{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1030cf4e",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "914b6400",
     "kernelId": "2916018f-6766-4ce9-a9f0-c3eb16ceb4f1",
     "source_hidden": false
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../../Src')\n",
    "from utils.preprocessing import *\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys, gc, time, warnings, pickle, psutil, random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from joblib import Parallel, delayed\n",
    "from math import ceil\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "SEED = 42\n",
    "seed_everything(SEED)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "SHIFT_DAY = 1\n",
    "WINDOW = 15\n",
    "TARGET = 'Demanda'         # Our main target\n",
    "PATH_DATASET = '../../dataset/'\n",
    "PATH_RESULTS = '../../results/Demanda/'\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_theme(style=\"ticks\", color_codes=True)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "gc.collect()\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a931a8e",
   "metadata": {},
   "source": [
    "# Creacion de los archivos en formato pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a05b04ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_original = pd.read_csv(os.path.join(PATH_DATASET,'train/train_converted.csv'))\n",
    "df_test_original = pd.read_csv(os.path.join(PATH_DATASET,'test/test_converted.csv'))\n",
    "\n",
    "df_train = df_train_original[['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA','Z_WEEK','Z_WEEK_DATE','Demanda']].groupby(['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA','Z_WEEK','Z_WEEK_DATE']).sum().reset_index()\n",
    "df_test = df_test_original[['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA','Z_WEEK','Z_WEEK_DATE','Demanda']].groupby(['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA','Z_WEEK','Z_WEEK_DATE']).sum().reset_index()\n",
    "df_train = df_train.merge(df_train_original[['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA','Z_WEEK','Z_MARCA','Z_DEPARTAMENTO']],on=['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA','Z_WEEK'],how='left')\n",
    "df_test  = df_test.merge(df_test_original[['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA','Z_WEEK','Z_MARCA','Z_DEPARTAMENTO']],on=['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA','Z_WEEK'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd17e35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creacion de fecha datetime e identificador\n",
    "df_train['Z_WEEK_DATE'] = pd.to_datetime(df_train['Z_WEEK_DATE'], errors='coerce')\n",
    "df_test['Z_WEEK_DATE'] = pd.to_datetime(df_test['Z_WEEK_DATE'], errors='coerce')\n",
    "dates = (set(df_train['Z_WEEK'].unique()) | set(df_test['Z_WEEK'].unique()))#df_auxiliar['Z_WEEK'].unique()\n",
    "dates = sorted(dates)\n",
    "\n",
    "dict_dates = {}\n",
    "for idx,date in enumerate(dates):\n",
    "    dict_dates[date] =idx\n",
    "df_train['date_block_num'] = df_train['Z_WEEK'].map(dict_dates)\n",
    "df_test['date_block_num'] = df_test['Z_WEEK'].map(dict_dates)\n",
    "df_test['Demanda'] = 0\n",
    "\n",
    "# Creacion de identificadores por modelo punto venta y gamma \n",
    "\n",
    "df_train[\"item_id\"] = df_train[\"Z_MODELO\"].astype(str) +\"|\"+ df_train[\"Z_PUNTO_VENTA\"].astype(str) +\"|\"+ df_train[\"Z_GAMA\"].astype(str) \n",
    "df_test[\"item_id\"]  = df_test[\"Z_MODELO\"].astype(str) +\"|\"+ df_test[\"Z_PUNTO_VENTA\"].astype(str) +\"|\"+ df_test[\"Z_GAMA\"].astype(str) \n",
    "df_train.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00b7a66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.22 s ± 72.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit df_train.to_pickle(os.path.join(PATH_RESULTS,'df_train.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7c6d728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418 ms ± 85.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit df_test.to_pickle(os.path.join(PATH_RESULTS,'df_test.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1552ee2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# limpieza de memoria\n",
    "del df_train\n",
    "del df_test\n",
    "del df_train_original\n",
    "del df_test_original\n",
    "del dict_dates\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2ebb35",
   "metadata": {},
   "source": [
    "# Feature Engineering with temporal information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ef18c5",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "2b0d2fad",
     "kernelId": "2916018f-6766-4ce9-a9f0-c3eb16ceb4f1"
    }
   },
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a60ff8f7",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "1d92c12b",
     "kernelId": "2916018f-6766-4ce9-a9f0-c3eb16ceb4f1",
     "source_hidden": false
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle(os.path.join(PATH_RESULTS,'dataset','df_train.pkl'))\n",
    "df_test  = pd.read_pickle(os.path.join(PATH_RESULTS,'dataset','df_test.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d155ac97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.replace([np.inf, -np.inf, np.nan],0,inplace=True)\n",
    "df_test.replace([np.inf, -np.inf, np.nan],0,inplace=True)\n",
    "\n",
    "N_test  = df_test.shape[0]\n",
    "N_train = df_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe41f975",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['DATE'] = pd.to_datetime(df_train['Z_WEEK_DATE'], errors='coerce')\n",
    "df_test['DATE']  = pd.to_datetime(df_test['Z_WEEK_DATE'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cc0b203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2830380, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creacion de la data total training y testing para generar los features temporales\n",
    "df_train_aux = pd.read_pickle(os.path.join(PATH_RESULTS,'dataset','df_train.pkl'))\n",
    "df_test_aux  = pd.read_pickle(os.path.join(PATH_RESULTS,'dataset','df_test.pkl'))    \n",
    "\n",
    "df_auxiliar = pd.concat([df_train_aux,df_test_aux])\n",
    "\n",
    "df_auxiliar.replace([np.inf, -np.inf,np.nan],0 , inplace=True)\n",
    "df_auxiliar.reset_index(inplace=True,drop=True)\n",
    "print(df_auxiliar.shape)\n",
    "df_auxiliar.head(2)\n",
    "\n",
    "df_auxiliar['Z_WEEK_DATE']  = pd.to_datetime(df_auxiliar['Z_WEEK_DATE'], errors='coerce')\n",
    "\n",
    "# limpieza de memoria\n",
    "del df_train_aux\n",
    "del df_test_aux\n",
    "del df_train['Z_WEEK']\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b108bb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 215.94 MB\n",
      "Memory usage after optimization is: 98.58 MB\n",
      "Decreased by 54.4%\n",
      "********************\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "# reducimos memoria\n",
    "df_auxiliar = reduce_mem_usage(df_auxiliar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab9003e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos el identificador para cada dataset\n",
    "df_auxiliar[\"item_id\"] = df_auxiliar[\"Z_MODELO\"].astype(str) +\"|\"+ df_auxiliar[\"Z_PUNTO_VENTA\"].astype(str) +\"|\"+ df_auxiliar[\"Z_GAMA\"].astype(str) \n",
    "\n",
    "df_train[\"item_id\"] = df_train[\"Z_MODELO\"].astype(str) +\"|\"+ df_train[\"Z_PUNTO_VENTA\"].astype(str) +\"|\"+ df_train[\"Z_GAMA\"].astype(str) \n",
    "df_test[\"item_id\"]  = df_test[\"Z_MODELO\"].astype(str) +\"|\"+ df_test[\"Z_PUNTO_VENTA\"].astype(str) +\"|\"+ df_test[\"Z_GAMA\"].astype(str) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "277bd4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nos aseguramos que los datos esten ordenados para hacer los lags por tiempo\n",
    "df_auxiliar.sort_values(by=['item_id','Z_WEEK_DATE'], ascending=[True, True],inplace=True)\n",
    "df_test.sort_values(by=['item_id','Z_WEEK_DATE'], ascending=[True, True],inplace=True)\n",
    "df_train.sort_values(by=['item_id','Z_WEEK_DATE'], ascending=[True, True],inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6432a4",
   "metadata": {},
   "source": [
    "## Creacion de la data total (training and testing) con identificador de ultima venta\n",
    "##### union de dataset training y testing para poder hacer la misma procesamiento en los dos datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9acac321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release week\n",
      "1 df_auxiliar (2830380, 10)\n",
      "2 df_auxiliar (2830380, 11)\n"
     ]
    }
   ],
   "source": [
    "print('Release week')\n",
    "# columna release significa la fecha en el cual se empezo a vender (demanda mayor que 0)\n",
    "release_df = df_train[['item_id','date_block_num']][df_train[TARGET]>0].groupby(['item_id'])['date_block_num'].agg(['min']).reset_index()\n",
    "release_df.columns = ['item_id','release']\n",
    "print('1 df_auxiliar',df_auxiliar.shape)\n",
    "df_auxiliar = merge_by_concat(df_auxiliar, release_df, ['item_id'])\n",
    "print('2 df_auxiliar',df_auxiliar.shape)\n",
    "# limpieza de memoria\n",
    "del release_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e12ddb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 df_auxiliar (2830380, 11)\n"
     ]
    }
   ],
   "source": [
    "# las ventas que nunca se empezaron y tienen todo 0 en el release aparecen como NaN por ende lo mandamos a un valor por defecto 100\n",
    "df_auxiliar['release'].fillna(100.0, inplace=True)\n",
    "\n",
    "df_auxiliar['release'] = df_auxiliar['release'].astype(np.int16)\n",
    "df_auxiliar = df_auxiliar.reset_index(drop=True)\n",
    "print('3 df_auxiliar',df_auxiliar.shape)\n",
    "\n",
    "# actualizamos nuestro release con respecto a la minima fecha de venta \n",
    "df_auxiliar['release'] = df_auxiliar['release'] - df_auxiliar['release'].min()\n",
    "df_auxiliar['release'] = df_auxiliar['release'].astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3c4121e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Part 1\n",
      "Size: (2830380, 11)\n"
     ]
    }
   ],
   "source": [
    "# guardamos el primer dataset referencial de toda la data para analizar training y testing con fecha de venta referencial\n",
    "print('Save Part 1')\n",
    "df_auxiliar.to_pickle(os.path.join(PATH_RESULTS,'dataset','grid_part_1.pkl'))\n",
    "print('Size:', df_auxiliar.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8b9d1d",
   "metadata": {},
   "source": [
    "# Creacion de Features Temporales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d484bbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END_TRAIN  : 59\n",
      "TARGET     : Demanda\n",
      "END_TRAIN  : 59\n",
      "MAIN_INDEX : ['item_id', 'date_block_num']\n",
      "START_TRAIN  : 0\n"
     ]
    }
   ],
   "source": [
    "# Definir nuestros parametros de trabajo con respecto a los puntos de corte temporales\n",
    "END_TRAIN = df_auxiliar['date_block_num'].max()         # Last day in train set\n",
    "print('END_TRAIN  :',END_TRAIN)\n",
    "\n",
    "MAIN_INDEX = ['item_id','date_block_num']  # We can identify item by these columns\n",
    "print('TARGET     :',TARGET)\n",
    "print('END_TRAIN  :',END_TRAIN)\n",
    "print('MAIN_INDEX :',MAIN_INDEX)\n",
    "START_TRAIN = df_auxiliar['date_block_num'].min()         # First day in train set\n",
    "print('START_TRAIN  :',START_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ba63724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df_auxiliar\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dacd7db",
   "metadata": {},
   "source": [
    "## Creacion de variables temporales del Z_WEEK_DATE (training and testing)\n",
    "##### union de dataset training y testing para poder hacer la misma procesamiento en los dos datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2210e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_auxiliar (2830380, 3)\n",
      "Save part 2\n",
      "Size: (2830380, 14)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creacion de features del dia de venta\n",
    "\n",
    "# union de dataset training y testing para poder hacer la misma procesamiento en los dos datasets\n",
    "df_train_aux = pd.read_pickle(os.path.join(PATH_RESULTS,'dataset','df_train.pkl'))\n",
    "df_test_aux  = pd.read_pickle(os.path.join(PATH_RESULTS,'dataset','df_test.pkl'))    \n",
    "\n",
    "df_auxiliar = pd.concat([df_train_aux,df_test_aux])\n",
    "del df_train_aux\n",
    "del df_test_aux\n",
    "gc.collect()\n",
    "\n",
    "df_auxiliar = df_auxiliar[['Z_WEEK_DATE', 'item_id', 'date_block_num']]\n",
    "print('df_auxiliar',df_auxiliar.shape)\n",
    "\n",
    "# Creacion de features del dia de venta\n",
    "df_auxiliar             = fe_dates(\"Z_WEEK_DATE\",df_auxiliar)\n",
    "df_auxiliar['tm_wm']    = df_auxiliar['day'].apply(lambda x: ceil(x/7)).astype(np.int8) # 오늘 몇째주?\n",
    "df_auxiliar['tm_w_end'] = (df_auxiliar['day_of_week']>=5).astype(np.int8)\n",
    "df_auxiliar['tm_m_end'] = (df_auxiliar['tm_wm']>=3).astype(np.int8)\n",
    "del df_auxiliar['Z_WEEK_DATE']\n",
    "gc.collect()\n",
    "# save features dates\n",
    "print('Save part 2')\n",
    "\n",
    "# Safe part 3\n",
    "df_auxiliar.to_pickle(os.path.join(PATH_RESULTS,'dataset','grid_part_2.pkl'))\n",
    "print('Size:', df_auxiliar.shape)\n",
    "\n",
    "# We don't need calendar_df anymore\n",
    "del df_auxiliar\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80b5d37",
   "metadata": {},
   "source": [
    "## Creacion de variables temporales del pasado (training and testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4196f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df = pd.read_pickle(os.path.join(PATH_RESULTS,'dataset','grid_part_1.pkl'))\n",
    "grid_df = grid_df.reset_index(drop=True)\n",
    "grid_df.sort_values(by=['item_id','Z_WEEK_DATE'], ascending=[True, True],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6eb7df43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create lags\n",
      "len LAG_DAYS 15\n",
      "grid_df (2830380, 26)\n",
      "1.21 min: Lags\n"
     ]
    }
   ],
   "source": [
    "# Lags ( demanda de los dias anteriores)\n",
    "start_time = time.time()\n",
    "print('Create lags')\n",
    "LAG_DAYS = [col for col in range(SHIFT_DAY,SHIFT_DAY+WINDOW)]\n",
    "print('len LAG_DAYS',len(LAG_DAYS))\n",
    "grid_df = grid_df.assign(**{\n",
    "        '{}_lag_shift_{}'.format(col, l): grid_df.groupby(['item_id'])[col].transform(lambda x: x.shift(l))\n",
    "        for l in LAG_DAYS\n",
    "        for col in [TARGET]\n",
    "    })\n",
    "\n",
    "grid_df.replace([np.inf, -np.inf,np.nan],0 , inplace=True)\n",
    "print('grid_df',grid_df.shape)\n",
    "grid_df.replace([np.inf, -np.inf, np.nan],0,inplace=True)\n",
    "grid_df = grid_df.reset_index(drop=True)\n",
    "gc.collect()\n",
    "# Minify lag columns\n",
    "for col in list(grid_df):\n",
    "    if 'lag' in col:\n",
    "        grid_df[col] = grid_df[col].astype(np.float16)\n",
    "\n",
    "print('%0.2f min: Lags' % ((time.time() - start_time) / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f03d589",
   "metadata": {},
   "source": [
    "## Creacion de variables temporales del pasado tendencias de venta por modelo, puntop de venta , departamento y mas variables (training and testing)\n",
    "\n",
    "### PROCESAMIENTO MULTITHREAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5b0a33e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create rolling aggs\n",
      "Shifting period: 1\n",
      "Shifting period: 2\n",
      "Shifting period: 3\n",
      "Shifting period: 4\n",
      "Shifting period: 5\n",
      "Shifting period: 6\n",
      "Shifting period: 7\n",
      "Shifting period: 8\n",
      "Shifting period: 9\n",
      "Shifting period: 10\n",
      "Shifting period: 11\n",
      "Shifting period: 12\n",
      "Shifting period: 13\n",
      "Shifting period: 14\n",
      "Shifting period: 15\n",
      "Shifting period: 16\n",
      "Inicio del procesamiento paralelo para crear features temporales por grupo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [04:04<00:00,  4.18it/s]\n",
      "100%|██████████| 1024/1024 [00:03<00:00, 313.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin del procesamiento paralelo para crear features temporales por grupo\n",
      "OK\n",
      "10.59 min: Lags\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "grid_df[['Z_MODELO', 'Z_PUNTO_VENTA', 'Z_GAMA']] = grid_df['item_id'].str.split('|',expand=True)\n",
    "\n",
    "# defino los grupos para hacer el analisis temporal\n",
    "icols =  [  ['item_id'],\n",
    "            ['Z_MODELO'],\n",
    "            ['Z_PUNTO_VENTA'],\n",
    "            ['Z_GAMA'],\n",
    "            ['Z_MARCA'],\n",
    "            ['Z_DEPARTAMENTO'],\n",
    "    \n",
    "            ['Z_MODELO', 'Z_PUNTO_VENTA'],\n",
    "          \n",
    "            ['Z_MODELO', 'Z_GAMA'],\n",
    "            ['Z_MODELO', 'Z_MARCA'],\n",
    "            ['Z_MODELO', 'Z_DEPARTAMENTO'],\n",
    "    \n",
    "            ['Z_PUNTO_VENTA', 'Z_GAMA'],\n",
    "            ['Z_PUNTO_VENTA', 'Z_MARCA'],\n",
    "            ['Z_PUNTO_VENTA', 'Z_DEPARTAMENTO'],\n",
    "    \n",
    "            ['Z_GAMA', 'Z_MARCA'],\n",
    "            ['Z_GAMA', 'Z_DEPARTAMENTO'],\n",
    "    \n",
    "            ['Z_MARCA', 'Z_DEPARTAMENTO'],\n",
    "        \n",
    "            ]\n",
    "# Rollings\n",
    "start_time = time.time()\n",
    "print('Create rolling aggs')\n",
    "global grid_df\n",
    "# Rollings\n",
    "# with sliding shift\n",
    "\n",
    "\n",
    "total_combinations = []\n",
    "for d_shift in range(SHIFT_DAY,SHIFT_DAY+WINDOW+1): \n",
    "    print('Shifting period:', d_shift)\n",
    "    for d_window in [2,4]:\n",
    "        col_name = 'shift_'+str(d_shift)+'_roll_'+str(d_window)\n",
    "        for group_columns in icols:\n",
    "            for tipo in ['mean','std']:\n",
    "                total_combinations.append([d_shift,d_window,group_columns,tipo])\n",
    "\n",
    "\n",
    "def process_lags(x):\n",
    "    global grid_df\n",
    "    d_shift = x[0]\n",
    "    d_window = x[1]\n",
    "    group_columns = x[2]\n",
    "    tipo = x[3]\n",
    "    col_name = 'shift_'+str(d_shift)+'_roll_'+str(d_window)\n",
    "    if tipo == 'mean':\n",
    "        var = grid_df.groupby(group_columns)[TARGET].transform(lambda x: x.shift(d_shift).fillna(0).rolling(d_window,min_periods=1).mean()).astype(np.float16)\n",
    "        return [col_name+'_mean_'+'_'.join(group_columns),var]\n",
    "    if tipo == 'std':\n",
    "        var = grid_df.groupby(group_columns)[TARGET].transform(lambda x: x.shift(d_shift).fillna(0).rolling(d_window,min_periods=1).std(ddof=0)).astype(np.float16)\n",
    "        return [col_name+'_std_'+'_'.join(group_columns),var]\n",
    "    if tipo == 'max':\n",
    "        var = grid_df.groupby(group_columns)[TARGET].transform(lambda x: x.shift(d_shift).fillna(0).rolling(d_window,min_periods=1).max()).astype(np.float16)\n",
    "        return [col_name+'_std_'+'_'.join(group_columns),var]\n",
    "\n",
    "\n",
    "print('Inicio del procesamiento paralelo para crear features temporales por grupo')\n",
    "results = Parallel(n_jobs=8, batch_size=64, backend=\"loky\", verbose=0)(delayed(process_lags)(n) for n in tqdm(total_combinations))\n",
    "n= len(results)\n",
    "\n",
    "for i in tqdm(range(n)):\n",
    "    va = results.pop()\n",
    "    grid_df[va[0]] = va[1]    \n",
    "    \n",
    "print('Fin del procesamiento paralelo para crear features temporales por grupo')\n",
    "print('OK')\n",
    "print('%0.2f min: Lags' % ((time.time() - start_time) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52dfa4e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8123"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del results\n",
    "del grid_df['Z_MODELO']\n",
    "del grid_df['Z_PUNTO_VENTA']\n",
    "del grid_df['Z_GAMA']\n",
    "del grid_df['Z_MARCA']\n",
    "del grid_df['Z_DEPARTAMENTO']\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc5ea381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save lags and rollings\n",
      "../../results/Demanda/dataset/lags_df_1_completed.pkl\n",
      "Size: (2830380, 1045)\n"
     ]
    }
   ],
   "source": [
    "# guardar toda la data temporal\n",
    "print('Save lags and rollings')\n",
    "print(os.path.join(PATH_RESULTS,'dataset','lags_df_'+str(SHIFT_DAY)+'_completed.pkl'))\n",
    "print('Size:', grid_df.shape)\n",
    "grid_df.to_pickle(os.path.join(PATH_RESULTS,'dataset','lags_df_'+str(SHIFT_DAY)+'_completed.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a413c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del grid_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d1c101",
   "metadata": {},
   "source": [
    "## Creacion de variables temporales del pasado tendencias global de venta por modelo, punto de venta , departamento y mas variables de las 10 semanas anteriores a SEMANA_50 (training and testing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57a7dbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding ['Z_MODELO']\n",
      "Encoding ['Z_PUNTO_VENTA']\n",
      "Encoding ['Z_GAMA']\n",
      "Encoding ['Z_MARCA']\n",
      "Encoding ['Z_DEPARTAMENTO']\n",
      "Encoding ['Z_MODELO', 'Z_PUNTO_VENTA']\n",
      "Encoding ['Z_MODELO', 'Z_GAMA']\n",
      "Encoding ['Z_MODELO', 'Z_MARCA']\n",
      "Encoding ['Z_MODELO', 'Z_DEPARTAMENTO']\n",
      "Encoding ['Z_PUNTO_VENTA', 'Z_GAMA']\n",
      "Encoding ['Z_PUNTO_VENTA', 'Z_MARCA']\n",
      "Encoding ['Z_PUNTO_VENTA', 'Z_DEPARTAMENTO']\n",
      "Encoding ['Z_GAMA', 'Z_MARCA']\n",
      "Encoding ['Z_GAMA', 'Z_DEPARTAMENTO']\n",
      "Encoding ['Z_MARCA', 'Z_DEPARTAMENTO']\n",
      "Encoding ['Z_MODELO', 'Z_PUNTO_VENTA', 'Z_GAMA']\n",
      "Save Mean/Std encoding\n",
      "../../results/Demanda/dataset/mean_encoding_df.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_df = pd.read_pickle(os.path.join(PATH_RESULTS,'dataset','grid_part_1.pkl'))\n",
    "grid_df[TARGET][grid_df['date_block_num']>(END_TRAIN-10)] = np.nan\n",
    "base_cols = list(grid_df)\n",
    "\n",
    "icols =  [\n",
    "            ['Z_MODELO'],\n",
    "            ['Z_PUNTO_VENTA'],\n",
    "            ['Z_GAMA'],\n",
    "            ['Z_MARCA'],\n",
    "            ['Z_DEPARTAMENTO'],\n",
    "    \n",
    "            ['Z_MODELO', 'Z_PUNTO_VENTA'],\n",
    "            ['Z_MODELO', 'Z_GAMA'],\n",
    "            ['Z_MODELO', 'Z_MARCA'],\n",
    "            ['Z_MODELO', 'Z_DEPARTAMENTO'],\n",
    "    \n",
    "            ['Z_PUNTO_VENTA', 'Z_GAMA'],\n",
    "            ['Z_PUNTO_VENTA', 'Z_MARCA'],\n",
    "            ['Z_PUNTO_VENTA', 'Z_DEPARTAMENTO'],\n",
    "    \n",
    "            ['Z_GAMA', 'Z_MARCA'],\n",
    "            ['Z_GAMA', 'Z_DEPARTAMENTO'],\n",
    "    \n",
    "            ['Z_MARCA', 'Z_DEPARTAMENTO'],\n",
    "        \n",
    "            ['Z_MODELO', 'Z_PUNTO_VENTA', 'Z_GAMA'],\n",
    "            ]\n",
    "\n",
    "for col in icols:\n",
    "    print('Encoding', col)\n",
    "    col_name = '_'+'_'.join(col)+'_'\n",
    "    grid_df['enc'+col_name+'mean'] = grid_df.groupby(col)[TARGET].transform('mean').astype(np.float32)\n",
    "    grid_df['enc'+col_name+'std'] = grid_df.groupby(col)[TARGET].transform('std').astype(np.float32)\n",
    "    grid_df['enc'+col_name+'max'] = grid_df.groupby(col)[TARGET].transform('max').astype(np.float32)\n",
    "keep_cols = [col for col in list(grid_df) if col not in base_cols]\n",
    "\n",
    "keep_cols = [col for col in list(grid_df) if col not in base_cols]\n",
    "grid_df = grid_df[['Z_WEEK_DATE', 'item_id', 'date_block_num']+keep_cols]\n",
    "\n",
    "# guardar la data global de tendencia\n",
    "print('Save Mean/Std encoding')\n",
    "print(os.path.join(PATH_RESULTS,'dataset','mean_encoding_df.pkl'))\n",
    "grid_df.to_pickle(os.path.join(PATH_RESULTS,'dataset','mean_encoding_df.pkl'))\n",
    "\n",
    "del grid_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bf3073c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Archivos de variables temporales creadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c616261b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f11d99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
