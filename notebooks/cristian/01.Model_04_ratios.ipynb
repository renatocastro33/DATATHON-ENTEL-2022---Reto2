{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from time import sleep\n",
    "\n",
    "def get_distance_from_paydays(date):\n",
    "    end_of_month = date.daysinmonth\n",
    "    distance_to_1st = 0 if date.day >=15 else 15 - date.day\n",
    "    distance_to15th = 0 if date.day < 15 else end_of_month - date.day\n",
    "    return distance_to_1st + distance_to15th\n",
    "\n",
    "def std(x): return np.std(x)\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            if str(col_type) == numerics:\n",
    "                c_min = df[col].min()\n",
    "                c_max = df[col].max()\n",
    "\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            if str(col_type)[:5] == 'float':\n",
    "                c_min = df[col].min()\n",
    "                c_max = df[col].max()\n",
    "\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "## Simple \"Memory profilers\" to see memory usage\n",
    "def get_memory_usage():\n",
    "    return np.round(psutil.Process(os.getpid()).memory_info()[0]/2.**30, 2) \n",
    "        \n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f%s%s\" % (num, 'Yi', suffix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 107.97 MB\n",
      "Memory usage after optimization is: 33.78 MB\n",
      "Decreased by 68.7%\n",
      "Memory usage of dataframe is 21.59 MB\n",
      "Memory usage after optimization is: 4.08 MB\n",
      "Decreased by 81.1%\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    df_train = pd.read_csv('../../dataset/train/train_converted.csv')\n",
    "    df_test  = pd.read_csv('../../dataset/test/test_converted.csv')\n",
    "    df_train = df_train[['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA','Z_WEEK','Z_WEEK_DATE','Demanda']].groupby(['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA','Z_WEEK','Z_WEEK_DATE']).sum().reset_index()\n",
    "    df_test = df_test[['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA','Z_WEEK','Z_WEEK_DATE','Demanda']].groupby(['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA','Z_WEEK','Z_WEEK_DATE']).sum().reset_index()\n",
    "\n",
    "else:\n",
    "\n",
    "    df_train = pd.read_pickle('../../dataset/train/train_converted_fill.pkl')\n",
    "    df_test  = pd.read_pickle('../../dataset/test/test_converted_fill.pkl')\n",
    "\n",
    "df_train = reduce_mem_usage(df_train)\n",
    "df_test = reduce_mem_usage(df_test)\n",
    "\n",
    "\n",
    "df_train.replace(['',np.inf, -np.inf, np.nan],0,inplace=True)\n",
    "df_test.replace(['',np.inf, -np.inf, np.nan],0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA','Z_WEEK']:\n",
    "    df_train[column] = df_train[column].astype(str)\n",
    "    df_test[column] = df_test[column].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['ID'] = df_train['Z_MODELO'] + '|' + df_train['Z_PUNTO_VENTA'] + '|' + df_train['Z_GAMA']\n",
    "df_test['ID'] = df_test['Z_MODELO'] + '|' + df_test['Z_PUNTO_VENTA'] + '|' + df_test['Z_GAMA']\n",
    "\n",
    "base = df_train[['ID', 'Demanda', 'Z_WEEK_DATE']].groupby(['ID', 'Z_WEEK_DATE']).sum().sort_values('Demanda' , ascending = [False]).reset_index()\n",
    "base_cum = df_train[['Demanda', 'Z_WEEK_DATE']].groupby([ 'Z_WEEK_DATE']).sum().sort_values('Demanda' , ascending = [False]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Z_MODELO</th>\n",
       "      <th>Z_PUNTO_VENTA</th>\n",
       "      <th>Z_GAMA</th>\n",
       "      <th>Z_WEEK</th>\n",
       "      <th>Z_WEEK_DATE</th>\n",
       "      <th>Demanda</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>SEMANA_01</td>\n",
       "      <td>2021-05-17</td>\n",
       "      <td>0</td>\n",
       "      <td>MOD_1|PVENT_1|GAM_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Z_MODELO Z_PUNTO_VENTA Z_GAMA     Z_WEEK Z_WEEK_DATE  Demanda  \\\n",
       "0    MOD_1       PVENT_1  GAM_1  SEMANA_01  2021-05-17        0   \n",
       "\n",
       "                    ID  \n",
       "0  MOD_1|PVENT_1|GAM_1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = base.merge(base_cum[['Z_WEEK_DATE', 'Demanda']].rename(columns = {'Demanda': 'Demanda_Total'}), on = 'Z_WEEK_DATE', how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Z_WEEK_DATE</th>\n",
       "      <th>Demanda</th>\n",
       "      <th>Demanda_Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MOD_3|PVENT_1|GAM_4</td>\n",
       "      <td>2021-07-19</td>\n",
       "      <td>1451</td>\n",
       "      <td>45037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MOD_3|PVENT_1|GAM_4</td>\n",
       "      <td>2021-07-12</td>\n",
       "      <td>1201</td>\n",
       "      <td>47069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MOD_27|PVENT_1|GAM_1</td>\n",
       "      <td>2021-06-07</td>\n",
       "      <td>1032</td>\n",
       "      <td>38374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MOD_3|PVENT_1|GAM_4</td>\n",
       "      <td>2021-06-21</td>\n",
       "      <td>1005</td>\n",
       "      <td>40481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MOD_2|PVENT_1|GAM_1</td>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>934</td>\n",
       "      <td>43389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358645</th>\n",
       "      <td>MOD_25|PVENT_409|GAM_1</td>\n",
       "      <td>2022-02-21</td>\n",
       "      <td>0</td>\n",
       "      <td>32408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358646</th>\n",
       "      <td>MOD_25|PVENT_409|GAM_1</td>\n",
       "      <td>2022-02-14</td>\n",
       "      <td>0</td>\n",
       "      <td>33511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358647</th>\n",
       "      <td>MOD_25|PVENT_409|GAM_1</td>\n",
       "      <td>2022-02-07</td>\n",
       "      <td>0</td>\n",
       "      <td>35144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358648</th>\n",
       "      <td>MOD_25|PVENT_409|GAM_1</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>0</td>\n",
       "      <td>32240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358649</th>\n",
       "      <td>MOD_9|PVENT_9|GAM_1</td>\n",
       "      <td>2022-04-25</td>\n",
       "      <td>0</td>\n",
       "      <td>28228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2358650 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             ID Z_WEEK_DATE  Demanda  Demanda_Total\n",
       "0           MOD_3|PVENT_1|GAM_4  2021-07-19     1451          45037\n",
       "1           MOD_3|PVENT_1|GAM_4  2021-07-12     1201          47069\n",
       "2          MOD_27|PVENT_1|GAM_1  2021-06-07     1032          38374\n",
       "3           MOD_3|PVENT_1|GAM_4  2021-06-21     1005          40481\n",
       "4           MOD_2|PVENT_1|GAM_1  2021-12-06      934          43389\n",
       "...                         ...         ...      ...            ...\n",
       "2358645  MOD_25|PVENT_409|GAM_1  2022-02-21        0          32408\n",
       "2358646  MOD_25|PVENT_409|GAM_1  2022-02-14        0          33511\n",
       "2358647  MOD_25|PVENT_409|GAM_1  2022-02-07        0          35144\n",
       "2358648  MOD_25|PVENT_409|GAM_1  2022-01-31        0          32240\n",
       "2358649     MOD_9|PVENT_9|GAM_1  2022-04-25        0          28228\n",
       "\n",
       "[2358650 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base['ratio'] = base['Demanda']*100/base['Demanda_Total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Z_WEEK_DATE</th>\n",
       "      <th>Demanda</th>\n",
       "      <th>Demanda_Total</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MOD_3|PVENT_1|GAM_4</td>\n",
       "      <td>2021-07-19</td>\n",
       "      <td>1451</td>\n",
       "      <td>45037</td>\n",
       "      <td>3.221795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MOD_3|PVENT_1|GAM_4</td>\n",
       "      <td>2021-07-12</td>\n",
       "      <td>1201</td>\n",
       "      <td>47069</td>\n",
       "      <td>2.551573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MOD_27|PVENT_1|GAM_1</td>\n",
       "      <td>2021-06-07</td>\n",
       "      <td>1032</td>\n",
       "      <td>38374</td>\n",
       "      <td>2.689321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MOD_3|PVENT_1|GAM_4</td>\n",
       "      <td>2021-06-21</td>\n",
       "      <td>1005</td>\n",
       "      <td>40481</td>\n",
       "      <td>2.482646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MOD_2|PVENT_1|GAM_1</td>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>934</td>\n",
       "      <td>43389</td>\n",
       "      <td>2.152619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358645</th>\n",
       "      <td>MOD_25|PVENT_409|GAM_1</td>\n",
       "      <td>2022-02-21</td>\n",
       "      <td>0</td>\n",
       "      <td>32408</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358646</th>\n",
       "      <td>MOD_25|PVENT_409|GAM_1</td>\n",
       "      <td>2022-02-14</td>\n",
       "      <td>0</td>\n",
       "      <td>33511</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358647</th>\n",
       "      <td>MOD_25|PVENT_409|GAM_1</td>\n",
       "      <td>2022-02-07</td>\n",
       "      <td>0</td>\n",
       "      <td>35144</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358648</th>\n",
       "      <td>MOD_25|PVENT_409|GAM_1</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>0</td>\n",
       "      <td>32240</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358649</th>\n",
       "      <td>MOD_9|PVENT_9|GAM_1</td>\n",
       "      <td>2022-04-25</td>\n",
       "      <td>0</td>\n",
       "      <td>28228</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2358650 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             ID Z_WEEK_DATE  Demanda  Demanda_Total     ratio\n",
       "0           MOD_3|PVENT_1|GAM_4  2021-07-19     1451          45037  3.221795\n",
       "1           MOD_3|PVENT_1|GAM_4  2021-07-12     1201          47069  2.551573\n",
       "2          MOD_27|PVENT_1|GAM_1  2021-06-07     1032          38374  2.689321\n",
       "3           MOD_3|PVENT_1|GAM_4  2021-06-21     1005          40481  2.482646\n",
       "4           MOD_2|PVENT_1|GAM_1  2021-12-06      934          43389  2.152619\n",
       "...                         ...         ...      ...            ...       ...\n",
       "2358645  MOD_25|PVENT_409|GAM_1  2022-02-21        0          32408  0.000000\n",
       "2358646  MOD_25|PVENT_409|GAM_1  2022-02-14        0          33511  0.000000\n",
       "2358647  MOD_25|PVENT_409|GAM_1  2022-02-07        0          35144  0.000000\n",
       "2358648  MOD_25|PVENT_409|GAM_1  2022-01-31        0          32240  0.000000\n",
       "2358649     MOD_9|PVENT_9|GAM_1  2022-04-25        0          28228  0.000000\n",
       "\n",
       "[2358650 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.merge(base[['ID','Z_WEEK_DATE','ratio']],how='left') # ,'Demanda_Total'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test  = df_test.merge(base[['ID','Z_WEEK_DATE','ratio']],how='left') # ,'Demanda_Total'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Z_MODELO</th>\n",
       "      <th>Z_PUNTO_VENTA</th>\n",
       "      <th>Z_GAMA</th>\n",
       "      <th>Z_WEEK</th>\n",
       "      <th>Z_WEEK_DATE</th>\n",
       "      <th>Demanda</th>\n",
       "      <th>ID</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>SEMANA_01</td>\n",
       "      <td>2021-05-17</td>\n",
       "      <td>0</td>\n",
       "      <td>MOD_1|PVENT_1|GAM_1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Z_MODELO Z_PUNTO_VENTA Z_GAMA     Z_WEEK Z_WEEK_DATE  Demanda  \\\n",
       "0    MOD_1       PVENT_1  GAM_1  SEMANA_01  2021-05-17        0   \n",
       "\n",
       "                    ID  ratio  \n",
       "0  MOD_1|PVENT_1|GAM_1    0.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(columns = ['ID','Demanda'],inplace=True)\n",
    "df_test.drop(columns = ['ID','Demanda'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'ratio'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating date_block_num ...\n",
      "(2358650, 6) (471730, 6)\n",
      "Creating date_block_num completed!\n",
      "Preprocessing TRAINING DATASET ...\n",
      "Preprocessing TRAINING DATASET COMPLETED!\n",
      "Preprocessing TESTING DATASET ...\n",
      "Preprocessing TESTING DATASET COMPLETED!\n"
     ]
    }
   ],
   "source": [
    "print('Creating date_block_num ...')\n",
    "N_submission = df_test.shape[0]\n",
    "N_sales      = df_train.shape[0]\n",
    "\n",
    "print(df_train.shape,df_test.shape)\n",
    "\n",
    "dates = (set(df_train['Z_WEEK'].unique()) | set(df_test['Z_WEEK'].unique()))#df_auxiliar['Z_WEEK'].unique()\n",
    "dates = sorted(dates)\n",
    "\n",
    "dict_dates = {}\n",
    "for idx,date in enumerate(dates):\n",
    "    dict_dates[date] =idx\n",
    "    \n",
    "    \n",
    "df_train['date_block_num'] = df_train['Z_WEEK'].map(dict_dates)\n",
    "df_test['date_block_num'] = df_test['Z_WEEK'].map(dict_dates)\n",
    "\n",
    "df_train.replace(['',np.inf, -np.inf, np.nan],0,inplace=True)\n",
    "df_test.replace(['',np.inf, -np.inf, np.nan],0,inplace=True)\n",
    "\n",
    "print('Creating date_block_num completed!')\n",
    "\n",
    "\n",
    "print('Preprocessing TRAINING DATASET ...')\n",
    "\n",
    "\n",
    "df_train['Z_WEEK_DATE'] = pd.to_datetime(df_train['Z_WEEK_DATE'])\n",
    "#df_train['days_from_payday'] = df_train['Z_WEEK_DATE'].apply(get_distance_from_paydays)\n",
    "\n",
    "\n",
    "statistics_columns = [ ]\n",
    "\n",
    "#df_train[\"log_ratio\"] = np.log(df_train['ratio'] + 1e-8)\n",
    "#df_test[\"log_Demanda\"] = np.log(1e-8)\n",
    "\n",
    "#statistics_columns.append('log_ratio')\n",
    "\n",
    "'''\n",
    "bar1 = tqdm([\n",
    "    ['Z_MODELO'],\n",
    "    ['Z_PUNTO_VENTA'],\n",
    "    ['Z_GAMA'],\n",
    "    ['Z_MODELO','Z_PUNTO_VENTA'],\n",
    "    ['Z_MODELO','Z_GAMA'],\n",
    "    ['Z_PUNTO_VENTA','Z_GAMA'],\n",
    "    ['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA']], position=0, desc=\"i\",colour='green', ncols=80)\n",
    "time.sleep(1)\n",
    "\n",
    "bar2 = tqdm(['mean'], position=1, desc=\"j\", colour='red', ncols=80, leave=False) #'std','max','min','sum'\n",
    "time.sleep(1)\n",
    "\n",
    "unique_columns = [ ]\n",
    "        \n",
    "for column_names in bar1:\n",
    "    bar1.update()\n",
    "    bar2.refresh()  #force print final state\n",
    "    time.sleep(0.1)\n",
    "    bar2.reset()  #reuse bar\n",
    "    for statistic in bar2:\n",
    "        \n",
    "        new_column_name = statistic+'_sales_by_'+'_'.join(column_names)\n",
    "        #df_train[new_column_name] = df_train.groupby([\"Z_WEEK_DATE\"]+column_names, observed=True).Demanda.transform(statistic)\n",
    "        if statistic == 'mean':\n",
    "            df_agg = df_train.groupby([\"Z_WEEK\"]+column_names, observed=True).ratio.mean()\n",
    "        if statistic == 'std':\n",
    "            df_agg = df_train.groupby([\"Z_WEEK\"]+column_names, observed=True).ratio.std(ddof=0)\n",
    "        if statistic == 'max':\n",
    "            df_agg = df_train.groupby([\"Z_WEEK\"]+column_names, observed=True).ratio.max()\n",
    "        if statistic == 'min':\n",
    "            df_agg = df_train.groupby([\"Z_WEEK\"]+column_names, observed=True).ratio.min()\n",
    "        if statistic == 'sum':\n",
    "            df_agg = df_train.groupby([\"Z_WEEK\"]+column_names, observed=True).ratio.sum()        \n",
    "        if df_agg.shape[0] >= df_train.shape[0]*0.7:\n",
    "            unique_columns.append([[\"Z_WEEK\"]+column_names,new_column_name])\n",
    "            continue\n",
    "        \n",
    "        df_agg = df_agg.reset_index()\n",
    "        df_agg.columns = df_agg.columns.str.replace('ratio', new_column_name)\n",
    "        \n",
    "        df_train = df_train.merge(df_agg,on=[\"Z_WEEK\"]+column_names,how='left')\n",
    "        statistics_columns.append(new_column_name)\n",
    "        bar2.update()\n",
    "        time.sleep(0.05)\n",
    "#'''\n",
    "\n",
    "\n",
    "#df_train['dayofweek'] = df_train['Z_WEEK_DATE'].dt.dayofweek.astype('str').astype('category')\n",
    "#df_train['month'] = df_train['Z_WEEK_DATE'].dt.month.astype('str').astype('category')\n",
    "#df_train['dayofyear'] = df_train['Z_WEEK_DATE'].dt.dayofyear.astype('str').astype('category')\n",
    "\n",
    "df_train.drop(columns=['Z_WEEK_DATE'],inplace=True)\n",
    "df_train.drop(columns=['Z_WEEK'],inplace=True)\n",
    "\n",
    "print('Preprocessing TRAINING DATASET COMPLETED!')\n",
    "print('Preprocessing TESTING DATASET ...')\n",
    "\n",
    "\n",
    "df_test['Z_WEEK_DATE'] = pd.to_datetime(df_test['Z_WEEK_DATE'])\n",
    "#df_test['days_from_payday'] = df_test['Z_WEEK_DATE'].apply(get_distance_from_paydays)\n",
    "\n",
    "inv_dict_dates = {v: k for k, v in dict_dates.items()}\n",
    "#df_test['Z_WEEK'] = df_test['date_block_num'].map(inv_dict_dates)\n",
    "df_test = df_test[['date_block_num','Z_MODELO','Z_PUNTO_VENTA','Z_GAMA',\"ratio\",\"Z_WEEK_DATE\"]]\n",
    "\n",
    "#df_test['dayofweek'] = df_test['Z_WEEK_DATE'].dt.dayofweek.astype('str').astype('category')\n",
    "#df_test['month'] = df_test['Z_WEEK_DATE'].dt.month.astype('str').astype('category')\n",
    "#df_test['dayofyear'] = df_test['Z_WEEK_DATE'].dt.dayofyear.astype('str').astype('category')\n",
    "\n",
    "#df_test['days_from_payday'] = df_test['Z_WEEK_DATE'].apply(get_distance_from_paydays)\n",
    "\n",
    "df_test.drop(columns=['Z_WEEK_DATE'],inplace=True)\n",
    "\n",
    "print('Preprocessing TESTING DATASET COMPLETED!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 107.97 MB\n",
      "Memory usage after optimization is: 51.77 MB\n",
      "Decreased by 52.1%\n"
     ]
    }
   ],
   "source": [
    "df_train = reduce_mem_usage(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 21.59 MB\n",
      "Memory usage after optimization is: 10.38 MB\n",
      "Decreased by 51.9%\n"
     ]
    }
   ],
   "source": [
    "df_test = reduce_mem_usage(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%timeit df_train.to_pickle('../../dataset/train/train_converted_fill_process.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%timeit df_test.to_pickle('../../dataset/test/test_converted_fill_process.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2358650, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Z_MODELO</th>\n",
       "      <th>Z_PUNTO_VENTA</th>\n",
       "      <th>Z_GAMA</th>\n",
       "      <th>ratio</th>\n",
       "      <th>date_block_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Z_MODELO Z_PUNTO_VENTA Z_GAMA  ratio  date_block_num\n",
       "0    MOD_1       PVENT_1  GAM_1    0.0               0\n",
       "1    MOD_1       PVENT_1  GAM_1    0.0               1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "#print(list(df_train.columns))\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ratio', 'date_block_num'], dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "\n",
    "numeric_columns = df_train.select_dtypes(include=numerics).columns\n",
    "numeric_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio\n",
      "date_block_num\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "scaler = StandardScaler()\n",
    "for column in numeric_columns:\n",
    "    print(column)\n",
    "    if column in ['ratio','date_block_num']:\n",
    "        continue\n",
    "    else:\n",
    "        \n",
    "        df_train[column] = scaler.fit_transform(df_train[column].values.reshape(-1,1))\n",
    "        \n",
    "        if column in df_test.columns:\n",
    "            df_test[column]  = scaler.transform(df_test[column].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/cristian/Extreme SSD/Investigacion/DATATHONES/entel-2022/my_3.8_env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/media/cristian/Extreme SSD/Investigacion/DATATHONES/entel-2022/my_3.8_env/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has\n",
      "                not been set for this class (SMAPE). The property determines if `update` by\n",
      "                default needs access to the full metric state. If this is not the case, significant speedups can be\n",
      "                achieved and we recommend setting this to `False`.\n",
      "                We provide an checking function\n",
      "                `from torchmetrics.utilities import check_forward_full_state_property`\n",
      "                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,\n",
      "                default for now) or if `full_state_update=False` can be used safely.\n",
      "                \n",
      "  warnings.warn(*args, **kwargs)\n",
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "import pytorch_forecasting\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer,EncoderNormalizer\n",
    "\n",
    "from pytorch_forecasting.metrics import SMAPE, PoissonLoss, QuantileLoss\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "# configure network and trainer\n",
    "pl.seed_everything(42)\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_steps = df_test['date_block_num'].nunique()\n",
    "prediction_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2358650 entries, 0 to 2358649\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Dtype   \n",
      "---  ------          -----   \n",
      " 0   Z_MODELO        category\n",
      " 1   Z_PUNTO_VENTA   category\n",
      " 2   Z_GAMA          category\n",
      " 3   ratio           float16 \n",
      " 4   date_block_num  int64   \n",
      "dtypes: category(3), float16(1), int64(1)\n",
      "memory usage: 51.8 MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['date_block_num'] = df_train['date_block_num'].astype(int)\n",
    "df_test['date_block_num'] = df_test['date_block_num'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['ratio'] = df_train['ratio'].astype(np.float32)\n",
    "df_test['ratio'] = df_test['ratio'].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z_MODELO train (0, 5)\n",
      "Z_MODELO test  (0, 5)\n",
      "Z_PUNTO_VENTA train (0, 5)\n",
      "Z_PUNTO_VENTA test  (0, 5)\n",
      "Z_GAMA train (0, 5)\n",
      "Z_GAMA test  (0, 5)\n",
      "ratio train (0, 5)\n",
      "ratio test  (0, 5)\n",
      "date_block_num train (0, 5)\n",
      "date_block_num test  (0, 5)\n"
     ]
    }
   ],
   "source": [
    "for column in df_train.columns:\n",
    "    print(column,'train',df_train[df_train[column]==358].shape)\n",
    "    if column in df_test.columns:\n",
    "        print(column,'test ',df_test[df_test[column]==358].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Z_MODELO</th>\n",
       "      <th>Z_PUNTO_VENTA</th>\n",
       "      <th>Z_GAMA</th>\n",
       "      <th>ratio</th>\n",
       "      <th>date_block_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Z_MODELO Z_PUNTO_VENTA Z_GAMA  ratio  date_block_num\n",
       "0    MOD_1       PVENT_1  GAM_1    0.0               0\n",
       "1    MOD_1       PVENT_1  GAM_1    0.0               1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA']:\n",
    "    df_train[column] = df_train[column].astype(str)\n",
    "    df_test[column] = df_test[column].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2358650 entries, 0 to 2358649\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Dtype  \n",
      "---  ------          -----  \n",
      " 0   Z_MODELO        object \n",
      " 1   Z_PUNTO_VENTA   object \n",
      " 2   Z_GAMA          object \n",
      " 3   ratio           float32\n",
      " 4   date_block_num  int64  \n",
      "dtypes: float32(1), int64(1), object(3)\n",
      "memory usage: 99.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#'day_of_week', 'day', 'is_month_end', 'day_of_year',  'is_quarter_start', 'year', 'month', 'is_year_start', 'is_month_start', \n",
    "# 'I103','S103', 'C101','I100' , 'C100', 'ID', 'I102','S102',, 'S101', 'S100', 'item_id', 'date_block_num', 'I101'\n",
    "max_prediction_length = prediction_steps\n",
    "\n",
    "max_encoder_length = 60\n",
    "\n",
    "training_cutoff = df_train['date_block_num'].max() - max_prediction_length\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    df_train[lambda x: x['date_block_num'] <= training_cutoff],\n",
    "    time_idx='date_block_num',\n",
    "    target=\"ratio\",\n",
    "    group_ids=['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA'],\n",
    "    min_encoder_length= max_encoder_length // 2,   \n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "        \n",
    "    static_categoricals=['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA'],\n",
    "    \n",
    "    time_varying_known_categoricals=[\n",
    "                                     \"month\", \n",
    "                                     \"dayofweek\",\n",
    "                                     \"dayofyear\"],\n",
    "    \n",
    "    time_varying_known_reals=['days_from_payday'],#\"date_block_num\",\n",
    "    time_varying_unknown_categoricals=['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA'],  \n",
    "    time_varying_unknown_reals= statistics_columns,#+['date_block_num'],#'Demanda',  statistics_columns+['Demanda'],#'date_block_num'],\n",
    "       \n",
    "    #target_normalizer=GroupNormalizer(\n",
    "    #    groups=['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA'], transformation=\"softplus\"\n",
    "    #),  # use softplus and normalize by group    \n",
    "    \n",
    "    categorical_encoders={                          \n",
    "                          \"Z_GAMA\":  pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "                          \"Z_MODELO\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "                          \"Z_PUNTO_VENTA\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "                          \"dayofweek\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "                          \"month\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "                          \"dayofyear\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "                          \"date_block_num\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "                         },\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'day_of_week', 'day', 'is_month_end', 'day_of_year',  'is_quarter_start', 'year', 'month', 'is_year_start', 'is_month_start', \n",
    "# 'I103','S103', 'C101','I100' , 'C100', 'ID', 'I102','S102',, 'S101', 'S100', 'item_id', 'date_block_num', 'I101'\n",
    "max_prediction_length = prediction_steps\n",
    "\n",
    "max_encoder_length = 60\n",
    "\n",
    "training_cutoff = df_train['date_block_num'].max() - max_prediction_length\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    df_train[lambda x: x['date_block_num'] <= training_cutoff],\n",
    "    time_idx='date_block_num',\n",
    "    target=\"ratio\",\n",
    "    group_ids=['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA'],\n",
    "    min_encoder_length= max_encoder_length // 2,   \n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "        \n",
    "    static_categoricals=['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA'],\n",
    "    \n",
    "    time_varying_known_categoricals=[],\n",
    "    \n",
    "    time_varying_known_reals=[],#\"date_block_num\",\n",
    "    time_varying_unknown_categoricals=['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA'],  \n",
    "    time_varying_unknown_reals= statistics_columns,#+['date_block_num'],#'Demanda',  statistics_columns+['Demanda'],#'date_block_num'],\n",
    "       \n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA'], transformation=\"softplus\"\n",
    "    ),  # use softplus and normalize by group    \n",
    "    \n",
    "    categorical_encoders={                          \n",
    "                          \"Z_GAMA\":  pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "                          \"Z_MODELO\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "                          \"Z_PUNTO_VENTA\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "                          #\"dayofweek\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "                          #\"month\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "                          #\"dayofyear\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "                          \"date_block_num\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "                         },\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = TimeSeriesDataSet.from_dataset(training, df_train, predict=True, stop_randomization=True)\n",
    "\n",
    "batch_size = 128\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "val_dataloader   = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting.metrics import MultiHorizonMetric\n",
    "from typing import Dict, List, Tuple, Union\n",
    "from pytorch_forecasting.metrics import TweedieLoss,NegativeBinomialDistributionLoss,BetaDistributionLoss,RMSE\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "'''\n",
    "def tweedieloss(y_true, y_pred):\n",
    "    p=1.5\n",
    "    a = K.pow(y_true, 2-p)/((1-p) * (2-p))\n",
    "    b = K.pow(y_pred, 1-p)/(1-p)\n",
    "    c = K.pow(y_pred, 2-p)/(2-p)\n",
    "    dev = 2 * (a -y_true *b  +c)\n",
    "    return K.mean(dev)\n",
    "'''\n",
    "class new_tweedieloss(MultiHorizonMetric):\n",
    "    def __init__(self, reduction=\"none\", **kwargs):\n",
    "        super().__init__(reduction=reduction, **kwargs)\n",
    "    def loss(self, y_pred: Dict[str, torch.Tensor], target):\n",
    "        p = 1.15#torch.tensor([1.01], dtype=torch.float16)\n",
    "        eps = 1e-10\n",
    "        \n",
    "        factor = 1\n",
    "        \n",
    "        if y_pred.sum() <= eps:\n",
    "            print(\"wtahsd!!\")\n",
    "            factor = 1e19\n",
    "            # y_pred = np.random.rand(len(y_pred))\n",
    "        else:\n",
    "            # y_pred = np.where(y_pred<0, eps, y_pred)  #Filter 0 and negative values \n",
    "            y_pred = torch.abs(y_pred)\n",
    "        #.requires_grad_(True)\n",
    "        preds = self.to_prediction(y_pred) + eps\n",
    "        #'''\n",
    "        \n",
    "        a = target*(torch.pow(preds,1-p))/(1-p)\n",
    "        b = torch.pow(preds,2-p)/(2-p)\n",
    "        tweddie = torch.mean((-a+b)/factor)\n",
    "        '''\n",
    "        a = torch.pow(target, 2-p)/((1-p) * (2-p))\n",
    "        b = torch.pow(preds, 1-p)/(1-p)\n",
    "        c = torch.pow(preds, 2-p)/(2-p)\n",
    "        tweddie = -2 * (a -target *b  +c)\n",
    "        '''\n",
    "        return tweddie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 30.8k\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer = pl.Trainer(gpus=1,gradient_clip_val=0.1)\n",
    "trainer.enforce_positive_output=True\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "\n",
    "    # not meaningful for finding the learning rate but otherwise very important\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=16,  # most important hyperparameter apart from learning rate\n",
    "    # number of attention heads. Set to up to 4 for large datasets\n",
    "    attention_head_size=1,\n",
    "    dropout=0.1,  # between 0.1 and 0.3 are good values\n",
    "    hidden_continuous_size=8,  # set to <= hidden_size\n",
    "    output_size=1,  # 7 quantiles by default\n",
    "    loss = RMSE(),#new_tweedieloss(),#.to(device),    \n",
    "    # reduce learning rate if no improvement in validation loss after x epochs\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Finding best initial lr: 100%|██████████████████████████████████████████████████████████████████████████| 100/100 [00:12<00:00,  7.49it/s]`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Finding best initial lr: 100%|██████████████████████████████████████████████████████████████████████████| 100/100 [00:12<00:00,  7.77it/s]\n",
      "Restoring states from the checkpoint path at /media/cristian/Extreme SSD/Investigacion/DATATHONES/entel-2022/DATATHON-ENTEL-2022---Reto2/notebooks/cristian/.lr_find_1ee73dc0-bb67-4a07-b9d9-750e1a9031c9.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suggested learning rate: 9.120108393559097e-07\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# find optimal learning rate\n",
    "res = trainer.tuner.lr_find(\n",
    "    \n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    "    max_lr=0.1,\n",
    "    min_lr=1e-7,\n",
    ")\n",
    "\n",
    "print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "fig = res.plot(show=True, suggest=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.120108393559097e-07"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.suggestion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 30.5k\n"
     ]
    }
   ],
   "source": [
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate= res.suggestion(),\n",
    "    hidden_size=16,\n",
    "    attention_head_size=4,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    output_size=1, \n",
    "    loss = RMSE(),#new_tweedieloss(),\n",
    "    log_interval=20,  \n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "holas\n"
     ]
    }
   ],
   "source": [
    "#Early Stopping \n",
    "MIN_DELTA  = 1e-7\n",
    "PATIENCE   = 30\n",
    "\n",
    "#PL Trainer\n",
    "MAX_EPOCHS = 500\n",
    "\n",
    "GPUS = 1\n",
    "\n",
    "\n",
    "\n",
    "# configure network and trainer\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=MIN_DELTA, patience=PATIENCE, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor(logging_interval='epoch')  # log the learning rate\n",
    "logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='../../results/models/tft/',\n",
    "    filename='MODEL_tft-tweedie-loss-epoch_{epoch:02d}-val_loss_{val_loss:.3f}',\n",
    "    auto_insert_metric_name=False,\n",
    "    \n",
    " )\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    gpus=GPUS,\n",
    "    #weights_summary=\"top\",\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=30,#oment in for training, running valiation every 30 batches\n",
    "    #fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "    callbacks=[lr_logger, early_stop_callback,checkpoint_callback],\n",
    "    #log_every_n_steps=10,\n",
    "    logger=logger,\n",
    ")\n",
    "trainer.enforce_positive_output=True\n",
    "\n",
    "\n",
    "print('holas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | RMSE                            | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 12.9 K\n",
      "3  | prescalers                         | ModuleDict                      | 64    \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 2.3 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 1.0 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 528   \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K \n",
      "11 | lstm_encoder                       | LSTM                            | 2.2 K \n",
      "12 | lstm_decoder                       | LSTM                            | 2.2 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 676   \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 576   \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 576   \n",
      "20 | output_layer                       | Linear                          | 17    \n",
      "----------------------------------------------------------------------------------------\n",
      "30.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "30.5 K    Total params\n",
      "0.122     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  45%|███████████████████▋                        | 30/67 [00:04<00:05,  7.29it/s, loss=0.0138, v_num=16, train_loss_step=0.00421]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  46%|████████████████████▎                       | 31/67 [00:04<00:05,  6.39it/s, loss=0.0138, v_num=16, train_loss_step=0.00421]\u001b[A\n",
      "Epoch 0:  48%|█████████████████████                       | 32/67 [00:05<00:06,  5.83it/s, loss=0.0138, v_num=16, train_loss_step=0.00421]\u001b[A\n",
      "Epoch 0:  49%|█████████████████████▋                      | 33/67 [00:06<00:06,  5.39it/s, loss=0.0138, v_num=16, train_loss_step=0.00421]\u001b[A\n",
      "Epoch 0:  51%|██████████████████████▎                     | 34/67 [00:06<00:06,  4.98it/s, loss=0.0138, v_num=16, train_loss_step=0.00421]\u001b[A\n",
      "Epoch 0:  52%|██████████████████████▉                     | 35/67 [00:07<00:06,  4.70it/s, loss=0.0138, v_num=16, train_loss_step=0.00421]\u001b[A\n",
      "Epoch 0:  54%|███████████████████████▋                    | 36/67 [00:08<00:06,  4.46it/s, loss=0.0138, v_num=16, train_loss_step=0.00421]\u001b[A\n",
      "Epoch 0:  55%|████████████████████████▎                   | 37/67 [00:08<00:07,  4.26it/s, loss=0.0138, v_num=16, train_loss_step=0.00421]\u001b[A\n",
      "Epoch 0:  57%|████████████████████████▉                   | 38/67 [00:09<00:07,  4.08it/s, loss=0.0138, v_num=16, train_loss_step=0.00421]\u001b[A\n",
      "Epoch 0:  58%|█████████████████████████▌                  | 39/67 [00:09<00:07,  3.93it/s, loss=0.0138, v_num=16, train_loss_step=0.00421]\u001b[A\n",
      "Epoch 0:  60%|██████████████████████████▎                 | 40/67 [00:10<00:07,  3.76it/s, loss=0.0138, v_num=16, train_loss_step=0.00421]\u001b[A\n",
      "Epoch 0:  61%|██████████████████████████▉                 | 41/67 [00:11<00:07,  3.64it/s, loss=0.0138, v_num=16, train_loss_step=0.00421]\u001b[A\n",
      "Epoch 0:  63%|███████████████████████████▌                | 42/67 [00:11<00:07,  3.54it/s, loss=0.0138, v_num=16, train_loss_step=0.00421]\u001b[A\n",
      "Epoch 0:  64%|████████████████████████████▏               | 43/67 [00:12<00:06,  3.45it/s, loss=0.0138, v_num=16, train_loss_step=0.00421]\u001b[A\n",
      "Epoch 0:  66%|████████████████████████████▉               | 44/67 [00:13<00:06,  3.37it/s, loss=0.0138, v_num=16, train_loss_step=0.00421]\u001b[A\n",
      "Epoch 0:  67%|█████████████████████████████▌              | 45/67 [00:13<00:06,  3.30it/s, loss=0.0138, v_num=16, train_loss_step=0.00421]\u001b[A\n",
      "Epoch 0:  69%|██████████████████████████████▏             | 46/67 [00:14<00:06,  3.21it/s, loss=0.0138, v_num=16, train_loss_step=0.00421]\u001b[A\n",
      "Epoch 0:  70%|██████████████████████████████▊             | 47/67 [00:14<00:06,  3.15it/s, loss=0.0138, v_num=16, train_loss_step=0.00421]\u001b[A\n",
      "Epoch 0:  72%|███████████████████████████████▌            | 48/67 [00:15<00:06,  3.07it/s, loss=0.0138, v_num=16, train_loss_step=0.00421]\u001b[A\n",
      "Epoch 0:  73%|████████████████████████████████▏           | 49/67 [00:16<00:05,  3.02it/s, loss=0.0138, v_num=16, train_loss_step=0.00421]\u001b[A\n",
      "Epoch 0:  75%|████████████████████████████████▊           | 50/67 [00:16<00:05,  2.97it/s, loss=0.0138, v_num=16, train_loss_step=0.00421]\u001b[A\n",
      "Epoch 0:  76%|█████████████████████████████████▍          | 51/67 [00:17<00:05,  2.91it/s, loss=0.0138, v_num=16, train_loss_step=0.00421]\u001b[A\n",
      "Epoch 0:  78%|██████████████████████████████████▏         | 52/67 [00:18<00:05,  2.84it/s, loss=0.0138, v_num=16, train_loss_step=0.00421]\u001b[A\n",
      "Epoch 0:  79%|██████████████████████████████████▊         | 53/67 [00:18<00:04,  2.81it/s, loss=0.0138, v_num=16, train_loss_step=0.00421]\u001b[A\n",
      "Epoch 0:  81%|███████████████████████████████████▍        | 54/67 [00:19<00:04,  2.76it/s, loss=0.0138, v_num=16, train_loss_step=0.00421]\u001b[A\n",
      "Epoch 0:  82%|████████████████████████████████████        | 55/67 [00:20<00:04,  2.73it/s, loss=0.0138, v_num=16, train_loss_step=0.00421]\u001b[A\n",
      "Epoch 0:  84%|████████████████████████████████████▊       | 56/67 [00:20<00:04,  2.69it/s, loss=0.0138, v_num=16, train_loss_step=0.00421]\u001b[A\n",
      "Epoch 0:  85%|█████████████████████████████████████▍      | 57/67 [00:21<00:03,  2.66it/s, loss=0.0138, v_num=16, train_loss_step=0.00421]\u001b[A\n",
      "Epoch 0:  87%|██████████████████████████████████████      | 58/67 [00:22<00:03,  2.62it/s, loss=0.0138, v_num=16, train_loss_step=0.00421]\u001b[A\n",
      "Epoch 0:  88%|██████████████████████████████████████▋     | 59/67 [00:22<00:03,  2.59it/s, loss=0.0138, v_num=16, train_loss_step=0.00421]\u001b[A\n",
      "Epoch 0:  90%|███████████████████████████████████████▍    | 60/67 [00:23<00:02,  2.56it/s, loss=0.0138, v_num=16, train_loss_step=0.00421]\u001b[A\n",
      "Epoch 0:  91%|████████████████████████████████████████    | 61/67 [00:24<00:02,  2.54it/s, loss=0.0138, v_num=16, train_loss_step=0.00421]\u001b[A\n",
      "Epoch 0:  93%|████████████████████████████████████████▋   | 62/67 [00:24<00:01,  2.51it/s, loss=0.0138, v_num=16, train_loss_step=0.00421]\u001b[A\n",
      "Epoch 0:  94%|█████████████████████████████████████████▎  | 63/67 [00:25<00:01,  2.49it/s, loss=0.0138, v_num=16, train_loss_step=0.00421]\u001b[A\n",
      "Epoch 0:  96%|██████████████████████████████████████████  | 64/67 [00:25<00:01,  2.47it/s, loss=0.0138, v_num=16, train_loss_step=0.00421]\u001b[A\n",
      "Epoch 0:  97%|██████████████████████████████████████████▋ | 65/67 [00:26<00:00,  2.44it/s, loss=0.0138, v_num=16, train_loss_step=0.00421]\u001b[A\n",
      "Epoch 0:  99%|███████████████████████████████████████████▎| 66/67 [00:27<00:00,  2.42it/s, loss=0.0138, v_num=16, train_loss_step=0.00421]\u001b[A\n",
      "Epoch 0: 100%|███████████████████████████| 67/67 [00:28<00:00,  2.39it/s, loss=0.0138, v_num=16, train_loss_step=0.00421, val_loss=0.0106]\u001b[A\n",
      "Epoch 1:  45%|█▊  | 30/67 [00:04<00:05,  7.10it/s, loss=0.01, v_num=16, train_loss_step=0.00275, val_loss=0.0106, train_loss_epoch=0.0135]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  46%|█▊  | 31/67 [00:04<00:05,  6.27it/s, loss=0.01, v_num=16, train_loss_step=0.00275, val_loss=0.0106, train_loss_epoch=0.0135]\u001b[A\n",
      "Epoch 1:  48%|█▉  | 32/67 [00:05<00:06,  5.61it/s, loss=0.01, v_num=16, train_loss_step=0.00275, val_loss=0.0106, train_loss_epoch=0.0135]\u001b[A\n",
      "Epoch 1:  49%|█▉  | 33/67 [00:06<00:06,  5.22it/s, loss=0.01, v_num=16, train_loss_step=0.00275, val_loss=0.0106, train_loss_epoch=0.0135]\u001b[A\n",
      "Epoch 1:  51%|██  | 34/67 [00:06<00:06,  4.88it/s, loss=0.01, v_num=16, train_loss_step=0.00275, val_loss=0.0106, train_loss_epoch=0.0135]\u001b[A\n",
      "Epoch 1:  52%|██  | 35/67 [00:07<00:06,  4.60it/s, loss=0.01, v_num=16, train_loss_step=0.00275, val_loss=0.0106, train_loss_epoch=0.0135]\u001b[A\n",
      "Epoch 1:  54%|██▏ | 36/67 [00:08<00:07,  4.37it/s, loss=0.01, v_num=16, train_loss_step=0.00275, val_loss=0.0106, train_loss_epoch=0.0135]\u001b[A\n",
      "Epoch 1:  55%|██▏ | 37/67 [00:08<00:07,  4.17it/s, loss=0.01, v_num=16, train_loss_step=0.00275, val_loss=0.0106, train_loss_epoch=0.0135]\u001b[A\n",
      "Epoch 1:  57%|██▎ | 38/67 [00:09<00:07,  3.99it/s, loss=0.01, v_num=16, train_loss_step=0.00275, val_loss=0.0106, train_loss_epoch=0.0135]\u001b[A\n",
      "Epoch 1:  58%|██▎ | 39/67 [00:10<00:07,  3.80it/s, loss=0.01, v_num=16, train_loss_step=0.00275, val_loss=0.0106, train_loss_epoch=0.0135]\u001b[A\n",
      "Epoch 1:  60%|██▍ | 40/67 [00:10<00:07,  3.68it/s, loss=0.01, v_num=16, train_loss_step=0.00275, val_loss=0.0106, train_loss_epoch=0.0135]\u001b[A\n",
      "Epoch 1:  61%|██▍ | 41/67 [00:11<00:07,  3.57it/s, loss=0.01, v_num=16, train_loss_step=0.00275, val_loss=0.0106, train_loss_epoch=0.0135]\u001b[A\n",
      "Epoch 1:  63%|██▌ | 42/67 [00:12<00:07,  3.47it/s, loss=0.01, v_num=16, train_loss_step=0.00275, val_loss=0.0106, train_loss_epoch=0.0135]\u001b[A\n",
      "Epoch 1:  64%|██▌ | 43/67 [00:12<00:07,  3.38it/s, loss=0.01, v_num=16, train_loss_step=0.00275, val_loss=0.0106, train_loss_epoch=0.0135]\u001b[A\n",
      "Epoch 1:  66%|██▋ | 44/67 [00:13<00:06,  3.29it/s, loss=0.01, v_num=16, train_loss_step=0.00275, val_loss=0.0106, train_loss_epoch=0.0135]\u001b[A\n",
      "Epoch 1:  67%|██▋ | 45/67 [00:14<00:06,  3.20it/s, loss=0.01, v_num=16, train_loss_step=0.00275, val_loss=0.0106, train_loss_epoch=0.0135]\u001b[A\n",
      "Epoch 1:  69%|██▋ | 46/67 [00:14<00:06,  3.13it/s, loss=0.01, v_num=16, train_loss_step=0.00275, val_loss=0.0106, train_loss_epoch=0.0135]\u001b[A\n",
      "Epoch 1:  70%|██▊ | 47/67 [00:15<00:06,  3.07it/s, loss=0.01, v_num=16, train_loss_step=0.00275, val_loss=0.0106, train_loss_epoch=0.0135]\u001b[A\n",
      "Epoch 1:  72%|██▊ | 48/67 [00:16<00:06,  3.00it/s, loss=0.01, v_num=16, train_loss_step=0.00275, val_loss=0.0106, train_loss_epoch=0.0135]\u001b[A\n",
      "Epoch 1:  73%|██▉ | 49/67 [00:16<00:06,  2.94it/s, loss=0.01, v_num=16, train_loss_step=0.00275, val_loss=0.0106, train_loss_epoch=0.0135]\u001b[A\n",
      "Epoch 1:  75%|██▉ | 50/67 [00:17<00:05,  2.90it/s, loss=0.01, v_num=16, train_loss_step=0.00275, val_loss=0.0106, train_loss_epoch=0.0135]\u001b[A\n",
      "Epoch 1:  76%|███ | 51/67 [00:17<00:05,  2.84it/s, loss=0.01, v_num=16, train_loss_step=0.00275, val_loss=0.0106, train_loss_epoch=0.0135]\u001b[A\n",
      "Epoch 1:  78%|███ | 52/67 [00:18<00:05,  2.78it/s, loss=0.01, v_num=16, train_loss_step=0.00275, val_loss=0.0106, train_loss_epoch=0.0135]\u001b[A\n",
      "Epoch 1:  79%|███▏| 53/67 [00:19<00:05,  2.74it/s, loss=0.01, v_num=16, train_loss_step=0.00275, val_loss=0.0106, train_loss_epoch=0.0135]\u001b[A\n",
      "Epoch 1:  81%|███▏| 54/67 [00:20<00:04,  2.70it/s, loss=0.01, v_num=16, train_loss_step=0.00275, val_loss=0.0106, train_loss_epoch=0.0135]\u001b[A\n",
      "Epoch 1:  82%|███▎| 55/67 [00:20<00:04,  2.66it/s, loss=0.01, v_num=16, train_loss_step=0.00275, val_loss=0.0106, train_loss_epoch=0.0135]\u001b[A\n",
      "Epoch 1:  84%|███▎| 56/67 [00:21<00:04,  2.63it/s, loss=0.01, v_num=16, train_loss_step=0.00275, val_loss=0.0106, train_loss_epoch=0.0135]\u001b[A\n",
      "Epoch 1:  85%|███▍| 57/67 [00:21<00:03,  2.60it/s, loss=0.01, v_num=16, train_loss_step=0.00275, val_loss=0.0106, train_loss_epoch=0.0135]\u001b[A\n",
      "Epoch 1:  87%|███▍| 58/67 [00:22<00:03,  2.56it/s, loss=0.01, v_num=16, train_loss_step=0.00275, val_loss=0.0106, train_loss_epoch=0.0135]\u001b[A\n",
      "Epoch 1:  88%|███▌| 59/67 [00:23<00:03,  2.53it/s, loss=0.01, v_num=16, train_loss_step=0.00275, val_loss=0.0106, train_loss_epoch=0.0135]\u001b[A\n",
      "Epoch 1:  90%|███▌| 60/67 [00:23<00:02,  2.51it/s, loss=0.01, v_num=16, train_loss_step=0.00275, val_loss=0.0106, train_loss_epoch=0.0135]\u001b[A\n",
      "Epoch 1:  91%|███▋| 61/67 [00:24<00:02,  2.48it/s, loss=0.01, v_num=16, train_loss_step=0.00275, val_loss=0.0106, train_loss_epoch=0.0135]\u001b[A\n",
      "Epoch 1:  93%|███▋| 62/67 [00:25<00:02,  2.46it/s, loss=0.01, v_num=16, train_loss_step=0.00275, val_loss=0.0106, train_loss_epoch=0.0135]\u001b[A\n",
      "Epoch 1:  94%|███▊| 63/67 [00:25<00:01,  2.44it/s, loss=0.01, v_num=16, train_loss_step=0.00275, val_loss=0.0106, train_loss_epoch=0.0135]\u001b[A\n",
      "Epoch 1:  96%|███▊| 64/67 [00:26<00:01,  2.41it/s, loss=0.01, v_num=16, train_loss_step=0.00275, val_loss=0.0106, train_loss_epoch=0.0135]\u001b[A\n",
      "Epoch 1:  97%|███▉| 65/67 [00:27<00:00,  2.39it/s, loss=0.01, v_num=16, train_loss_step=0.00275, val_loss=0.0106, train_loss_epoch=0.0135]\u001b[A\n",
      "Epoch 1:  99%|███▉| 66/67 [00:27<00:00,  2.37it/s, loss=0.01, v_num=16, train_loss_step=0.00275, val_loss=0.0106, train_loss_epoch=0.0135]\u001b[A\n",
      "Epoch 1: 100%|████| 67/67 [00:28<00:00,  2.33it/s, loss=0.01, v_num=16, train_loss_step=0.00275, val_loss=0.0106, train_loss_epoch=0.0135]\u001b[A\n",
      "Epoch 2:  45%|▍| 30/67 [00:04<00:05,  7.12it/s, loss=0.0112, v_num=16, train_loss_step=0.00745, val_loss=0.0106, train_loss_epoch=0.00986]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  46%|▍| 31/67 [00:04<00:05,  6.22it/s, loss=0.0112, v_num=16, train_loss_step=0.00745, val_loss=0.0106, train_loss_epoch=0.00986]\u001b[A\n",
      "Epoch 2:  48%|▍| 32/67 [00:05<00:06,  5.67it/s, loss=0.0112, v_num=16, train_loss_step=0.00745, val_loss=0.0106, train_loss_epoch=0.00986]\u001b[A\n",
      "Epoch 2:  49%|▍| 33/67 [00:06<00:06,  5.18it/s, loss=0.0112, v_num=16, train_loss_step=0.00745, val_loss=0.0106, train_loss_epoch=0.00986]\u001b[A\n",
      "Epoch 2:  51%|▌| 34/67 [00:06<00:06,  4.86it/s, loss=0.0112, v_num=16, train_loss_step=0.00745, val_loss=0.0106, train_loss_epoch=0.00986]\u001b[A\n",
      "Epoch 2:  52%|▌| 35/67 [00:07<00:06,  4.59it/s, loss=0.0112, v_num=16, train_loss_step=0.00745, val_loss=0.0106, train_loss_epoch=0.00986]\u001b[A\n",
      "Epoch 2:  54%|▌| 36/67 [00:08<00:07,  4.35it/s, loss=0.0112, v_num=16, train_loss_step=0.00745, val_loss=0.0106, train_loss_epoch=0.00986]\u001b[A\n",
      "Epoch 2:  55%|▌| 37/67 [00:08<00:07,  4.15it/s, loss=0.0112, v_num=16, train_loss_step=0.00745, val_loss=0.0106, train_loss_epoch=0.00986]\u001b[A\n",
      "Epoch 2:  57%|▌| 38/67 [00:09<00:07,  3.99it/s, loss=0.0112, v_num=16, train_loss_step=0.00745, val_loss=0.0106, train_loss_epoch=0.00986]\u001b[A\n",
      "Epoch 2:  58%|▌| 39/67 [00:10<00:07,  3.84it/s, loss=0.0112, v_num=16, train_loss_step=0.00745, val_loss=0.0106, train_loss_epoch=0.00986]\u001b[A\n",
      "Epoch 2:  60%|▌| 40/67 [00:10<00:07,  3.68it/s, loss=0.0112, v_num=16, train_loss_step=0.00745, val_loss=0.0106, train_loss_epoch=0.00986]\u001b[A\n",
      "Epoch 2:  61%|▌| 41/67 [00:11<00:07,  3.57it/s, loss=0.0112, v_num=16, train_loss_step=0.00745, val_loss=0.0106, train_loss_epoch=0.00986]\u001b[A\n",
      "Epoch 2:  63%|▋| 42/67 [00:12<00:07,  3.48it/s, loss=0.0112, v_num=16, train_loss_step=0.00745, val_loss=0.0106, train_loss_epoch=0.00986]\u001b[A\n",
      "Epoch 2:  64%|▋| 43/67 [00:12<00:07,  3.40it/s, loss=0.0112, v_num=16, train_loss_step=0.00745, val_loss=0.0106, train_loss_epoch=0.00986]\u001b[A\n",
      "Epoch 2:  66%|▋| 44/67 [00:13<00:06,  3.31it/s, loss=0.0112, v_num=16, train_loss_step=0.00745, val_loss=0.0106, train_loss_epoch=0.00986]\u001b[A\n",
      "Epoch 2:  67%|▋| 45/67 [00:13<00:06,  3.24it/s, loss=0.0112, v_num=16, train_loss_step=0.00745, val_loss=0.0106, train_loss_epoch=0.00986]\u001b[A\n",
      "Epoch 2:  69%|▋| 46/67 [00:14<00:06,  3.17it/s, loss=0.0112, v_num=16, train_loss_step=0.00745, val_loss=0.0106, train_loss_epoch=0.00986]\u001b[A\n",
      "Epoch 2:  70%|▋| 47/67 [00:15<00:06,  3.11it/s, loss=0.0112, v_num=16, train_loss_step=0.00745, val_loss=0.0106, train_loss_epoch=0.00986]\u001b[A\n",
      "Epoch 2:  72%|▋| 48/67 [00:15<00:06,  3.02it/s, loss=0.0112, v_num=16, train_loss_step=0.00745, val_loss=0.0106, train_loss_epoch=0.00986]\u001b[A\n",
      "Epoch 2:  73%|▋| 49/67 [00:16<00:06,  2.97it/s, loss=0.0112, v_num=16, train_loss_step=0.00745, val_loss=0.0106, train_loss_epoch=0.00986]\u001b[A\n",
      "Epoch 2:  75%|▋| 50/67 [00:17<00:05,  2.93it/s, loss=0.0112, v_num=16, train_loss_step=0.00745, val_loss=0.0106, train_loss_epoch=0.00986]\u001b[A\n",
      "Epoch 2:  76%|▊| 51/67 [00:17<00:05,  2.87it/s, loss=0.0112, v_num=16, train_loss_step=0.00745, val_loss=0.0106, train_loss_epoch=0.00986]\u001b[A\n",
      "Epoch 2:  78%|▊| 52/67 [00:18<00:05,  2.82it/s, loss=0.0112, v_num=16, train_loss_step=0.00745, val_loss=0.0106, train_loss_epoch=0.00986]\u001b[A\n",
      "Epoch 2:  79%|▊| 53/67 [00:19<00:05,  2.78it/s, loss=0.0112, v_num=16, train_loss_step=0.00745, val_loss=0.0106, train_loss_epoch=0.00986]\u001b[A\n",
      "Epoch 2:  81%|▊| 54/67 [00:19<00:04,  2.73it/s, loss=0.0112, v_num=16, train_loss_step=0.00745, val_loss=0.0106, train_loss_epoch=0.00986]\u001b[A\n",
      "Epoch 2:  82%|▊| 55/67 [00:20<00:04,  2.69it/s, loss=0.0112, v_num=16, train_loss_step=0.00745, val_loss=0.0106, train_loss_epoch=0.00986]\u001b[A\n",
      "Epoch 2:  84%|▊| 56/67 [00:21<00:04,  2.66it/s, loss=0.0112, v_num=16, train_loss_step=0.00745, val_loss=0.0106, train_loss_epoch=0.00986]\u001b[A\n",
      "Epoch 2:  85%|▊| 57/67 [00:21<00:03,  2.63it/s, loss=0.0112, v_num=16, train_loss_step=0.00745, val_loss=0.0106, train_loss_epoch=0.00986]\u001b[A\n",
      "Epoch 2:  87%|▊| 58/67 [00:22<00:03,  2.60it/s, loss=0.0112, v_num=16, train_loss_step=0.00745, val_loss=0.0106, train_loss_epoch=0.00986]\u001b[A\n",
      "Epoch 2:  88%|▉| 59/67 [00:22<00:03,  2.57it/s, loss=0.0112, v_num=16, train_loss_step=0.00745, val_loss=0.0106, train_loss_epoch=0.00986]\u001b[A\n",
      "Epoch 2:  90%|▉| 60/67 [00:23<00:02,  2.54it/s, loss=0.0112, v_num=16, train_loss_step=0.00745, val_loss=0.0106, train_loss_epoch=0.00986]\u001b[A\n",
      "Epoch 2:  91%|▉| 61/67 [00:24<00:02,  2.51it/s, loss=0.0112, v_num=16, train_loss_step=0.00745, val_loss=0.0106, train_loss_epoch=0.00986]\u001b[A\n",
      "Epoch 2:  93%|▉| 62/67 [00:24<00:02,  2.49it/s, loss=0.0112, v_num=16, train_loss_step=0.00745, val_loss=0.0106, train_loss_epoch=0.00986]\u001b[A\n",
      "Epoch 2:  94%|▉| 63/67 [00:25<00:01,  2.46it/s, loss=0.0112, v_num=16, train_loss_step=0.00745, val_loss=0.0106, train_loss_epoch=0.00986]\u001b[A\n",
      "Epoch 2:  96%|▉| 64/67 [00:26<00:01,  2.44it/s, loss=0.0112, v_num=16, train_loss_step=0.00745, val_loss=0.0106, train_loss_epoch=0.00986]\u001b[A\n",
      "Epoch 2:  97%|▉| 65/67 [00:26<00:00,  2.42it/s, loss=0.0112, v_num=16, train_loss_step=0.00745, val_loss=0.0106, train_loss_epoch=0.00986]\u001b[A\n",
      "Epoch 2:  99%|▉| 66/67 [00:27<00:00,  2.40it/s, loss=0.0112, v_num=16, train_loss_step=0.00745, val_loss=0.0106, train_loss_epoch=0.00986]\u001b[A\n",
      "Epoch 2: 100%|█| 67/67 [00:28<00:00,  2.36it/s, loss=0.0112, v_num=16, train_loss_step=0.00745, val_loss=0.0106, train_loss_epoch=0.00986]\u001b[A\n",
      "Epoch 3:  45%|▉ | 30/67 [00:04<00:05,  7.13it/s, loss=0.0168, v_num=16, train_loss_step=0.00387, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  46%|▉ | 31/67 [00:04<00:05,  6.24it/s, loss=0.0168, v_num=16, train_loss_step=0.00387, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 3:  48%|▉ | 32/67 [00:05<00:06,  5.69it/s, loss=0.0168, v_num=16, train_loss_step=0.00387, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 3:  49%|▉ | 33/67 [00:06<00:06,  5.28it/s, loss=0.0168, v_num=16, train_loss_step=0.00387, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 3:  51%|█ | 34/67 [00:06<00:06,  4.93it/s, loss=0.0168, v_num=16, train_loss_step=0.00387, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 3:  52%|█ | 35/67 [00:07<00:06,  4.66it/s, loss=0.0168, v_num=16, train_loss_step=0.00387, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 3:  54%|█ | 36/67 [00:08<00:07,  4.37it/s, loss=0.0168, v_num=16, train_loss_step=0.00387, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 3:  55%|█ | 37/67 [00:08<00:07,  4.17it/s, loss=0.0168, v_num=16, train_loss_step=0.00387, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 3:  57%|█▏| 38/67 [00:09<00:07,  4.00it/s, loss=0.0168, v_num=16, train_loss_step=0.00387, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 3:  58%|█▏| 39/67 [00:10<00:07,  3.86it/s, loss=0.0168, v_num=16, train_loss_step=0.00387, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 3:  60%|█▏| 40/67 [00:10<00:07,  3.74it/s, loss=0.0168, v_num=16, train_loss_step=0.00387, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 3:  61%|█▏| 41/67 [00:11<00:07,  3.61it/s, loss=0.0168, v_num=16, train_loss_step=0.00387, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 3:  63%|█▎| 42/67 [00:11<00:07,  3.50it/s, loss=0.0168, v_num=16, train_loss_step=0.00387, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 3:  64%|█▎| 43/67 [00:12<00:07,  3.41it/s, loss=0.0168, v_num=16, train_loss_step=0.00387, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 3:  66%|█▎| 44/67 [00:13<00:06,  3.30it/s, loss=0.0168, v_num=16, train_loss_step=0.00387, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 3:  67%|█▎| 45/67 [00:13<00:06,  3.22it/s, loss=0.0168, v_num=16, train_loss_step=0.00387, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 3:  69%|█▎| 46/67 [00:14<00:06,  3.15it/s, loss=0.0168, v_num=16, train_loss_step=0.00387, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 3:  70%|█▍| 47/67 [00:15<00:06,  3.09it/s, loss=0.0168, v_num=16, train_loss_step=0.00387, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 3:  72%|█▍| 48/67 [00:15<00:06,  3.02it/s, loss=0.0168, v_num=16, train_loss_step=0.00387, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 3:  73%|█▍| 49/67 [00:16<00:06,  2.97it/s, loss=0.0168, v_num=16, train_loss_step=0.00387, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 3:  75%|█▍| 50/67 [00:17<00:05,  2.92it/s, loss=0.0168, v_num=16, train_loss_step=0.00387, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 3:  76%|█▌| 51/67 [00:17<00:05,  2.84it/s, loss=0.0168, v_num=16, train_loss_step=0.00387, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 3:  78%|█▌| 52/67 [00:18<00:05,  2.79it/s, loss=0.0168, v_num=16, train_loss_step=0.00387, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 3:  79%|█▌| 53/67 [00:19<00:05,  2.75it/s, loss=0.0168, v_num=16, train_loss_step=0.00387, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 3:  81%|█▌| 54/67 [00:19<00:04,  2.71it/s, loss=0.0168, v_num=16, train_loss_step=0.00387, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 3:  82%|█▋| 55/67 [00:20<00:04,  2.67it/s, loss=0.0168, v_num=16, train_loss_step=0.00387, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 3:  84%|█▋| 56/67 [00:21<00:04,  2.64it/s, loss=0.0168, v_num=16, train_loss_step=0.00387, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 3:  85%|█▋| 57/67 [00:21<00:03,  2.60it/s, loss=0.0168, v_num=16, train_loss_step=0.00387, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 3:  87%|█▋| 58/67 [00:22<00:03,  2.56it/s, loss=0.0168, v_num=16, train_loss_step=0.00387, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 3:  88%|█▊| 59/67 [00:23<00:03,  2.53it/s, loss=0.0168, v_num=16, train_loss_step=0.00387, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 3:  90%|█▊| 60/67 [00:23<00:02,  2.51it/s, loss=0.0168, v_num=16, train_loss_step=0.00387, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 3:  91%|█▊| 61/67 [00:24<00:02,  2.48it/s, loss=0.0168, v_num=16, train_loss_step=0.00387, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 3:  93%|█▊| 62/67 [00:25<00:02,  2.45it/s, loss=0.0168, v_num=16, train_loss_step=0.00387, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 3:  94%|█▉| 63/67 [00:25<00:01,  2.43it/s, loss=0.0168, v_num=16, train_loss_step=0.00387, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 3:  96%|█▉| 64/67 [00:26<00:01,  2.41it/s, loss=0.0168, v_num=16, train_loss_step=0.00387, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 3:  97%|█▉| 65/67 [00:27<00:00,  2.39it/s, loss=0.0168, v_num=16, train_loss_step=0.00387, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 3:  99%|█▉| 66/67 [00:27<00:00,  2.36it/s, loss=0.0168, v_num=16, train_loss_step=0.00387, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 3: 100%|██| 67/67 [00:28<00:00,  2.33it/s, loss=0.0168, v_num=16, train_loss_step=0.00387, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 4:  45%|█▎ | 30/67 [00:04<00:05,  7.00it/s, loss=0.0145, v_num=16, train_loss_step=0.0176, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  46%|█▍ | 31/67 [00:05<00:05,  6.16it/s, loss=0.0145, v_num=16, train_loss_step=0.0176, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 4:  48%|█▍ | 32/67 [00:05<00:06,  5.66it/s, loss=0.0145, v_num=16, train_loss_step=0.0176, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 4:  49%|█▍ | 33/67 [00:06<00:06,  5.26it/s, loss=0.0145, v_num=16, train_loss_step=0.0176, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 4:  51%|█▌ | 34/67 [00:07<00:06,  4.85it/s, loss=0.0145, v_num=16, train_loss_step=0.0176, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 4:  52%|█▌ | 35/67 [00:07<00:06,  4.58it/s, loss=0.0145, v_num=16, train_loss_step=0.0176, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 4:  54%|█▌ | 36/67 [00:08<00:07,  4.34it/s, loss=0.0145, v_num=16, train_loss_step=0.0176, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 4:  55%|█▋ | 37/67 [00:08<00:07,  4.14it/s, loss=0.0145, v_num=16, train_loss_step=0.0176, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 4:  57%|█▋ | 38/67 [00:09<00:07,  3.97it/s, loss=0.0145, v_num=16, train_loss_step=0.0176, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 4:  58%|█▋ | 39/67 [00:10<00:07,  3.82it/s, loss=0.0145, v_num=16, train_loss_step=0.0176, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 4:  60%|█▊ | 40/67 [00:10<00:07,  3.69it/s, loss=0.0145, v_num=16, train_loss_step=0.0176, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 4:  61%|█▊ | 41/67 [00:11<00:07,  3.57it/s, loss=0.0145, v_num=16, train_loss_step=0.0176, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 4:  63%|█▉ | 42/67 [00:12<00:07,  3.44it/s, loss=0.0145, v_num=16, train_loss_step=0.0176, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 4:  64%|█▉ | 43/67 [00:12<00:07,  3.35it/s, loss=0.0145, v_num=16, train_loss_step=0.0176, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 4:  66%|█▉ | 44/67 [00:13<00:07,  3.27it/s, loss=0.0145, v_num=16, train_loss_step=0.0176, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 4:  67%|██ | 45/67 [00:14<00:06,  3.20it/s, loss=0.0145, v_num=16, train_loss_step=0.0176, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 4:  69%|██ | 46/67 [00:14<00:06,  3.13it/s, loss=0.0145, v_num=16, train_loss_step=0.0176, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 4:  70%|██ | 47/67 [00:15<00:06,  3.07it/s, loss=0.0145, v_num=16, train_loss_step=0.0176, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 4:  72%|██▏| 48/67 [00:16<00:06,  3.00it/s, loss=0.0145, v_num=16, train_loss_step=0.0176, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 4:  73%|██▏| 49/67 [00:16<00:06,  2.95it/s, loss=0.0145, v_num=16, train_loss_step=0.0176, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 4:  75%|██▏| 50/67 [00:17<00:05,  2.88it/s, loss=0.0145, v_num=16, train_loss_step=0.0176, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 4:  76%|██▎| 51/67 [00:18<00:05,  2.82it/s, loss=0.0145, v_num=16, train_loss_step=0.0176, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 4:  78%|██▎| 52/67 [00:18<00:05,  2.78it/s, loss=0.0145, v_num=16, train_loss_step=0.0176, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 4:  79%|██▎| 53/67 [00:19<00:05,  2.74it/s, loss=0.0145, v_num=16, train_loss_step=0.0176, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 4:  81%|██▍| 54/67 [00:20<00:04,  2.70it/s, loss=0.0145, v_num=16, train_loss_step=0.0176, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 4:  82%|██▍| 55/67 [00:20<00:04,  2.66it/s, loss=0.0145, v_num=16, train_loss_step=0.0176, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 4:  84%|██▌| 56/67 [00:21<00:04,  2.63it/s, loss=0.0145, v_num=16, train_loss_step=0.0176, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 4:  85%|██▌| 57/67 [00:21<00:03,  2.59it/s, loss=0.0145, v_num=16, train_loss_step=0.0176, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 4:  87%|██▌| 58/67 [00:22<00:03,  2.55it/s, loss=0.0145, v_num=16, train_loss_step=0.0176, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 4:  88%|██▋| 59/67 [00:23<00:03,  2.53it/s, loss=0.0145, v_num=16, train_loss_step=0.0176, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 4:  90%|██▋| 60/67 [00:24<00:02,  2.50it/s, loss=0.0145, v_num=16, train_loss_step=0.0176, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 4:  91%|██▋| 61/67 [00:24<00:02,  2.48it/s, loss=0.0145, v_num=16, train_loss_step=0.0176, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 4:  93%|██▊| 62/67 [00:25<00:02,  2.45it/s, loss=0.0145, v_num=16, train_loss_step=0.0176, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 4:  94%|██▊| 63/67 [00:25<00:01,  2.43it/s, loss=0.0145, v_num=16, train_loss_step=0.0176, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 4:  96%|██▊| 64/67 [00:26<00:01,  2.41it/s, loss=0.0145, v_num=16, train_loss_step=0.0176, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 4:  97%|██▉| 65/67 [00:27<00:00,  2.39it/s, loss=0.0145, v_num=16, train_loss_step=0.0176, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 4:  99%|██▉| 66/67 [00:27<00:00,  2.36it/s, loss=0.0145, v_num=16, train_loss_step=0.0176, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 4: 100%|███| 67/67 [00:28<00:00,  2.33it/s, loss=0.0145, v_num=16, train_loss_step=0.0176, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 5:  45%|█▎ | 30/67 [00:04<00:05,  7.13it/s, loss=0.0145, v_num=16, train_loss_step=0.0108, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5:  46%|█▍ | 31/67 [00:04<00:05,  6.25it/s, loss=0.0145, v_num=16, train_loss_step=0.0108, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 5:  48%|█▍ | 32/67 [00:05<00:06,  5.69it/s, loss=0.0145, v_num=16, train_loss_step=0.0108, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 5:  49%|█▍ | 33/67 [00:06<00:06,  5.27it/s, loss=0.0145, v_num=16, train_loss_step=0.0108, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 5:  51%|█▌ | 34/67 [00:06<00:06,  4.91it/s, loss=0.0145, v_num=16, train_loss_step=0.0108, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 5:  52%|█▌ | 35/67 [00:07<00:07,  4.56it/s, loss=0.0145, v_num=16, train_loss_step=0.0108, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 5:  54%|█▌ | 36/67 [00:08<00:07,  4.33it/s, loss=0.0145, v_num=16, train_loss_step=0.0108, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 5:  55%|█▋ | 37/67 [00:08<00:07,  4.14it/s, loss=0.0145, v_num=16, train_loss_step=0.0108, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 5:  57%|█▋ | 38/67 [00:09<00:07,  3.97it/s, loss=0.0145, v_num=16, train_loss_step=0.0108, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 5:  58%|█▋ | 39/67 [00:10<00:07,  3.82it/s, loss=0.0145, v_num=16, train_loss_step=0.0108, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 5:  60%|█▊ | 40/67 [00:10<00:07,  3.69it/s, loss=0.0145, v_num=16, train_loss_step=0.0108, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 5:  61%|█▊ | 41/67 [00:11<00:07,  3.57it/s, loss=0.0145, v_num=16, train_loss_step=0.0108, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 5:  63%|█▉ | 42/67 [00:12<00:07,  3.47it/s, loss=0.0145, v_num=16, train_loss_step=0.0108, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 5:  64%|█▉ | 43/67 [00:12<00:07,  3.35it/s, loss=0.0145, v_num=16, train_loss_step=0.0108, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 5:  66%|█▉ | 44/67 [00:13<00:07,  3.28it/s, loss=0.0145, v_num=16, train_loss_step=0.0108, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 5:  67%|██ | 45/67 [00:14<00:06,  3.21it/s, loss=0.0145, v_num=16, train_loss_step=0.0108, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 5:  69%|██ | 46/67 [00:14<00:06,  3.14it/s, loss=0.0145, v_num=16, train_loss_step=0.0108, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 5:  70%|██ | 47/67 [00:15<00:06,  3.07it/s, loss=0.0145, v_num=16, train_loss_step=0.0108, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 5:  72%|██▏| 48/67 [00:15<00:06,  3.00it/s, loss=0.0145, v_num=16, train_loss_step=0.0108, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 5:  73%|██▏| 49/67 [00:16<00:06,  2.95it/s, loss=0.0145, v_num=16, train_loss_step=0.0108, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 5:  75%|██▏| 50/67 [00:17<00:05,  2.91it/s, loss=0.0145, v_num=16, train_loss_step=0.0108, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 5:  76%|██▎| 51/67 [00:17<00:05,  2.84it/s, loss=0.0145, v_num=16, train_loss_step=0.0108, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 5:  78%|██▎| 52/67 [00:18<00:05,  2.78it/s, loss=0.0145, v_num=16, train_loss_step=0.0108, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 5:  79%|██▎| 53/67 [00:19<00:05,  2.74it/s, loss=0.0145, v_num=16, train_loss_step=0.0108, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 5:  81%|██▍| 54/67 [00:19<00:04,  2.70it/s, loss=0.0145, v_num=16, train_loss_step=0.0108, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 5:  82%|██▍| 55/67 [00:20<00:04,  2.66it/s, loss=0.0145, v_num=16, train_loss_step=0.0108, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 5:  84%|██▌| 56/67 [00:21<00:04,  2.63it/s, loss=0.0145, v_num=16, train_loss_step=0.0108, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 5:  85%|██▌| 57/67 [00:21<00:03,  2.60it/s, loss=0.0145, v_num=16, train_loss_step=0.0108, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 5:  87%|██▌| 58/67 [00:22<00:03,  2.57it/s, loss=0.0145, v_num=16, train_loss_step=0.0108, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 5:  88%|██▋| 59/67 [00:23<00:03,  2.54it/s, loss=0.0145, v_num=16, train_loss_step=0.0108, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 5:  90%|██▋| 60/67 [00:23<00:02,  2.50it/s, loss=0.0145, v_num=16, train_loss_step=0.0108, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 5:  91%|██▋| 61/67 [00:24<00:02,  2.48it/s, loss=0.0145, v_num=16, train_loss_step=0.0108, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 5:  93%|██▊| 62/67 [00:25<00:02,  2.46it/s, loss=0.0145, v_num=16, train_loss_step=0.0108, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 5:  94%|██▊| 63/67 [00:25<00:01,  2.43it/s, loss=0.0145, v_num=16, train_loss_step=0.0108, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 5:  96%|██▊| 64/67 [00:26<00:01,  2.41it/s, loss=0.0145, v_num=16, train_loss_step=0.0108, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 5:  97%|██▉| 65/67 [00:27<00:00,  2.40it/s, loss=0.0145, v_num=16, train_loss_step=0.0108, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 5:  99%|██▉| 66/67 [00:27<00:00,  2.38it/s, loss=0.0145, v_num=16, train_loss_step=0.0108, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 5: 100%|███| 67/67 [00:28<00:00,  2.35it/s, loss=0.0145, v_num=16, train_loss_step=0.0108, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 6:  45%|▉ | 30/67 [00:04<00:05,  7.10it/s, loss=0.00712, v_num=16, train_loss_step=0.0119, val_loss=0.0106, train_loss_epoch=0.0128]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6:  46%|▉ | 31/67 [00:05<00:05,  6.08it/s, loss=0.00712, v_num=16, train_loss_step=0.0119, val_loss=0.0106, train_loss_epoch=0.0128]\u001b[A\n",
      "Epoch 6:  48%|▉ | 32/67 [00:05<00:06,  5.57it/s, loss=0.00712, v_num=16, train_loss_step=0.0119, val_loss=0.0106, train_loss_epoch=0.0128]\u001b[A\n",
      "Epoch 6:  49%|▉ | 33/67 [00:06<00:06,  5.17it/s, loss=0.00712, v_num=16, train_loss_step=0.0119, val_loss=0.0106, train_loss_epoch=0.0128]\u001b[A\n",
      "Epoch 6:  51%|█ | 34/67 [00:07<00:06,  4.84it/s, loss=0.00712, v_num=16, train_loss_step=0.0119, val_loss=0.0106, train_loss_epoch=0.0128]\u001b[A\n",
      "Epoch 6:  52%|█ | 35/67 [00:07<00:07,  4.57it/s, loss=0.00712, v_num=16, train_loss_step=0.0119, val_loss=0.0106, train_loss_epoch=0.0128]\u001b[A\n",
      "Epoch 6:  54%|█ | 36/67 [00:08<00:07,  4.33it/s, loss=0.00712, v_num=16, train_loss_step=0.0119, val_loss=0.0106, train_loss_epoch=0.0128]\u001b[A\n",
      "Epoch 6:  55%|█ | 37/67 [00:08<00:07,  4.12it/s, loss=0.00712, v_num=16, train_loss_step=0.0119, val_loss=0.0106, train_loss_epoch=0.0128]\u001b[A\n",
      "Epoch 6:  57%|█▏| 38/67 [00:09<00:07,  3.95it/s, loss=0.00712, v_num=16, train_loss_step=0.0119, val_loss=0.0106, train_loss_epoch=0.0128]\u001b[A\n",
      "Epoch 6:  58%|█▏| 39/67 [00:10<00:07,  3.76it/s, loss=0.00712, v_num=16, train_loss_step=0.0119, val_loss=0.0106, train_loss_epoch=0.0128]\u001b[A\n",
      "Epoch 6:  60%|█▏| 40/67 [00:10<00:07,  3.64it/s, loss=0.00712, v_num=16, train_loss_step=0.0119, val_loss=0.0106, train_loss_epoch=0.0128]\u001b[A\n",
      "Epoch 6:  61%|█▏| 41/67 [00:11<00:07,  3.53it/s, loss=0.00712, v_num=16, train_loss_step=0.0119, val_loss=0.0106, train_loss_epoch=0.0128]\u001b[A\n",
      "Epoch 6:  63%|█▎| 42/67 [00:12<00:07,  3.43it/s, loss=0.00712, v_num=16, train_loss_step=0.0119, val_loss=0.0106, train_loss_epoch=0.0128]\u001b[A\n",
      "Epoch 6:  64%|█▎| 43/67 [00:12<00:07,  3.35it/s, loss=0.00712, v_num=16, train_loss_step=0.0119, val_loss=0.0106, train_loss_epoch=0.0128]\u001b[A\n",
      "Epoch 6:  66%|█▎| 44/67 [00:13<00:07,  3.27it/s, loss=0.00712, v_num=16, train_loss_step=0.0119, val_loss=0.0106, train_loss_epoch=0.0128]\u001b[A\n",
      "Epoch 6:  67%|█▎| 45/67 [00:14<00:06,  3.20it/s, loss=0.00712, v_num=16, train_loss_step=0.0119, val_loss=0.0106, train_loss_epoch=0.0128]\u001b[A\n",
      "Epoch 6:  69%|█▎| 46/67 [00:14<00:06,  3.13it/s, loss=0.00712, v_num=16, train_loss_step=0.0119, val_loss=0.0106, train_loss_epoch=0.0128]\u001b[A\n",
      "Epoch 6:  70%|█▍| 47/67 [00:15<00:06,  3.07it/s, loss=0.00712, v_num=16, train_loss_step=0.0119, val_loss=0.0106, train_loss_epoch=0.0128]\u001b[A\n",
      "Epoch 6:  72%|█▍| 48/67 [00:16<00:06,  2.98it/s, loss=0.00712, v_num=16, train_loss_step=0.0119, val_loss=0.0106, train_loss_epoch=0.0128]\u001b[A\n",
      "Epoch 6:  73%|█▍| 49/67 [00:16<00:06,  2.92it/s, loss=0.00712, v_num=16, train_loss_step=0.0119, val_loss=0.0106, train_loss_epoch=0.0128]\u001b[A\n",
      "Epoch 6:  75%|█▍| 50/67 [00:17<00:05,  2.88it/s, loss=0.00712, v_num=16, train_loss_step=0.0119, val_loss=0.0106, train_loss_epoch=0.0128]\u001b[A\n",
      "Epoch 6:  76%|█▌| 51/67 [00:18<00:05,  2.82it/s, loss=0.00712, v_num=16, train_loss_step=0.0119, val_loss=0.0106, train_loss_epoch=0.0128]\u001b[A\n",
      "Epoch 6:  78%|█▌| 52/67 [00:18<00:05,  2.77it/s, loss=0.00712, v_num=16, train_loss_step=0.0119, val_loss=0.0106, train_loss_epoch=0.0128]\u001b[A\n",
      "Epoch 6:  79%|█▌| 53/67 [00:19<00:05,  2.74it/s, loss=0.00712, v_num=16, train_loss_step=0.0119, val_loss=0.0106, train_loss_epoch=0.0128]\u001b[A\n",
      "Epoch 6:  81%|█▌| 54/67 [00:20<00:04,  2.70it/s, loss=0.00712, v_num=16, train_loss_step=0.0119, val_loss=0.0106, train_loss_epoch=0.0128]\u001b[A\n",
      "Epoch 6:  82%|█▋| 55/67 [00:20<00:04,  2.66it/s, loss=0.00712, v_num=16, train_loss_step=0.0119, val_loss=0.0106, train_loss_epoch=0.0128]\u001b[A\n",
      "Epoch 6:  84%|█▋| 56/67 [00:21<00:04,  2.61it/s, loss=0.00712, v_num=16, train_loss_step=0.0119, val_loss=0.0106, train_loss_epoch=0.0128]\u001b[A\n",
      "Epoch 6:  85%|█▋| 57/67 [00:22<00:03,  2.58it/s, loss=0.00712, v_num=16, train_loss_step=0.0119, val_loss=0.0106, train_loss_epoch=0.0128]\u001b[A\n",
      "Epoch 6:  87%|█▋| 58/67 [00:22<00:03,  2.55it/s, loss=0.00712, v_num=16, train_loss_step=0.0119, val_loss=0.0106, train_loss_epoch=0.0128]\u001b[A\n",
      "Epoch 6:  88%|█▊| 59/67 [00:23<00:03,  2.52it/s, loss=0.00712, v_num=16, train_loss_step=0.0119, val_loss=0.0106, train_loss_epoch=0.0128]\u001b[A\n",
      "Epoch 6:  90%|█▊| 60/67 [00:24<00:02,  2.50it/s, loss=0.00712, v_num=16, train_loss_step=0.0119, val_loss=0.0106, train_loss_epoch=0.0128]\u001b[A\n",
      "Epoch 6:  91%|█▊| 61/67 [00:24<00:02,  2.48it/s, loss=0.00712, v_num=16, train_loss_step=0.0119, val_loss=0.0106, train_loss_epoch=0.0128]\u001b[A\n",
      "Epoch 6:  93%|█▊| 62/67 [00:25<00:02,  2.45it/s, loss=0.00712, v_num=16, train_loss_step=0.0119, val_loss=0.0106, train_loss_epoch=0.0128]\u001b[A\n",
      "Epoch 6:  94%|█▉| 63/67 [00:25<00:01,  2.43it/s, loss=0.00712, v_num=16, train_loss_step=0.0119, val_loss=0.0106, train_loss_epoch=0.0128]\u001b[A\n",
      "Epoch 6:  96%|█▉| 64/67 [00:26<00:01,  2.41it/s, loss=0.00712, v_num=16, train_loss_step=0.0119, val_loss=0.0106, train_loss_epoch=0.0128]\u001b[A\n",
      "Epoch 6:  97%|█▉| 65/67 [00:27<00:00,  2.38it/s, loss=0.00712, v_num=16, train_loss_step=0.0119, val_loss=0.0106, train_loss_epoch=0.0128]\u001b[A\n",
      "Epoch 6:  99%|█▉| 66/67 [00:27<00:00,  2.36it/s, loss=0.00712, v_num=16, train_loss_step=0.0119, val_loss=0.0106, train_loss_epoch=0.0128]\u001b[A\n",
      "Epoch 6: 100%|██| 67/67 [00:28<00:00,  2.33it/s, loss=0.00712, v_num=16, train_loss_step=0.0119, val_loss=0.0106, train_loss_epoch=0.0128]\u001b[A\n",
      "Epoch 7:  45%|▉ | 30/67 [00:04<00:05,  7.09it/s, loss=0.0138, v_num=16, train_loss_step=0.0188, val_loss=0.0106, train_loss_epoch=0.00784]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7:  46%|▉ | 31/67 [00:04<00:05,  6.21it/s, loss=0.0138, v_num=16, train_loss_step=0.0188, val_loss=0.0106, train_loss_epoch=0.00784]\u001b[A\n",
      "Epoch 7:  48%|▉ | 32/67 [00:05<00:06,  5.67it/s, loss=0.0138, v_num=16, train_loss_step=0.0188, val_loss=0.0106, train_loss_epoch=0.00784]\u001b[A\n",
      "Epoch 7:  49%|▉ | 33/67 [00:06<00:06,  5.25it/s, loss=0.0138, v_num=16, train_loss_step=0.0188, val_loss=0.0106, train_loss_epoch=0.00784]\u001b[A\n",
      "Epoch 7:  51%|█ | 34/67 [00:06<00:06,  4.91it/s, loss=0.0138, v_num=16, train_loss_step=0.0188, val_loss=0.0106, train_loss_epoch=0.00784]\u001b[A\n",
      "Epoch 7:  52%|█ | 35/67 [00:07<00:07,  4.55it/s, loss=0.0138, v_num=16, train_loss_step=0.0188, val_loss=0.0106, train_loss_epoch=0.00784]\u001b[A\n",
      "Epoch 7:  54%|█ | 36/67 [00:08<00:07,  4.32it/s, loss=0.0138, v_num=16, train_loss_step=0.0188, val_loss=0.0106, train_loss_epoch=0.00784]\u001b[A\n",
      "Epoch 7:  55%|█ | 37/67 [00:09<00:07,  4.11it/s, loss=0.0138, v_num=16, train_loss_step=0.0188, val_loss=0.0106, train_loss_epoch=0.00784]\u001b[A\n",
      "Epoch 7:  57%|█▏| 38/67 [00:09<00:07,  3.95it/s, loss=0.0138, v_num=16, train_loss_step=0.0188, val_loss=0.0106, train_loss_epoch=0.00784]\u001b[A\n",
      "Epoch 7:  58%|█▏| 39/67 [00:10<00:07,  3.81it/s, loss=0.0138, v_num=16, train_loss_step=0.0188, val_loss=0.0106, train_loss_epoch=0.00784]\u001b[A\n",
      "Epoch 7:  60%|█▏| 40/67 [00:10<00:07,  3.68it/s, loss=0.0138, v_num=16, train_loss_step=0.0188, val_loss=0.0106, train_loss_epoch=0.00784]\u001b[A\n",
      "Epoch 7:  61%|█▏| 41/67 [00:11<00:07,  3.57it/s, loss=0.0138, v_num=16, train_loss_step=0.0188, val_loss=0.0106, train_loss_epoch=0.00784]\u001b[A\n",
      "Epoch 7:  63%|█▎| 42/67 [00:12<00:07,  3.48it/s, loss=0.0138, v_num=16, train_loss_step=0.0188, val_loss=0.0106, train_loss_epoch=0.00784]\u001b[A\n",
      "Epoch 7:  64%|█▎| 43/67 [00:12<00:07,  3.39it/s, loss=0.0138, v_num=16, train_loss_step=0.0188, val_loss=0.0106, train_loss_epoch=0.00784]\u001b[A\n",
      "Epoch 7:  66%|█▎| 44/67 [00:13<00:07,  3.28it/s, loss=0.0138, v_num=16, train_loss_step=0.0188, val_loss=0.0106, train_loss_epoch=0.00784]\u001b[A\n",
      "Epoch 7:  67%|█▎| 45/67 [00:14<00:06,  3.21it/s, loss=0.0138, v_num=16, train_loss_step=0.0188, val_loss=0.0106, train_loss_epoch=0.00784]\u001b[A\n",
      "Epoch 7:  69%|█▎| 46/67 [00:14<00:06,  3.14it/s, loss=0.0138, v_num=16, train_loss_step=0.0188, val_loss=0.0106, train_loss_epoch=0.00784]\u001b[A\n",
      "Epoch 7:  70%|█▍| 47/67 [00:15<00:06,  3.08it/s, loss=0.0138, v_num=16, train_loss_step=0.0188, val_loss=0.0106, train_loss_epoch=0.00784]\u001b[A\n",
      "Epoch 7:  72%|█▍| 48/67 [00:15<00:06,  3.01it/s, loss=0.0138, v_num=16, train_loss_step=0.0188, val_loss=0.0106, train_loss_epoch=0.00784]\u001b[A\n",
      "Epoch 7:  73%|█▍| 49/67 [00:16<00:06,  2.96it/s, loss=0.0138, v_num=16, train_loss_step=0.0188, val_loss=0.0106, train_loss_epoch=0.00784]\u001b[A\n",
      "Epoch 7:  75%|█▍| 50/67 [00:17<00:05,  2.91it/s, loss=0.0138, v_num=16, train_loss_step=0.0188, val_loss=0.0106, train_loss_epoch=0.00784]\u001b[A\n",
      "Epoch 7:  76%|█▌| 51/67 [00:17<00:05,  2.85it/s, loss=0.0138, v_num=16, train_loss_step=0.0188, val_loss=0.0106, train_loss_epoch=0.00784]\u001b[A\n",
      "Epoch 7:  78%|█▌| 52/67 [00:18<00:05,  2.80it/s, loss=0.0138, v_num=16, train_loss_step=0.0188, val_loss=0.0106, train_loss_epoch=0.00784]\u001b[A\n",
      "Epoch 7:  79%|█▌| 53/67 [00:19<00:05,  2.76it/s, loss=0.0138, v_num=16, train_loss_step=0.0188, val_loss=0.0106, train_loss_epoch=0.00784]\u001b[A\n",
      "Epoch 7:  81%|█▌| 54/67 [00:19<00:04,  2.70it/s, loss=0.0138, v_num=16, train_loss_step=0.0188, val_loss=0.0106, train_loss_epoch=0.00784]\u001b[A\n",
      "Epoch 7:  82%|█▋| 55/67 [00:20<00:04,  2.67it/s, loss=0.0138, v_num=16, train_loss_step=0.0188, val_loss=0.0106, train_loss_epoch=0.00784]\u001b[A\n",
      "Epoch 7:  84%|█▋| 56/67 [00:21<00:04,  2.63it/s, loss=0.0138, v_num=16, train_loss_step=0.0188, val_loss=0.0106, train_loss_epoch=0.00784]\u001b[A\n",
      "Epoch 7:  85%|█▋| 57/67 [00:21<00:03,  2.60it/s, loss=0.0138, v_num=16, train_loss_step=0.0188, val_loss=0.0106, train_loss_epoch=0.00784]\u001b[A\n",
      "Epoch 7:  87%|█▋| 58/67 [00:22<00:03,  2.57it/s, loss=0.0138, v_num=16, train_loss_step=0.0188, val_loss=0.0106, train_loss_epoch=0.00784]\u001b[A\n",
      "Epoch 7:  88%|█▊| 59/67 [00:23<00:03,  2.54it/s, loss=0.0138, v_num=16, train_loss_step=0.0188, val_loss=0.0106, train_loss_epoch=0.00784]\u001b[A\n",
      "Epoch 7:  90%|█▊| 60/67 [00:23<00:02,  2.52it/s, loss=0.0138, v_num=16, train_loss_step=0.0188, val_loss=0.0106, train_loss_epoch=0.00784]\u001b[A\n",
      "Epoch 7:  91%|█▊| 61/67 [00:24<00:02,  2.49it/s, loss=0.0138, v_num=16, train_loss_step=0.0188, val_loss=0.0106, train_loss_epoch=0.00784]\u001b[A\n",
      "Epoch 7:  93%|█▊| 62/67 [00:25<00:02,  2.47it/s, loss=0.0138, v_num=16, train_loss_step=0.0188, val_loss=0.0106, train_loss_epoch=0.00784]\u001b[A\n",
      "Epoch 7:  94%|█▉| 63/67 [00:25<00:01,  2.43it/s, loss=0.0138, v_num=16, train_loss_step=0.0188, val_loss=0.0106, train_loss_epoch=0.00784]\u001b[A\n",
      "Epoch 7:  96%|█▉| 64/67 [00:26<00:01,  2.41it/s, loss=0.0138, v_num=16, train_loss_step=0.0188, val_loss=0.0106, train_loss_epoch=0.00784]\u001b[A\n",
      "Epoch 7:  97%|█▉| 65/67 [00:27<00:00,  2.39it/s, loss=0.0138, v_num=16, train_loss_step=0.0188, val_loss=0.0106, train_loss_epoch=0.00784]\u001b[A\n",
      "Epoch 7:  99%|█▉| 66/67 [00:27<00:00,  2.37it/s, loss=0.0138, v_num=16, train_loss_step=0.0188, val_loss=0.0106, train_loss_epoch=0.00784]\u001b[A\n",
      "Epoch 7: 100%|██| 67/67 [00:28<00:00,  2.33it/s, loss=0.0138, v_num=16, train_loss_step=0.0188, val_loss=0.0106, train_loss_epoch=0.00784]\u001b[A\n",
      "Epoch 8:  45%|▉ | 30/67 [00:04<00:05,  6.90it/s, loss=0.0171, v_num=16, train_loss_step=0.00484, val_loss=0.0106, train_loss_epoch=0.0152]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8:  46%|▉ | 31/67 [00:05<00:05,  6.10it/s, loss=0.0171, v_num=16, train_loss_step=0.00484, val_loss=0.0106, train_loss_epoch=0.0152]\u001b[A\n",
      "Epoch 8:  48%|▉ | 32/67 [00:05<00:06,  5.59it/s, loss=0.0171, v_num=16, train_loss_step=0.00484, val_loss=0.0106, train_loss_epoch=0.0152]\u001b[A\n",
      "Epoch 8:  49%|▉ | 33/67 [00:06<00:06,  5.21it/s, loss=0.0171, v_num=16, train_loss_step=0.00484, val_loss=0.0106, train_loss_epoch=0.0152]\u001b[A\n",
      "Epoch 8:  51%|█ | 34/67 [00:07<00:06,  4.78it/s, loss=0.0171, v_num=16, train_loss_step=0.00484, val_loss=0.0106, train_loss_epoch=0.0152]\u001b[A\n",
      "Epoch 8:  52%|█ | 35/67 [00:07<00:07,  4.52it/s, loss=0.0171, v_num=16, train_loss_step=0.00484, val_loss=0.0106, train_loss_epoch=0.0152]\u001b[A\n",
      "Epoch 8:  54%|█ | 36/67 [00:08<00:07,  4.30it/s, loss=0.0171, v_num=16, train_loss_step=0.00484, val_loss=0.0106, train_loss_epoch=0.0152]\u001b[A\n",
      "Epoch 8:  55%|█ | 37/67 [00:09<00:07,  4.09it/s, loss=0.0171, v_num=16, train_loss_step=0.00484, val_loss=0.0106, train_loss_epoch=0.0152]\u001b[A\n",
      "Epoch 8:  57%|█▏| 38/67 [00:09<00:07,  3.92it/s, loss=0.0171, v_num=16, train_loss_step=0.00484, val_loss=0.0106, train_loss_epoch=0.0152]\u001b[A\n",
      "Epoch 8:  58%|█▏| 39/67 [00:10<00:07,  3.77it/s, loss=0.0171, v_num=16, train_loss_step=0.00484, val_loss=0.0106, train_loss_epoch=0.0152]\u001b[A\n",
      "Epoch 8:  60%|█▏| 40/67 [00:10<00:07,  3.65it/s, loss=0.0171, v_num=16, train_loss_step=0.00484, val_loss=0.0106, train_loss_epoch=0.0152]\u001b[A\n",
      "Epoch 8:  61%|█▏| 41/67 [00:11<00:07,  3.54it/s, loss=0.0171, v_num=16, train_loss_step=0.00484, val_loss=0.0106, train_loss_epoch=0.0152]\u001b[A\n",
      "Epoch 8:  63%|█▎| 42/67 [00:12<00:07,  3.43it/s, loss=0.0171, v_num=16, train_loss_step=0.00484, val_loss=0.0106, train_loss_epoch=0.0152]\u001b[A\n",
      "Epoch 8:  64%|█▎| 43/67 [00:12<00:07,  3.35it/s, loss=0.0171, v_num=16, train_loss_step=0.00484, val_loss=0.0106, train_loss_epoch=0.0152]\u001b[A\n",
      "Epoch 8:  66%|█▎| 44/67 [00:13<00:07,  3.23it/s, loss=0.0171, v_num=16, train_loss_step=0.00484, val_loss=0.0106, train_loss_epoch=0.0152]\u001b[A\n",
      "Epoch 8:  67%|█▎| 45/67 [00:14<00:06,  3.16it/s, loss=0.0171, v_num=16, train_loss_step=0.00484, val_loss=0.0106, train_loss_epoch=0.0152]\u001b[A\n",
      "Epoch 8:  69%|█▎| 46/67 [00:14<00:06,  3.08it/s, loss=0.0171, v_num=16, train_loss_step=0.00484, val_loss=0.0106, train_loss_epoch=0.0152]\u001b[A\n",
      "Epoch 8:  70%|█▍| 47/67 [00:15<00:06,  3.02it/s, loss=0.0171, v_num=16, train_loss_step=0.00484, val_loss=0.0106, train_loss_epoch=0.0152]\u001b[A\n",
      "Epoch 8:  72%|█▍| 48/67 [00:16<00:06,  2.96it/s, loss=0.0171, v_num=16, train_loss_step=0.00484, val_loss=0.0106, train_loss_epoch=0.0152]\u001b[A\n",
      "Epoch 8:  73%|█▍| 49/67 [00:16<00:06,  2.91it/s, loss=0.0171, v_num=16, train_loss_step=0.00484, val_loss=0.0106, train_loss_epoch=0.0152]\u001b[A\n",
      "Epoch 8:  75%|█▍| 50/67 [00:17<00:05,  2.86it/s, loss=0.0171, v_num=16, train_loss_step=0.00484, val_loss=0.0106, train_loss_epoch=0.0152]\u001b[A\n",
      "Epoch 8:  76%|█▌| 51/67 [00:18<00:05,  2.80it/s, loss=0.0171, v_num=16, train_loss_step=0.00484, val_loss=0.0106, train_loss_epoch=0.0152]\u001b[A\n",
      "Epoch 8:  78%|█▌| 52/67 [00:18<00:05,  2.76it/s, loss=0.0171, v_num=16, train_loss_step=0.00484, val_loss=0.0106, train_loss_epoch=0.0152]\u001b[A\n",
      "Epoch 8:  79%|█▌| 53/67 [00:19<00:05,  2.70it/s, loss=0.0171, v_num=16, train_loss_step=0.00484, val_loss=0.0106, train_loss_epoch=0.0152]\u001b[A\n",
      "Epoch 8:  81%|█▌| 54/67 [00:20<00:04,  2.66it/s, loss=0.0171, v_num=16, train_loss_step=0.00484, val_loss=0.0106, train_loss_epoch=0.0152]\u001b[A\n",
      "Epoch 8:  82%|█▋| 55/67 [00:20<00:04,  2.63it/s, loss=0.0171, v_num=16, train_loss_step=0.00484, val_loss=0.0106, train_loss_epoch=0.0152]\u001b[A\n",
      "Epoch 8:  84%|█▋| 56/67 [00:21<00:04,  2.60it/s, loss=0.0171, v_num=16, train_loss_step=0.00484, val_loss=0.0106, train_loss_epoch=0.0152]\u001b[A\n",
      "Epoch 8:  85%|█▋| 57/67 [00:22<00:03,  2.57it/s, loss=0.0171, v_num=16, train_loss_step=0.00484, val_loss=0.0106, train_loss_epoch=0.0152]\u001b[A\n",
      "Epoch 8:  87%|█▋| 58/67 [00:22<00:03,  2.54it/s, loss=0.0171, v_num=16, train_loss_step=0.00484, val_loss=0.0106, train_loss_epoch=0.0152]\u001b[A\n",
      "Epoch 8:  88%|█▊| 59/67 [00:23<00:03,  2.52it/s, loss=0.0171, v_num=16, train_loss_step=0.00484, val_loss=0.0106, train_loss_epoch=0.0152]\u001b[A\n",
      "Epoch 8:  90%|█▊| 60/67 [00:24<00:02,  2.49it/s, loss=0.0171, v_num=16, train_loss_step=0.00484, val_loss=0.0106, train_loss_epoch=0.0152]\u001b[A\n",
      "Epoch 8:  91%|█▊| 61/67 [00:24<00:02,  2.46it/s, loss=0.0171, v_num=16, train_loss_step=0.00484, val_loss=0.0106, train_loss_epoch=0.0152]\u001b[A\n",
      "Epoch 8:  93%|█▊| 62/67 [00:25<00:02,  2.44it/s, loss=0.0171, v_num=16, train_loss_step=0.00484, val_loss=0.0106, train_loss_epoch=0.0152]\u001b[A\n",
      "Epoch 8:  94%|█▉| 63/67 [00:26<00:01,  2.41it/s, loss=0.0171, v_num=16, train_loss_step=0.00484, val_loss=0.0106, train_loss_epoch=0.0152]\u001b[A\n",
      "Epoch 8:  96%|█▉| 64/67 [00:26<00:01,  2.38it/s, loss=0.0171, v_num=16, train_loss_step=0.00484, val_loss=0.0106, train_loss_epoch=0.0152]\u001b[A\n",
      "Epoch 8:  97%|█▉| 65/67 [00:27<00:00,  2.37it/s, loss=0.0171, v_num=16, train_loss_step=0.00484, val_loss=0.0106, train_loss_epoch=0.0152]\u001b[A\n",
      "Epoch 8:  99%|█▉| 66/67 [00:28<00:00,  2.35it/s, loss=0.0171, v_num=16, train_loss_step=0.00484, val_loss=0.0106, train_loss_epoch=0.0152]\u001b[A\n",
      "Epoch 8: 100%|██| 67/67 [00:28<00:00,  2.32it/s, loss=0.0171, v_num=16, train_loss_step=0.00484, val_loss=0.0106, train_loss_epoch=0.0152]\u001b[A\n",
      "Epoch 9:  45%|█▊  | 30/67 [00:04<00:05,  7.14it/s, loss=0.0158, v_num=16, train_loss_step=0.0734, val_loss=0.0106, train_loss_epoch=0.015]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9:  46%|█▊  | 31/67 [00:04<00:05,  6.27it/s, loss=0.0158, v_num=16, train_loss_step=0.0734, val_loss=0.0106, train_loss_epoch=0.015]\u001b[A\n",
      "Epoch 9:  48%|█▉  | 32/67 [00:05<00:06,  5.75it/s, loss=0.0158, v_num=16, train_loss_step=0.0734, val_loss=0.0106, train_loss_epoch=0.015]\u001b[A\n",
      "Epoch 9:  49%|█▉  | 33/67 [00:06<00:06,  5.34it/s, loss=0.0158, v_num=16, train_loss_step=0.0734, val_loss=0.0106, train_loss_epoch=0.015]\u001b[A\n",
      "Epoch 9:  51%|██  | 34/67 [00:06<00:06,  4.89it/s, loss=0.0158, v_num=16, train_loss_step=0.0734, val_loss=0.0106, train_loss_epoch=0.015]\u001b[A\n",
      "Epoch 9:  52%|██  | 35/67 [00:07<00:06,  4.62it/s, loss=0.0158, v_num=16, train_loss_step=0.0734, val_loss=0.0106, train_loss_epoch=0.015]\u001b[A\n",
      "Epoch 9:  54%|██▏ | 36/67 [00:08<00:07,  4.39it/s, loss=0.0158, v_num=16, train_loss_step=0.0734, val_loss=0.0106, train_loss_epoch=0.015]\u001b[A\n",
      "Epoch 9:  55%|██▏ | 37/67 [00:08<00:07,  4.19it/s, loss=0.0158, v_num=16, train_loss_step=0.0734, val_loss=0.0106, train_loss_epoch=0.015]\u001b[A\n",
      "Epoch 9:  57%|██▎ | 38/67 [00:09<00:07,  4.02it/s, loss=0.0158, v_num=16, train_loss_step=0.0734, val_loss=0.0106, train_loss_epoch=0.015]\u001b[A\n",
      "Epoch 9:  58%|██▎ | 39/67 [00:10<00:07,  3.88it/s, loss=0.0158, v_num=16, train_loss_step=0.0734, val_loss=0.0106, train_loss_epoch=0.015]\u001b[A\n",
      "Epoch 9:  60%|██▍ | 40/67 [00:10<00:07,  3.75it/s, loss=0.0158, v_num=16, train_loss_step=0.0734, val_loss=0.0106, train_loss_epoch=0.015]\u001b[A\n",
      "Epoch 9:  61%|██▍ | 41/67 [00:11<00:07,  3.63it/s, loss=0.0158, v_num=16, train_loss_step=0.0734, val_loss=0.0106, train_loss_epoch=0.015]\u001b[A\n",
      "Epoch 9:  63%|██▌ | 42/67 [00:11<00:07,  3.53it/s, loss=0.0158, v_num=16, train_loss_step=0.0734, val_loss=0.0106, train_loss_epoch=0.015]\u001b[A\n",
      "Epoch 9:  64%|██▌ | 43/67 [00:12<00:06,  3.44it/s, loss=0.0158, v_num=16, train_loss_step=0.0734, val_loss=0.0106, train_loss_epoch=0.015]\u001b[A\n",
      "Epoch 9:  66%|██▋ | 44/67 [00:13<00:06,  3.33it/s, loss=0.0158, v_num=16, train_loss_step=0.0734, val_loss=0.0106, train_loss_epoch=0.015]\u001b[A\n",
      "Epoch 9:  67%|██▋ | 45/67 [00:13<00:06,  3.26it/s, loss=0.0158, v_num=16, train_loss_step=0.0734, val_loss=0.0106, train_loss_epoch=0.015]\u001b[A\n",
      "Epoch 9:  69%|██▋ | 46/67 [00:14<00:06,  3.19it/s, loss=0.0158, v_num=16, train_loss_step=0.0734, val_loss=0.0106, train_loss_epoch=0.015]\u001b[A\n",
      "Epoch 9:  70%|██▊ | 47/67 [00:15<00:06,  3.13it/s, loss=0.0158, v_num=16, train_loss_step=0.0734, val_loss=0.0106, train_loss_epoch=0.015]\u001b[A\n",
      "Epoch 9:  72%|██▊ | 48/67 [00:15<00:06,  3.06it/s, loss=0.0158, v_num=16, train_loss_step=0.0734, val_loss=0.0106, train_loss_epoch=0.015]\u001b[A\n",
      "Epoch 9:  73%|██▉ | 49/67 [00:16<00:05,  3.01it/s, loss=0.0158, v_num=16, train_loss_step=0.0734, val_loss=0.0106, train_loss_epoch=0.015]\u001b[A\n",
      "Epoch 9:  75%|██▉ | 50/67 [00:16<00:05,  2.96it/s, loss=0.0158, v_num=16, train_loss_step=0.0734, val_loss=0.0106, train_loss_epoch=0.015]\u001b[A\n",
      "Epoch 9:  76%|███ | 51/67 [00:17<00:05,  2.90it/s, loss=0.0158, v_num=16, train_loss_step=0.0734, val_loss=0.0106, train_loss_epoch=0.015]\u001b[A\n",
      "Epoch 9:  78%|███ | 52/67 [00:18<00:05,  2.85it/s, loss=0.0158, v_num=16, train_loss_step=0.0734, val_loss=0.0106, train_loss_epoch=0.015]\u001b[A\n",
      "Epoch 9:  79%|███▏| 53/67 [00:18<00:04,  2.81it/s, loss=0.0158, v_num=16, train_loss_step=0.0734, val_loss=0.0106, train_loss_epoch=0.015]\u001b[A\n",
      "Epoch 9:  81%|███▏| 54/67 [00:19<00:04,  2.75it/s, loss=0.0158, v_num=16, train_loss_step=0.0734, val_loss=0.0106, train_loss_epoch=0.015]\u001b[A\n",
      "Epoch 9:  82%|███▎| 55/67 [00:20<00:04,  2.71it/s, loss=0.0158, v_num=16, train_loss_step=0.0734, val_loss=0.0106, train_loss_epoch=0.015]\u001b[A\n",
      "Epoch 9:  84%|███▎| 56/67 [00:20<00:04,  2.68it/s, loss=0.0158, v_num=16, train_loss_step=0.0734, val_loss=0.0106, train_loss_epoch=0.015]\u001b[A\n",
      "Epoch 9:  85%|███▍| 57/67 [00:21<00:03,  2.64it/s, loss=0.0158, v_num=16, train_loss_step=0.0734, val_loss=0.0106, train_loss_epoch=0.015]\u001b[A\n",
      "Epoch 9:  87%|███▍| 58/67 [00:22<00:03,  2.61it/s, loss=0.0158, v_num=16, train_loss_step=0.0734, val_loss=0.0106, train_loss_epoch=0.015]\u001b[A\n",
      "Epoch 9:  88%|███▌| 59/67 [00:22<00:03,  2.58it/s, loss=0.0158, v_num=16, train_loss_step=0.0734, val_loss=0.0106, train_loss_epoch=0.015]\u001b[A\n",
      "Epoch 9:  90%|███▌| 60/67 [00:23<00:02,  2.55it/s, loss=0.0158, v_num=16, train_loss_step=0.0734, val_loss=0.0106, train_loss_epoch=0.015]\u001b[A\n",
      "Epoch 9:  91%|███▋| 61/67 [00:24<00:02,  2.53it/s, loss=0.0158, v_num=16, train_loss_step=0.0734, val_loss=0.0106, train_loss_epoch=0.015]\u001b[A\n",
      "Epoch 9:  93%|███▋| 62/67 [00:24<00:01,  2.50it/s, loss=0.0158, v_num=16, train_loss_step=0.0734, val_loss=0.0106, train_loss_epoch=0.015]\u001b[A\n",
      "Epoch 9:  94%|███▊| 63/67 [00:25<00:01,  2.48it/s, loss=0.0158, v_num=16, train_loss_step=0.0734, val_loss=0.0106, train_loss_epoch=0.015]\u001b[A\n",
      "Epoch 9:  96%|███▊| 64/67 [00:26<00:01,  2.44it/s, loss=0.0158, v_num=16, train_loss_step=0.0734, val_loss=0.0106, train_loss_epoch=0.015]\u001b[A\n",
      "Epoch 9:  97%|███▉| 65/67 [00:26<00:00,  2.42it/s, loss=0.0158, v_num=16, train_loss_step=0.0734, val_loss=0.0106, train_loss_epoch=0.015]\u001b[A\n",
      "Epoch 9:  99%|███▉| 66/67 [00:27<00:00,  2.40it/s, loss=0.0158, v_num=16, train_loss_step=0.0734, val_loss=0.0106, train_loss_epoch=0.015]\u001b[A\n",
      "Epoch 9: 100%|████| 67/67 [00:28<00:00,  2.37it/s, loss=0.0158, v_num=16, train_loss_step=0.0734, val_loss=0.0106, train_loss_epoch=0.015]\u001b[A\n",
      "Epoch 10:  45%|▍| 30/67 [00:04<00:05,  7.02it/s, loss=0.00816, v_num=16, train_loss_step=0.0056, val_loss=0.0106, train_loss_epoch=0.0147]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10:  46%|▍| 31/67 [00:04<00:05,  6.20it/s, loss=0.00816, v_num=16, train_loss_step=0.0056, val_loss=0.0106, train_loss_epoch=0.0147]\u001b[A\n",
      "Epoch 10:  48%|▍| 32/67 [00:05<00:06,  5.67it/s, loss=0.00816, v_num=16, train_loss_step=0.0056, val_loss=0.0106, train_loss_epoch=0.0147]\u001b[A\n",
      "Epoch 10:  49%|▍| 33/67 [00:06<00:06,  5.27it/s, loss=0.00816, v_num=16, train_loss_step=0.0056, val_loss=0.0106, train_loss_epoch=0.0147]\u001b[A\n",
      "Epoch 10:  51%|▌| 34/67 [00:06<00:06,  4.94it/s, loss=0.00816, v_num=16, train_loss_step=0.0056, val_loss=0.0106, train_loss_epoch=0.0147]\u001b[A\n",
      "Epoch 10:  52%|▌| 35/67 [00:07<00:06,  4.66it/s, loss=0.00816, v_num=16, train_loss_step=0.0056, val_loss=0.0106, train_loss_epoch=0.0147]\u001b[A\n",
      "Epoch 10:  54%|▌| 36/67 [00:08<00:07,  4.34it/s, loss=0.00816, v_num=16, train_loss_step=0.0056, val_loss=0.0106, train_loss_epoch=0.0147]\u001b[A\n",
      "Epoch 10:  55%|▌| 37/67 [00:08<00:07,  4.14it/s, loss=0.00816, v_num=16, train_loss_step=0.0056, val_loss=0.0106, train_loss_epoch=0.0147]\u001b[A\n",
      "Epoch 10:  57%|▌| 38/67 [00:09<00:07,  3.97it/s, loss=0.00816, v_num=16, train_loss_step=0.0056, val_loss=0.0106, train_loss_epoch=0.0147]\u001b[A\n",
      "Epoch 10:  58%|▌| 39/67 [00:10<00:07,  3.82it/s, loss=0.00816, v_num=16, train_loss_step=0.0056, val_loss=0.0106, train_loss_epoch=0.0147]\u001b[A\n",
      "Epoch 10:  60%|▌| 40/67 [00:10<00:07,  3.69it/s, loss=0.00816, v_num=16, train_loss_step=0.0056, val_loss=0.0106, train_loss_epoch=0.0147]\u001b[A\n",
      "Epoch 10:  61%|▌| 41/67 [00:11<00:07,  3.57it/s, loss=0.00816, v_num=16, train_loss_step=0.0056, val_loss=0.0106, train_loss_epoch=0.0147]\u001b[A\n",
      "Epoch 10:  63%|▋| 42/67 [00:12<00:07,  3.46it/s, loss=0.00816, v_num=16, train_loss_step=0.0056, val_loss=0.0106, train_loss_epoch=0.0147]\u001b[A\n",
      "Epoch 10:  64%|▋| 43/67 [00:12<00:07,  3.38it/s, loss=0.00816, v_num=16, train_loss_step=0.0056, val_loss=0.0106, train_loss_epoch=0.0147]\u001b[A\n",
      "Epoch 10:  66%|▋| 44/67 [00:13<00:06,  3.30it/s, loss=0.00816, v_num=16, train_loss_step=0.0056, val_loss=0.0106, train_loss_epoch=0.0147]\u001b[A\n",
      "Epoch 10:  67%|▋| 45/67 [00:13<00:06,  3.23it/s, loss=0.00816, v_num=16, train_loss_step=0.0056, val_loss=0.0106, train_loss_epoch=0.0147]\u001b[A\n",
      "Epoch 10:  69%|▋| 46/67 [00:14<00:06,  3.16it/s, loss=0.00816, v_num=16, train_loss_step=0.0056, val_loss=0.0106, train_loss_epoch=0.0147]\u001b[A\n",
      "Epoch 10:  70%|▋| 47/67 [00:15<00:06,  3.07it/s, loss=0.00816, v_num=16, train_loss_step=0.0056, val_loss=0.0106, train_loss_epoch=0.0147]\u001b[A\n",
      "Epoch 10:  72%|▋| 48/67 [00:16<00:06,  3.00it/s, loss=0.00816, v_num=16, train_loss_step=0.0056, val_loss=0.0106, train_loss_epoch=0.0147]\u001b[A\n",
      "Epoch 10:  73%|▋| 49/67 [00:16<00:06,  2.95it/s, loss=0.00816, v_num=16, train_loss_step=0.0056, val_loss=0.0106, train_loss_epoch=0.0147]\u001b[A\n",
      "Epoch 10:  75%|▋| 50/67 [00:17<00:05,  2.90it/s, loss=0.00816, v_num=16, train_loss_step=0.0056, val_loss=0.0106, train_loss_epoch=0.0147]\u001b[A\n",
      "Epoch 10:  76%|▊| 51/67 [00:17<00:05,  2.84it/s, loss=0.00816, v_num=16, train_loss_step=0.0056, val_loss=0.0106, train_loss_epoch=0.0147]\u001b[A\n",
      "Epoch 10:  78%|▊| 52/67 [00:18<00:05,  2.79it/s, loss=0.00816, v_num=16, train_loss_step=0.0056, val_loss=0.0106, train_loss_epoch=0.0147]\u001b[A\n",
      "Epoch 10:  79%|▊| 53/67 [00:19<00:05,  2.75it/s, loss=0.00816, v_num=16, train_loss_step=0.0056, val_loss=0.0106, train_loss_epoch=0.0147]\u001b[A\n",
      "Epoch 10:  81%|▊| 54/67 [00:19<00:04,  2.71it/s, loss=0.00816, v_num=16, train_loss_step=0.0056, val_loss=0.0106, train_loss_epoch=0.0147]\u001b[A\n",
      "Epoch 10:  82%|▊| 55/67 [00:20<00:04,  2.67it/s, loss=0.00816, v_num=16, train_loss_step=0.0056, val_loss=0.0106, train_loss_epoch=0.0147]\u001b[A\n",
      "Epoch 10:  84%|▊| 56/67 [00:21<00:04,  2.64it/s, loss=0.00816, v_num=16, train_loss_step=0.0056, val_loss=0.0106, train_loss_epoch=0.0147]\u001b[A\n",
      "Epoch 10:  85%|▊| 57/67 [00:22<00:03,  2.59it/s, loss=0.00816, v_num=16, train_loss_step=0.0056, val_loss=0.0106, train_loss_epoch=0.0147]\u001b[A\n",
      "Epoch 10:  87%|▊| 58/67 [00:22<00:03,  2.56it/s, loss=0.00816, v_num=16, train_loss_step=0.0056, val_loss=0.0106, train_loss_epoch=0.0147]\u001b[A\n",
      "Epoch 10:  88%|▉| 59/67 [00:23<00:03,  2.54it/s, loss=0.00816, v_num=16, train_loss_step=0.0056, val_loss=0.0106, train_loss_epoch=0.0147]\u001b[A\n",
      "Epoch 10:  90%|▉| 60/67 [00:23<00:02,  2.51it/s, loss=0.00816, v_num=16, train_loss_step=0.0056, val_loss=0.0106, train_loss_epoch=0.0147]\u001b[A\n",
      "Epoch 10:  91%|▉| 61/67 [00:24<00:02,  2.49it/s, loss=0.00816, v_num=16, train_loss_step=0.0056, val_loss=0.0106, train_loss_epoch=0.0147]\u001b[A\n",
      "Epoch 10:  93%|▉| 62/67 [00:25<00:02,  2.47it/s, loss=0.00816, v_num=16, train_loss_step=0.0056, val_loss=0.0106, train_loss_epoch=0.0147]\u001b[A\n",
      "Epoch 10:  94%|▉| 63/67 [00:25<00:01,  2.45it/s, loss=0.00816, v_num=16, train_loss_step=0.0056, val_loss=0.0106, train_loss_epoch=0.0147]\u001b[A\n",
      "Epoch 10:  96%|▉| 64/67 [00:26<00:01,  2.42it/s, loss=0.00816, v_num=16, train_loss_step=0.0056, val_loss=0.0106, train_loss_epoch=0.0147]\u001b[A\n",
      "Epoch 10:  97%|▉| 65/67 [00:27<00:00,  2.40it/s, loss=0.00816, v_num=16, train_loss_step=0.0056, val_loss=0.0106, train_loss_epoch=0.0147]\u001b[A\n",
      "Epoch 10:  99%|▉| 66/67 [00:27<00:00,  2.38it/s, loss=0.00816, v_num=16, train_loss_step=0.0056, val_loss=0.0106, train_loss_epoch=0.0147]\u001b[A\n",
      "Epoch 10: 100%|█| 67/67 [00:28<00:00,  2.34it/s, loss=0.00816, v_num=16, train_loss_step=0.0056, val_loss=0.0106, train_loss_epoch=0.0147]\u001b[A\n",
      "Epoch 11:  45%|▍| 30/67 [00:04<00:05,  6.97it/s, loss=0.0168, v_num=16, train_loss_step=0.00705, val_loss=0.0106, train_loss_epoch=0.00796\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 11:  46%|▍| 31/67 [00:05<00:05,  6.15it/s, loss=0.0168, v_num=16, train_loss_step=0.00705, val_loss=0.0106, train_loss_epoch=0.00796\u001b[A\n",
      "Epoch 11:  48%|▍| 32/67 [00:05<00:06,  5.61it/s, loss=0.0168, v_num=16, train_loss_step=0.00705, val_loss=0.0106, train_loss_epoch=0.00796\u001b[A\n",
      "Epoch 11:  49%|▍| 33/67 [00:06<00:06,  5.21it/s, loss=0.0168, v_num=16, train_loss_step=0.00705, val_loss=0.0106, train_loss_epoch=0.00796\u001b[A\n",
      "Epoch 11:  51%|▌| 34/67 [00:06<00:06,  4.89it/s, loss=0.0168, v_num=16, train_loss_step=0.00705, val_loss=0.0106, train_loss_epoch=0.00796\u001b[A\n",
      "Epoch 11:  52%|▌| 35/67 [00:07<00:06,  4.61it/s, loss=0.0168, v_num=16, train_loss_step=0.00705, val_loss=0.0106, train_loss_epoch=0.00796\u001b[A\n",
      "Epoch 11:  54%|▌| 36/67 [00:08<00:07,  4.37it/s, loss=0.0168, v_num=16, train_loss_step=0.00705, val_loss=0.0106, train_loss_epoch=0.00796\u001b[A\n",
      "Epoch 11:  55%|▌| 37/67 [00:08<00:07,  4.17it/s, loss=0.0168, v_num=16, train_loss_step=0.00705, val_loss=0.0106, train_loss_epoch=0.00796\u001b[A\n",
      "Epoch 11:  57%|▌| 38/67 [00:09<00:07,  3.99it/s, loss=0.0168, v_num=16, train_loss_step=0.00705, val_loss=0.0106, train_loss_epoch=0.00796\u001b[A\n",
      "Epoch 11:  58%|▌| 39/67 [00:10<00:07,  3.79it/s, loss=0.0168, v_num=16, train_loss_step=0.00705, val_loss=0.0106, train_loss_epoch=0.00796\u001b[A\n",
      "Epoch 11:  60%|▌| 40/67 [00:10<00:07,  3.66it/s, loss=0.0168, v_num=16, train_loss_step=0.00705, val_loss=0.0106, train_loss_epoch=0.00796\u001b[A\n",
      "Epoch 11:  61%|▌| 41/67 [00:11<00:07,  3.55it/s, loss=0.0168, v_num=16, train_loss_step=0.00705, val_loss=0.0106, train_loss_epoch=0.00796\u001b[A\n",
      "Epoch 11:  63%|▋| 42/67 [00:12<00:07,  3.45it/s, loss=0.0168, v_num=16, train_loss_step=0.00705, val_loss=0.0106, train_loss_epoch=0.00796\u001b[A\n",
      "Epoch 11:  64%|▋| 43/67 [00:12<00:07,  3.36it/s, loss=0.0168, v_num=16, train_loss_step=0.00705, val_loss=0.0106, train_loss_epoch=0.00796\u001b[A\n",
      "Epoch 11:  66%|▋| 44/67 [00:13<00:07,  3.28it/s, loss=0.0168, v_num=16, train_loss_step=0.00705, val_loss=0.0106, train_loss_epoch=0.00796\u001b[A\n",
      "Epoch 11:  67%|▋| 45/67 [00:14<00:06,  3.21it/s, loss=0.0168, v_num=16, train_loss_step=0.00705, val_loss=0.0106, train_loss_epoch=0.00796\u001b[A\n",
      "Epoch 11:  69%|▋| 46/67 [00:14<00:06,  3.14it/s, loss=0.0168, v_num=16, train_loss_step=0.00705, val_loss=0.0106, train_loss_epoch=0.00796\u001b[A\n",
      "Epoch 11:  70%|▋| 47/67 [00:15<00:06,  3.08it/s, loss=0.0168, v_num=16, train_loss_step=0.00705, val_loss=0.0106, train_loss_epoch=0.00796\u001b[A\n",
      "Epoch 11:  72%|▋| 48/67 [00:15<00:06,  3.01it/s, loss=0.0168, v_num=16, train_loss_step=0.00705, val_loss=0.0106, train_loss_epoch=0.00796\u001b[A\n",
      "Epoch 11:  73%|▋| 49/67 [00:16<00:06,  2.96it/s, loss=0.0168, v_num=16, train_loss_step=0.00705, val_loss=0.0106, train_loss_epoch=0.00796\u001b[A\n",
      "Epoch 11:  75%|▋| 50/67 [00:17<00:05,  2.89it/s, loss=0.0168, v_num=16, train_loss_step=0.00705, val_loss=0.0106, train_loss_epoch=0.00796\u001b[A\n",
      "Epoch 11:  76%|▊| 51/67 [00:18<00:05,  2.83it/s, loss=0.0168, v_num=16, train_loss_step=0.00705, val_loss=0.0106, train_loss_epoch=0.00796\u001b[A\n",
      "Epoch 11:  78%|▊| 52/67 [00:18<00:05,  2.78it/s, loss=0.0168, v_num=16, train_loss_step=0.00705, val_loss=0.0106, train_loss_epoch=0.00796\u001b[A\n",
      "Epoch 11:  79%|▊| 53/67 [00:19<00:05,  2.74it/s, loss=0.0168, v_num=16, train_loss_step=0.00705, val_loss=0.0106, train_loss_epoch=0.00796\u001b[A\n",
      "Epoch 11:  81%|▊| 54/67 [00:20<00:04,  2.70it/s, loss=0.0168, v_num=16, train_loss_step=0.00705, val_loss=0.0106, train_loss_epoch=0.00796\u001b[A\n",
      "Epoch 11:  82%|▊| 55/67 [00:20<00:04,  2.66it/s, loss=0.0168, v_num=16, train_loss_step=0.00705, val_loss=0.0106, train_loss_epoch=0.00796\u001b[A\n",
      "Epoch 11:  84%|▊| 56/67 [00:21<00:04,  2.63it/s, loss=0.0168, v_num=16, train_loss_step=0.00705, val_loss=0.0106, train_loss_epoch=0.00796\u001b[A\n",
      "Epoch 11:  85%|▊| 57/67 [00:21<00:03,  2.60it/s, loss=0.0168, v_num=16, train_loss_step=0.00705, val_loss=0.0106, train_loss_epoch=0.00796\u001b[A\n",
      "Epoch 11:  87%|▊| 58/67 [00:22<00:03,  2.57it/s, loss=0.0168, v_num=16, train_loss_step=0.00705, val_loss=0.0106, train_loss_epoch=0.00796\u001b[A\n",
      "Epoch 11:  88%|▉| 59/67 [00:23<00:03,  2.54it/s, loss=0.0168, v_num=16, train_loss_step=0.00705, val_loss=0.0106, train_loss_epoch=0.00796\u001b[A\n",
      "Epoch 11:  90%|▉| 60/67 [00:23<00:02,  2.51it/s, loss=0.0168, v_num=16, train_loss_step=0.00705, val_loss=0.0106, train_loss_epoch=0.00796\u001b[A\n",
      "Epoch 11:  91%|▉| 61/67 [00:24<00:02,  2.47it/s, loss=0.0168, v_num=16, train_loss_step=0.00705, val_loss=0.0106, train_loss_epoch=0.00796\u001b[A\n",
      "Epoch 11:  93%|▉| 62/67 [00:25<00:02,  2.44it/s, loss=0.0168, v_num=16, train_loss_step=0.00705, val_loss=0.0106, train_loss_epoch=0.00796\u001b[A\n",
      "Epoch 11:  94%|▉| 63/67 [00:26<00:01,  2.42it/s, loss=0.0168, v_num=16, train_loss_step=0.00705, val_loss=0.0106, train_loss_epoch=0.00796\u001b[A\n",
      "Epoch 11:  96%|▉| 64/67 [00:26<00:01,  2.40it/s, loss=0.0168, v_num=16, train_loss_step=0.00705, val_loss=0.0106, train_loss_epoch=0.00796\u001b[A\n",
      "Epoch 11:  97%|▉| 65/67 [00:27<00:00,  2.38it/s, loss=0.0168, v_num=16, train_loss_step=0.00705, val_loss=0.0106, train_loss_epoch=0.00796\u001b[A\n",
      "Epoch 11:  99%|▉| 66/67 [00:27<00:00,  2.36it/s, loss=0.0168, v_num=16, train_loss_step=0.00705, val_loss=0.0106, train_loss_epoch=0.00796\u001b[A\n",
      "Epoch 11: 100%|█| 67/67 [00:28<00:00,  2.33it/s, loss=0.0168, v_num=16, train_loss_step=0.00705, val_loss=0.0106, train_loss_epoch=0.00796\u001b[A\n",
      "Epoch 12:  45%|▉ | 30/67 [00:04<00:05,  7.01it/s, loss=0.0124, v_num=16, train_loss_step=0.0088, val_loss=0.0106, train_loss_epoch=0.0168]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 12:  46%|▉ | 31/67 [00:05<00:05,  6.17it/s, loss=0.0124, v_num=16, train_loss_step=0.0088, val_loss=0.0106, train_loss_epoch=0.0168]\u001b[A\n",
      "Epoch 12:  48%|▉ | 32/67 [00:05<00:06,  5.64it/s, loss=0.0124, v_num=16, train_loss_step=0.0088, val_loss=0.0106, train_loss_epoch=0.0168]\u001b[A\n",
      "Epoch 12:  49%|▉ | 33/67 [00:06<00:06,  5.10it/s, loss=0.0124, v_num=16, train_loss_step=0.0088, val_loss=0.0106, train_loss_epoch=0.0168]\u001b[A\n",
      "Epoch 12:  51%|█ | 34/67 [00:07<00:06,  4.77it/s, loss=0.0124, v_num=16, train_loss_step=0.0088, val_loss=0.0106, train_loss_epoch=0.0168]\u001b[A\n",
      "Epoch 12:  52%|█ | 35/67 [00:07<00:07,  4.51it/s, loss=0.0124, v_num=16, train_loss_step=0.0088, val_loss=0.0106, train_loss_epoch=0.0168]\u001b[A\n",
      "Epoch 12:  54%|█ | 36/67 [00:08<00:07,  4.28it/s, loss=0.0124, v_num=16, train_loss_step=0.0088, val_loss=0.0106, train_loss_epoch=0.0168]\u001b[A\n",
      "Epoch 12:  55%|█ | 37/67 [00:09<00:07,  4.08it/s, loss=0.0124, v_num=16, train_loss_step=0.0088, val_loss=0.0106, train_loss_epoch=0.0168]\u001b[A\n",
      "Epoch 12:  57%|█▏| 38/67 [00:09<00:07,  3.91it/s, loss=0.0124, v_num=16, train_loss_step=0.0088, val_loss=0.0106, train_loss_epoch=0.0168]\u001b[A\n",
      "Epoch 12:  58%|█▏| 39/67 [00:10<00:07,  3.77it/s, loss=0.0124, v_num=16, train_loss_step=0.0088, val_loss=0.0106, train_loss_epoch=0.0168]\u001b[A\n",
      "Epoch 12:  60%|█▏| 40/67 [00:10<00:07,  3.64it/s, loss=0.0124, v_num=16, train_loss_step=0.0088, val_loss=0.0106, train_loss_epoch=0.0168]\u001b[A\n",
      "Epoch 12:  61%|█▏| 41/67 [00:11<00:07,  3.54it/s, loss=0.0124, v_num=16, train_loss_step=0.0088, val_loss=0.0106, train_loss_epoch=0.0168]\u001b[A\n",
      "Epoch 12:  63%|█▎| 42/67 [00:12<00:07,  3.44it/s, loss=0.0124, v_num=16, train_loss_step=0.0088, val_loss=0.0106, train_loss_epoch=0.0168]\u001b[A\n",
      "Epoch 12:  64%|█▎| 43/67 [00:12<00:07,  3.36it/s, loss=0.0124, v_num=16, train_loss_step=0.0088, val_loss=0.0106, train_loss_epoch=0.0168]\u001b[A\n",
      "Epoch 12:  66%|█▎| 44/67 [00:13<00:07,  3.24it/s, loss=0.0124, v_num=16, train_loss_step=0.0088, val_loss=0.0106, train_loss_epoch=0.0168]\u001b[A\n",
      "Epoch 12:  67%|█▎| 45/67 [00:14<00:06,  3.18it/s, loss=0.0124, v_num=16, train_loss_step=0.0088, val_loss=0.0106, train_loss_epoch=0.0168]\u001b[A\n",
      "Epoch 12:  69%|█▎| 46/67 [00:14<00:06,  3.11it/s, loss=0.0124, v_num=16, train_loss_step=0.0088, val_loss=0.0106, train_loss_epoch=0.0168]\u001b[A\n",
      "Epoch 12:  70%|█▍| 47/67 [00:15<00:06,  3.06it/s, loss=0.0124, v_num=16, train_loss_step=0.0088, val_loss=0.0106, train_loss_epoch=0.0168]\u001b[A\n",
      "Epoch 12:  72%|█▍| 48/67 [00:16<00:06,  2.98it/s, loss=0.0124, v_num=16, train_loss_step=0.0088, val_loss=0.0106, train_loss_epoch=0.0168]\u001b[A\n",
      "Epoch 12:  73%|█▍| 49/67 [00:16<00:06,  2.93it/s, loss=0.0124, v_num=16, train_loss_step=0.0088, val_loss=0.0106, train_loss_epoch=0.0168]\u001b[A\n",
      "Epoch 12:  75%|█▍| 50/67 [00:17<00:05,  2.89it/s, loss=0.0124, v_num=16, train_loss_step=0.0088, val_loss=0.0106, train_loss_epoch=0.0168]\u001b[A\n",
      "Epoch 12:  76%|█▌| 51/67 [00:18<00:05,  2.82it/s, loss=0.0124, v_num=16, train_loss_step=0.0088, val_loss=0.0106, train_loss_epoch=0.0168]\u001b[A\n",
      "Epoch 12:  78%|█▌| 52/67 [00:18<00:05,  2.78it/s, loss=0.0124, v_num=16, train_loss_step=0.0088, val_loss=0.0106, train_loss_epoch=0.0168]\u001b[A\n",
      "Epoch 12:  79%|█▌| 53/67 [00:19<00:05,  2.74it/s, loss=0.0124, v_num=16, train_loss_step=0.0088, val_loss=0.0106, train_loss_epoch=0.0168]\u001b[A\n",
      "Epoch 12:  81%|█▌| 54/67 [00:19<00:04,  2.70it/s, loss=0.0124, v_num=16, train_loss_step=0.0088, val_loss=0.0106, train_loss_epoch=0.0168]\u001b[A\n",
      "Epoch 12:  82%|█▋| 55/67 [00:20<00:04,  2.66it/s, loss=0.0124, v_num=16, train_loss_step=0.0088, val_loss=0.0106, train_loss_epoch=0.0168]\u001b[A\n",
      "Epoch 12:  84%|█▋| 56/67 [00:21<00:04,  2.61it/s, loss=0.0124, v_num=16, train_loss_step=0.0088, val_loss=0.0106, train_loss_epoch=0.0168]\u001b[A\n",
      "Epoch 12:  85%|█▋| 57/67 [00:22<00:03,  2.58it/s, loss=0.0124, v_num=16, train_loss_step=0.0088, val_loss=0.0106, train_loss_epoch=0.0168]\u001b[A\n",
      "Epoch 12:  87%|█▋| 58/67 [00:22<00:03,  2.56it/s, loss=0.0124, v_num=16, train_loss_step=0.0088, val_loss=0.0106, train_loss_epoch=0.0168]\u001b[A\n",
      "Epoch 12:  88%|█▊| 59/67 [00:23<00:03,  2.53it/s, loss=0.0124, v_num=16, train_loss_step=0.0088, val_loss=0.0106, train_loss_epoch=0.0168]\u001b[A\n",
      "Epoch 12:  90%|█▊| 60/67 [00:23<00:02,  2.50it/s, loss=0.0124, v_num=16, train_loss_step=0.0088, val_loss=0.0106, train_loss_epoch=0.0168]\u001b[A\n",
      "Epoch 12:  91%|█▊| 61/67 [00:24<00:02,  2.48it/s, loss=0.0124, v_num=16, train_loss_step=0.0088, val_loss=0.0106, train_loss_epoch=0.0168]\u001b[A\n",
      "Epoch 12:  93%|█▊| 62/67 [00:25<00:02,  2.45it/s, loss=0.0124, v_num=16, train_loss_step=0.0088, val_loss=0.0106, train_loss_epoch=0.0168]\u001b[A\n",
      "Epoch 12:  94%|█▉| 63/67 [00:25<00:01,  2.43it/s, loss=0.0124, v_num=16, train_loss_step=0.0088, val_loss=0.0106, train_loss_epoch=0.0168]\u001b[A\n",
      "Epoch 12:  96%|█▉| 64/67 [00:26<00:01,  2.41it/s, loss=0.0124, v_num=16, train_loss_step=0.0088, val_loss=0.0106, train_loss_epoch=0.0168]\u001b[A\n",
      "Epoch 12:  97%|█▉| 65/67 [00:27<00:00,  2.39it/s, loss=0.0124, v_num=16, train_loss_step=0.0088, val_loss=0.0106, train_loss_epoch=0.0168]\u001b[A\n",
      "Epoch 12:  99%|█▉| 66/67 [00:27<00:00,  2.37it/s, loss=0.0124, v_num=16, train_loss_step=0.0088, val_loss=0.0106, train_loss_epoch=0.0168]\u001b[A\n",
      "Epoch 12: 100%|██| 67/67 [00:28<00:00,  2.33it/s, loss=0.0124, v_num=16, train_loss_step=0.0088, val_loss=0.0106, train_loss_epoch=0.0168]\u001b[A\n",
      "Epoch 13:  45%|▍| 30/67 [00:04<00:05,  7.00it/s, loss=0.0137, v_num=16, train_loss_step=0.00556, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 13:  46%|▍| 31/67 [00:05<00:05,  6.18it/s, loss=0.0137, v_num=16, train_loss_step=0.00556, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 13:  48%|▍| 32/67 [00:05<00:06,  5.65it/s, loss=0.0137, v_num=16, train_loss_step=0.00556, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 13:  49%|▍| 33/67 [00:06<00:06,  5.24it/s, loss=0.0137, v_num=16, train_loss_step=0.00556, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 13:  51%|▌| 34/67 [00:06<00:06,  4.90it/s, loss=0.0137, v_num=16, train_loss_step=0.00556, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 13:  52%|▌| 35/67 [00:07<00:06,  4.63it/s, loss=0.0137, v_num=16, train_loss_step=0.00556, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 13:  54%|▌| 36/67 [00:08<00:07,  4.37it/s, loss=0.0137, v_num=16, train_loss_step=0.00556, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 13:  55%|▌| 37/67 [00:08<00:07,  4.17it/s, loss=0.0137, v_num=16, train_loss_step=0.00556, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 13:  57%|▌| 38/67 [00:09<00:07,  4.00it/s, loss=0.0137, v_num=16, train_loss_step=0.00556, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 13:  58%|▌| 39/67 [00:10<00:07,  3.85it/s, loss=0.0137, v_num=16, train_loss_step=0.00556, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 13:  60%|▌| 40/67 [00:10<00:07,  3.66it/s, loss=0.0137, v_num=16, train_loss_step=0.00556, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 13:  61%|▌| 41/67 [00:11<00:07,  3.55it/s, loss=0.0137, v_num=16, train_loss_step=0.00556, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 13:  63%|▋| 42/67 [00:12<00:07,  3.45it/s, loss=0.0137, v_num=16, train_loss_step=0.00556, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 13:  64%|▋| 43/67 [00:12<00:07,  3.37it/s, loss=0.0137, v_num=16, train_loss_step=0.00556, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 13:  66%|▋| 44/67 [00:13<00:06,  3.30it/s, loss=0.0137, v_num=16, train_loss_step=0.00556, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 13:  67%|▋| 45/67 [00:13<00:06,  3.23it/s, loss=0.0137, v_num=16, train_loss_step=0.00556, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 13:  69%|▋| 46/67 [00:14<00:06,  3.16it/s, loss=0.0137, v_num=16, train_loss_step=0.00556, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 13:  70%|▋| 47/67 [00:15<00:06,  3.10it/s, loss=0.0137, v_num=16, train_loss_step=0.00556, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 13:  72%|▋| 48/67 [00:15<00:06,  3.03it/s, loss=0.0137, v_num=16, train_loss_step=0.00556, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 13:  73%|▋| 49/67 [00:16<00:06,  2.98it/s, loss=0.0137, v_num=16, train_loss_step=0.00556, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 13:  75%|▋| 50/67 [00:17<00:05,  2.93it/s, loss=0.0137, v_num=16, train_loss_step=0.00556, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 13:  76%|▊| 51/67 [00:17<00:05,  2.86it/s, loss=0.0137, v_num=16, train_loss_step=0.00556, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 13:  78%|▊| 52/67 [00:18<00:05,  2.79it/s, loss=0.0137, v_num=16, train_loss_step=0.00556, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 13:  79%|▊| 53/67 [00:19<00:05,  2.76it/s, loss=0.0137, v_num=16, train_loss_step=0.00556, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 13:  81%|▊| 54/67 [00:19<00:04,  2.72it/s, loss=0.0137, v_num=16, train_loss_step=0.00556, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 13:  82%|▊| 55/67 [00:20<00:04,  2.68it/s, loss=0.0137, v_num=16, train_loss_step=0.00556, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 13:  84%|▊| 56/67 [00:21<00:04,  2.65it/s, loss=0.0137, v_num=16, train_loss_step=0.00556, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 13:  85%|▊| 57/67 [00:21<00:03,  2.61it/s, loss=0.0137, v_num=16, train_loss_step=0.00556, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 13:  87%|▊| 58/67 [00:22<00:03,  2.58it/s, loss=0.0137, v_num=16, train_loss_step=0.00556, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 13:  88%|▉| 59/67 [00:23<00:03,  2.55it/s, loss=0.0137, v_num=16, train_loss_step=0.00556, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 13:  90%|▉| 60/67 [00:23<00:02,  2.52it/s, loss=0.0137, v_num=16, train_loss_step=0.00556, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 13:  91%|▉| 61/67 [00:24<00:02,  2.50it/s, loss=0.0137, v_num=16, train_loss_step=0.00556, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 13:  93%|▉| 62/67 [00:25<00:02,  2.47it/s, loss=0.0137, v_num=16, train_loss_step=0.00556, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 13:  94%|▉| 63/67 [00:25<00:01,  2.45it/s, loss=0.0137, v_num=16, train_loss_step=0.00556, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 13:  96%|▉| 64/67 [00:26<00:01,  2.41it/s, loss=0.0137, v_num=16, train_loss_step=0.00556, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 13:  97%|▉| 65/67 [00:27<00:00,  2.39it/s, loss=0.0137, v_num=16, train_loss_step=0.00556, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 13:  99%|▉| 66/67 [00:27<00:00,  2.37it/s, loss=0.0137, v_num=16, train_loss_step=0.00556, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 13: 100%|█| 67/67 [00:28<00:00,  2.33it/s, loss=0.0137, v_num=16, train_loss_step=0.00556, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 14:  45%|█▊  | 30/67 [00:04<00:05,  6.91it/s, loss=0.0153, v_num=16, train_loss_step=0.024, val_loss=0.0106, train_loss_epoch=0.012]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 14:  46%|█▊  | 31/67 [00:05<00:05,  6.11it/s, loss=0.0153, v_num=16, train_loss_step=0.024, val_loss=0.0106, train_loss_epoch=0.012]\u001b[A\n",
      "Epoch 14:  48%|█▉  | 32/67 [00:05<00:06,  5.59it/s, loss=0.0153, v_num=16, train_loss_step=0.024, val_loss=0.0106, train_loss_epoch=0.012]\u001b[A\n",
      "Epoch 14:  49%|█▉  | 33/67 [00:06<00:06,  5.18it/s, loss=0.0153, v_num=16, train_loss_step=0.024, val_loss=0.0106, train_loss_epoch=0.012]\u001b[A\n",
      "Epoch 14:  51%|██  | 34/67 [00:07<00:06,  4.85it/s, loss=0.0153, v_num=16, train_loss_step=0.024, val_loss=0.0106, train_loss_epoch=0.012]\u001b[A\n",
      "Epoch 14:  52%|██  | 35/67 [00:07<00:06,  4.58it/s, loss=0.0153, v_num=16, train_loss_step=0.024, val_loss=0.0106, train_loss_epoch=0.012]\u001b[A\n",
      "Epoch 14:  54%|██▏ | 36/67 [00:08<00:07,  4.34it/s, loss=0.0153, v_num=16, train_loss_step=0.024, val_loss=0.0106, train_loss_epoch=0.012]\u001b[A\n",
      "Epoch 14:  55%|██▏ | 37/67 [00:09<00:07,  4.06it/s, loss=0.0153, v_num=16, train_loss_step=0.024, val_loss=0.0106, train_loss_epoch=0.012]\u001b[A\n",
      "Epoch 14:  57%|██▎ | 38/67 [00:09<00:07,  3.89it/s, loss=0.0153, v_num=16, train_loss_step=0.024, val_loss=0.0106, train_loss_epoch=0.012]\u001b[A\n",
      "Epoch 14:  58%|██▎ | 39/67 [00:10<00:07,  3.75it/s, loss=0.0153, v_num=16, train_loss_step=0.024, val_loss=0.0106, train_loss_epoch=0.012]\u001b[A\n",
      "Epoch 14:  60%|██▍ | 40/67 [00:11<00:07,  3.63it/s, loss=0.0153, v_num=16, train_loss_step=0.024, val_loss=0.0106, train_loss_epoch=0.012]\u001b[A\n",
      "Epoch 14:  61%|██▍ | 41/67 [00:11<00:07,  3.52it/s, loss=0.0153, v_num=16, train_loss_step=0.024, val_loss=0.0106, train_loss_epoch=0.012]\u001b[A\n",
      "Epoch 14:  63%|██▌ | 42/67 [00:12<00:07,  3.43it/s, loss=0.0153, v_num=16, train_loss_step=0.024, val_loss=0.0106, train_loss_epoch=0.012]\u001b[A\n",
      "Epoch 14:  64%|██▌ | 43/67 [00:12<00:07,  3.34it/s, loss=0.0153, v_num=16, train_loss_step=0.024, val_loss=0.0106, train_loss_epoch=0.012]\u001b[A\n",
      "Epoch 14:  66%|██▋ | 44/67 [00:13<00:07,  3.26it/s, loss=0.0153, v_num=16, train_loss_step=0.024, val_loss=0.0106, train_loss_epoch=0.012]\u001b[A\n",
      "Epoch 14:  67%|██▋ | 45/67 [00:14<00:06,  3.19it/s, loss=0.0153, v_num=16, train_loss_step=0.024, val_loss=0.0106, train_loss_epoch=0.012]\u001b[A\n",
      "Epoch 14:  69%|██▋ | 46/67 [00:14<00:06,  3.12it/s, loss=0.0153, v_num=16, train_loss_step=0.024, val_loss=0.0106, train_loss_epoch=0.012]\u001b[A\n",
      "Epoch 14:  70%|██▊ | 47/67 [00:15<00:06,  3.06it/s, loss=0.0153, v_num=16, train_loss_step=0.024, val_loss=0.0106, train_loss_epoch=0.012]\u001b[A\n",
      "Epoch 14:  72%|██▊ | 48/67 [00:16<00:06,  2.99it/s, loss=0.0153, v_num=16, train_loss_step=0.024, val_loss=0.0106, train_loss_epoch=0.012]\u001b[A\n",
      "Epoch 14:  73%|██▉ | 49/67 [00:16<00:06,  2.90it/s, loss=0.0153, v_num=16, train_loss_step=0.024, val_loss=0.0106, train_loss_epoch=0.012]\u001b[A\n",
      "Epoch 14:  75%|██▉ | 50/67 [00:17<00:05,  2.86it/s, loss=0.0153, v_num=16, train_loss_step=0.024, val_loss=0.0106, train_loss_epoch=0.012]\u001b[A\n",
      "Epoch 14:  76%|███ | 51/67 [00:18<00:05,  2.80it/s, loss=0.0153, v_num=16, train_loss_step=0.024, val_loss=0.0106, train_loss_epoch=0.012]\u001b[A\n",
      "Epoch 14:  78%|███ | 52/67 [00:18<00:05,  2.76it/s, loss=0.0153, v_num=16, train_loss_step=0.024, val_loss=0.0106, train_loss_epoch=0.012]\u001b[A\n",
      "Epoch 14:  79%|███▏| 53/67 [00:19<00:05,  2.72it/s, loss=0.0153, v_num=16, train_loss_step=0.024, val_loss=0.0106, train_loss_epoch=0.012]\u001b[A\n",
      "Epoch 14:  81%|███▏| 54/67 [00:20<00:04,  2.68it/s, loss=0.0153, v_num=16, train_loss_step=0.024, val_loss=0.0106, train_loss_epoch=0.012]\u001b[A\n",
      "Epoch 14:  82%|███▎| 55/67 [00:20<00:04,  2.65it/s, loss=0.0153, v_num=16, train_loss_step=0.024, val_loss=0.0106, train_loss_epoch=0.012]\u001b[A\n",
      "Epoch 14:  84%|███▎| 56/67 [00:21<00:04,  2.61it/s, loss=0.0153, v_num=16, train_loss_step=0.024, val_loss=0.0106, train_loss_epoch=0.012]\u001b[A\n",
      "Epoch 14:  85%|███▍| 57/67 [00:22<00:03,  2.58it/s, loss=0.0153, v_num=16, train_loss_step=0.024, val_loss=0.0106, train_loss_epoch=0.012]\u001b[A\n",
      "Epoch 14:  87%|███▍| 58/67 [00:22<00:03,  2.55it/s, loss=0.0153, v_num=16, train_loss_step=0.024, val_loss=0.0106, train_loss_epoch=0.012]\u001b[A\n",
      "Epoch 14:  88%|███▌| 59/67 [00:23<00:03,  2.53it/s, loss=0.0153, v_num=16, train_loss_step=0.024, val_loss=0.0106, train_loss_epoch=0.012]\u001b[A\n",
      "Epoch 14:  90%|███▌| 60/67 [00:23<00:02,  2.50it/s, loss=0.0153, v_num=16, train_loss_step=0.024, val_loss=0.0106, train_loss_epoch=0.012]\u001b[A\n",
      "Epoch 14:  91%|███▋| 61/67 [00:24<00:02,  2.48it/s, loss=0.0153, v_num=16, train_loss_step=0.024, val_loss=0.0106, train_loss_epoch=0.012]\u001b[A\n",
      "Epoch 14:  93%|███▋| 62/67 [00:25<00:02,  2.44it/s, loss=0.0153, v_num=16, train_loss_step=0.024, val_loss=0.0106, train_loss_epoch=0.012]\u001b[A\n",
      "Epoch 14:  94%|███▊| 63/67 [00:26<00:01,  2.41it/s, loss=0.0153, v_num=16, train_loss_step=0.024, val_loss=0.0106, train_loss_epoch=0.012]\u001b[A\n",
      "Epoch 14:  96%|███▊| 64/67 [00:26<00:01,  2.39it/s, loss=0.0153, v_num=16, train_loss_step=0.024, val_loss=0.0106, train_loss_epoch=0.012]\u001b[A\n",
      "Epoch 14:  97%|███▉| 65/67 [00:27<00:00,  2.37it/s, loss=0.0153, v_num=16, train_loss_step=0.024, val_loss=0.0106, train_loss_epoch=0.012]\u001b[A\n",
      "Epoch 14:  99%|███▉| 66/67 [00:28<00:00,  2.36it/s, loss=0.0153, v_num=16, train_loss_step=0.024, val_loss=0.0106, train_loss_epoch=0.012]\u001b[A\n",
      "Epoch 14: 100%|████| 67/67 [00:28<00:00,  2.33it/s, loss=0.0153, v_num=16, train_loss_step=0.024, val_loss=0.0106, train_loss_epoch=0.012]\u001b[A\n",
      "Epoch 15:  45%|▍| 30/67 [00:04<00:05,  7.04it/s, loss=0.00955, v_num=16, train_loss_step=0.0185, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 15:  46%|▍| 31/67 [00:05<00:05,  6.19it/s, loss=0.00955, v_num=16, train_loss_step=0.0185, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 15:  48%|▍| 32/67 [00:05<00:06,  5.67it/s, loss=0.00955, v_num=16, train_loss_step=0.0185, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 15:  49%|▍| 33/67 [00:06<00:06,  5.26it/s, loss=0.00955, v_num=16, train_loss_step=0.0185, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 15:  51%|▌| 34/67 [00:06<00:06,  4.90it/s, loss=0.00955, v_num=16, train_loss_step=0.0185, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 15:  52%|▌| 35/67 [00:07<00:06,  4.63it/s, loss=0.00955, v_num=16, train_loss_step=0.0185, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 15:  54%|▌| 36/67 [00:08<00:07,  4.29it/s, loss=0.00955, v_num=16, train_loss_step=0.0185, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 15:  55%|▌| 37/67 [00:09<00:07,  4.09it/s, loss=0.00955, v_num=16, train_loss_step=0.0185, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 15:  57%|▌| 38/67 [00:09<00:07,  3.92it/s, loss=0.00955, v_num=16, train_loss_step=0.0185, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 15:  58%|▌| 39/67 [00:10<00:07,  3.77it/s, loss=0.00955, v_num=16, train_loss_step=0.0185, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 15:  60%|▌| 40/67 [00:10<00:07,  3.65it/s, loss=0.00955, v_num=16, train_loss_step=0.0185, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 15:  61%|▌| 41/67 [00:11<00:07,  3.53it/s, loss=0.00955, v_num=16, train_loss_step=0.0185, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 15:  63%|▋| 42/67 [00:12<00:07,  3.43it/s, loss=0.00955, v_num=16, train_loss_step=0.0185, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 15:  64%|▋| 43/67 [00:12<00:07,  3.34it/s, loss=0.00955, v_num=16, train_loss_step=0.0185, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 15:  66%|▋| 44/67 [00:13<00:07,  3.25it/s, loss=0.00955, v_num=16, train_loss_step=0.0185, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 15:  67%|▋| 45/67 [00:14<00:06,  3.19it/s, loss=0.00955, v_num=16, train_loss_step=0.0185, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 15:  69%|▋| 46/67 [00:14<00:06,  3.12it/s, loss=0.00955, v_num=16, train_loss_step=0.0185, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 15:  70%|▋| 47/67 [00:15<00:06,  3.06it/s, loss=0.00955, v_num=16, train_loss_step=0.0185, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 15:  72%|▋| 48/67 [00:16<00:06,  2.99it/s, loss=0.00955, v_num=16, train_loss_step=0.0185, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 15:  73%|▋| 49/67 [00:16<00:06,  2.91it/s, loss=0.00955, v_num=16, train_loss_step=0.0185, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 15:  75%|▋| 50/67 [00:17<00:05,  2.86it/s, loss=0.00955, v_num=16, train_loss_step=0.0185, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 15:  76%|▊| 51/67 [00:18<00:05,  2.80it/s, loss=0.00955, v_num=16, train_loss_step=0.0185, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 15:  78%|▊| 52/67 [00:18<00:05,  2.75it/s, loss=0.00955, v_num=16, train_loss_step=0.0185, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 15:  79%|▊| 53/67 [00:19<00:05,  2.72it/s, loss=0.00955, v_num=16, train_loss_step=0.0185, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 15:  81%|▊| 54/67 [00:20<00:04,  2.68it/s, loss=0.00955, v_num=16, train_loss_step=0.0185, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 15:  82%|▊| 55/67 [00:20<00:04,  2.64it/s, loss=0.00955, v_num=16, train_loss_step=0.0185, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 15:  84%|▊| 56/67 [00:21<00:04,  2.61it/s, loss=0.00955, v_num=16, train_loss_step=0.0185, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 15:  85%|▊| 57/67 [00:22<00:03,  2.58it/s, loss=0.00955, v_num=16, train_loss_step=0.0185, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 15:  87%|▊| 58/67 [00:22<00:03,  2.55it/s, loss=0.00955, v_num=16, train_loss_step=0.0185, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 15:  88%|▉| 59/67 [00:23<00:03,  2.52it/s, loss=0.00955, v_num=16, train_loss_step=0.0185, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 15:  90%|▉| 60/67 [00:24<00:02,  2.50it/s, loss=0.00955, v_num=16, train_loss_step=0.0185, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 15:  91%|▉| 61/67 [00:24<00:02,  2.47it/s, loss=0.00955, v_num=16, train_loss_step=0.0185, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 15:  93%|▉| 62/67 [00:25<00:02,  2.43it/s, loss=0.00955, v_num=16, train_loss_step=0.0185, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 15:  94%|▉| 63/67 [00:26<00:01,  2.41it/s, loss=0.00955, v_num=16, train_loss_step=0.0185, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 15:  96%|▉| 64/67 [00:26<00:01,  2.38it/s, loss=0.00955, v_num=16, train_loss_step=0.0185, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 15:  97%|▉| 65/67 [00:27<00:00,  2.37it/s, loss=0.00955, v_num=16, train_loss_step=0.0185, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 15:  99%|▉| 66/67 [00:28<00:00,  2.35it/s, loss=0.00955, v_num=16, train_loss_step=0.0185, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 15: 100%|█| 67/67 [00:28<00:00,  2.32it/s, loss=0.00955, v_num=16, train_loss_step=0.0185, val_loss=0.0106, train_loss_epoch=0.0141]\u001b[A\n",
      "Epoch 16:  45%|▍| 30/67 [00:04<00:05,  6.95it/s, loss=0.0126, v_num=16, train_loss_step=0.00841, val_loss=0.0106, train_loss_epoch=0.0108]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 16:  46%|▍| 31/67 [00:05<00:05,  6.10it/s, loss=0.0126, v_num=16, train_loss_step=0.00841, val_loss=0.0106, train_loss_epoch=0.0108]\u001b[A\n",
      "Epoch 16:  48%|▍| 32/67 [00:05<00:06,  5.58it/s, loss=0.0126, v_num=16, train_loss_step=0.00841, val_loss=0.0106, train_loss_epoch=0.0108]\u001b[A\n",
      "Epoch 16:  49%|▍| 33/67 [00:06<00:06,  5.19it/s, loss=0.0126, v_num=16, train_loss_step=0.00841, val_loss=0.0106, train_loss_epoch=0.0108]\u001b[A\n",
      "Epoch 16:  51%|▌| 34/67 [00:06<00:06,  4.86it/s, loss=0.0126, v_num=16, train_loss_step=0.00841, val_loss=0.0106, train_loss_epoch=0.0108]\u001b[A\n",
      "Epoch 16:  52%|▌| 35/67 [00:07<00:06,  4.59it/s, loss=0.0126, v_num=16, train_loss_step=0.00841, val_loss=0.0106, train_loss_epoch=0.0108]\u001b[A\n",
      "Epoch 16:  54%|▌| 36/67 [00:08<00:07,  4.26it/s, loss=0.0126, v_num=16, train_loss_step=0.00841, val_loss=0.0106, train_loss_epoch=0.0108]\u001b[A\n",
      "Epoch 16:  55%|▌| 37/67 [00:09<00:07,  4.06it/s, loss=0.0126, v_num=16, train_loss_step=0.00841, val_loss=0.0106, train_loss_epoch=0.0108]\u001b[A\n",
      "Epoch 16:  57%|▌| 38/67 [00:09<00:07,  3.90it/s, loss=0.0126, v_num=16, train_loss_step=0.00841, val_loss=0.0106, train_loss_epoch=0.0108]\u001b[A\n",
      "Epoch 16:  58%|▌| 39/67 [00:10<00:07,  3.76it/s, loss=0.0126, v_num=16, train_loss_step=0.00841, val_loss=0.0106, train_loss_epoch=0.0108]\u001b[A\n",
      "Epoch 16:  60%|▌| 40/67 [00:11<00:07,  3.63it/s, loss=0.0126, v_num=16, train_loss_step=0.00841, val_loss=0.0106, train_loss_epoch=0.0108]\u001b[A\n",
      "Epoch 16:  61%|▌| 41/67 [00:11<00:07,  3.52it/s, loss=0.0126, v_num=16, train_loss_step=0.00841, val_loss=0.0106, train_loss_epoch=0.0108]\u001b[A\n",
      "Epoch 16:  63%|▋| 42/67 [00:12<00:07,  3.42it/s, loss=0.0126, v_num=16, train_loss_step=0.00841, val_loss=0.0106, train_loss_epoch=0.0108]\u001b[A\n",
      "Epoch 16:  64%|▋| 43/67 [00:12<00:07,  3.34it/s, loss=0.0126, v_num=16, train_loss_step=0.00841, val_loss=0.0106, train_loss_epoch=0.0108]\u001b[A\n",
      "Epoch 16:  66%|▋| 44/67 [00:13<00:07,  3.26it/s, loss=0.0126, v_num=16, train_loss_step=0.00841, val_loss=0.0106, train_loss_epoch=0.0108]\u001b[A\n",
      "Epoch 16:  67%|▋| 45/67 [00:14<00:06,  3.19it/s, loss=0.0126, v_num=16, train_loss_step=0.00841, val_loss=0.0106, train_loss_epoch=0.0108]\u001b[A\n",
      "Epoch 16:  69%|▋| 46/67 [00:14<00:06,  3.12it/s, loss=0.0126, v_num=16, train_loss_step=0.00841, val_loss=0.0106, train_loss_epoch=0.0108]\u001b[A\n",
      "Epoch 16:  70%|▋| 47/67 [00:15<00:06,  3.06it/s, loss=0.0126, v_num=16, train_loss_step=0.00841, val_loss=0.0106, train_loss_epoch=0.0108]\u001b[A\n",
      "Epoch 16:  72%|▋| 48/67 [00:16<00:06,  2.99it/s, loss=0.0126, v_num=16, train_loss_step=0.00841, val_loss=0.0106, train_loss_epoch=0.0108]\u001b[A\n",
      "Epoch 16:  73%|▋| 49/67 [00:16<00:06,  2.91it/s, loss=0.0126, v_num=16, train_loss_step=0.00841, val_loss=0.0106, train_loss_epoch=0.0108]\u001b[A\n",
      "Epoch 16:  75%|▋| 50/67 [00:17<00:05,  2.86it/s, loss=0.0126, v_num=16, train_loss_step=0.00841, val_loss=0.0106, train_loss_epoch=0.0108]\u001b[A\n",
      "Epoch 16:  76%|▊| 51/67 [00:18<00:05,  2.80it/s, loss=0.0126, v_num=16, train_loss_step=0.00841, val_loss=0.0106, train_loss_epoch=0.0108]\u001b[A\n",
      "Epoch 16:  78%|▊| 52/67 [00:18<00:05,  2.75it/s, loss=0.0126, v_num=16, train_loss_step=0.00841, val_loss=0.0106, train_loss_epoch=0.0108]\u001b[A\n",
      "Epoch 16:  79%|▊| 53/67 [00:19<00:05,  2.71it/s, loss=0.0126, v_num=16, train_loss_step=0.00841, val_loss=0.0106, train_loss_epoch=0.0108]\u001b[A\n",
      "Epoch 16:  81%|▊| 54/67 [00:20<00:04,  2.68it/s, loss=0.0126, v_num=16, train_loss_step=0.00841, val_loss=0.0106, train_loss_epoch=0.0108]\u001b[A\n",
      "Epoch 16:  82%|▊| 55/67 [00:20<00:04,  2.64it/s, loss=0.0126, v_num=16, train_loss_step=0.00841, val_loss=0.0106, train_loss_epoch=0.0108]\u001b[A\n",
      "Epoch 16:  84%|▊| 56/67 [00:21<00:04,  2.60it/s, loss=0.0126, v_num=16, train_loss_step=0.00841, val_loss=0.0106, train_loss_epoch=0.0108]\u001b[A\n",
      "Epoch 16:  85%|▊| 57/67 [00:22<00:03,  2.57it/s, loss=0.0126, v_num=16, train_loss_step=0.00841, val_loss=0.0106, train_loss_epoch=0.0108]\u001b[A\n",
      "Epoch 16:  87%|▊| 58/67 [00:22<00:03,  2.55it/s, loss=0.0126, v_num=16, train_loss_step=0.00841, val_loss=0.0106, train_loss_epoch=0.0108]\u001b[A\n",
      "Epoch 16:  88%|▉| 59/67 [00:23<00:03,  2.52it/s, loss=0.0126, v_num=16, train_loss_step=0.00841, val_loss=0.0106, train_loss_epoch=0.0108]\u001b[A\n",
      "Epoch 16:  90%|▉| 60/67 [00:24<00:02,  2.49it/s, loss=0.0126, v_num=16, train_loss_step=0.00841, val_loss=0.0106, train_loss_epoch=0.0108]\u001b[A\n",
      "Epoch 16:  91%|▉| 61/67 [00:24<00:02,  2.47it/s, loss=0.0126, v_num=16, train_loss_step=0.00841, val_loss=0.0106, train_loss_epoch=0.0108]\u001b[A\n",
      "Epoch 16:  93%|▉| 62/67 [00:25<00:02,  2.44it/s, loss=0.0126, v_num=16, train_loss_step=0.00841, val_loss=0.0106, train_loss_epoch=0.0108]\u001b[A\n",
      "Epoch 16:  94%|▉| 63/67 [00:26<00:01,  2.41it/s, loss=0.0126, v_num=16, train_loss_step=0.00841, val_loss=0.0106, train_loss_epoch=0.0108]\u001b[A\n",
      "Epoch 16:  96%|▉| 64/67 [00:26<00:01,  2.39it/s, loss=0.0126, v_num=16, train_loss_step=0.00841, val_loss=0.0106, train_loss_epoch=0.0108]\u001b[A\n",
      "Epoch 16:  97%|▉| 65/67 [00:27<00:00,  2.37it/s, loss=0.0126, v_num=16, train_loss_step=0.00841, val_loss=0.0106, train_loss_epoch=0.0108]\u001b[A\n",
      "Epoch 16:  99%|▉| 66/67 [00:28<00:00,  2.35it/s, loss=0.0126, v_num=16, train_loss_step=0.00841, val_loss=0.0106, train_loss_epoch=0.0108]\u001b[A\n",
      "Epoch 16: 100%|█| 67/67 [00:28<00:00,  2.31it/s, loss=0.0126, v_num=16, train_loss_step=0.00841, val_loss=0.0106, train_loss_epoch=0.0108]\u001b[A\n",
      "Epoch 17:  45%|▉ | 30/67 [00:04<00:05,  7.02it/s, loss=0.014, v_num=16, train_loss_step=0.00692, val_loss=0.0106, train_loss_epoch=0.0137]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 17:  46%|▉ | 31/67 [00:05<00:05,  6.17it/s, loss=0.014, v_num=16, train_loss_step=0.00692, val_loss=0.0106, train_loss_epoch=0.0137]\u001b[A\n",
      "Epoch 17:  48%|▉ | 32/67 [00:05<00:06,  5.64it/s, loss=0.014, v_num=16, train_loss_step=0.00692, val_loss=0.0106, train_loss_epoch=0.0137]\u001b[A\n",
      "Epoch 17:  49%|▉ | 33/67 [00:06<00:06,  5.25it/s, loss=0.014, v_num=16, train_loss_step=0.00692, val_loss=0.0106, train_loss_epoch=0.0137]\u001b[A\n",
      "Epoch 17:  51%|█ | 34/67 [00:06<00:06,  4.91it/s, loss=0.014, v_num=16, train_loss_step=0.00692, val_loss=0.0106, train_loss_epoch=0.0137]\u001b[A\n",
      "Epoch 17:  52%|█ | 35/67 [00:07<00:06,  4.63it/s, loss=0.014, v_num=16, train_loss_step=0.00692, val_loss=0.0106, train_loss_epoch=0.0137]\u001b[A\n",
      "Epoch 17:  54%|█ | 36/67 [00:08<00:07,  4.39it/s, loss=0.014, v_num=16, train_loss_step=0.00692, val_loss=0.0106, train_loss_epoch=0.0137]\u001b[A\n",
      "Epoch 17:  55%|█ | 37/67 [00:09<00:07,  4.10it/s, loss=0.014, v_num=16, train_loss_step=0.00692, val_loss=0.0106, train_loss_epoch=0.0137]\u001b[A\n",
      "Epoch 17:  57%|█▏| 38/67 [00:09<00:07,  3.93it/s, loss=0.014, v_num=16, train_loss_step=0.00692, val_loss=0.0106, train_loss_epoch=0.0137]\u001b[A\n",
      "Epoch 17:  58%|█▏| 39/67 [00:10<00:07,  3.79it/s, loss=0.014, v_num=16, train_loss_step=0.00692, val_loss=0.0106, train_loss_epoch=0.0137]\u001b[A\n",
      "Epoch 17:  60%|█▏| 40/67 [00:10<00:07,  3.66it/s, loss=0.014, v_num=16, train_loss_step=0.00692, val_loss=0.0106, train_loss_epoch=0.0137]\u001b[A\n",
      "Epoch 17:  61%|█▏| 41/67 [00:11<00:07,  3.55it/s, loss=0.014, v_num=16, train_loss_step=0.00692, val_loss=0.0106, train_loss_epoch=0.0137]\u001b[A\n",
      "Epoch 17:  63%|█▎| 42/67 [00:12<00:07,  3.45it/s, loss=0.014, v_num=16, train_loss_step=0.00692, val_loss=0.0106, train_loss_epoch=0.0137]\u001b[A\n",
      "Epoch 17:  64%|█▎| 43/67 [00:12<00:07,  3.37it/s, loss=0.014, v_num=16, train_loss_step=0.00692, val_loss=0.0106, train_loss_epoch=0.0137]\u001b[A\n",
      "Epoch 17:  66%|█▎| 44/67 [00:13<00:06,  3.29it/s, loss=0.014, v_num=16, train_loss_step=0.00692, val_loss=0.0106, train_loss_epoch=0.0137]\u001b[A\n",
      "Epoch 17:  67%|█▎| 45/67 [00:13<00:06,  3.21it/s, loss=0.014, v_num=16, train_loss_step=0.00692, val_loss=0.0106, train_loss_epoch=0.0137]\u001b[A\n",
      "Epoch 17:  69%|█▎| 46/67 [00:14<00:06,  3.14it/s, loss=0.014, v_num=16, train_loss_step=0.00692, val_loss=0.0106, train_loss_epoch=0.0137]\u001b[A\n",
      "Epoch 17:  70%|█▍| 47/67 [00:15<00:06,  3.08it/s, loss=0.014, v_num=16, train_loss_step=0.00692, val_loss=0.0106, train_loss_epoch=0.0137]\u001b[A\n",
      "Epoch 17:  72%|█▍| 48/67 [00:15<00:06,  3.01it/s, loss=0.014, v_num=16, train_loss_step=0.00692, val_loss=0.0106, train_loss_epoch=0.0137]\u001b[A\n",
      "Epoch 17:  73%|█▍| 49/67 [00:16<00:06,  2.96it/s, loss=0.014, v_num=16, train_loss_step=0.00692, val_loss=0.0106, train_loss_epoch=0.0137]\u001b[A\n",
      "Epoch 17:  75%|█▍| 50/67 [00:17<00:05,  2.92it/s, loss=0.014, v_num=16, train_loss_step=0.00692, val_loss=0.0106, train_loss_epoch=0.0137]\u001b[A\n",
      "Epoch 17:  76%|█▌| 51/67 [00:18<00:05,  2.83it/s, loss=0.014, v_num=16, train_loss_step=0.00692, val_loss=0.0106, train_loss_epoch=0.0137]\u001b[A\n",
      "Epoch 17:  78%|█▌| 52/67 [00:18<00:05,  2.78it/s, loss=0.014, v_num=16, train_loss_step=0.00692, val_loss=0.0106, train_loss_epoch=0.0137]\u001b[A\n",
      "Epoch 17:  79%|█▌| 53/67 [00:19<00:05,  2.74it/s, loss=0.014, v_num=16, train_loss_step=0.00692, val_loss=0.0106, train_loss_epoch=0.0137]\u001b[A\n",
      "Epoch 17:  81%|█▌| 54/67 [00:20<00:04,  2.70it/s, loss=0.014, v_num=16, train_loss_step=0.00692, val_loss=0.0106, train_loss_epoch=0.0137]\u001b[A\n",
      "Epoch 17:  82%|█▋| 55/67 [00:20<00:04,  2.66it/s, loss=0.014, v_num=16, train_loss_step=0.00692, val_loss=0.0106, train_loss_epoch=0.0137]\u001b[A\n",
      "Epoch 17:  84%|█▋| 56/67 [00:21<00:04,  2.63it/s, loss=0.014, v_num=16, train_loss_step=0.00692, val_loss=0.0106, train_loss_epoch=0.0137]\u001b[A\n",
      "Epoch 17:  85%|█▋| 57/67 [00:21<00:03,  2.60it/s, loss=0.014, v_num=16, train_loss_step=0.00692, val_loss=0.0106, train_loss_epoch=0.0137]\u001b[A\n",
      "Epoch 17:  87%|█▋| 58/67 [00:22<00:03,  2.57it/s, loss=0.014, v_num=16, train_loss_step=0.00692, val_loss=0.0106, train_loss_epoch=0.0137]\u001b[A\n",
      "Epoch 17:  88%|█▊| 59/67 [00:23<00:03,  2.54it/s, loss=0.014, v_num=16, train_loss_step=0.00692, val_loss=0.0106, train_loss_epoch=0.0137]\u001b[A\n",
      "Epoch 17:  90%|█▊| 60/67 [00:23<00:02,  2.52it/s, loss=0.014, v_num=16, train_loss_step=0.00692, val_loss=0.0106, train_loss_epoch=0.0137]\u001b[A\n",
      "Epoch 17:  91%|█▊| 61/67 [00:24<00:02,  2.49it/s, loss=0.014, v_num=16, train_loss_step=0.00692, val_loss=0.0106, train_loss_epoch=0.0137]\u001b[A\n",
      "Epoch 17:  93%|█▊| 62/67 [00:25<00:02,  2.47it/s, loss=0.014, v_num=16, train_loss_step=0.00692, val_loss=0.0106, train_loss_epoch=0.0137]\u001b[A\n",
      "Epoch 17:  94%|█▉| 63/67 [00:25<00:01,  2.44it/s, loss=0.014, v_num=16, train_loss_step=0.00692, val_loss=0.0106, train_loss_epoch=0.0137]\u001b[A\n",
      "Epoch 17:  96%|█▉| 64/67 [00:26<00:01,  2.40it/s, loss=0.014, v_num=16, train_loss_step=0.00692, val_loss=0.0106, train_loss_epoch=0.0137]\u001b[A\n",
      "Epoch 17:  97%|█▉| 65/67 [00:27<00:00,  2.38it/s, loss=0.014, v_num=16, train_loss_step=0.00692, val_loss=0.0106, train_loss_epoch=0.0137]\u001b[A\n",
      "Epoch 17:  99%|█▉| 66/67 [00:27<00:00,  2.36it/s, loss=0.014, v_num=16, train_loss_step=0.00692, val_loss=0.0106, train_loss_epoch=0.0137]\u001b[A\n",
      "Epoch 17: 100%|██| 67/67 [00:28<00:00,  2.33it/s, loss=0.014, v_num=16, train_loss_step=0.00692, val_loss=0.0106, train_loss_epoch=0.0137]\u001b[A\n",
      "Epoch 18:  45%|█▎ | 30/67 [00:04<00:05,  6.94it/s, loss=0.0122, v_num=16, train_loss_step=0.019, val_loss=0.0106, train_loss_epoch=0.0136]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 18:  46%|█▍ | 31/67 [00:05<00:05,  6.10it/s, loss=0.0122, v_num=16, train_loss_step=0.019, val_loss=0.0106, train_loss_epoch=0.0136]\u001b[A\n",
      "Epoch 18:  48%|█▍ | 32/67 [00:05<00:06,  5.56it/s, loss=0.0122, v_num=16, train_loss_step=0.019, val_loss=0.0106, train_loss_epoch=0.0136]\u001b[A\n",
      "Epoch 18:  49%|█▍ | 33/67 [00:06<00:06,  5.15it/s, loss=0.0122, v_num=16, train_loss_step=0.019, val_loss=0.0106, train_loss_epoch=0.0136]\u001b[A\n",
      "Epoch 18:  51%|█▌ | 34/67 [00:07<00:06,  4.82it/s, loss=0.0122, v_num=16, train_loss_step=0.019, val_loss=0.0106, train_loss_epoch=0.0136]\u001b[A\n",
      "Epoch 18:  52%|█▌ | 35/67 [00:07<00:07,  4.55it/s, loss=0.0122, v_num=16, train_loss_step=0.019, val_loss=0.0106, train_loss_epoch=0.0136]\u001b[A\n",
      "Epoch 18:  54%|█▌ | 36/67 [00:08<00:07,  4.32it/s, loss=0.0122, v_num=16, train_loss_step=0.019, val_loss=0.0106, train_loss_epoch=0.0136]\u001b[A\n",
      "Epoch 18:  55%|█▋ | 37/67 [00:09<00:07,  4.10it/s, loss=0.0122, v_num=16, train_loss_step=0.019, val_loss=0.0106, train_loss_epoch=0.0136]\u001b[A\n",
      "Epoch 18:  57%|█▋ | 38/67 [00:09<00:07,  3.93it/s, loss=0.0122, v_num=16, train_loss_step=0.019, val_loss=0.0106, train_loss_epoch=0.0136]\u001b[A\n",
      "Epoch 18:  58%|█▋ | 39/67 [00:10<00:07,  3.77it/s, loss=0.0122, v_num=16, train_loss_step=0.019, val_loss=0.0106, train_loss_epoch=0.0136]\u001b[A\n",
      "Epoch 18:  60%|█▊ | 40/67 [00:11<00:07,  3.57it/s, loss=0.0122, v_num=16, train_loss_step=0.019, val_loss=0.0106, train_loss_epoch=0.0136]\u001b[A\n",
      "Epoch 18:  61%|█▊ | 41/67 [00:11<00:07,  3.47it/s, loss=0.0122, v_num=16, train_loss_step=0.019, val_loss=0.0106, train_loss_epoch=0.0136]\u001b[A\n",
      "Epoch 18:  63%|█▉ | 42/67 [00:12<00:07,  3.38it/s, loss=0.0122, v_num=16, train_loss_step=0.019, val_loss=0.0106, train_loss_epoch=0.0136]\u001b[A\n",
      "Epoch 18:  64%|█▉ | 43/67 [00:13<00:07,  3.30it/s, loss=0.0122, v_num=16, train_loss_step=0.019, val_loss=0.0106, train_loss_epoch=0.0136]\u001b[A\n",
      "Epoch 18:  66%|█▉ | 44/67 [00:13<00:07,  3.22it/s, loss=0.0122, v_num=16, train_loss_step=0.019, val_loss=0.0106, train_loss_epoch=0.0136]\u001b[A\n",
      "Epoch 18:  67%|██ | 45/67 [00:14<00:06,  3.16it/s, loss=0.0122, v_num=16, train_loss_step=0.019, val_loss=0.0106, train_loss_epoch=0.0136]\u001b[A\n",
      "Epoch 18:  69%|██ | 46/67 [00:14<00:06,  3.10it/s, loss=0.0122, v_num=16, train_loss_step=0.019, val_loss=0.0106, train_loss_epoch=0.0136]\u001b[A\n",
      "Epoch 18:  70%|██ | 47/67 [00:15<00:06,  3.04it/s, loss=0.0122, v_num=16, train_loss_step=0.019, val_loss=0.0106, train_loss_epoch=0.0136]\u001b[A\n",
      "Epoch 18:  72%|██▏| 48/67 [00:16<00:06,  2.97it/s, loss=0.0122, v_num=16, train_loss_step=0.019, val_loss=0.0106, train_loss_epoch=0.0136]\u001b[A\n",
      "Epoch 18:  73%|██▏| 49/67 [00:16<00:06,  2.92it/s, loss=0.0122, v_num=16, train_loss_step=0.019, val_loss=0.0106, train_loss_epoch=0.0136]\u001b[A\n",
      "Epoch 18:  75%|██▏| 50/67 [00:17<00:05,  2.87it/s, loss=0.0122, v_num=16, train_loss_step=0.019, val_loss=0.0106, train_loss_epoch=0.0136]\u001b[A\n",
      "Epoch 18:  76%|██▎| 51/67 [00:18<00:05,  2.81it/s, loss=0.0122, v_num=16, train_loss_step=0.019, val_loss=0.0106, train_loss_epoch=0.0136]\u001b[A\n",
      "Epoch 18:  78%|██▎| 52/67 [00:18<00:05,  2.77it/s, loss=0.0122, v_num=16, train_loss_step=0.019, val_loss=0.0106, train_loss_epoch=0.0136]\u001b[A\n",
      "Epoch 18:  79%|██▎| 53/67 [00:19<00:05,  2.73it/s, loss=0.0122, v_num=16, train_loss_step=0.019, val_loss=0.0106, train_loss_epoch=0.0136]\u001b[A\n",
      "Epoch 18:  81%|██▍| 54/67 [00:20<00:04,  2.67it/s, loss=0.0122, v_num=16, train_loss_step=0.019, val_loss=0.0106, train_loss_epoch=0.0136]\u001b[A\n",
      "Epoch 18:  82%|██▍| 55/67 [00:20<00:04,  2.63it/s, loss=0.0122, v_num=16, train_loss_step=0.019, val_loss=0.0106, train_loss_epoch=0.0136]\u001b[A\n",
      "Epoch 18:  84%|██▌| 56/67 [00:21<00:04,  2.60it/s, loss=0.0122, v_num=16, train_loss_step=0.019, val_loss=0.0106, train_loss_epoch=0.0136]\u001b[A\n",
      "Epoch 18:  85%|██▌| 57/67 [00:22<00:03,  2.57it/s, loss=0.0122, v_num=16, train_loss_step=0.019, val_loss=0.0106, train_loss_epoch=0.0136]\u001b[A\n",
      "Epoch 18:  87%|██▌| 58/67 [00:22<00:03,  2.54it/s, loss=0.0122, v_num=16, train_loss_step=0.019, val_loss=0.0106, train_loss_epoch=0.0136]\u001b[A\n",
      "Epoch 18:  88%|██▋| 59/67 [00:23<00:03,  2.52it/s, loss=0.0122, v_num=16, train_loss_step=0.019, val_loss=0.0106, train_loss_epoch=0.0136]\u001b[A\n",
      "Epoch 18:  90%|██▋| 60/67 [00:24<00:02,  2.49it/s, loss=0.0122, v_num=16, train_loss_step=0.019, val_loss=0.0106, train_loss_epoch=0.0136]\u001b[A\n",
      "Epoch 18:  91%|██▋| 61/67 [00:24<00:02,  2.47it/s, loss=0.0122, v_num=16, train_loss_step=0.019, val_loss=0.0106, train_loss_epoch=0.0136]\u001b[A\n",
      "Epoch 18:  93%|██▊| 62/67 [00:25<00:02,  2.44it/s, loss=0.0122, v_num=16, train_loss_step=0.019, val_loss=0.0106, train_loss_epoch=0.0136]\u001b[A\n",
      "Epoch 18:  94%|██▊| 63/67 [00:26<00:01,  2.42it/s, loss=0.0122, v_num=16, train_loss_step=0.019, val_loss=0.0106, train_loss_epoch=0.0136]\u001b[A\n",
      "Epoch 18:  96%|██▊| 64/67 [00:26<00:01,  2.40it/s, loss=0.0122, v_num=16, train_loss_step=0.019, val_loss=0.0106, train_loss_epoch=0.0136]\u001b[A\n",
      "Epoch 18:  97%|██▉| 65/67 [00:27<00:00,  2.38it/s, loss=0.0122, v_num=16, train_loss_step=0.019, val_loss=0.0106, train_loss_epoch=0.0136]\u001b[A\n",
      "Epoch 18:  99%|██▉| 66/67 [00:27<00:00,  2.36it/s, loss=0.0122, v_num=16, train_loss_step=0.019, val_loss=0.0106, train_loss_epoch=0.0136]\u001b[A\n",
      "Epoch 18: 100%|███| 67/67 [00:28<00:00,  2.32it/s, loss=0.0122, v_num=16, train_loss_step=0.019, val_loss=0.0106, train_loss_epoch=0.0136]\u001b[A\n",
      "Epoch 19:  45%|█▎ | 30/67 [00:04<00:05,  6.65it/s, loss=0.012, v_num=16, train_loss_step=0.0219, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 19:  46%|█▍ | 31/67 [00:05<00:06,  5.88it/s, loss=0.012, v_num=16, train_loss_step=0.0219, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 19:  48%|█▍ | 32/67 [00:05<00:06,  5.41it/s, loss=0.012, v_num=16, train_loss_step=0.0219, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 19:  49%|█▍ | 33/67 [00:06<00:06,  5.03it/s, loss=0.012, v_num=16, train_loss_step=0.0219, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 19:  51%|█▌ | 34/67 [00:07<00:06,  4.72it/s, loss=0.012, v_num=16, train_loss_step=0.0219, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 19:  52%|█▌ | 35/67 [00:07<00:07,  4.46it/s, loss=0.012, v_num=16, train_loss_step=0.0219, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 19:  54%|█▌ | 36/67 [00:08<00:07,  4.24it/s, loss=0.012, v_num=16, train_loss_step=0.0219, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 19:  55%|█▋ | 37/67 [00:09<00:07,  4.05it/s, loss=0.012, v_num=16, train_loss_step=0.0219, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 19:  57%|█▋ | 38/67 [00:09<00:07,  3.88it/s, loss=0.012, v_num=16, train_loss_step=0.0219, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 19:  58%|█▋ | 39/67 [00:10<00:07,  3.74it/s, loss=0.012, v_num=16, train_loss_step=0.0219, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 19:  60%|█▊ | 40/67 [00:11<00:07,  3.61it/s, loss=0.012, v_num=16, train_loss_step=0.0219, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 19:  61%|█▊ | 41/67 [00:11<00:07,  3.50it/s, loss=0.012, v_num=16, train_loss_step=0.0219, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 19:  63%|█▉ | 42/67 [00:12<00:07,  3.41it/s, loss=0.012, v_num=16, train_loss_step=0.0219, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 19:  64%|█▉ | 43/67 [00:12<00:07,  3.32it/s, loss=0.012, v_num=16, train_loss_step=0.0219, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 19:  66%|█▉ | 44/67 [00:13<00:07,  3.20it/s, loss=0.012, v_num=16, train_loss_step=0.0219, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 19:  67%|██ | 45/67 [00:14<00:07,  3.14it/s, loss=0.012, v_num=16, train_loss_step=0.0219, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 19:  69%|██ | 46/67 [00:14<00:06,  3.07it/s, loss=0.012, v_num=16, train_loss_step=0.0219, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 19:  70%|██ | 47/67 [00:15<00:06,  3.02it/s, loss=0.012, v_num=16, train_loss_step=0.0219, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 19:  72%|██▏| 48/67 [00:16<00:06,  2.95it/s, loss=0.012, v_num=16, train_loss_step=0.0219, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 19:  73%|██▏| 49/67 [00:16<00:06,  2.90it/s, loss=0.012, v_num=16, train_loss_step=0.0219, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 19:  75%|██▏| 50/67 [00:17<00:05,  2.86it/s, loss=0.012, v_num=16, train_loss_step=0.0219, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 19:  76%|██▎| 51/67 [00:18<00:05,  2.80it/s, loss=0.012, v_num=16, train_loss_step=0.0219, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 19:  78%|██▎| 52/67 [00:18<00:05,  2.76it/s, loss=0.012, v_num=16, train_loss_step=0.0219, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 19:  79%|██▎| 53/67 [00:19<00:05,  2.72it/s, loss=0.012, v_num=16, train_loss_step=0.0219, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 19:  81%|██▍| 54/67 [00:20<00:04,  2.68it/s, loss=0.012, v_num=16, train_loss_step=0.0219, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 19:  82%|██▍| 55/67 [00:20<00:04,  2.64it/s, loss=0.012, v_num=16, train_loss_step=0.0219, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 19:  84%|██▌| 56/67 [00:21<00:04,  2.61it/s, loss=0.012, v_num=16, train_loss_step=0.0219, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 19:  85%|██▌| 57/67 [00:22<00:03,  2.58it/s, loss=0.012, v_num=16, train_loss_step=0.0219, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 19:  87%|██▌| 58/67 [00:22<00:03,  2.55it/s, loss=0.012, v_num=16, train_loss_step=0.0219, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 19:  88%|██▋| 59/67 [00:23<00:03,  2.50it/s, loss=0.012, v_num=16, train_loss_step=0.0219, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 19:  90%|██▋| 60/67 [00:24<00:02,  2.48it/s, loss=0.012, v_num=16, train_loss_step=0.0219, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 19:  91%|██▋| 61/67 [00:24<00:02,  2.45it/s, loss=0.012, v_num=16, train_loss_step=0.0219, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 19:  93%|██▊| 62/67 [00:25<00:02,  2.43it/s, loss=0.012, v_num=16, train_loss_step=0.0219, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 19:  94%|██▊| 63/67 [00:26<00:01,  2.40it/s, loss=0.012, v_num=16, train_loss_step=0.0219, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 19:  96%|██▊| 64/67 [00:26<00:01,  2.38it/s, loss=0.012, v_num=16, train_loss_step=0.0219, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 19:  97%|██▉| 65/67 [00:27<00:00,  2.37it/s, loss=0.012, v_num=16, train_loss_step=0.0219, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 19:  99%|██▉| 66/67 [00:28<00:00,  2.34it/s, loss=0.012, v_num=16, train_loss_step=0.0219, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 19: 100%|███| 67/67 [00:28<00:00,  2.31it/s, loss=0.012, v_num=16, train_loss_step=0.0219, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 20:  45%|▉ | 30/67 [00:04<00:05,  6.99it/s, loss=0.0143, v_num=16, train_loss_step=0.0111, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 20:  46%|▉ | 31/67 [00:05<00:05,  6.15it/s, loss=0.0143, v_num=16, train_loss_step=0.0111, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 20:  48%|▉ | 32/67 [00:05<00:06,  5.63it/s, loss=0.0143, v_num=16, train_loss_step=0.0111, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 20:  49%|▉ | 33/67 [00:06<00:06,  5.23it/s, loss=0.0143, v_num=16, train_loss_step=0.0111, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 20:  51%|█ | 34/67 [00:06<00:06,  4.89it/s, loss=0.0143, v_num=16, train_loss_step=0.0111, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 20:  52%|█ | 35/67 [00:07<00:07,  4.50it/s, loss=0.0143, v_num=16, train_loss_step=0.0111, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 20:  54%|█ | 36/67 [00:08<00:07,  4.27it/s, loss=0.0143, v_num=16, train_loss_step=0.0111, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 20:  55%|█ | 37/67 [00:09<00:07,  4.06it/s, loss=0.0143, v_num=16, train_loss_step=0.0111, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 20:  57%|█▏| 38/67 [00:09<00:07,  3.89it/s, loss=0.0143, v_num=16, train_loss_step=0.0111, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 20:  58%|█▏| 39/67 [00:10<00:07,  3.75it/s, loss=0.0143, v_num=16, train_loss_step=0.0111, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 20:  60%|█▏| 40/67 [00:11<00:07,  3.61it/s, loss=0.0143, v_num=16, train_loss_step=0.0111, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 20:  61%|█▏| 41/67 [00:11<00:07,  3.50it/s, loss=0.0143, v_num=16, train_loss_step=0.0111, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 20:  63%|█▎| 42/67 [00:12<00:07,  3.40it/s, loss=0.0143, v_num=16, train_loss_step=0.0111, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 20:  64%|█▎| 43/67 [00:12<00:07,  3.32it/s, loss=0.0143, v_num=16, train_loss_step=0.0111, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 20:  66%|█▎| 44/67 [00:13<00:07,  3.25it/s, loss=0.0143, v_num=16, train_loss_step=0.0111, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 20:  67%|█▎| 45/67 [00:14<00:06,  3.18it/s, loss=0.0143, v_num=16, train_loss_step=0.0111, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 20:  69%|█▎| 46/67 [00:14<00:06,  3.11it/s, loss=0.0143, v_num=16, train_loss_step=0.0111, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 20:  70%|█▍| 47/67 [00:15<00:06,  3.05it/s, loss=0.0143, v_num=16, train_loss_step=0.0111, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 20:  72%|█▍| 48/67 [00:16<00:06,  2.97it/s, loss=0.0143, v_num=16, train_loss_step=0.0111, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 20:  73%|█▍| 49/67 [00:16<00:06,  2.93it/s, loss=0.0143, v_num=16, train_loss_step=0.0111, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 20:  75%|█▍| 50/67 [00:17<00:05,  2.85it/s, loss=0.0143, v_num=16, train_loss_step=0.0111, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 20:  76%|█▌| 51/67 [00:18<00:05,  2.79it/s, loss=0.0143, v_num=16, train_loss_step=0.0111, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 20:  78%|█▌| 52/67 [00:18<00:05,  2.74it/s, loss=0.0143, v_num=16, train_loss_step=0.0111, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 20:  79%|█▌| 53/67 [00:19<00:05,  2.70it/s, loss=0.0143, v_num=16, train_loss_step=0.0111, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 20:  81%|█▌| 54/67 [00:20<00:04,  2.67it/s, loss=0.0143, v_num=16, train_loss_step=0.0111, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 20:  82%|█▋| 55/67 [00:20<00:04,  2.63it/s, loss=0.0143, v_num=16, train_loss_step=0.0111, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 20:  84%|█▋| 56/67 [00:21<00:04,  2.60it/s, loss=0.0143, v_num=16, train_loss_step=0.0111, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 20:  85%|█▋| 57/67 [00:22<00:03,  2.57it/s, loss=0.0143, v_num=16, train_loss_step=0.0111, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 20:  87%|█▋| 58/67 [00:22<00:03,  2.54it/s, loss=0.0143, v_num=16, train_loss_step=0.0111, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 20:  88%|█▊| 59/67 [00:23<00:03,  2.52it/s, loss=0.0143, v_num=16, train_loss_step=0.0111, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 20:  90%|█▊| 60/67 [00:24<00:02,  2.49it/s, loss=0.0143, v_num=16, train_loss_step=0.0111, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 20:  91%|█▊| 61/67 [00:24<00:02,  2.46it/s, loss=0.0143, v_num=16, train_loss_step=0.0111, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 20:  93%|█▊| 62/67 [00:25<00:02,  2.44it/s, loss=0.0143, v_num=16, train_loss_step=0.0111, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 20:  94%|█▉| 63/67 [00:26<00:01,  2.42it/s, loss=0.0143, v_num=16, train_loss_step=0.0111, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 20:  96%|█▉| 64/67 [00:26<00:01,  2.40it/s, loss=0.0143, v_num=16, train_loss_step=0.0111, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 20:  97%|█▉| 65/67 [00:27<00:00,  2.36it/s, loss=0.0143, v_num=16, train_loss_step=0.0111, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 20:  99%|█▉| 66/67 [00:28<00:00,  2.34it/s, loss=0.0143, v_num=16, train_loss_step=0.0111, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 20: 100%|██| 67/67 [00:28<00:00,  2.31it/s, loss=0.0143, v_num=16, train_loss_step=0.0111, val_loss=0.0106, train_loss_epoch=0.0115]\u001b[A\n",
      "Epoch 21:  45%|▉ | 30/67 [00:04<00:05,  6.90it/s, loss=0.0161, v_num=16, train_loss_step=0.0105, val_loss=0.0106, train_loss_epoch=0.0117]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 21:  46%|▉ | 31/67 [00:05<00:05,  6.08it/s, loss=0.0161, v_num=16, train_loss_step=0.0105, val_loss=0.0106, train_loss_epoch=0.0117]\u001b[A\n",
      "Epoch 21:  48%|▉ | 32/67 [00:05<00:06,  5.57it/s, loss=0.0161, v_num=16, train_loss_step=0.0105, val_loss=0.0106, train_loss_epoch=0.0117]\u001b[A\n",
      "Epoch 21:  49%|▉ | 33/67 [00:06<00:06,  5.18it/s, loss=0.0161, v_num=16, train_loss_step=0.0105, val_loss=0.0106, train_loss_epoch=0.0117]\u001b[A\n",
      "Epoch 21:  51%|█ | 34/67 [00:07<00:06,  4.84it/s, loss=0.0161, v_num=16, train_loss_step=0.0105, val_loss=0.0106, train_loss_epoch=0.0117]\u001b[A\n",
      "Epoch 21:  52%|█ | 35/67 [00:07<00:07,  4.56it/s, loss=0.0161, v_num=16, train_loss_step=0.0105, val_loss=0.0106, train_loss_epoch=0.0117]\u001b[A\n",
      "Epoch 21:  54%|█ | 36/67 [00:08<00:07,  4.34it/s, loss=0.0161, v_num=16, train_loss_step=0.0105, val_loss=0.0106, train_loss_epoch=0.0117]\u001b[A\n",
      "Epoch 21:  55%|█ | 37/67 [00:08<00:07,  4.14it/s, loss=0.0161, v_num=16, train_loss_step=0.0105, val_loss=0.0106, train_loss_epoch=0.0117]\u001b[A\n",
      "Epoch 21:  57%|█▏| 38/67 [00:09<00:07,  3.97it/s, loss=0.0161, v_num=16, train_loss_step=0.0105, val_loss=0.0106, train_loss_epoch=0.0117]\u001b[A\n",
      "Epoch 21:  58%|█▏| 39/67 [00:10<00:07,  3.83it/s, loss=0.0161, v_num=16, train_loss_step=0.0105, val_loss=0.0106, train_loss_epoch=0.0117]\u001b[A\n",
      "Epoch 21:  60%|█▏| 40/67 [00:10<00:07,  3.70it/s, loss=0.0161, v_num=16, train_loss_step=0.0105, val_loss=0.0106, train_loss_epoch=0.0117]\u001b[A\n",
      "Epoch 21:  61%|█▏| 41/67 [00:11<00:07,  3.58it/s, loss=0.0161, v_num=16, train_loss_step=0.0105, val_loss=0.0106, train_loss_epoch=0.0117]\u001b[A\n",
      "Epoch 21:  63%|█▎| 42/67 [00:12<00:07,  3.42it/s, loss=0.0161, v_num=16, train_loss_step=0.0105, val_loss=0.0106, train_loss_epoch=0.0117]\u001b[A\n",
      "Epoch 21:  64%|█▎| 43/67 [00:12<00:07,  3.33it/s, loss=0.0161, v_num=16, train_loss_step=0.0105, val_loss=0.0106, train_loss_epoch=0.0117]\u001b[A\n",
      "Epoch 21:  66%|█▎| 44/67 [00:13<00:07,  3.25it/s, loss=0.0161, v_num=16, train_loss_step=0.0105, val_loss=0.0106, train_loss_epoch=0.0117]\u001b[A\n",
      "Epoch 21:  67%|█▎| 45/67 [00:14<00:06,  3.18it/s, loss=0.0161, v_num=16, train_loss_step=0.0105, val_loss=0.0106, train_loss_epoch=0.0117]\u001b[A\n",
      "Epoch 21:  69%|█▎| 46/67 [00:14<00:06,  3.11it/s, loss=0.0161, v_num=16, train_loss_step=0.0105, val_loss=0.0106, train_loss_epoch=0.0117]\u001b[A\n",
      "Epoch 21:  70%|█▍| 47/67 [00:15<00:06,  3.05it/s, loss=0.0161, v_num=16, train_loss_step=0.0105, val_loss=0.0106, train_loss_epoch=0.0117]\u001b[A\n",
      "Epoch 21:  72%|█▍| 48/67 [00:16<00:06,  2.98it/s, loss=0.0161, v_num=16, train_loss_step=0.0105, val_loss=0.0106, train_loss_epoch=0.0117]\u001b[A\n",
      "Epoch 21:  73%|█▍| 49/67 [00:16<00:06,  2.93it/s, loss=0.0161, v_num=16, train_loss_step=0.0105, val_loss=0.0106, train_loss_epoch=0.0117]\u001b[A\n",
      "Epoch 21:  75%|█▍| 50/67 [00:17<00:05,  2.88it/s, loss=0.0161, v_num=16, train_loss_step=0.0105, val_loss=0.0106, train_loss_epoch=0.0117]\u001b[A\n",
      "Epoch 21:  76%|█▌| 51/67 [00:18<00:05,  2.82it/s, loss=0.0161, v_num=16, train_loss_step=0.0105, val_loss=0.0106, train_loss_epoch=0.0117]\u001b[A\n",
      "Epoch 21:  78%|█▌| 52/67 [00:18<00:05,  2.77it/s, loss=0.0161, v_num=16, train_loss_step=0.0105, val_loss=0.0106, train_loss_epoch=0.0117]\u001b[A\n",
      "Epoch 21:  79%|█▌| 53/67 [00:19<00:05,  2.73it/s, loss=0.0161, v_num=16, train_loss_step=0.0105, val_loss=0.0106, train_loss_epoch=0.0117]\u001b[A\n",
      "Epoch 21:  81%|█▌| 54/67 [00:20<00:04,  2.69it/s, loss=0.0161, v_num=16, train_loss_step=0.0105, val_loss=0.0106, train_loss_epoch=0.0117]\u001b[A\n",
      "Epoch 21:  82%|█▋| 55/67 [00:20<00:04,  2.66it/s, loss=0.0161, v_num=16, train_loss_step=0.0105, val_loss=0.0106, train_loss_epoch=0.0117]\u001b[A\n",
      "Epoch 21:  84%|█▋| 56/67 [00:21<00:04,  2.63it/s, loss=0.0161, v_num=16, train_loss_step=0.0105, val_loss=0.0106, train_loss_epoch=0.0117]\u001b[A\n",
      "Epoch 21:  85%|█▋| 57/67 [00:22<00:03,  2.57it/s, loss=0.0161, v_num=16, train_loss_step=0.0105, val_loss=0.0106, train_loss_epoch=0.0117]\u001b[A\n",
      "Epoch 21:  87%|█▋| 58/67 [00:22<00:03,  2.54it/s, loss=0.0161, v_num=16, train_loss_step=0.0105, val_loss=0.0106, train_loss_epoch=0.0117]\u001b[A\n",
      "Epoch 21:  88%|█▊| 59/67 [00:23<00:03,  2.52it/s, loss=0.0161, v_num=16, train_loss_step=0.0105, val_loss=0.0106, train_loss_epoch=0.0117]\u001b[A\n",
      "Epoch 21:  90%|█▊| 60/67 [00:24<00:02,  2.49it/s, loss=0.0161, v_num=16, train_loss_step=0.0105, val_loss=0.0106, train_loss_epoch=0.0117]\u001b[A\n",
      "Epoch 21:  91%|█▊| 61/67 [00:24<00:02,  2.46it/s, loss=0.0161, v_num=16, train_loss_step=0.0105, val_loss=0.0106, train_loss_epoch=0.0117]\u001b[A\n",
      "Epoch 21:  93%|█▊| 62/67 [00:25<00:02,  2.44it/s, loss=0.0161, v_num=16, train_loss_step=0.0105, val_loss=0.0106, train_loss_epoch=0.0117]\u001b[A\n",
      "Epoch 21:  94%|█▉| 63/67 [00:26<00:01,  2.42it/s, loss=0.0161, v_num=16, train_loss_step=0.0105, val_loss=0.0106, train_loss_epoch=0.0117]\u001b[A\n",
      "Epoch 21:  96%|█▉| 64/67 [00:26<00:01,  2.39it/s, loss=0.0161, v_num=16, train_loss_step=0.0105, val_loss=0.0106, train_loss_epoch=0.0117]\u001b[A\n",
      "Epoch 21:  97%|█▉| 65/67 [00:27<00:00,  2.38it/s, loss=0.0161, v_num=16, train_loss_step=0.0105, val_loss=0.0106, train_loss_epoch=0.0117]\u001b[A\n",
      "Epoch 21:  99%|█▉| 66/67 [00:27<00:00,  2.36it/s, loss=0.0161, v_num=16, train_loss_step=0.0105, val_loss=0.0106, train_loss_epoch=0.0117]\u001b[A\n",
      "Epoch 21: 100%|██| 67/67 [00:28<00:00,  2.32it/s, loss=0.0161, v_num=16, train_loss_step=0.0105, val_loss=0.0106, train_loss_epoch=0.0117]\u001b[A\n",
      "Epoch 22:  45%|▍| 30/67 [00:04<00:05,  6.85it/s, loss=0.0128, v_num=16, train_loss_step=0.00758, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 22:  46%|▍| 31/67 [00:05<00:05,  6.04it/s, loss=0.0128, v_num=16, train_loss_step=0.00758, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 22:  48%|▍| 32/67 [00:05<00:06,  5.51it/s, loss=0.0128, v_num=16, train_loss_step=0.00758, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 22:  49%|▍| 33/67 [00:06<00:06,  5.14it/s, loss=0.0128, v_num=16, train_loss_step=0.00758, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 22:  51%|▌| 34/67 [00:07<00:07,  4.67it/s, loss=0.0128, v_num=16, train_loss_step=0.00758, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 22:  52%|▌| 35/67 [00:07<00:07,  4.41it/s, loss=0.0128, v_num=16, train_loss_step=0.00758, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 22:  54%|▌| 36/67 [00:08<00:07,  4.19it/s, loss=0.0128, v_num=16, train_loss_step=0.00758, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 22:  55%|▌| 37/67 [00:09<00:07,  4.01it/s, loss=0.0128, v_num=16, train_loss_step=0.00758, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 22:  57%|▌| 38/67 [00:09<00:07,  3.85it/s, loss=0.0128, v_num=16, train_loss_step=0.00758, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 22:  58%|▌| 39/67 [00:10<00:07,  3.71it/s, loss=0.0128, v_num=16, train_loss_step=0.00758, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 22:  60%|▌| 40/67 [00:11<00:07,  3.59it/s, loss=0.0128, v_num=16, train_loss_step=0.00758, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 22:  61%|▌| 41/67 [00:11<00:07,  3.48it/s, loss=0.0128, v_num=16, train_loss_step=0.00758, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 22:  63%|▋| 42/67 [00:12<00:07,  3.39it/s, loss=0.0128, v_num=16, train_loss_step=0.00758, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 22:  64%|▋| 43/67 [00:13<00:07,  3.30it/s, loss=0.0128, v_num=16, train_loss_step=0.00758, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 22:  66%|▋| 44/67 [00:13<00:07,  3.23it/s, loss=0.0128, v_num=16, train_loss_step=0.00758, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 22:  67%|▋| 45/67 [00:14<00:06,  3.16it/s, loss=0.0128, v_num=16, train_loss_step=0.00758, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 22:  69%|▋| 46/67 [00:14<00:06,  3.08it/s, loss=0.0128, v_num=16, train_loss_step=0.00758, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 22:  70%|▋| 47/67 [00:15<00:06,  3.02it/s, loss=0.0128, v_num=16, train_loss_step=0.00758, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 22:  72%|▋| 48/67 [00:16<00:06,  2.95it/s, loss=0.0128, v_num=16, train_loss_step=0.00758, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 22:  73%|▋| 49/67 [00:16<00:06,  2.91it/s, loss=0.0128, v_num=16, train_loss_step=0.00758, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 22:  75%|▋| 50/67 [00:17<00:06,  2.83it/s, loss=0.0128, v_num=16, train_loss_step=0.00758, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 22:  76%|▊| 51/67 [00:18<00:05,  2.76it/s, loss=0.0128, v_num=16, train_loss_step=0.00758, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 22:  78%|▊| 52/67 [00:19<00:05,  2.71it/s, loss=0.0128, v_num=16, train_loss_step=0.00758, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 22:  79%|▊| 53/67 [00:19<00:05,  2.68it/s, loss=0.0128, v_num=16, train_loss_step=0.00758, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 22:  81%|▊| 54/67 [00:20<00:04,  2.63it/s, loss=0.0128, v_num=16, train_loss_step=0.00758, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 22:  82%|▊| 55/67 [00:21<00:04,  2.60it/s, loss=0.0128, v_num=16, train_loss_step=0.00758, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 22:  84%|▊| 56/67 [00:21<00:04,  2.57it/s, loss=0.0128, v_num=16, train_loss_step=0.00758, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 22:  85%|▊| 57/67 [00:22<00:03,  2.54it/s, loss=0.0128, v_num=16, train_loss_step=0.00758, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 22:  87%|▊| 58/67 [00:23<00:03,  2.51it/s, loss=0.0128, v_num=16, train_loss_step=0.00758, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 22:  88%|▉| 59/67 [00:23<00:03,  2.48it/s, loss=0.0128, v_num=16, train_loss_step=0.00758, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 22:  90%|▉| 60/67 [00:24<00:02,  2.45it/s, loss=0.0128, v_num=16, train_loss_step=0.00758, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 22:  91%|▉| 61/67 [00:25<00:02,  2.42it/s, loss=0.0128, v_num=16, train_loss_step=0.00758, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 22:  93%|▉| 62/67 [00:25<00:02,  2.40it/s, loss=0.0128, v_num=16, train_loss_step=0.00758, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 22:  94%|▉| 63/67 [00:26<00:01,  2.37it/s, loss=0.0128, v_num=16, train_loss_step=0.00758, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 22:  96%|▉| 64/67 [00:27<00:01,  2.35it/s, loss=0.0128, v_num=16, train_loss_step=0.00758, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 22:  97%|▉| 65/67 [00:27<00:00,  2.34it/s, loss=0.0128, v_num=16, train_loss_step=0.00758, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 22:  99%|▉| 66/67 [00:28<00:00,  2.30it/s, loss=0.0128, v_num=16, train_loss_step=0.00758, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 22: 100%|█| 67/67 [00:29<00:00,  2.27it/s, loss=0.0128, v_num=16, train_loss_step=0.00758, val_loss=0.0106, train_loss_epoch=0.0132]\u001b[A\n",
      "Epoch 23:  45%|▍| 30/67 [00:04<00:05,  6.55it/s, loss=0.00932, v_num=16, train_loss_step=0.00775, val_loss=0.0106, train_loss_epoch=0.0117\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 23:  46%|▍| 31/67 [00:05<00:06,  5.78it/s, loss=0.00932, v_num=16, train_loss_step=0.00775, val_loss=0.0106, train_loss_epoch=0.0117\u001b[A\n",
      "Epoch 23:  48%|▍| 32/67 [00:06<00:06,  5.32it/s, loss=0.00932, v_num=16, train_loss_step=0.00775, val_loss=0.0106, train_loss_epoch=0.0117\u001b[A"
     ]
    }
   ],
   "source": [
    "# fit network\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/cristian/Extreme SSD/Investigacion/DATATHONES/entel-2022/DATATHON-ENTEL-2022---Reto2/results/models/tft/MODEL_tft-tweedie-loss-epoch_21-val_loss_0.011.ckpt\n",
      "best_model_name =  MODEL_tft-tweedie-loss-epoch_21-val_loss_0.011.ckpt\n"
     ]
    }
   ],
   "source": [
    "# load the best model according to the validation loss\n",
    "# (given that we use early stopping, this is not necessarily the last epoch)\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "print(best_model_path)\n",
    "best_model_name = best_model_path.split('/')[-1]\n",
    "print('best_model_name = ',best_model_name)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# raw predictions are a dictionary from which all kind of information including quantiles can be extracted\n",
    "raw_predictions, x = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)\n",
    "print('hola')\n",
    "\n",
    "for idx in range(5):  # plot 10 examples\n",
    "    best_tft.plot_prediction(x, raw_predictions, idx=idx, add_loss_to_title=True);\n",
    "    \n",
    "predictions, x = best_tft.predict(val_dataloader, return_x=True)\n",
    "predictions_vs_actuals = best_tft.calculate_prediction_actual_by_variable(x, predictions)\n",
    "best_tft.plot_prediction_actual_by_variable(predictions_vs_actuals);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse_val =  0.014899672009050846\n"
     ]
    }
   ],
   "source": [
    "# calcualte root mean squared error on validation set\n",
    "actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
    "val_predictions = best_tft.predict(val_dataloader)\n",
    "criterion = nn.MSELoss()\n",
    "rmse_val = torch.sqrt(criterion(actuals,val_predictions)).item()\n",
    "print('rmse_val = ',rmse_val)\n",
    "#rmse_val =  4.774808883666992"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='ratio'>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAEGCAYAAABbzE8LAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAPbklEQVR4nO3df2xd91nH8c9T+4YRAk3rRDRsYR4K/2yIH22oSvihVCSSXVUriAn1D3CKhJBAJPQHVRFx2pg47R+tWqlB1VRt0xyEYNUYqESxUQKpQEjrcNt16WKwTeomgVTL7hJvWe3k2n74495zub4+N7735vo+567vl2T1nHu+P55zknz89bnHt+buAgC03y3RBQDAhxUBDABBCGAACEIAA0AQAhgAgnQ30njTpk3e29u7RqUAwA+mN95449vuvrn69YYCuLe3V+Pj462rCgA+BMzsvbTXuQUBAEEIYAAIQgADQBACGACCEMAAEIQABoAgBDAABCGAASAIAQwAQQhgAAhCAANAEAIYAIIQwAAQhAAGgCAEMAAEIYABIAgBDABBCGAACEIAA0CQtgTwkSNHdOTIkXZMBQAdoy0BPDY2prGxsXZMBQAdg1sQABCEAAaAIAQwAAQhgAEgCAEMAEEIYAAIQgADQBACGACCEMAAEIQABoAgBDAABCGAASAIAQwAQQhgAAhCAANAEAIYAIIQwAAQhAAGgCAEMAAEIYABIAgBDABBCGAACEIAA0AQAhgAghDAABCEAAaAIAQwAAQhgAEgCAEMAEEIYAAIQgADQBACGACCEMAAEIQABoAgBDAABCGAASAIAQwAQQhgAAhCAANAEAIYAIIQwAAQhAAGgCDd7Zjkgw8+aMc0ANBR2hLA7t6OaQCgo3ALAgCCEMAAEIQABoAgBDAABCGAASAIAQwAQQhgAAhCAANAEAIYAIIQwAAQhAAGgCAEMAAEIYABIAgBDABBCGAACEIAA0AQAhgAghDAABCEAAaAIAQwAAQhgAEgCAEMAEEIYAAIQgADQBACGACCEMAAEIQABoAgBDAABCGAASAIAQwAQQhgAAhCAANAEAIYAIIQwAAQhAAGgCAEMAAEIYABIAgBDABButs52c6dO9s53Q+crq4uLS4uSpI2btyoK1euyMzk7jIzdXV1aWFhYVnb2267TZcvX5aZad26dbr99tt18eJF5XI5LSws6JFHHtFLL72kpaUlFQoFPfnkk3rllVd07do1Xbx4UY8//rieffZZbdmyRUtLS3r//fclSVu3btUTTzyhgwcP6sKFC7r11ls1Ozurxx57TDt27NDQ0JAGBgZ04MABbd68WZcuXdIdd9yhXC4nd9e1a9d0/vx5Pffcc7rrrruUz+c1NDSkp556SpJ04MABXb9+XevWrdOhQ4fU09MjScrn8xocHJSZ6dChQ5K0rF+ynbSXpOnpae3du1dbt27VM888s6JdMubCwoJyudyy+dJU1pr0Hxoa0r59+/Tiiy8um7+67WqSWhYXF9XV1aXh4eFV+zU6RzPjNDNHq+qKtpbnwQq4gyThK0lXrlyRJLl7+b9J+Fa2vXz5cvl4EqqSVCgU5O564YUXND8/r+vXr8vddfjwYU1MTOjs2bOam5vT008/rbm5OZ09e1YzMzOan5/X/Py8pqamNDw8rAsXLkiSZmdnJUnPP/+8RkZGdPr0aR08eFBzc3M6d+6c5ubm9O6772pyclJTU1M6d+6c3L0cnEmfo0ePamRkRGfOnNH09LTOnDmjo0ePls9rZGREExMT5der+yXblYaHhzU3N6fJycnUdsmYU1NTK+ZLk9b/9OnTGh4eXjF/rZpuNPbExIQmJyc1MTFRV79G52hmnGbmaFVd0dbyPNY8gFn1ZlsS4InKEE/brzQzM5M63rFjx+Tuunr16qrzX716VadOndLY2JjcXaOjoxodHV3WZnR0VPl8Xvl8ftmx48ePa3R0dFk/d9fY2Jjy+byk4uq3ss7KPmNjY5qenq45X5p8Pl+uNemf7M/MzCybv7ptrTErx26klrR6VpujmXGamaNVdUVb6/NgBYyWW1paaqj94cOHy30KhYIKhcKy44VCobxyrfyGUCgUyvuV24uLi+XVyvDw8IqxKtsNDw+v+CaTzJdmZGSkXGvSv/p8k/mr29azsm6klrR6ml2l3WicZuZoVV3R1vo8Vg1gM/sDMxs3s/FLly61dHJAKq6yk+CpXpEnr504cUInT55ccbzyFkyyvbCwoBMnTkiqvUpP2iWr1rT50pw8ebJca9I/7aeGpN7KtrXGrBy7kVrS6lltjmbGaWaOVtUVba3PY9UAdveX3X27u2/fvHlzSycHJKm7u1vd3cX3g81sxXEz0+7du7Vr164Vx5N9Mytvd3d3a/fu3ZKk3t7e1PGSdr29valjJv2r7dq1q1xr0j/ZrzyfpN7KtrXGrBy7kVrS6lltjmbGaWaOVtUVba3Pg1sQaLlbbmnsr9X+/fvLfXK5nHK53LLjuVxOAwMD2rNnz7Kwy+Vy5f3K7a6uLg0MDEiSBgcHV4xV2W5wcHBFgCbzpdmzZ0+51qR/9fkm81e3rTVm5diN1JJWz2pzNDNOM3O0qq5oa30eax7Ar7322lpPgZtQveJKW83VUmt1ef/998vMtGHDhlXn37Bhg+6991719fXJzNTf36/+/v5lbfr7+9XT06Oenp5lx+677z719/cv62dm6uvrKz8utG3btmV1Vvbp6+vTtm3bas6Xpqenp1xr0j/ZT1bTyfzVbVd7hKn6/FarJa2eZh+TutE4zczRqrqirfV5tPU5YNycdjwHvH///pt6DvjRRx/Vjh07NDMzU9dzwENDQ5KKK42kj1R8eiF5Drh6NTY1NSUzK79e2a9yOzE4OFh+DjitXTJm8hxwPSvV6v4zMzPl54Cr602r6UZjT01NlZ8Drne12cgczYzTzBytqivaWp6Hpb3pUcv27dt9fHy84UmSR9FYDQP4MDKzN9x9e/Xr3AMGgCAEMAAEIYABIAgBDABBCGAACEIAA0AQAhgAghDAABCEAAaAIAQwAAQhgAEgCAEMAEEIYAAIQgADQBACGACCEMAAEIQABoAgBDAABCGAASAIAQwAQQhgAAhCAANAEAIYAIIQwAAQhAAGgCAEMAAEIYABIAgBDABBCGAACEIAA0AQAhgAghDAABCEAAaAIAQwAAQhgAEgCAEMAEEIYAAIQgADQJDudkxiZu2YBgA6SlsCeP369e2YBgA6CrcgACAIAQwAQQhgAAhCAANAEAIYAIIQwAAQhAAGgCAEMAAEIYABIAgBDABBCGAACEIAA0AQAhgAghDAABCEAAaAIAQwAAQhgAEgCAEMAEEIYAAIQgADQBACGACCEMAAEIQABoAgBDAABCGAASAIAQwAQQhgAAhCAANAEAIYAIIQwAAQhAAGgCAEMAAEIYABIAgBDABBCGAACEIAA0AQAhgAghDAABCEAAaAIAQwAAQhgAEgSHc7Junr62vHNADQUdoSwHv37m3HNADQUbgFAQBBCGAACEIAA0AQAhgAghDAABCEAAaAIAQwAAQhgAEgCAEMAEEIYAAIQgADQBACGACCEMAAEIQABoAgBDAABCGAASAIAQwAQQhgAAhCAANAEAIYAIKYu9ff2OySpPeanGuTpG832Tcatcfp5PqpPUYWa/+4u2+ufrGhAL4ZZjbu7tvbMlmLUXucTq6f2mN0Uu3cggCAIAQwAARpZwC/3Ma5Wo3a43Ry/dQeo2Nqb9s9YADActyCAIAgBDAABGl5AJtZn5n9l5lNm9mfpRz/ITP7Uun462bW2+oamlVH7Q+Z2SUz+3rp6/cj6kxjZl8ws2+Z2Ts1jpuZvVg6t2+Y2Z3trrGWOmrfaWazFdf9yXbXWIuZbTWzU2Z2xsy+aWZ/ktImk9e+ztozee3N7CNm9jUze7tU+1BKm8xmTZm7t+xLUpek/5b0U5LWSXpb0ier2vyRpM+Wth+U9KVW1rDGtT8k6S+ja61R/69JulPSOzWO3ydpVJJJukfS69E1N1D7TknHouusUdsWSXeWtn9U0mTK35tMXvs6a8/ktS9dyw2l7Zyk1yXdU9Umk1lT+dXqFfDdkqbd/ay7X5f0t5IeqGrzgKSR0vaXJf26mVmL62hGPbVnlrv/q6Tv3KDJA5KOetFXJW00sy3tqe7G6qg9s9z9oru/Wdr+nqQJSR+tapbJa19n7ZlUupZXS7u50lf1EwVZzZqyVgfwRyWdr9i/oJV/oOU27r4gaVZST4vraEY9tUvSb5V+jPyymW1tT2ktUe/5ZdUvlX7cHDWzT0UXk6b0I+4vqLgaq5T5a3+D2qWMXnsz6zKzr0v6lqQT7l7zumcsa8p4E64x/yip191/VtIJ/f93V6ytN1X8Xfqfk3RE0j/ElrOSmW2Q9HeSHnb370bX04hVas/stXf3RXf/eUkfk3S3mf1McEkNa3UA/4+kylXhx0qvpbYxs25Jt0rKt7iOZqxau7vn3f1aafdzku5qU22tUM+fTSa5+3eTHzfd/biknJltCi6rzMxyKgbYX7v7V1KaZPbar1Z71q+9JLn7FUmnJPVVHcpq1pS1OoD/Q9JPm9knzGydije+X61q86qkPaXtz0j6Fy/dJQ+2au1V9+0+reI9s07xqqSB0jvy90iadfeL0UXVw8zuSO7dmdndKv69zcQ/pFJdn5c04e7P12iWyWtfT+1ZvfZmttnMNpa2f1jSbkn/WdUsq1lT1t3Kwdx9wcz+WNI/qfhUwRfc/Ztm9heSxt39VRX/wP/KzKZVfOPlwVbW0Kw6a99nZp+WtKBi7Q+FFVzFzP5GxXesN5nZBUlPqfjGhNz9s5KOq/hu/LSkDyT9XkylK9VR+2ck/aGZLUiak/Rghv4h/bKk35V0unQ/UpL+XNJPSpm/9vXUntVrv0XSiJl1qfhN4RV3P9YJWVOJX0UGgCC8CQcAQQhgAAhCAANAEAIYAIIQwAAQhABGxzOzh81sfcX+8eQZUSDLeAwNHaH0ywDm7kspx2YkbXf3rP2vyIEbYgWMzDKzXit+PvNRSe9I+ryZjVd+/quZ7ZP0E5JOmdmp0mszya/LmtmjZvZO6evhoFMBUrECRmaVPqHrrKQd7v5VM7vd3b9T+u2nf5a0z92/Ub0CTvYlfVzSF1X8DF5T8ZO+fsfd32r3uQBpWAEj694rfYauJP22mb0p6S1Jn5L0yVX6/oqkv3f375c+UOYrkn517UoFGtPSz4IA1sD3JcnMPiHpTyX9ortfNrMvSvpIZGHAzWIFjE7xYyqG8ayZ/bik/opj31Pxf6lT7d8k/YaZrTezH5H0m6XXgExgBYyO4O5vm9lbKn7k4HlJ/15x+GVJY2b2v+5+b0WfN0sr5a+VXvoc93+RJbwJBwBBuAUBAEEIYAAIQgADQBACGACCEMAAEIQABoAgBDAABPk/Be2GCQyWzlEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.boxplot(data=df_train,x='ratio')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in range(1):\n",
    "    print(actuals[i],val_predictions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_prediction_length: 10\n",
      "max_encoder_length   : 60\n"
     ]
    }
   ],
   "source": [
    "print('max_prediction_length:',max_prediction_length)\n",
    "print('max_encoder_length   :',max_encoder_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 49\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Z_MODELO</th>\n",
       "      <th>Z_PUNTO_VENTA</th>\n",
       "      <th>Z_GAMA</th>\n",
       "      <th>ratio</th>\n",
       "      <th>date_block_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358645</th>\n",
       "      <td>MOD_99</td>\n",
       "      <td>PVENT_99</td>\n",
       "      <td>GAM_3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358646</th>\n",
       "      <td>MOD_99</td>\n",
       "      <td>PVENT_99</td>\n",
       "      <td>GAM_3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358647</th>\n",
       "      <td>MOD_99</td>\n",
       "      <td>PVENT_99</td>\n",
       "      <td>GAM_3</td>\n",
       "      <td>0.005993</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358648</th>\n",
       "      <td>MOD_99</td>\n",
       "      <td>PVENT_99</td>\n",
       "      <td>GAM_3</td>\n",
       "      <td>0.003273</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358649</th>\n",
       "      <td>MOD_99</td>\n",
       "      <td>PVENT_99</td>\n",
       "      <td>GAM_3</td>\n",
       "      <td>0.003542</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2358650 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Z_MODELO Z_PUNTO_VENTA Z_GAMA     ratio  date_block_num\n",
       "0          MOD_1       PVENT_1  GAM_1  0.000000               0\n",
       "1          MOD_1       PVENT_1  GAM_1  0.000000               1\n",
       "2          MOD_1       PVENT_1  GAM_1  0.000000               2\n",
       "3          MOD_1       PVENT_1  GAM_1  0.000000               3\n",
       "4          MOD_1       PVENT_1  GAM_1  0.000000               4\n",
       "...          ...           ...    ...       ...             ...\n",
       "2358645   MOD_99      PVENT_99  GAM_3  0.000000              45\n",
       "2358646   MOD_99      PVENT_99  GAM_3  0.000000              46\n",
       "2358647   MOD_99      PVENT_99  GAM_3  0.005993              47\n",
       "2358648   MOD_99      PVENT_99  GAM_3  0.003273              48\n",
       "2358649   MOD_99      PVENT_99  GAM_3  0.003542              49\n",
       "\n",
       "[2358650 rows x 5 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select last 30 days from data (max_encoder_length is 24)\n",
    "encoder_data = df_train[lambda x: x.date_block_num > x.date_block_num.max() - max_encoder_length]\n",
    "\n",
    "print(encoder_data['date_block_num'].min(),encoder_data['date_block_num'].max())\n",
    "#print(encoder_data['DATE'].min(),encoder_data['DATE'].max())\n",
    "encoder_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "last_data = df_train[df_train['date_block_num'].isin([idx  -  max_prediction_length for idx in df_test['date_block_num'].unique()])]\n",
    "last_data['date_block_num'] = last_data['date_block_num'] + max_prediction_length\n",
    "\n",
    "decoder_data = pd.merge(df_test[[col for col in df_test.columns if 'ratio' not in col]], \n",
    "        last_data[['date_block_num','Z_MODELO','Z_PUNTO_VENTA','Z_GAMA',\"ratio\"]+statistics_columns],\n",
    "        on = ['date_block_num', 'Z_MODELO','Z_PUNTO_VENTA','Z_GAMA',],\n",
    "                        how='left'\n",
    "        )\n",
    "\n",
    "\n",
    "encoder_data.replace([np.inf, -np.inf, np.nan],0,inplace=True)\n",
    "decoder_data.replace([np.inf, -np.inf, np.nan],0,inplace=True)\n",
    "\n",
    "\n",
    "# combine encoder and decoder data\n",
    "new_prediction_data = pd.concat([encoder_data, decoder_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>Z_MODELO</th>\n",
       "      <th>Z_PUNTO_VENTA</th>\n",
       "      <th>Z_GAMA</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>50</td>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0.361084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>51</td>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0.548340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>52</td>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0.884277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>53</td>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>1.017578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>54</td>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>1.449219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358645</th>\n",
       "      <td>55</td>\n",
       "      <td>MOD_99</td>\n",
       "      <td>PVENT_99</td>\n",
       "      <td>GAM_3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358646</th>\n",
       "      <td>56</td>\n",
       "      <td>MOD_99</td>\n",
       "      <td>PVENT_99</td>\n",
       "      <td>GAM_3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358647</th>\n",
       "      <td>57</td>\n",
       "      <td>MOD_99</td>\n",
       "      <td>PVENT_99</td>\n",
       "      <td>GAM_3</td>\n",
       "      <td>0.005993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358648</th>\n",
       "      <td>58</td>\n",
       "      <td>MOD_99</td>\n",
       "      <td>PVENT_99</td>\n",
       "      <td>GAM_3</td>\n",
       "      <td>0.003273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358649</th>\n",
       "      <td>59</td>\n",
       "      <td>MOD_99</td>\n",
       "      <td>PVENT_99</td>\n",
       "      <td>GAM_3</td>\n",
       "      <td>0.003542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>471730 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date_block_num Z_MODELO Z_PUNTO_VENTA Z_GAMA     ratio\n",
       "40                   50    MOD_1       PVENT_1  GAM_1  0.361084\n",
       "41                   51    MOD_1       PVENT_1  GAM_1  0.548340\n",
       "42                   52    MOD_1       PVENT_1  GAM_1  0.884277\n",
       "43                   53    MOD_1       PVENT_1  GAM_1  1.017578\n",
       "44                   54    MOD_1       PVENT_1  GAM_1  1.449219\n",
       "...                 ...      ...           ...    ...       ...\n",
       "2358645              55   MOD_99      PVENT_99  GAM_3  0.000000\n",
       "2358646              56   MOD_99      PVENT_99  GAM_3  0.000000\n",
       "2358647              57   MOD_99      PVENT_99  GAM_3  0.005993\n",
       "2358648              58   MOD_99      PVENT_99  GAM_3  0.003273\n",
       "2358649              59   MOD_99      PVENT_99  GAM_3  0.003542\n",
       "\n",
       "[471730 rows x 5 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_data[['date_block_num','Z_MODELO','Z_PUNTO_VENTA','Z_GAMA',\"ratio\"]+statistics_columns]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "new_raw_predictions, new_x = best_tft.predict(new_prediction_data, mode=\"raw\", return_x=True)\n",
    "\n",
    "for idx in range(10):  # plot 10 examples\n",
    "    best_tft.plot_prediction(new_x, new_raw_predictions, idx=idx, show_future_observed=False);\n",
    "    \n",
    "interpretation = best_tft.interpret_output(new_raw_predictions, reduction=\"sum\")\n",
    "best_tft.plot_interpretation(interpretation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_raw_predictions = best_tft.predict(new_prediction_data, mode=\"prediction\", return_x=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(new_raw_predictions.numpy()).T\n",
    "predictions['date_block_num'] = sorted(df_test['date_block_num'].unique())\n",
    "predictions = pd.melt(predictions, id_vars=['date_block_num'])\n",
    "predictions = predictions.sort_values(['date_block_num', 'variable']).reset_index(drop=True)\n",
    "df_test[['date_block_num','Z_MODELO','Z_PUNTO_VENTA','Z_GAMA']].sort_values(['date_block_num', 'Z_MODELO','Z_PUNTO_VENTA','Z_GAMA']).reset_index(drop=True)\n",
    "df_test2 = df_test.join(predictions['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>Z_MODELO</th>\n",
       "      <th>Z_PUNTO_VENTA</th>\n",
       "      <th>Z_GAMA</th>\n",
       "      <th>ratio</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.535267e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51</td>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.987189e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52</td>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.269469e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.702113e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54</td>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.720216e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   date_block_num Z_MODELO Z_PUNTO_VENTA Z_GAMA  ratio         value\n",
       "0              50    MOD_1       PVENT_1  GAM_1    0.0  1.535267e-07\n",
       "1              51    MOD_1       PVENT_1  GAM_1    0.0  1.987189e-07\n",
       "2              52    MOD_1       PVENT_1  GAM_1    0.0  1.269469e-07\n",
       "3              53    MOD_1       PVENT_1  GAM_1    0.0  2.702113e-07\n",
       "4              54    MOD_1       PVENT_1  GAM_1    0.0  1.720216e-07"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "reverse_mapping_file = '../../utils/reverse_dict_mapping_list.txt'\n",
    "\n",
    "with open(reverse_mapping_file, 'rb') as f:\n",
    "    reverse_mapping = pickle.load( f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reverse_mapping#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#descriptive_columns = ['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA']\n",
    "descriptive_columns = ['Z_MARCA', 'Z_GAMA', 'Z_MODELO',\n",
    "                       'Z_DEPARTAMENTO', 'Z_PUNTO_VENTA']\n",
    "i=0\n",
    "for column in descriptive_columns:\n",
    "    if column in df_test2.columns:\n",
    "        df_test2[column] = df_test2[column].map(reverse_mapping[i])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>Z_MODELO</th>\n",
       "      <th>Z_PUNTO_VENTA</th>\n",
       "      <th>Z_GAMA</th>\n",
       "      <th>ratio</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "      <td>da45328ba820604eb99694768f2a430cd933d161601dcb...</td>\n",
       "      <td>76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.535267e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "      <td>da45328ba820604eb99694768f2a430cd933d161601dcb...</td>\n",
       "      <td>76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.987189e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "      <td>da45328ba820604eb99694768f2a430cd933d161601dcb...</td>\n",
       "      <td>76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.269469e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "      <td>da45328ba820604eb99694768f2a430cd933d161601dcb...</td>\n",
       "      <td>76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.702113e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "      <td>da45328ba820604eb99694768f2a430cd933d161601dcb...</td>\n",
       "      <td>76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.720216e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   date_block_num                                           Z_MODELO  \\\n",
       "0              50  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...   \n",
       "1              51  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...   \n",
       "2              52  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...   \n",
       "3              53  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...   \n",
       "4              54  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...   \n",
       "\n",
       "                                       Z_PUNTO_VENTA  \\\n",
       "0  da45328ba820604eb99694768f2a430cd933d161601dcb...   \n",
       "1  da45328ba820604eb99694768f2a430cd933d161601dcb...   \n",
       "2  da45328ba820604eb99694768f2a430cd933d161601dcb...   \n",
       "3  da45328ba820604eb99694768f2a430cd933d161601dcb...   \n",
       "4  da45328ba820604eb99694768f2a430cd933d161601dcb...   \n",
       "\n",
       "                                              Z_GAMA  ratio         value  \n",
       "0  76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...    0.0  1.535267e-07  \n",
       "1  76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...    0.0  1.987189e-07  \n",
       "2  76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...    0.0  1.269469e-07  \n",
       "3  76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...    0.0  2.702113e-07  \n",
       "4  76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...    0.0  1.720216e-07  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inv_dict_dates = {v: k for k, v in dict_dates.items()}\n",
    "df_test2['Z_WEEK'] = df_test2['date_block_num'].map(inv_dict_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA','Z_WEEK']:\n",
    "    df_test2[column] = df_test2[column].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test2['ID'] = df_test2['Z_MODELO'] + '|' + df_test2['Z_PUNTO_VENTA'] + '|' + df_test2['Z_GAMA'] + '|' + df_test2['Z_WEEK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test2['ratio'] = np.maximum(df_test2['value'],0)\n",
    "submission = df_test2[['Z_WEEK','ID','ratio']]#.groupby('ID').sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>Z_MODELO</th>\n",
       "      <th>Z_PUNTO_VENTA</th>\n",
       "      <th>Z_GAMA</th>\n",
       "      <th>ratio</th>\n",
       "      <th>value</th>\n",
       "      <th>Z_WEEK</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "      <td>da45328ba820604eb99694768f2a430cd933d161601dcb...</td>\n",
       "      <td>76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...</td>\n",
       "      <td>1.535267e-07</td>\n",
       "      <td>1.535267e-07</td>\n",
       "      <td>SEMANA_51</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "      <td>da45328ba820604eb99694768f2a430cd933d161601dcb...</td>\n",
       "      <td>76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...</td>\n",
       "      <td>1.987189e-07</td>\n",
       "      <td>1.987189e-07</td>\n",
       "      <td>SEMANA_52</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "      <td>da45328ba820604eb99694768f2a430cd933d161601dcb...</td>\n",
       "      <td>76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...</td>\n",
       "      <td>1.269469e-07</td>\n",
       "      <td>1.269469e-07</td>\n",
       "      <td>SEMANA_53</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "      <td>da45328ba820604eb99694768f2a430cd933d161601dcb...</td>\n",
       "      <td>76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...</td>\n",
       "      <td>2.702113e-07</td>\n",
       "      <td>2.702113e-07</td>\n",
       "      <td>SEMANA_54</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "      <td>da45328ba820604eb99694768f2a430cd933d161601dcb...</td>\n",
       "      <td>76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...</td>\n",
       "      <td>1.720216e-07</td>\n",
       "      <td>1.720216e-07</td>\n",
       "      <td>SEMANA_55</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>55</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "      <td>da45328ba820604eb99694768f2a430cd933d161601dcb...</td>\n",
       "      <td>76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...</td>\n",
       "      <td>2.279860e-07</td>\n",
       "      <td>2.279860e-07</td>\n",
       "      <td>SEMANA_56</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>56</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "      <td>da45328ba820604eb99694768f2a430cd933d161601dcb...</td>\n",
       "      <td>76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...</td>\n",
       "      <td>1.202381e-07</td>\n",
       "      <td>1.202381e-07</td>\n",
       "      <td>SEMANA_57</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>57</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "      <td>da45328ba820604eb99694768f2a430cd933d161601dcb...</td>\n",
       "      <td>76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...</td>\n",
       "      <td>1.624551e-07</td>\n",
       "      <td>1.624551e-07</td>\n",
       "      <td>SEMANA_58</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>58</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "      <td>da45328ba820604eb99694768f2a430cd933d161601dcb...</td>\n",
       "      <td>76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...</td>\n",
       "      <td>1.627685e-07</td>\n",
       "      <td>1.627685e-07</td>\n",
       "      <td>SEMANA_59</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>59</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "      <td>da45328ba820604eb99694768f2a430cd933d161601dcb...</td>\n",
       "      <td>76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...</td>\n",
       "      <td>1.550223e-07</td>\n",
       "      <td>1.550223e-07</td>\n",
       "      <td>SEMANA_60</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   date_block_num                                           Z_MODELO  \\\n",
       "0              50  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...   \n",
       "1              51  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...   \n",
       "2              52  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...   \n",
       "3              53  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...   \n",
       "4              54  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...   \n",
       "5              55  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...   \n",
       "6              56  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...   \n",
       "7              57  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...   \n",
       "8              58  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...   \n",
       "9              59  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...   \n",
       "\n",
       "                                       Z_PUNTO_VENTA  \\\n",
       "0  da45328ba820604eb99694768f2a430cd933d161601dcb...   \n",
       "1  da45328ba820604eb99694768f2a430cd933d161601dcb...   \n",
       "2  da45328ba820604eb99694768f2a430cd933d161601dcb...   \n",
       "3  da45328ba820604eb99694768f2a430cd933d161601dcb...   \n",
       "4  da45328ba820604eb99694768f2a430cd933d161601dcb...   \n",
       "5  da45328ba820604eb99694768f2a430cd933d161601dcb...   \n",
       "6  da45328ba820604eb99694768f2a430cd933d161601dcb...   \n",
       "7  da45328ba820604eb99694768f2a430cd933d161601dcb...   \n",
       "8  da45328ba820604eb99694768f2a430cd933d161601dcb...   \n",
       "9  da45328ba820604eb99694768f2a430cd933d161601dcb...   \n",
       "\n",
       "                                              Z_GAMA         ratio  \\\n",
       "0  76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...  1.535267e-07   \n",
       "1  76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...  1.987189e-07   \n",
       "2  76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...  1.269469e-07   \n",
       "3  76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...  2.702113e-07   \n",
       "4  76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...  1.720216e-07   \n",
       "5  76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...  2.279860e-07   \n",
       "6  76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...  1.202381e-07   \n",
       "7  76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...  1.624551e-07   \n",
       "8  76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...  1.627685e-07   \n",
       "9  76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...  1.550223e-07   \n",
       "\n",
       "          value     Z_WEEK                                                 ID  \n",
       "0  1.535267e-07  SEMANA_51  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...  \n",
       "1  1.987189e-07  SEMANA_52  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...  \n",
       "2  1.269469e-07  SEMANA_53  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...  \n",
       "3  2.702113e-07  SEMANA_54  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...  \n",
       "4  1.720216e-07  SEMANA_55  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...  \n",
       "5  2.279860e-07  SEMANA_56  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...  \n",
       "6  1.202381e-07  SEMANA_57  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...  \n",
       "7  1.624551e-07  SEMANA_58  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...  \n",
       "8  1.627685e-07  SEMANA_59  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...  \n",
       "9  1.550223e-07  SEMANA_60  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "aux = df_test2['ratio'].values\n",
    "\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "plt.plot(aux,'o')\n",
    "plt.show()\n",
    "# rmse daba ratios muy pequenos de 0 a 0.01\n",
    "# TWEDDIE P 1.5 daba ratios muy pequenos de 0 a 0.01\n",
    "# TWEDDIE P 1.01  daba ratios muy pequenos de 0 a 0.14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIEAAAE8CAYAAABXWqHNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAABQX0lEQVR4nO3df5Rc5X3n+c+jVgm3sEODLW+sMliYMO21IkttFBtH5+yxycm2fwTcww9jD84mezz28UyyiWymdyUPayQvOyirjZ3Jj8kck/jkB4QICU0dEZHIOQdyklEQtnBLKLLRGmwjKJhBQWpsozIqdT/7R/dtVVff5/6uuvfWfb/O4SDdKlU9VXV/Pd/n+3wfY60VAAAAAAAABtuyvBsAAAAAAACA3iMIBAAAAAAAUAEEgQAAAAAAACqAIBAAAAAAAEAFEAQCAAAAAACoAIJAAAAAAAAAFbA8rzd+05veZNesWZPX2wMAAAAAAAycJ5544p+ttav8HsstCLRmzRodPnw4r7cHAAAAAAAYOMaYZ12PMR0MAAAAAACgAggCAQAAAAAAVABBIAAAAAAAgAogCAQAAAAAAFABBIEAAAAAAAAqgCAQAAAAAABABRAEAgAAAAAAqIDleTcAAABE05hqaueBE3phuqXVI8OaHB/VxFg972YBAACgJAgCAQBQAo2pprbuPaZWe0aS1JxuaeveY5JEIAgAAACRMB0MAIAS2HngxEIAyNNqz2jngRM5tQgAAABlQxAIAIASeGG6FWs7AAAA0I0gEAAAJbB6ZDjWdgAAAKAbQSAAAEpgcnxUw7WhRduGa0OaHB/NqUUAAAAoGwpDAwBQAl7xZ1YHAwAAQFIEgQAAKImJsTpBHwAAACTGdDAAAAAAAIAKIAgEAAAAAABQAQSBAAAAAAAAKoAgEAAAAAAAQAUQBAIAAAAAAKgAgkAAAAAAAAAVQBAIAAAAAACgAggCAQAAAAAAVABBIAAAAAAAgAoIDQIZY15njPmGMeaoMea4MWa7z3MuMsbsMsY8bYx53BizpietBQAAAAAAQCJRMoFek3SdtXa9pA2SPmiMubbrOZ+SdMZa+zOSviLptzJtJQAAAAAAAFIJDQLZOT+e/2tt/j/b9bSPSvrT+T/vkfQLxhiTWSsBAAAAAACQSqSaQMaYIWPMEUkvSfpba+3jXU+pS3pOkqy15yW9IumNPq/zGWPMYWPM4VOnTqVqOAAAAAAAAKKLFASy1s5YazdIequk9xhjfjbJm1lrv2qt3Wit3bhq1aokLwEAAAAAAIAEYq0OZq2dlvSopA92PdSUdLkkGWOWS7pE0ssZtA8AAAAAAAAZiLI62CpjzMj8n4cl/aKkp7qetk/Sr8z/+WZJj1hru+sGAQAAAAAAICfLIzznLZL+1BgzpLmg0QPW2r8yxnxJ0mFr7T5Jfyzpz40xT0s6LenjPWsxAAAAAAAAYgsNAllrn5Q05rP9ix1//omkW7JtGgAAAAAAALISqyYQAAAAAAAAyokgEAAAAAAAQAUQBAIAAAAAAKgAgkAAAAAAAAAVQBAIAAAAAACgAggCAQAAAAAAVABBIAAAAAAAgAogCAQAAAAAAFABBIEAAAAAAAAqgCAQAAAAAABABRAEAgAAAAAAqACCQAAAAAAAABVAEAgAAAAAAKACCAIBAAAAAABUAEEgAAAAAACACliedwMAACijxlRTOw+c0AvTLa0eGdbk+Kgmxup5NwsAAABwIggEAEBMjammtu49plZ7RpLUnG5p695jkkQgCAAAAIXFdDAAAGLaeeDEQgDI02rPaOeBEzm1CAAAAAhHEAgAgJhemG7F2g4AAAAUAUEgAABiWj0yHGs7AAAAUAQEgQAAiGlyfFTDtaFF24ZrQ5ocH82pRQAAAEA4CkMDABCTV/yZ1cEAAABQJgSBAABIYGKsTtAHAAAApcJ0MAAAAAAAgAogCAQAAAAAAFABBIEAAAAAAAAqgJpAAACURGOqSTFqAAAAJEYQCACAEmhMNbV17zG12jOSpOZ0S1v3HpMkAkEAAACIhOlgAACUwM4DJxYCQJ5We0Y7D5zIqUUAAAAom9AgkDHmcmPMo8aYbxtjjhtjftPnOe83xrxijDky/98Xe9NcAACq6YXpVqztAAAAQLco08HOS7rdWvstY8wbJD1hjPlba+23u573D9baX8q+iQAAYPXIsJo+AZ/VI8M5tAYAAABlFJoJZK190Vr7rfk//0jSdyRRfAAAgD6aHB9Vbcgs2lYbMpocH82pRQAAACibWDWBjDFrJI1Jetzn4fcZY44aY/7aGLM2i8YBAIAONuTvAAAAQIDIQSBjzOslPShps7X2h10Pf0vS26y16yX9nqSG4zU+Y4w5bIw5fOrUqYRNBgCgenYeOKH27OKoT3vWUhgaAAAAkUUKAhljapoLAN1nrd3b/bi19ofW2h/P//lhSTVjzJt8nvdVa+1Ga+3GVatWpWw6AADVQWFoAAAApBVldTAj6Y8lfcda+2XHc356/nkyxrxn/nVfzrKhAABUmasANIWhAQAAEFWUTKBNkn5Z0nUdS8B/2BjzWWPMZ+efc7OkfzLGHJX0u5I+bq2lUgEAABmZHB/VcG1o0bbh2hCFoQEAABBZ6BLx1tr/KsmEPOf3Jf1+Vo0CAACLTYzNLcy588AJvTDd0uqRYU2Ojy5sBwAAAMKEBoEAAEAxTIzVCfoAAAAgsVhLxAMAAAAAAKCcCAIBAAAAAABUAEEgAAAAAACACiAIBAAAAAAAUAEEgQAAAAAAACqAIBAAAAAAAEAFEAQCAAAAAACogOV5NwAAAETTmGpq54ETemG6pdUjw5ocH9XEWD3vZgEAAKAkCAIBAFACjammtu49plZ7RpLUnG5p695jkkQgCAAAAJEwHQwAgBLYeeDEQgDI02rPaOeBEzm1CAAAAGVDEAgAgBJ4YboVazsAAADQjSAQAAAlsHpkONZ2AAAAoBtBIAAASmByfFTDtaFF24ZrQ5ocH82pRQAAACgbCkMDAFACXvFnVgcDAABAUgSBAAAoiYmxOkEfAAAAJMZ0MAAAAAAAgAogEwgAKqYx1WRKUQb4HgEAAFA2BIEAoEIaU01N7j6q9qyVJDWnW5rcfVSSCGDE0JhqauveY2q1ZyTNfY9b9x6TxPcIAACA4mI6GABUyLZ9xxcCQJ72rNW2fcdzalE57TxwYiEA5Gm1Z7TzwImcWgQAAACEIwgEABUy3WrH2g5/L0y3Ym0HAAAAioAgEAAAMa0eGY61HQAAACgCgkAAUCGXrqzF2g5/k+OjGq4NLdo2XBvS5PhoTi0CAAAAwhEEAoAKufP6taoNmUXbakNGd16/NqcWldPEWF1337hO9ZFhGUn1kWHdfeM6ikIDAACg0FgdDAAqxAtSsLR5ehNjdb43AAAAlApBIACoGIIXAAAAQDUxHQwAAAAAAKACCAIBAAAAAABUANPBAAAAUHqNqSb1zgAACBGaCWSMudwY86gx5tvGmOPGmN/0eY4xxvyuMeZpY8yTxph396a5AAAAwGKNqaa27j2m5nRLVlJzuqWte4+pMdXMu2kAABRKlOlg5yXdbq19p6RrJf2aMeadXc/5kKSr5//7jKQ/zLSVAAAAgMPOAyfUas8s2tZqz2jngRM5tQgAgGIKDQJZa1+01n5r/s8/kvQdSd25tR+V9Gd2ziFJI8aYt2TeWgAAAKDLC9OtWNsBAKiqWDWBjDFrJI1Jerzrobqk5zr+/vz8thfTNA4AAFxAzRPA3+qRYTV9Aj6rR4ZzaA0AAMUVeXUwY8zrJT0oabO19odJ3swY8xljzGFjzOFTp04leQkAACqJmieA2+T4qIZrQ4u2DdeGNDk+mlOLAAAopkhBIGNMTXMBoPustXt9ntKUdHnH3986v20Ra+1XrbUbrbUbV61alaS9AAAUQmOqqU07HtGVW/Zr045Heh6MoeYJ4DYxVtfdN65TfWRYRlJ9ZFh337iOTDkAALqETgczxhhJfyzpO9baLzuetk/Srxtj/lLSeyW9Yq1lKhgAYCB5WTleUMbLypHUs04nNU+AYBNjdYI+AACEiJIJtEnSL0u6zhhzZP6/DxtjPmuM+ez8cx6W9D1JT0u6R9K/7U1zAQDIXx5ZOa7aJtQ8AQAAQFShmUDW2v8qyYQ8x0r6tawaBQDoPYoMJ5dHVs7k+Oii7COJmicAAACIJ9bqYACAwZDHdKZBksdKRN7vQuAOAAAASREEAoAKCprORFAhXF5ZOdQ8AQAAQBoEgQCggigynA5ZOQAAACgjgkAAUEF5TGcaNGTlAAAAoGyirA4GABgwk+OjGq4NLdpGkWEAAABgsJEJBAAVxHQmAAAAoHoIAgFARTGdCQAAAKgWgkAAAJREY6pJ9hYAAAASIwgEAEAJNKaamtxzVO0ZK0lqTrc0ueeoJBEIAgAAQCQEgQCgYsgmKaftDx1fCAB52jNW2x86zu8HAACASAgCAUCFNKaa2rr3mFrtGUlz2SRb9x6TRDZJ0Z052461HQAAAOjGEvEAUCE7D5xYCAB5Wu0Z7TxwIqcWAQAAAOgXgkAAUCEvTLdibUdxjAzXYm0HAAAAuhEEAoAKWT0yHGs7imPbDWtVW2YWbastM9p2w9qcWgQAAICyIQgEABXygXesirUdxTExVtfOW9arPjIsI6k+Mqydt6ynlhMAAAAiozA0AFTIo0+d8t1+/+PPaePbLiOgECLvldUmxur8RgAAAEiMIBAAVIir9s+MtZrcc1QSq4S5sLIaAAAAyo7pYABQIUG1f9ozVtsfOt7H1pQLK6sBAACg7AgCAUCFTI6Parg25Hz8zNl2H1tTLqysBgAAgLIjCAQAFTIxVtfdN67LuxmlxMpqAAAAKDuCQABQMRNjdY0M13wfc22HfxbVcG1Ik+OjObUIAAAAiIcgUAqNqaY27XhEV27Zr007HlFjqpl3kwAgkm03rFVtmVm0rbbMaNsNa3NqUfF5WVSdS7TffeM6ikIDAACgNFgdLCFWiQFQZt55Ks/lzsuIJdoBAABQZgSBEgpaJYYOAoAyIKBRPo2pJoE7AAAAJEYQKCFWiQEA9BMZqAAAAEiLmkAJsUoMAKCfXBmotz9wlJp0AAAAiIQgUEKsEgOgrChqX06uTNMZa7V17zF+RwAAAIQiCJQQq8QAKCNvSlFzuiWrC1OKCCAUX1CmqVeTDgAAAAhCTaAUKKoKoGwoal9ek+Oji2oCdaMmHQAAAMKQCQQAFUJR+/LyMlCHjPF9nJp0AAAACBMaBDLGfM0Y85Ix5p8cj7/fGPOKMebI/H9fzL6ZAIAsUNS+3CbG6vrtj62nJh3gg3pnAACEi5IJ9CeSPhjynH+w1m6Y/+9L6ZsFAOgFv6L2knT23Hk6TCVBTTpgKeqdAQAQTWhNIGvt3xtj1vShLQCAHvMCBdv2Hdd0q72w/czZtrbuPbboOWXSmGpq54ETemG6pdUjw5ocHy3l54iKmnTAYtQ7AwAgmqxqAr3PGHPUGPPXxpi1ricZYz5jjDlsjDl86tSpjN4aABDHxFhdF1+0dAygrCtMkQEAgHpnAABEk0UQ6FuS3matXS/p9yQ1XE+01n7VWrvRWrtx1apVGbx1vph7DqCsBqnDFJQBAKAaqHcGAEA0qYNA1tofWmt/PP/nhyXVjDFvSt2ygmPkGUCZDVKHaZACWgCS8at3RsF0AACWSh0EMsb8tDFz69UaY94z/5ovp33domPkGUCZDVKHaZACWgCSoWA6AADRhBaGNsbcL+n9kt5kjHle0p2SapJkrf3Pkm6W9G+MMecltSR93Fpre9bigmDkGXmrWiFcZMfbd1rtGQ0ZoxlrVS/xPjQ5Pqqte48tCsyXLaDF8QykR8F0AADCRVkd7BMhj/++pN/PrEUlsXpkWE2fgA8jz+gHbzqi1+n1piNK5VzZCf3Tve/MWLsQMCnrvuO1u6xBlDjHM8EiAAAApBEaBIK/QRh5RnmxFC6SGtR9p8wZAFF/E4K/AAAASIsgUEJlH3lGuTEdEUn5ZTAGbUfvRT2eBzWAB2SNjDkAANwIAqVQ5pFnlBvTEZHUMiPN+lRtW2b63xbMiXo8E/wFwpExBwBAsNSrgwHov0Fa2Qn95RcACtqO3ot6PI+srPn++5GVNTWmmtq04xFduWW/Nu14RI2pZs/aCxQZq7cCABCMTCCghJiOiF5oTDXZh3IQ9Xh2rbv5k/YMmQ/APDLmACAeptBWD0EgoKSYjogkjCRX0g+Bg/xEOZ5fabV9t7fasz7bqBWEamK6NABExxTaamI6GABUSNCsL6ZMFFvcTiyZD6gipksDQHRMoa0mgkAAUCEjw/51ZTwEDorL1bm91FEriMwHVNHEWF1337hO9ZFhGUn1kWHdfeM6RrQBwAdTaKuJ6WAAUCEmZBWwsgYOqjCffWKsrt2HT+rgM6cXtr37ikt0y8YrFqVyS2Q+oJq6zwNfuXXDwJ0HACBLTKGtJjKBAKBCzpz1rysjlTdw4M1nb063ZHVhPvugrZB1R+PYogCQJB185rQOP3uazAdUXlXOAwCQJabQVpOxruVGemzjxo328OHDubw3AFTV27fudy4H/zslHTXftOMR31Gs+siwDm65LocW9cZVWx/WjM81e8gYPXP3h3NoEVAcVTkPAEDWOrMoLxmuyRhp+mx7YDOrq8IY84S1dqPfY2QCAUCFuAJAZebX8QvaXlZ+AaCg7UCVVOU8AABZmxir6+CW6/SVWzfotfOzOnO2TUblgCMIBAAF0ZhqatOOR3Tllv3atOORvl90y3qhH3IUOnJtL6uqfE4AANB/rBRWHQSBAKAA+lXPImh1sLJe6KuSIfOJ914eazsAAEBUrBRWHQSBAKAA+jX6su2Gtaotc2eOlPFC71oi3bW9iKJkgd01sU6fvPaKhcyfIWP0yWuv0F0T6/rdXKBwXAlxJMoBQDSuFcFYKWzwsEQ8UDJVWAq7ivo1+uLtK7c/cNQ3U6aMF3pXwk9ZEoG8LDAvCOhlgUlacmzfNbGOoA/gY3j5Mp1tz/puBwCE+8A7VuneQyd9t2OwcGUESoQlcAdXP0dfJsbqzilEZbzQv9LyX/betb0X0tRzYg4+kF7LJwAUtB0AsNijT52KtR3lRRAIKBE6i4NrcnxUw7WhRduGa0OaHB/tyfvtf/LFWNuLLO/05bTBWebgA+nlfR4AgLLjfqQ6CAIBJcLJuTfyXpVLmsvOufvGdaqPDMtIqo8M6+4b1/Vkql9jqqkzZ/2zZFzbi6zfAbRuaYOzdF6B9PI+DwBA2XE/Uh3UBEqB2izot9Ujw2r6BHw4OScXpx5Lr02M1Xv+nt7nHSTed5bX+ThtcHZyfFSTe46qPXOhiFFtyNB5BWLI+zwAAGU3OT666J5YIpg+qAgCJVSkjiOqg5Nz9oKyOAbxWPb7vJ2ClpAvsn4E0FwyCc52F7EuSVFroEjyPA8AQNkRTK8OgkAJVa3jiGLg5Jy9qk2xC/pctWVG225Y28fWDIa0wdmdB06oPbs46tOetVxPgJi8DO3mdEtDxmjGWtW5TgJAZATTq4EgUEKujlRzuqXGVJODBz3DyTlbVZtiN7Ky5qz7854rL2XfSiBtcDZOIJJpyIC/7gztGTsXWCVTGwCAxSgMnVBQB5Elu4HyqFoxURswzejgM6d1R2Ow6gX1y8RYXQe3XKfv7/iIDm65LlZnM2ohxrSrkAGDLGiqK6toAgBwAUGghPw6jh5uNoDy6OeqXEXwSit49a/7H3+uTy2BZ3J8VLVlZtG22rKlhaHTrkIGDLKwKbyDOsUXAIC4mA6WkNdB3LzriO/j3GwA5VGlKXau6W+emaBUISyS6dQsE/J3Va9+FRBH2LltUKf4AgAQF5lAKUyM1VWPmMYPAEUQlMVYZo2ppjbteERXbtmvTTse6fkUqSynZu08cGLR8vCS1J6xSzJ8ok4bA6pozRvdx8EgT/EFACAugkApVa2eCDCI+h1AyJM3/W2Q5FErJ8upWa7she7tXG8At0PfO+N8bJCn+AIAEBdBoBS8qQCt9oyGzFzu/qDXEwEGTRWL7U6M1RfOWd1c24vMFZC5/YGjPfsdg1aIjBtIjPpbVK1+FRBH0FRWjhEAAC4IrQlkjPmapF+S9JK19md9HjeS/qOkD0s6K+lXrbXfyrqhReO3FKk3IsvNBlAeQRkdg3osN6aazg5TGWsCuQIyM9b2bGnooPojcZekjvNbVKl+FRDHMiPNOk5fjakmxw0AAPOiZAL9iaQPBjz+IUlXz//3GUl/mL5ZxccqLcBgqFqxXS+A7bKyVr4E0ZGVNedjvTovh9VWivO+rtpyru0AlrpoufvcNbm7d1mBAACUTejdvrX27yWdDnjKRyX9mZ1zSNKIMeYtWTWwqKrWcQQGVdWK7foFsDu1zs/2sTXZCEte6sV5uXNqVtr3pdYPkN5P2u5zV3vWatu+431sDQCUR5VqY2JOFkO+dUnPdfz9+fltA61qHUdgUBWpA35H45iu2vqw1mzZr6u2Pqw7Gu6MnaTCAhMlnA2m6VY78PFenZcnxuo6uOW61KtEUusHSO+SYXdGoBR+ngCAKqpibUxEqAmUJWPMZzQ3ZUxXXHFFP986c5Pjo4tqAkmM3KK/vMLkL0y3tHpkmHpUCXnfWd7f5R2NY7r30MmFv89Yu/D3uyayW80rqJbNIOrHeTmL60HUWj+33fOYDj5zITl301WX6b5Pvy9eg4EBVMKa9gCQuyrWxkQ2QaCmpMs7/v7W+W1LWGu/KumrkrRx48YSjjdfUJSOI6qpuzB53EK0WKwIxXbvf/w55/agIFDcYKBfwGKQdWfU9CJ42q/rQXcASJIOPnNat93zGIEgVN6Zs2T6AEBclDippiyCQPsk/box5i8lvVfSK9baFzN43cIrQscR1UTUfvAkWa0rSTDQ2377A0d9X3uQixE3ppqa3HNU7Zm5z92cbmlyz1FJ6YOn/bgedAeAwrZnhaxDlMGQMYHny0sDCsgDQFW5MsQpcTLYQmsCGWPul/SYpFFjzPPGmE8ZYz5rjPns/FMelvQ9SU9LukfSv+1ZawFIImo/iIYC5jK45mUnXaVwYqyu3/7Y+sLUQuqlznnt2x86vhAA8rRnrD7/wJFCzX1PUqCxF/WjvLZQKwBlEBQAqg0Z3Xn92j62BgDKoUi1MdE/oZlA1tpPhDxuJf1aZi0qIUZJ0W9E7QfPtW+/1JnR4cruSRMMHKQprfWAOkedGXKu6SKz1v0d91tQdleQXtSPksg6RHkEnQdu/bnL2V8BwMcg3Q8iuixWB6s0RkmRB6L2g+cHL7sDN67snrSrFHqrW31/x0d0cMt1pb3g+x0PnaIExaJkUPVDUNDl6jdfHPhvXXWl0iDrEGUxOT6q2jL/jMoHn2hyXwYADoNyP4joCAKl0Jhq6vYHjiaajgEk5WWetdozC1OIWFK6/MJW7PLrdKcNBiaZdlRE3hLrril1XlBsJGQJ6SIENoKCLmfPzQb+26DpMEmlDTQCfeWYVct9GQAAFxAESqgx1dTkbv/CqlIxOhMYPJ2ZZ9Jcp8/r9BMAKrew5Y39Ot1e8KM+Miyj6MHAxlRTG7Z/XZt3HRmYLMYodY623bDWmSkgpQtspA2o3dE4pqu2PixXGMc1BbTXXFlWZ8+dL+2+gsG088CJJTW/OnFfBgDAnCxWB6ukbfuOqz3rvtlglBS9QH2OwRWUxBGU3RN3VSovgO13/ir7vhQ2r937/7Z9xzXdWlwfKM10yiSrtHW6o3FsoaaPywfesUr3P/5cYLZPSBwxEdd3duZsuzB1lAApPMjDfRkAAHMIAiXU3YHoVBsy1GZBT1Cfo5puuia75cfDAthl35fCgmLe41kW9E8bnI1Sy+fRp06FTveykq7csj/zoo4TY3XtPHBiyXWv7EFDDJagbDlq5gEAcAFBoF7IviwDIIlVwQbZyHDNGVx+8ImmNr7tskw620EBbKk6+1LcDKogaYOzUWr5vDDdClz9yNM5tU/KLkuHADSKbnJ8dFFGXqd3X3EJwUoAAOZREyihS1e6C4y2Zy0FCNETrAo2uILq1fSrqClZjMn0o3iyl90TVNOoU9b7zCWOotqu7UC/TYzVddM1/oGeg8+c1h2NY31uEQAAxUQmUEJ3Xr9Wk3uOOosQMjqKXgireYLy8n7DzbuO+D6e1Tnl0pU1nTnryAYqcRajN72rOd3SkDGasVb1FMdHnOliH3jHKt+aPh94x6rY7+tnUaA3RuGfLK9DrsLlYQXNgX5pTDX14BPuYuX3PX5Sd02s62OLAAAoJoJACXmdgdsf8F8hrCpTKtB/WU5jQfF4AYxuWZ1TggLYXhZj2fav7sLM3vfnNy0qSnAnbqHnR5865dsu1/Y4OgNZm3Y8Erj6Ubes9pnGVNMZOJx2BRSBPvOrzdUpwqxLAAAqgSBQSm943fJMV5kBwmRZ0BZzivCdeqt2uerDZJVV0q+Mo34K6vx1Fi+OGtyJW+g5bb0cV3bWpStrOrjlutivJ2V3HfK+MxcGPFAUZTx3AQCQB2oCJeTdGHcHgC5dWdPdN66jU46eaEw1NbnnqJrTrYUCsJN7jqox5U6BRzDvWO78TrfuPdb37zRs1a4ssko8E2N1jQxQjZewzp/3eFBwJ8rrubanrQl05/VrNeRT6+fM2bY27XhkYV+M8npGc9lDWV2HggJsDHigSMp47gIAIA8EgRJy3RivXLG89AGgxlRTm3Y8oiu37F/UAUH+tj90fMl0kPaM1faHjufUovKLGhjotbBVu7Ie5R6kGi9hwRHvcdd32JxuLTrPxQ3qZFGw3ToywJrTLU3ungv0hmWDXbxiSN/f8REd3HJdz1cFk8SABwrl1deYmggAQBQEgRKK2pmQyhVUKUpWBPy56nI4C/0iVFmWvs562o2rlksZa7xMjo+qNuQfveoMxgR9h53nOVewxbV9Yqyuu29cp/rIcKJMnO0PHVdAEpjas1bb9h3XXx19MfB1Xj3nroeSlOs7GypjtBADrT0b/HiNO14AACQRBEosqDPhjdpK5QuqFCUrAuiXfizvHcWlK91TGXqxdHtRPndmfIIo3dNz/TJ2PJ3nuSSFnifG6jq45bpEmThRgrjTrXZotpikzAcbXAG2GWsLfS0DuoUFiQAAqAqCQAkFdSa8UVupfEGVsmRFVJVr7J0x+eSymMqThY+86y3Ox2ZirAgVVdafO8+Mx50HTvjWU+qenutl7Lh457kynwd7Mtjg2P2KfC1D9fiU1QIAAD4IAiU0MVbXTde4R3q9EduydSYGLjtgwLhCAax8m1zaqTxZ2f+ke6rPrLQQWM5K1M8dJbiTd8ZjnOm5E2N11UPOc/0+D7qKdHcLyhbrllWAxhVg8xT1Wobqed/bLwt8PM7xAwDAICMIlFBjqqkHnwjv4JQtqFKUrAj4G6RivkWSZipPVsKmBEWZChSVF9j53Pwy8V+5dYPv5/aWrV+0Gt3upavR5Z3xGLXWjyfsPNfv8+C2G9aGXoyNca8i5pJFgCbsNYp6LUP1/OBl975aGzK68/q1fWwNAADFtTzvBpRV0LK50oURp8nxUW3de2zRc4scVPE6gTsPnNAL0y2tHhnW5PgoK8AUhGMBIed2RNOYamrngRNqTrc0ZIxmrFV9QPd9L2vHOyd5WTuSlnxWv2Xrvemunc9Nk/HoffdpzjcfeMcq3XvopO9jXjDKe03v/VrtGedvncd5cGjIaDZg2p93jNugCtJdsgjQrB4ZVtPxOxb5WobqCTrf7Lx5/cCdywEASIogUEJBNxudI05lDKpMjNUL3T4gS91BkZn53nZQcCQPWU1lCMra6f6cruyj7u2uQEFYECJOQCpIUMFm6cL52vVbnz13fsm/6ed5cOeBE2pHqPu0bd9xRa1tm1WAxhVgu3jFkP7vf8kS8SiOkZU134zKS1fW2E8BAOhAECghV6dnyJglI04EVZCVi1cMOZeBvqNxTHdNuIvewl9QVp8rONJvWU5l6EWdsqQZj3ECUkGiTlly/dZnzrZzDfhF/e7DpgQOGaNZazMdbHAF2EZWrsj9uAA6/cRxHndtBwDEk0X2NoqBmkAJuVYH+6nhpXG1PFfNwWCpDbkP2XsPndQdjWN9bM1gCOuAF6Hw7a0/d3lmF9k4dcpW1tz7W+d5LGlx7awCUmEZRx94x6rQ181zpaus6urMWpt5XSvXVDDXdiAvLcca8K32LPddAJBS3ouAIFsEgRLyOj3dq7p4I8reAcEBgyy9EpIJcP/jz/WpJYMjrAPer8K3K4bcBX//4vGTmZ0zJsdHVesqLlxbZnyzdi7yCXR7ugMmncW1J8dHtfPAidDAd1aF811Bec9fHX0x0uvmFfCbHB9VLeD3j+qSiKuMxTHkqDrv2g4UkV9BewBAdHkvAoJsEQRKYWKsrosvWpr503lAcMAgS2Gd2BkqRMfmZYkkfTwrfucSz6yVvrD3yezerLv/7ujPTwesWObKBGlMNTW5p2tFsT2LO2BedmRzurXkrZPUspkYq+uma9yZL940qpUrgi95fsdX3zI5Qw7dKIuCtWeiVgyKznVOmbGWzEMUSlDdNK+gPQAgmV6UE0B+CAKlFHZAlPGAYfpacYVlDDA6H19YUeGwx7MStkT8WcdUh7j8ihC3Z6xvYDoo6Oja17Y/dNz39bc/NNcB61x2Xloc+4g6jaxbY6qpXd8Mz4L77kuvOh/zCz71K5Nz54ETS1Zh6zZrg6fnSXLWC0ujHrAPMAUVRXLn9WsDg6VhNbUAAG5ZZW+jGAgCpTTiGHnytq9c4T9FwfXv8tbZQVsYxSeNulgC+oqfeO/l/WvHgAirbVLkgG0SYYHpziDw6Vdfc76OK0PEFczytvstOy9JI8O1xLVs/AJPnaJk0dx0zeIC/o2ppm5/4GhfMjmj7GP1kWH9hxvfFemzZClsqt19PiuHVdEdjWO6auvDWrNlv67a+jDBsZwM9fsAAYCKcGXG9ytjHtlidbCUXLNvftKe0YbtX3eOzL5W0NUq/DpoXho11d/zFyVjAPEY4z6OpcEb4Qhazr17CXVXoVUpedZZ1GXn4wjLoopyyDz4RFMb33aZJsbqC9+DK9DVGTDLYpUM12/i8bKUvNfevOtI7PdIKuw9ORvNBYDu7QiGzVi78HdWbOwfvyzHTkHTxQCgyqLcz7gy4/uVMY9skQmUQmOq6ey4tNqzgZ2arKZ2ZC1pB63sU8jK0v6wrJV7GZWPLSgAVBvyL5hcZkEjOa4l1P24AiTdxfLDtveDN6Vp01WXOZ8TVsutU2fALIupYn7ZNl6IrXuK3MRY3ZkNRBJEPlwF+SnU319BGXVDy4zuvH5taa71ANAvUe9nyljiBG4EgRLyDhiUfwoZK7jBaQDTHIJGcuIs++2qFbPthrW+q49tu2GtpODR+F50yjpr/dz36fcFBoK8zx8lKyfLov9eYWsvu2rIGN127RX6gWO5d1dmUy+SBL3zO9yCimejf4KyNmdmrQ4/e5prPQB0iXo/Q02gwUIQKKE4I+Z+8hwVD+LqoAV13IKmkJUBK7hV28WOul3S3H7cr/0g6Tkh7sh20EhOnClea97of9GfGKtr5y3rVR8ZltFcsGjnLesXAhl3Xr/WWdw8aacsKAPm3VdcsiiIct+n3xe67HnQ9+Bl5WQ5ItaYaurBJ5oLQYMZa/XgE03f7yGo1kxQEeekXDWcPJ+89orM37NswvYn9EdYXYr7Dp3kWo++yCLjjKw19IvrvqU53Vq071ETaLBECgIZYz5ojDlhjHnaGLPF5/FfNcacMsYcmf/vX2ff1GJJm/rmjYoXjV8HrTY0l0bt0osaH/1EemO1zYaM1sfJjknjRz+Jf7wkyWILGsmJk7nw2PdOx22upPkg0c3rnQGLJJ2yoAyYg8+cXhQ4aUw1QzM3gr4HL6CU5YiYKxC9edeRRTdg3bVnuvXiRizoPD60zGjj29yZVVXhKshPof7+2v/ki4GPu45qrvXIUhbZ5WSoo5+C7ls69z1qAg2W0CCQMWZI0h9I+pCkd0r6hDHmnT5P3WWt3TD/3x9l3M7CcR0w9ZHhUhcf7OygLYzi37x+oItCh63wViQXLSd5L2tBxY+lC7VZei2gnqlTkiy2oJGcONlIrsBLlJvXibG6Dm65zvnacQNvYRkwXm2WxlRTn3/gSOjruF5vyJiFz+Gq49M9chZFUCe08/sLq/nV7xuxmT5myhXZXRPr9Mlrr1iS+fPoU6fotPVRWIF4F6YyIEtZZJeToY5+ClsF1Nv3GDQfLFF6lO+R9LS19nvW2nOS/lLSR3vbrOLzO2C8WhFB0x08RT6Rex207zvqUXRLMoWsSFyD/kUs5xCyW/VkOkjVxdkN+r1MdJILsitQcP/jz6Wa4uqJcvPqpbm7xJ1GE3YD42X2/B8PPukMXtWWXSgCPjk+uqSukfc6XkBmYqyuu29ct3DMGV3YV+KO2oZ1QqPe/PfiRizsPM7N35y7Jtbptz+2ftG1vznd0uSe8tTHqwLXfRuQlSw6ynS20U/d9zN+vFXD/BBIL6coQaC6pM4lLp6f39btJmPMk8aYPcaYgc+B7j5ghoxZvLrMzesDOzJFPpHHnYf8kXe9Jdb2onnFMd3BtT1PQavKdXZiEV1WwUpvqk7nlKJ7D53saSAoyQXZde6ZsVavnY++auFwzf/y4cri8bZ3Zgq5xC2o652PXbwzcdDn637E9czO87wXMK+PDC8JFkYJ3Hjn2iiZT1GuGSsD6lslFXYe5+ZvTmOqqc89cGTJEuXtGavtD5WjPl7ZrXSckzzDtWW6+8Z1izIeXxfyb4C4sugo09lG0XjLxhNIHxxZXf0ekrTGWvsuSX8r6U/9nmSM+Ywx5rAx5vCpU+WfPzgxVl84ILxOizcCvPvwycCOzCUFLQydZB7y3ieej7W9aAbmYksN0kTuvH6thjJYWzvtMtFJ+iJJLshZ7dd33/iuRX+Pmt0Tpah+knq6E2N1Z5HvKMGRmdkLnfXtDx3XTEChoe6gTZJR2yjBsE5Rfrez59JncnULmmKW5uavTEVPw9rqraDmuuQnnaaEeFYsDz7Ob7rmrZIWB4PPnG1TawWZyqKjTGcb/RR2P+Lte50JEF7JEG+xDJTP8gjPaUrqzOx56/y2Bdbalzv++keS/h+/F7LWflXSVyVp48aNBZxsE59r6sPBZ4KLprZnoo+495Pr89z+wNwSwX4Huis7JShrpUjWvHHY98TnWv0oTyPDNWeh1vbMXH0OTsbxLZPk6j5HrZOTdpnonbds0OZdRyI91+P91t5cbW+kJmgfmBwf1da9x1JN/br6zRcveg+v1k5QgWbve4iS0ZJ0KqYrCBI1OOJ11qN02q/csl+XDNdkjHvKYFDgJs4Kk94NWNj+0YuLatDvlfTmz7vh9D6/N9gg+V9j8hSlrWErqKE/wrJ3H33qlB596pRzumrR9j2UU5Lrci9eA4gq6H6k3rXvTYzV2Q8HRJQg0DclXW2MuVJzwZ+PS/pXnU8wxrzFWusty3CDpO9k2soCS7py0Ks9GLHNQtBUkaLepKf1j45Vjlzb87TthrWa3H3U2eEo8jTDotp54ERgB+6X1keb1jhkjG/AJ2p9G++4ShIISnJzGfd9On33pVd12z2P6b5Pv0+S9IW97lo7Hm/q7OoR/6BrWo2pppY5fgMvGPM/vGGF/vuPzmXyflbBK2eFjdpG/Q46b8DCfrNeJAO6fq/6yHDia0FQ3aiiXV+itLUsK2EOupGVtcAAbtAxx7UTWcqio0xnG/3iOv8ZKXARD5Rb6AQEa+15Sb8u6YDmgjsPWGuPG2O+ZIy5Yf5pv2GMOW6MOSrpNyT9aq8aXDRxC5gWXdDItavGhesrKMtXU6bC0BNjde28xV1vqnRT2Aog7OY/6opLvV4mOoMZawvTWj6XIgDk6cx2DMv66wyIhBVxTsLL1vALAHW+9z//OLiz7mV9rQirwB4iSop0L64dKxyrB6aZeuX6vc6eO594Ck2Zip7m2dYyTZkrgjTXbK6dAKpqYMpiIJYomUCy1j4s6eGubV/s+PNWSVuzbVo5xC1g6omzFHM/feAdqwKXIfa78X3DiiH98LWlmU1v6EGRUswFgg4/e9r3d3It/w23sMyUqBkbd03MFSa+//HnNGOthozRJ957+cL2MF4gwyXtbJPuaS394pdKLM1lWLi+27jFul2pzEPGLArGhJ2vvayv7uK+cUUZOYt67eicfrRM7oLV0lytkyu37NfqkWF94B2r9OhTp9ScbvmuXCZFy+r0zjf3HTq5aLrZmbNtTe52TxMO4jrminjDmVdbyzRlriiSLuZArRUAVeZXJoDz4uBjWYSUki7Jve2GtRm3JBthWQ9+N75+AaCg7WVSxJHXxlRT9zkCdX919EXf7XALy0yJk4Fz18Q6PXP3h/WDHR/RM3d/OHIASAqvEZM2byRODZq4grIBD265bkmn9fCzp/XfXvmJ77+pDRndeX2882PQNNbDz57WHY1jumrrw77P6eSd//qRBBgnEcjLwoxSZc0r6H/voZMLwQu/lcs27zoSKbukMdXUg080fb+T9qzVtn3RV77qXA2t++MX9YYzrwKtQdPQ4C9pYO7dV1xCYA1AZVHwuZoiZQLBzRU9ffcVlwQWhy7qgRWW4l61TJOijbx6o8OuTiq1KeKbGKtr9+GTzuM1TgZOY6qZuJBj2LFnpYUOtFd/qDvLJs3rp/Hzb7/M9/uzVrpq68P6xHsv18a3XRaY/SMtzRqKKiibKyizsZv3Gq76TlFt2vFI6OeI+/K9qKMUJbskLHgY9ZzTmGrq9t1HF1Zd6/z4SX/3fsirQKvr9+7FfjAokha9/8dnTqsx1Szk/gcA/UANquohCJRS0A3imi37c25dfGFTY/7q6IuxshvKIKjDV7Ripb3M5qiqOxrHQlfziyLt9I0oBZO9x739Nc57ZF2QuTOT4wcvu193xlrde+ik7v/Gc4HLrktzn2f34ZOxj7ewaaxxXfv2S333iYuWL1u0vLSL97scfva0Hn3qVKFXdwk7x2UVPPz3/+WY8/dP+rv3Sx43x2kLzZdRmiC6lLzovZUKdZ0HgCJKe45GsTAdLIXuIqtfuXWD79SHMgnL9PEb9b36zRf7Pte1vWjCRvyLVKy0SG0ZFPc9Hhw8qEU8S6advrHmjcmmMkR9j6wLMnceNVGCS2EBIM/BZ07rtnsei9WWqMW7o3IFtaIEgDyt9szClCxvitbndh3RHY25oF3cuke9FHRecRWb9kT9HGErYib53QfVbfc85rwupclQKzIviN55vGzdeyz2lOyk919kWKFovD7Gmi37ddXWh7WGAvHIUVbn6KJg4QWCQIkN2sHgSVJT5tc+cHWs7WXzuqhRgD5YGVJse7hAbS2LsD5V1BrBaVcRSpONFOU9vDnfvdhHss5OiPtdZB0c7VWw1Uq679BJNaaauvP6taqlXIUsK0G1VIICX0nqNwXJIiOv7G6757HA7yFpHcKiy7IGUpIAaxarLwJZ6exjSIuzfyf3HC19XwPl4zpHx6kLWBSNqaYm9xxd1Iev4nFFjzEh18Fw+wNHdeWW/fof/8+/dv7bIt9shNV38Gu76yZtUApYxhn977WzIaPpr8t46W1ErwmU5xKbYcFBz8RYXd/5vz6kT157Rabvn3d2wkjGWTW9/M2spG37jmtirK6dN69fKMSYV/w2TZHjW3/u8siZFwW+7BVKUACotswUsnh2FtIG0T2/+OW/05mz8WvjpV19EfCTNNsgaOp/e8Zq+0Pl63ijPPz2W1e25HSrXbrgyfaHji9ZBbaKxxVBoISCVqOxklptd+Bg1qq06Wd+N0pZ3bwVVZFuDsOaMp3g5rfqwjqnUYO2rqmU/SimHhYc7DZodb1ey7hOVtZT57p5N00TY3Ud3HKdvnLrBi0f6n8AN+0KIA8+0Yx8Dfv5qy5L9B6DYGVGEb7zRboYZeySYf9Armu7n1/88t/puy+9mlWTgFTSzBgIu39OEugEovDbbyd3Hw38N9sfOl6q6VWu46dqxxVBoITSjhQXdQrZxSEZBX6p6M6bNFPMJdbjKlIhzrC2ZJ0RUQVh3aqoM3ZcdWmyrlfjJ27XMIvjstdHRZw6CGcDgu5JeFPneqkzhTqPgu/1keFINewuCqgJFGe6zrdf/FHoc8pSRy6OZUb6Dze+K5PXspK+sPfJTF6raFyXtjiXXwJAKJI0Uxz7kUEMdPKCOJt3HVmy37ZDBiDOnG0PZImUQUcQKKHJ8dFMOkFJ57z3Sm0oeJfwy2poz/h3wKzVQJwE3r5qZd5NWPCJ914e+PiA1gztqbAaG1HjC2VZ0tkb5Ukr6hS0NDrrIGzedURjX/p6384nE2P1wABIWp1Tb/PImoz6nmHTYaO+TpQRtrPn8pt6m2YUcyQgW+VfvfeKTBeLyDrgWRR5j8wWaKwHAyJNlvzk+KhqAWnIQeccIK7uGlRJZFXTDf1DECihibF67NF3lyJNm3olpCbQ/ieXFo4OWvVlEE4CT58qzujiXRPrAuu5hP1+WGrlimxOg64sraiZZP3qg2SVddJ53Per7d5oU3cHPcti1xu2f11rtuzXmi37+1YPLI9R36hZg2H7b9jqYXG8MN3SHY1jCxlgV219eGE1tV5Ku9DDthvWOjtscabMlVUW0wDSnj/TYgAFWUtdJ9Cx69eWGW27IbuC/ECvspGL1L/FUgSBUshqlY4ipX2Gzb9PMipXtEyIbmEjKkW7ObxrYp1z38t6Xwq7uU/7eBGETSGI2gdJu6Rzv3azrC7KnX3efh4ifoHlLLuJYcXxe6HXNYj8RD2vhe2/UQNlUQJ1VtK9h04uvOeMtbr30MmeBoIaU03d/sDRVKOYE2N13foe/yzNXgyEFOmcmtVKqWnPn2mxsiay5ndej1qIf+eBE0sK10pzQdGdt6zPNLtw0JXhPjRvae8LXfdgRerfYimueilkVfC1H4Vjozp3PvtIcJFXQ5OkX1r/lrybEMsdjWO+J+w0q/z4Cbu5T/t4WVgbrdPlCswVbUnnrC7KnVPEsxqtj/o63ft/GafIrKwtW7g5/dyuI7po+bJES1snFTVrMKtpB8tS7CP3P/5cJm3o5p2jXIGGqDfGjammdn3T3casB0KKdE7Namn3vM+fQYt5AElMjNV10zX1hevakDG66Zp6pACO65wxY23fAkB5ZGVmjaXAo8mizm3SgCfyQxAoocZUU7u+kc2NaT8Kx0YV1plK0iEo+oImRfr+w9zROKZ7D51cknlhpMg3F1GF3dynfbxMOm8gNu86orVf/JslNxFr3uh/EXVt79avOf69uCiH1aqK6rc/tl61CJW48x5dyiKwbaVFQdLpVls/ac/qk9de0ZesoKjfYVazcYKmDYfpVTZIWAp81ClzfsvN9kve59SsVgdNkzUBuOSVBdKYamrD9q8vyWyMOj007+mR3r1mP7Mye4GlwKOJep8a5O4b52YpGKVfeTRPVQoQEgRKaOeBE6HV0qMq+nQpz6DOQw67WS1SIpNrRNxK2vWN5zI9eYXd3Kd9vMxePTezZDTpse+d9n2ua3u3fh1bWV2UO5e93vi2bJb/3rbvuGZCzqt+HcN+ZtBI2QS2W+1Z3yDpo0+dWjJ6nLXO7zCskzQdMgV4Ux+Wfu9VxyfsXPRaSI0E77tLMk06y85pc7qV241r6ron87wV+dJ0Ioq0kqfENJS85ZWN7L2v39TiqEHbvKdHuu41e5WV2St5F5wvi3+MeJ/qcvGKIU2M1XVwy3X6/o6PRFp5tKjyzq7tJ4JACWXZkS3SjYurM7XMaGDnIYfdrBYpkSnoBqA9axctO52WaxTc2x528++qLxVWd8qlaDfU7Rm76GbOFRiIGjAo27HVuSpgVvvddKsd+n35dQw/8q5yTekM0pxu6cEnmotGYLPU2bmO0kkKOz/esvFCofpeHaNZZZp1C/tsXmas3+dKs5pK0PeeNMtscnc+UxyKlMHTrw5yFIMyHbrM8spGDsswbM4XwA8S1C+47Z7Hen4/lHcQCv2V9mftRSmRXrr6zRc7H8s7u7afCAIllOV0hCKdVF2dqVk7d2Eryg1MlhfAPIqy9sp0q53Zd+MaBfe2h938dwYJOrm2B2lMNTW5u2ted4pOT+d3lEaWweB+HVtZpXN3zhztVzHl+siwb7Bs7xPP9+X9s3TxCvc5pxerdEhzHYvOEbqgTpJ3jIQFObybpV52erPKNOsWpR6f69yz/aHjkX+n7prDQd/7qteviNr8RbIeBIgqTd2TTlnsP8UZThus6dBl5Tp39Tr7Psp9QdjUqqB+wcFnTuvzDxwhwIjCKFtJtbPnghs8CDMWoiAIlFCWo1z9nsoQJKg+TtoLTRbBCW+e9eZd2V0AvTR0lwIlakXyuYy+G1d9KG972M2/qwZIktog2/YdXzL9Mmmnp7uzkYaX1RT0/UZddaZfnYOypXN36j7veueUMhaGnrW278Hnt69auejvQZ0kL/ARxrtZ6mWnt1fHxv4nXwx8fGS45jz3xJlO8PrXXbjGN6aazu/1hemW/vuPzkV+3W69CMaGFYdtTDWXZK5FrXvSKYv9pzjDae5ORJ5T96omr7o6UQeJg67FYfcN3RmzBBiRt7SFxPuZ7R8W5Ek6Y6FsCAIllOXUjelWuzBTXMJu+pNeaLIY5ctinrVL0O/pDcgUYaWEKDcv3TfCvbo58Lv5v/fQSY196euZ78euzk2STk9YqnYc0622Nmz/uib3HPV9fJmku29816Jtrv2oH7XBGlPNzDIP8171L810nCJotWcXaqD0y3dfelUbtn9dY1/6emgWXNSad16Hp5c1wHr1GwcFcrwaeFkEVry6Sl5WkUveBc+7RSkO6wrebN51JNY9zaDVkFsZkOmXZdZGEe5LiiqvKU1Rs8uD2vHa+fgDG1kGGF0LVfRrAYusrHAsNOHa7lK0cgRFlKaQeL+nz4ZOBT93vifvWzTL824ALgQZvJ1eyq8+yJAxoRdI70ITp41Bo3xRXyes857mZrEx1XR+9vrI8MLNsMc7wUnSXRPuLKKsNKaa2nngROKbl17cSLt+jzNn2wv7cRFl/V0EdRLfd9Vli/Zv137Uua1XvItsVq5adbGu/sL+vqYBf/6BI/rcriNaPTKss+fO92zaVL9MjM1lzl25ZX/fshiyzBYxmuvwzNWy8T9/ZhHY8AKO3nnwhemWVo8Ma3J8tGfXSq8G3uZdR5zPqQ2ZSKuCed+BX1ZRpw+8Y1Xqc0HQdxT3+wsqDutd94LOp949zeFnT+vRp04Fvu/qkWHfYF/RAmNRnQ3IeI177+MS5b6k18dMP4/JuEaGa77nu14HMibG6jr87Gnd57OSa1RJFx/Iqg+R5XT+PJ1znJ9d2/14y8x753pvmXmpfLUcXVbWlvUkozpqPymLPmIcYdfaOPtHmREESuhdd/5NT163lzt9FFGDDHEvNEHp71GFjQanKTg8ueeo87OvXLHMebK47/GTPQ8CeZ33NB3eqEsdd7p0Zc13pNybvhj027XaMzLGv9hcP6Y/Bt2YujobvXDwmdO6o3FsYR/JcypWlhlQ0lxWSb/NdgTMB0k/98ksWUm7D5/Ut06+4nv+zKpA8Kxdeh7s9aDJ7sNz53zXeUySZOcCVGEdNq/2UFgA7sEnnteQkdLcfy7prMzXLzpzti2jC5mind+fJN/zZZRMikscHW1Pqz2zqDPs+t0mx0eXXOfKvER82E+YxfEeFqTr9THT72MyLlfidK+n+Demmtr1zedSBfY7j9U4supDZDmdv+yClpkvwn6ehYtqQz2bVh8lENTvTNCg0idVwnSwhH74Wu9OhM3plm6757HMXi9OGmPUqQlZTTFyjfIlSb1MOkLhd4LvFNTZtVY9Tw3NovOeJIHozuvXqtaVMlsbMrrz+rmlzENXVfN5z85/3ythaaVRisFmqfNGvZ9F4LuPoTIGGQbZytqyyMWXi+zgM6ed56fXddS1SDvtLWjaUS+mwBx85rQ+t+tI4LmzPWv1U68LD2pHLVzeas/qp1JmKSzprHTUL/KbKrxt33FN7ukqfL0nvOi+d92L0qGOMkU5iyXiyyRqXZqge6GwIF2vi1NHff28ptLktTx42D2lJ2gfSHOnUObrST9F3RfLuMx83GNuusefJWwANGy14axxjMwhE6igDj5zWrfd85ju+/T7Ur1O0EiNtHT0z280ziWLg2jliqVxyLA2uyQdoUh7Iu/16FcW37PfSO0djWO6//HnNGOthozRJ957+aJIfecKQn4ZNWH7it9I1nvWXJr5d9Sd9eM3TahzdKzfIwAz1mrTjkc0OT6aeHQvLr9jCMXSnplNneFXdJ1TQyfHRwOnVgVZMWQCRwR7NTU3yrH6SoTpdd4I68UrhkKvU72+GV/yfj7t90a5g3hZRlFrR/n9+27e9MgqiDIgEJZpE3Q9uaNxrOej62Gvf0fj2JIpUf3MFnJl8fU6EyjqPeWMtbFLKyC6sMBHc7qlzz9wRFLyfbFov19jqqlt+44vOq9HOebCMjrTCtvXXdOzejVoG+VevGi/bS+QCVRgB585nfo1XCM12/Yd982WkKSbrqn3balVvywbV5uLLKvRNb/ofS9+iygFP8N4q4O5+J1gDz5zOvNRwNu7lm923YB5nY48AiLN6ZY27zrSt7ovWU/9Qvbas8U/r2Uhi+kJ52asloX03PKaahl1pLIx1Yw0UBH2OfslSkc2aQBI8s+C8K5/a7bsX/RflpnRWUmb3RKlsH5Yps2K5e5b+HsPnXQWp44zuh70OV3TzEdW1hbuMfz2kH6tZOWKs8VNyO1lJtPmXUd6spgGoq0qOWulL+x9MvA5QTWkelm8OK4oi+e49uV+1HoK+q5cg7O9GrSNcgq4fXd4RmzZEQQaYEFL0U632s7g0F88nryYXVpBbQ6T96oFaUfXvNpE3Wn5Wf0WnQGeoFoCYe3xToqNqaZ2fSN+x2ty9xHfC1HSG62ZGB0RrwD4oCvrijoYTC9Mt1JP2QrLnIg71TKrm7s1b4zWoY6SBWXU3ymjeer+nJ3Xm25eZnSvRd0n/KYcfy7mtMQoly3XvZC3OEfYClKvnptZMogUp85SY6qpz+06suRzet/Ta44g9pmz7dAC52XJTm1MNZcMNIV1DuPei5452+7ZtNYkgj5bme6eot4HhdXC+aX1b3E+lmVAM02wsTHV1OceOBI4sOQN9vuVSuhHraeg76qIq0POzFrdPp8pNqiYDjagGlNNfT5B6n3cdMBNOx6J/R4uYasXhaXvrXrDCufr9mP1is76F3F47fO7KYoyrzyqziLWYbUEulcd6WyPVwwvbKUbl/bs3CpPnUV+P//AEQ0tM4sKmm7edWShOGtWvrD3yYHtZHWmrpa10DAG03DNXVw/a1HP91nduGeRsesZzDOTv+5MmLA6Kl4WaS+v5Z0Za0Hvtf2h40s6W1bSfYdOauPbLsusTa4VS42iBRW9dnXq7IiFtfPf7V46CGXntx9+9nSqQrKuTDC/7zzp7x51+nXn1HgjaeWKIZ09N6PVI8P65x+/tmSgaWbW6t//l2POtm27YW2iqZL3Zrz/JBV0bkxzjuouQXDt2y/VD15u9ex4HnEsbhJHY6qp+yIENK/csj/VZ0hTZL0x1dTk7qOhGW7GLM0+7ldWnsd1X1rU1SFnrBYt7jJoCAKVjN8FR9JCECHKEu9ZyrKjGTaFJexTffelV5fM4XQt7di5ZG1WWu3Z2HNIG1PNxHUy4vJ2i7DRBVcAyONdVNPMH+6+N5q10qxPByDLDpYUPuJTZpt3HdH2h47rzuvXZrLUNJCVfh13fjfSm3cd0eZdR5bUPSNbLl/d14AonbXOjrVXk0jKrraMdz8TtCR0UFut5s7Df/DodyO936YdjwR2gF33cmnv8KIucX3eEcQ4P2tDO8ZhvM/WOQjWvXqdd+x2t90b4Oy+1+ushXLpylrg9+Tdq3Xf71hdqC8ZdH/76rkZZ8f97hvX6db3XJ7oGpzn6sCeKOfGsJqSfs/v/D5mrF10fxfleI4bJIzTFZr7LZ9Ua/5atcxI73v7ZfrWyVciHW+dmXJBn8ElzRLpOw+ciBRwdH0f/R4w/MUv/53+9vPvX7Rtcnx0SeC0tsxkvjqkt6/Ece+h3q8CnReCQCXgV+hLunDCWT50IYMiTgCotsykmtOftSxuyid3H1m4KPh9Z9JcNkuvOshxL+CTu4/0pB1Bwgp+Rrm5G/R5smV15mxbk3uOaibDDDKgLG5/4GhgluO9h07qvkMn9ZVbN1Qq66ao4g6adN+vtGftQsbot05Op26Pl5viWhL687uOKEo4M2hF0U6ddeo2J+w8JtWesfrC3icTZ1Zlcfx0B1Givuas5oJtOw+cWOgkdgeLwoKK3r1ammBWUM3NpINkveiQRwmexPk912zZv+jv3rlVchfnj1KzrT1rtW2f/7LrroDb4WdP68Enmr4ZNHF+g+79Z9YmG4S0kv73PUdjB7Jc/Z8o+0PZsr6/+9KrvgsfdV+7s05o6A7uQzI2p6kRGzdutIcPH87lvbPQfRLslU9ee4X+4tDJSDceZfWDHR+RJI196euFXnIxKu/zRNGv/cjzgx0fyeQ960w3AgCkMGSMfvtj6zUxVu/7tdAlq2tkmvf39LsdRtJt116x0JHv9ftncR8xtGxuCbAk/bp+rdYZVxbfy+/cumEh06l7hTYj6eevukzf+P6ZzAeCvf23O9gR5/MYaUlQctOOR2K9Rn1kWP/tlZ/kNv3frx+QNAhx9ZsvXpI506ko5864Or+jDdu/7hu0Gxmu6cid//PC3zv3q+XL5spLeDZddVngitqjd/x1aB21KG0tG2PME9bajX6PkQlUcFWa0vGTAVkpp/OEfOnKmu68fq0OP3t6yYU4D1kV1yQABABIY8bawDqAVfQzX3hY/+8t63OZEmTV37o0WdxHxFkYolve92MuWXwvXkFbv/tOq+yn2ne67Z7Hlkz1isObWuVNB0wSFCviPWpY3TOXqJmFZRMleNUZGOrer7pnmR985rR+8ct/p1M/OrdoWuid16/V3Q9/O3EAaJCRCZRQWSOvRfTJ+ZEnvlMAAJCXvDOBisBI+sqtG/pWrxC9QcZ2frzMkc7aSUjmd27doMPPns41KWJQM4EiBYGMMR+U9B8lDUn6I2vtjq7HL5L0Z5KukfSypFuttT8Iek2CQAAAAAAAoIgGNQgUuqa1MWZI0h9I+pCkd0r6hDHmnV1P+5SkM9ban5H0FUm/la7JAAAAAAAAyFJoEEjSeyQ9ba39nrX2nKS/lPTRrud8VNKfzv95j6RfMMYYAQAAAAAAoBCiBIHqkjrX9nt+fpvvc6y15yW9IumN3S9kjPmMMeawMebwqVOnkrUYAAAAAAAAsUUJAmXGWvtVa+1Ga+3GVatW9fOtAQAAAAAAKi1KEKgp6fKOv791fpvvc4wxyyVdorkC0QAAAAAAACiAKEGgb0q62hhzpTFmhaSPS9rX9Zx9kn5l/s83S3rE5rX2fJ+UuVI4AAAAAADwN8j9/eVhT7DWnjfG/LqkA5pbIv5r1trjxpgvSTpsrd0n6Y8l/bkx5mlJpzUXKBp4g7xjAAAAAACAwRIaBJIka+3Dkh7u2vbFjj//RNIt2TYNAAAAAAAAWelrYWgAAAAAAADkgyAQAAAAAABABRAEAgAAAAAAqACCQAAAAAAAABVAEAgAAAAAAKACCAIBAAAAAABUAEEgAAAAAACACjDW2nze2JhTkp7N5c2z9yZJ/5x3I4ABxLEF9AbHFtAbHFtAb3BsAfG8zVq7yu+B3IJAg8QYc9hauzHvdgCDhmML6A2OLaA3OLaA3uDYArLDdDAAAAAAAIAKIAgEAAAAAABQAQSBsvHVvBsADCiOLaA3OLaA3uDYAnqDYwvICDWBAAAAAAAAKoBMIAAAAAAAgAogCBSDMeaDxpgTxpinjTFbfB6/yBiza/7xx40xa3JoJlA6EY6tXzXGnDLGHJn/71/n0U6gTIwxXzPGvGSM+SfH48YY87vzx92Txph397uNQBlFOLbeb4x5peOa9cV+txEoI2PM5caYR40x3zbGHDfG/KbPc7h2ASkRBIrIGDMk6Q8kfUjSOyV9whjzzq6nfUrSGWvtz0j6iqTf6m8rgfKJeGxJ0i5r7Yb5//6or40EyulPJH0w4PEPSbp6/r/PSPrDPrQJGAR/ouBjS5L+oeOa9aU+tAkYBOcl3W6tfaekayX9ms89IdcuICWCQNG9R9LT1trvWWvPSfpLSR/tes5HJf3p/J/3SPoFY4zpYxuBMopybAGIyVr795JOBzzlo5L+zM45JGnEGPOW/rQOKK8IxxaABKy1L1prvzX/5x9J+o6ketfTuHYBKREEiq4u6bmOvz+vpSelhedYa89LekXSG/vSOqC8ohxbknTTfNrvHmPM5f1pGjDQoh57AOJ7nzHmqDHmr40xa/NuDFA282U1xiQ93vUQ1y4gJYJAAMrgIUlrrLXvkvS3upBxBwBA0XxL0tustesl/Z6kRr7NAcrFGPN6SQ9K2myt/WHe7QEGDUGg6JqSOrMP3jq/zfc5xpjlki6R9HJfWgeUV+ixZa192Vr72vxf/0jSNX1qGzDIolzXAMRkrf2htfbH839+WFLNGPOmnJsFlIIxpqa5ANB91tq9Pk/h2gWkRBAoum9KutoYc6UxZoWkj0va1/WcfZJ+Zf7PN0t6xFpr+9hGoIxCj62uud43aG6OOIB09kn6X+ZXWrlW0ivW2hfzbhRQdsaYn/ZqQhpj3qO5+20GBYEQ88fNH0v6jrX2y46nce0CUlqedwPKwlp73hjz65IOSBqS9DVr7XFjzJckHbbW7tPcSevPjTFPa65g4MfzazFQDhGPrd8wxtyguVUjTkv61dwaDJSEMeZ+Se+X9CZjzPOS7pRUkyRr7X+W9LCkD0t6WtJZSf9rPi0FyiXCsXWzpH9jjDkvqSXp4wwKApFskvTLko4ZY47Mb/uCpCskrl1AVgzXJAAAAAAAgMHHdDAAAAAAAIAKIAgEAAAAAABQAQSBAAAAAAAAKoAgEAAAAAAAQAUQBAIAAAAAAMiZMeZrxpiXjDH/FPH5HzPGfNsYc9wY8xeR/g2rgwEAAAAAAOTLGPM/SfqxpD+z1v5syHOvlvSApOustWeMMW+21r4U9h5kAgEAAAAAAOTMWvv3kk53bjPGXGWM+RtjzBPGmH8wxrxj/qFPS/oDa+2Z+X8bGgCSCAIBAAAAAAAU1Vcl/W/W2msk/TtJ/2l++7+Q9C+MMQeNMYeMMR+M8mLLe9RIAAAAAAAAJGSMeb2kn5e02xjjbb5o/v/LJV0t6f2S3irp740x66y100GvSRAIAAAAAACgeJZJmrbWbvB57HlJj1tr25K+b4z5/zQXFPpm2AsCAAAAAACgQKy1P9RcgOcWSTJz1s8/3NBcFpCMMW/S3PSw74W9JkEgAAAAAACAnBlj7pf0mKRRY8zzxphPSbpN0qeMMUclHZf00fmnH5D0sjHm25IelTRprX059D1YIh4AAAAAAGDwkQkEAAAAAABQAQSBAAAAAAAAKoAgEAAAAAAAQAUQBAIAAAAAAKgAgkAAAAAAAAAVQBAIAAAAAACgAggCAQAAAAAAVABBIAAAAAAAgAr4/wHv+MZXzAK1MgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "aux = df_train['ratio'].values\n",
    "\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "plt.plot(aux,'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "prediction = submission['ratio'].values\n",
    "df_baseline = pd.read_csv('../../results/Submission_ratios_01.csv')\n",
    "real = df_baseline['ratios']\n",
    "\n",
    "\n",
    "y_actual = df_baseline['Demanda']\n",
    "y_predicted = submission['Demanda'].values\n",
    "\n",
    "rms = mean_squared_error(y_actual, y_predicted, squared=False)\n",
    "rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "fig = plt.figure(figsize=(30,5),facecolor='white')\n",
    "\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(prediction,'bo')\n",
    "plt.title('prediction')\n",
    "\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(real,'go')\n",
    "plt.title('real')\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(prediction,'bo',alpha=0.7,label='prediction')\n",
    "plt.plot(real,'go',alpha=0.1,label='real')\n",
    "plt.title('real')\n",
    "\n",
    "plt.suptitle(best_model_name+'_MAX_EPOCHS_'+str(MAX_EPOCHS)+'_val_'+str(np.round(rmse_val,2))+'_refencia_best_pred_rmse_'+str(np.round(rms,2))+'_PREDICTIONS')\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(\n",
    "            '../../results/models/tft/'+best_model_name+'_MAX_EPOCHS_'+str(MAX_EPOCHS)+'_val_'+str(np.round(rmse_val,2))+'_refencia_best_pred_rmse_'+str(np.round(rms,2))+'_PREDICTIONS.png') "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_actual = df_baseline['Demanda']\n",
    "y_predicted = submission['Demanda'].values\n",
    "\n",
    "rms = mean_squared_error(y_actual, y_predicted, squared=False)\n",
    "rms"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#rmse_val = 3\n",
    "submission['Demanda_real'] = df_baseline['Demanda']\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "tendencia_semanal = submission[['Z_WEEK','Demanda','Demanda_real']].groupby(['Z_WEEK']).sum().reset_index()\n",
    "\n",
    "graph = pd.melt(tendencia_semanal,id_vars=['Z_WEEK'],value_vars=['Demanda','Demanda_real'],)\n",
    "\n",
    "#fig = px.line(graph,x='Z_WEEK',y='value',color='variable')\n",
    "#fig.show()\n",
    "import seaborn as sns\n",
    "sns.set(style='white')\n",
    "sns_fig = sns.catplot(x=\"Z_WEEK\", y=\"value\", hue=\"variable\", kind=\"point\", data=graph, height=4.27, aspect=25.7/8.27,facecolor='w')\n",
    "plt.title(best_model_name+'_MAX_EPOCHS_'+str(MAX_EPOCHS)+'_val_'+str(np.round(rmse_val,2))+'_refencia_best_pred_rmse_'+str(np.round(rms,2))+'_DEMANDA_SEMANAL_TOTAL')\n",
    "fig = sns_fig.figure\n",
    "fig.savefig(\n",
    "            '../../results/models/tft/'+best_model_name+'_MAX_EPOCHS_'+str(MAX_EPOCHS)+'_val_'+str(np.round(rmse_val,2))+'_refencia_best_pred_rmse_'+str(np.round(rms,2))+'_DEMANDA_SEMANAL_TOTAL.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submission['Demanda'] = 0.9#submission['value']\n",
    "import numpy as np\n",
    "# +'_refencia_best_pred_rmse_'+str(np.round(rms,2))+\n",
    "submission[['ID', 'ratio']].to_csv('../../results/models/tft/'+best_model_name+'_MAX_EPOCHS_'+str(MAX_EPOCHS)+'_val_'+str(np.round(rmse_val,2))+'_RATIOS.csv', index = False, sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Z_WEEK</th>\n",
       "      <th>ID</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SEMANA_51</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "      <td>1.535267e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Z_WEEK                                                 ID         ratio\n",
       "0  SEMANA_51  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...  1.535267e-07"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Z_WEEK</th>\n",
       "      <th>ID</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SEMANA_51</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "      <td>1.535267e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Z_WEEK                                                 ID         ratio\n",
       "0  SEMANA_51  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...  1.535267e-07"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "bb936826e9998cf8c0ce19ca18d08375e9e9e4488ea4df72de7138dd75138a24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
