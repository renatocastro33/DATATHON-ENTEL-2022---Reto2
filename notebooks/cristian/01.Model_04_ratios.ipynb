{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from time import sleep\n",
    "\n",
    "def get_distance_from_paydays(date):\n",
    "    end_of_month = date.daysinmonth\n",
    "    distance_to_1st = 0 if date.day >=15 else 15 - date.day\n",
    "    distance_to15th = 0 if date.day < 15 else end_of_month - date.day\n",
    "    return distance_to_1st + distance_to15th\n",
    "\n",
    "def std(x): return np.std(x)\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            if str(col_type) == numerics:\n",
    "                c_min = df[col].min()\n",
    "                c_max = df[col].max()\n",
    "\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            if str(col_type)[:5] == 'float':\n",
    "                c_min = df[col].min()\n",
    "                c_max = df[col].max()\n",
    "\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "## Simple \"Memory profilers\" to see memory usage\n",
    "def get_memory_usage():\n",
    "    return np.round(psutil.Process(os.getpid()).memory_info()[0]/2.**30, 2) \n",
    "        \n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f%s%s\" % (num, 'Yi', suffix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 107.97 MB\n",
      "Memory usage after optimization is: 33.78 MB\n",
      "Decreased by 68.7%\n",
      "Memory usage of dataframe is 21.59 MB\n",
      "Memory usage after optimization is: 4.08 MB\n",
      "Decreased by 81.1%\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    df_train = pd.read_csv('../../dataset/train/train_converted.csv')\n",
    "    df_test  = pd.read_csv('../../dataset/test/test_converted.csv')\n",
    "    df_train = df_train[['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA','Z_WEEK','Z_WEEK_DATE','Demanda']].groupby(['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA','Z_WEEK','Z_WEEK_DATE']).sum().reset_index()\n",
    "    df_test = df_test[['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA','Z_WEEK','Z_WEEK_DATE','Demanda']].groupby(['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA','Z_WEEK','Z_WEEK_DATE']).sum().reset_index()\n",
    "\n",
    "else:\n",
    "\n",
    "    df_train = pd.read_pickle('../../dataset/train/train_converted_fill.pkl')\n",
    "    df_test  = pd.read_pickle('../../dataset/test/test_converted_fill.pkl')\n",
    "\n",
    "df_train = reduce_mem_usage(df_train)\n",
    "df_test = reduce_mem_usage(df_test)\n",
    "\n",
    "\n",
    "df_train.replace(['',np.inf, -np.inf, np.nan],0,inplace=True)\n",
    "df_test.replace(['',np.inf, -np.inf, np.nan],0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA','Z_WEEK']:\n",
    "    df_train[column] = df_train[column].astype(str)\n",
    "    df_test[column] = df_test[column].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['ID'] = df_train['Z_MODELO'] + '|' + df_train['Z_PUNTO_VENTA'] + '|' + df_train['Z_GAMA']\n",
    "df_test['ID'] = df_test['Z_MODELO'] + '|' + df_test['Z_PUNTO_VENTA'] + '|' + df_test['Z_GAMA']\n",
    "\n",
    "base = df_train[['ID', 'Demanda', 'Z_WEEK_DATE']].groupby(['ID', 'Z_WEEK_DATE']).sum().sort_values('Demanda' , ascending = [False]).reset_index()\n",
    "base_cum = df_train[['Demanda', 'Z_WEEK_DATE']].groupby([ 'Z_WEEK_DATE']).sum().sort_values('Demanda' , ascending = [False]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Z_MODELO</th>\n",
       "      <th>Z_PUNTO_VENTA</th>\n",
       "      <th>Z_GAMA</th>\n",
       "      <th>Z_WEEK</th>\n",
       "      <th>Z_WEEK_DATE</th>\n",
       "      <th>Demanda</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>SEMANA_01</td>\n",
       "      <td>2021-05-17</td>\n",
       "      <td>0</td>\n",
       "      <td>MOD_1|PVENT_1|GAM_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Z_MODELO Z_PUNTO_VENTA Z_GAMA     Z_WEEK Z_WEEK_DATE  Demanda  \\\n",
       "0    MOD_1       PVENT_1  GAM_1  SEMANA_01  2021-05-17        0   \n",
       "\n",
       "                    ID  \n",
       "0  MOD_1|PVENT_1|GAM_1  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = base.merge(base_cum[['Z_WEEK_DATE', 'Demanda']].rename(columns = {'Demanda': 'Demanda_Total'}), on = 'Z_WEEK_DATE', how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Z_WEEK_DATE</th>\n",
       "      <th>Demanda</th>\n",
       "      <th>Demanda_Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MOD_3|PVENT_1|GAM_4</td>\n",
       "      <td>2021-07-19</td>\n",
       "      <td>1451</td>\n",
       "      <td>45037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MOD_3|PVENT_1|GAM_4</td>\n",
       "      <td>2021-07-12</td>\n",
       "      <td>1201</td>\n",
       "      <td>47069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MOD_27|PVENT_1|GAM_1</td>\n",
       "      <td>2021-06-07</td>\n",
       "      <td>1032</td>\n",
       "      <td>38374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MOD_3|PVENT_1|GAM_4</td>\n",
       "      <td>2021-06-21</td>\n",
       "      <td>1005</td>\n",
       "      <td>40481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MOD_2|PVENT_1|GAM_1</td>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>934</td>\n",
       "      <td>43389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358645</th>\n",
       "      <td>MOD_25|PVENT_409|GAM_1</td>\n",
       "      <td>2022-02-21</td>\n",
       "      <td>0</td>\n",
       "      <td>32408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358646</th>\n",
       "      <td>MOD_25|PVENT_409|GAM_1</td>\n",
       "      <td>2022-02-14</td>\n",
       "      <td>0</td>\n",
       "      <td>33511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358647</th>\n",
       "      <td>MOD_25|PVENT_409|GAM_1</td>\n",
       "      <td>2022-02-07</td>\n",
       "      <td>0</td>\n",
       "      <td>35144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358648</th>\n",
       "      <td>MOD_25|PVENT_409|GAM_1</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>0</td>\n",
       "      <td>32240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358649</th>\n",
       "      <td>MOD_9|PVENT_9|GAM_1</td>\n",
       "      <td>2022-04-25</td>\n",
       "      <td>0</td>\n",
       "      <td>28228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2358650 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             ID Z_WEEK_DATE  Demanda  Demanda_Total\n",
       "0           MOD_3|PVENT_1|GAM_4  2021-07-19     1451          45037\n",
       "1           MOD_3|PVENT_1|GAM_4  2021-07-12     1201          47069\n",
       "2          MOD_27|PVENT_1|GAM_1  2021-06-07     1032          38374\n",
       "3           MOD_3|PVENT_1|GAM_4  2021-06-21     1005          40481\n",
       "4           MOD_2|PVENT_1|GAM_1  2021-12-06      934          43389\n",
       "...                         ...         ...      ...            ...\n",
       "2358645  MOD_25|PVENT_409|GAM_1  2022-02-21        0          32408\n",
       "2358646  MOD_25|PVENT_409|GAM_1  2022-02-14        0          33511\n",
       "2358647  MOD_25|PVENT_409|GAM_1  2022-02-07        0          35144\n",
       "2358648  MOD_25|PVENT_409|GAM_1  2022-01-31        0          32240\n",
       "2358649     MOD_9|PVENT_9|GAM_1  2022-04-25        0          28228\n",
       "\n",
       "[2358650 rows x 4 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "base['ratio'] = base['Demanda']*100/base['Demanda_Total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Z_WEEK_DATE</th>\n",
       "      <th>Demanda</th>\n",
       "      <th>Demanda_Total</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MOD_3|PVENT_1|GAM_4</td>\n",
       "      <td>2021-07-19</td>\n",
       "      <td>1451</td>\n",
       "      <td>45037</td>\n",
       "      <td>3.221795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MOD_3|PVENT_1|GAM_4</td>\n",
       "      <td>2021-07-12</td>\n",
       "      <td>1201</td>\n",
       "      <td>47069</td>\n",
       "      <td>2.551573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MOD_27|PVENT_1|GAM_1</td>\n",
       "      <td>2021-06-07</td>\n",
       "      <td>1032</td>\n",
       "      <td>38374</td>\n",
       "      <td>2.689321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MOD_3|PVENT_1|GAM_4</td>\n",
       "      <td>2021-06-21</td>\n",
       "      <td>1005</td>\n",
       "      <td>40481</td>\n",
       "      <td>2.482646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MOD_2|PVENT_1|GAM_1</td>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>934</td>\n",
       "      <td>43389</td>\n",
       "      <td>2.152619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358645</th>\n",
       "      <td>MOD_25|PVENT_409|GAM_1</td>\n",
       "      <td>2022-02-21</td>\n",
       "      <td>0</td>\n",
       "      <td>32408</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358646</th>\n",
       "      <td>MOD_25|PVENT_409|GAM_1</td>\n",
       "      <td>2022-02-14</td>\n",
       "      <td>0</td>\n",
       "      <td>33511</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358647</th>\n",
       "      <td>MOD_25|PVENT_409|GAM_1</td>\n",
       "      <td>2022-02-07</td>\n",
       "      <td>0</td>\n",
       "      <td>35144</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358648</th>\n",
       "      <td>MOD_25|PVENT_409|GAM_1</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>0</td>\n",
       "      <td>32240</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358649</th>\n",
       "      <td>MOD_9|PVENT_9|GAM_1</td>\n",
       "      <td>2022-04-25</td>\n",
       "      <td>0</td>\n",
       "      <td>28228</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2358650 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             ID Z_WEEK_DATE  Demanda  Demanda_Total     ratio\n",
       "0           MOD_3|PVENT_1|GAM_4  2021-07-19     1451          45037  3.221795\n",
       "1           MOD_3|PVENT_1|GAM_4  2021-07-12     1201          47069  2.551573\n",
       "2          MOD_27|PVENT_1|GAM_1  2021-06-07     1032          38374  2.689321\n",
       "3           MOD_3|PVENT_1|GAM_4  2021-06-21     1005          40481  2.482646\n",
       "4           MOD_2|PVENT_1|GAM_1  2021-12-06      934          43389  2.152619\n",
       "...                         ...         ...      ...            ...       ...\n",
       "2358645  MOD_25|PVENT_409|GAM_1  2022-02-21        0          32408  0.000000\n",
       "2358646  MOD_25|PVENT_409|GAM_1  2022-02-14        0          33511  0.000000\n",
       "2358647  MOD_25|PVENT_409|GAM_1  2022-02-07        0          35144  0.000000\n",
       "2358648  MOD_25|PVENT_409|GAM_1  2022-01-31        0          32240  0.000000\n",
       "2358649     MOD_9|PVENT_9|GAM_1  2022-04-25        0          28228  0.000000\n",
       "\n",
       "[2358650 rows x 5 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.merge(base[['ID','Z_WEEK_DATE','ratio']],how='left') # ,'Demanda_Total'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test  = df_test.merge(base[['ID','Z_WEEK_DATE','ratio']],how='left') # ,'Demanda_Total'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Z_MODELO</th>\n",
       "      <th>Z_PUNTO_VENTA</th>\n",
       "      <th>Z_GAMA</th>\n",
       "      <th>Z_WEEK</th>\n",
       "      <th>Z_WEEK_DATE</th>\n",
       "      <th>Demanda</th>\n",
       "      <th>ID</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>SEMANA_01</td>\n",
       "      <td>2021-05-17</td>\n",
       "      <td>0</td>\n",
       "      <td>MOD_1|PVENT_1|GAM_1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Z_MODELO Z_PUNTO_VENTA Z_GAMA     Z_WEEK Z_WEEK_DATE  Demanda  \\\n",
       "0    MOD_1       PVENT_1  GAM_1  SEMANA_01  2021-05-17        0   \n",
       "\n",
       "                    ID  ratio  \n",
       "0  MOD_1|PVENT_1|GAM_1    0.0  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(columns = ['ID','Demanda'],inplace=True)\n",
    "df_test.drop(columns = ['ID','Demanda'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'ratio'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating date_block_num ...\n",
      "(2358650, 6) (471730, 6)\n",
      "Creating date_block_num completed!\n",
      "Preprocessing TRAINING DATASET ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:   0%|\u001b[32m                                                  \u001b[0m| 0/7 [00:00<?, ?it/s]\u001b[0m\n",
      "i:  14%|\u001b[32m██████                                    \u001b[0m| 1/7 [00:02<00:12,  2.00s/it]\u001b[0m\u001b[A\n",
      "j:   0%|\u001b[31m                                                  \u001b[0m| 0/1 [00:01<?, ?it/s]\u001b[0m\u001b[A\n",
      "j:   0%|\u001b[31m                                                  \u001b[0m| 0/1 [00:00<?, ?it/s]\u001b[0m\u001b[A\n",
      "j: 100%|\u001b[31m██████████████████████████████████████████\u001b[0m| 1/1 [00:00<00:00,  1.09it/s]\u001b[0m\u001b[A\n",
      "i: 100%|\u001b[32m██████████████████████████████████████████\u001b[0m| 7/7 [00:09<00:00,  1.39s/it]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing TRAINING DATASET COMPLETED!\n",
      "Preprocessing TESTING DATASET ...\n",
      "Preprocessing TESTING DATASET COMPLETED!\n"
     ]
    }
   ],
   "source": [
    "print('Creating date_block_num ...')\n",
    "N_submission = df_test.shape[0]\n",
    "N_sales      = df_train.shape[0]\n",
    "\n",
    "print(df_train.shape,df_test.shape)\n",
    "\n",
    "dates = (set(df_train['Z_WEEK'].unique()) | set(df_test['Z_WEEK'].unique()))#df_auxiliar['Z_WEEK'].unique()\n",
    "dates = sorted(dates)\n",
    "\n",
    "dict_dates = {}\n",
    "for idx,date in enumerate(dates):\n",
    "    dict_dates[date] =idx\n",
    "    \n",
    "    \n",
    "df_train['date_block_num'] = df_train['Z_WEEK'].map(dict_dates)\n",
    "df_test['date_block_num'] = df_test['Z_WEEK'].map(dict_dates)\n",
    "\n",
    "df_train.replace(['',np.inf, -np.inf, np.nan],0,inplace=True)\n",
    "df_test.replace(['',np.inf, -np.inf, np.nan],0,inplace=True)\n",
    "\n",
    "print('Creating date_block_num completed!')\n",
    "\n",
    "\n",
    "print('Preprocessing TRAINING DATASET ...')\n",
    "\n",
    "\n",
    "df_train['Z_WEEK_DATE'] = pd.to_datetime(df_train['Z_WEEK_DATE'])\n",
    "df_train['days_from_payday'] = df_train['Z_WEEK_DATE'].apply(get_distance_from_paydays)\n",
    "\n",
    "\n",
    "statistics_columns = [ ]\n",
    "\n",
    "df_train[\"log_ratio\"] = np.log(df_train['ratio'] + 1e-8)\n",
    "#df_test[\"log_Demanda\"] = np.log(1e-8)\n",
    "\n",
    "statistics_columns.append('log_ratio')\n",
    "\n",
    "#'''\n",
    "bar1 = tqdm([\n",
    "    ['Z_MODELO'],\n",
    "    ['Z_PUNTO_VENTA'],\n",
    "    ['Z_GAMA'],\n",
    "    ['Z_MODELO','Z_PUNTO_VENTA'],\n",
    "    ['Z_MODELO','Z_GAMA'],\n",
    "    ['Z_PUNTO_VENTA','Z_GAMA'],\n",
    "    ['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA']], position=0, desc=\"i\",colour='green', ncols=80)\n",
    "time.sleep(1)\n",
    "\n",
    "bar2 = tqdm(['mean'], position=1, desc=\"j\", colour='red', ncols=80, leave=False) #'std','max','min','sum'\n",
    "time.sleep(1)\n",
    "\n",
    "unique_columns = [ ]\n",
    "        \n",
    "for column_names in bar1:\n",
    "    bar1.update()\n",
    "    bar2.refresh()  #force print final state\n",
    "    time.sleep(0.1)\n",
    "    bar2.reset()  #reuse bar\n",
    "    for statistic in bar2:\n",
    "        \n",
    "        new_column_name = statistic+'_sales_by_'+'_'.join(column_names)\n",
    "        #df_train[new_column_name] = df_train.groupby([\"Z_WEEK_DATE\"]+column_names, observed=True).Demanda.transform(statistic)\n",
    "        if statistic == 'mean':\n",
    "            df_agg = df_train.groupby([\"Z_WEEK\"]+column_names, observed=True).ratio.mean()\n",
    "        if statistic == 'std':\n",
    "            df_agg = df_train.groupby([\"Z_WEEK\"]+column_names, observed=True).ratio.std(ddof=0)\n",
    "        if statistic == 'max':\n",
    "            df_agg = df_train.groupby([\"Z_WEEK\"]+column_names, observed=True).ratio.max()\n",
    "        if statistic == 'min':\n",
    "            df_agg = df_train.groupby([\"Z_WEEK\"]+column_names, observed=True).ratio.min()\n",
    "        if statistic == 'sum':\n",
    "            df_agg = df_train.groupby([\"Z_WEEK\"]+column_names, observed=True).ratio.sum()        \n",
    "        if df_agg.shape[0] >= df_train.shape[0]*0.7:\n",
    "            unique_columns.append([[\"Z_WEEK\"]+column_names,new_column_name])\n",
    "            continue\n",
    "        \n",
    "        df_agg = df_agg.reset_index()\n",
    "        df_agg.columns = df_agg.columns.str.replace('ratio', new_column_name)\n",
    "        \n",
    "        df_train = df_train.merge(df_agg,on=[\"Z_WEEK\"]+column_names,how='left')\n",
    "        statistics_columns.append(new_column_name)\n",
    "        bar2.update()\n",
    "        time.sleep(0.05)\n",
    "#'''\n",
    "df_train['dayofweek'] = df_train['Z_WEEK_DATE'].dt.dayofweek.astype('str').astype('category')\n",
    "df_train['month'] = df_train['Z_WEEK_DATE'].dt.month.astype('str').astype('category')\n",
    "df_train['dayofyear'] = df_train['Z_WEEK_DATE'].dt.dayofyear.astype('str').astype('category')\n",
    "\n",
    "df_train.drop(columns=['Z_WEEK_DATE'],inplace=True)\n",
    "df_train.drop(columns=['Z_WEEK'],inplace=True)\n",
    "\n",
    "print('Preprocessing TRAINING DATASET COMPLETED!')\n",
    "print('Preprocessing TESTING DATASET ...')\n",
    "\n",
    "\n",
    "df_test['Z_WEEK_DATE'] = pd.to_datetime(df_test['Z_WEEK_DATE'])\n",
    "df_test['days_from_payday'] = df_test['Z_WEEK_DATE'].apply(get_distance_from_paydays)\n",
    "\n",
    "inv_dict_dates = {v: k for k, v in dict_dates.items()}\n",
    "#df_test['Z_WEEK'] = df_test['date_block_num'].map(inv_dict_dates)\n",
    "df_test = df_test[['date_block_num','Z_MODELO','Z_PUNTO_VENTA','Z_GAMA',\"ratio\",\"Z_WEEK_DATE\"]]\n",
    "\n",
    "df_test['dayofweek'] = df_test['Z_WEEK_DATE'].dt.dayofweek.astype('str').astype('category')\n",
    "df_test['month'] = df_test['Z_WEEK_DATE'].dt.month.astype('str').astype('category')\n",
    "df_test['dayofyear'] = df_test['Z_WEEK_DATE'].dt.dayofyear.astype('str').astype('category')\n",
    "\n",
    "\n",
    "\n",
    "df_test['days_from_payday'] = df_test['Z_WEEK_DATE'].apply(get_distance_from_paydays)\n",
    "df_test.drop(columns=['Z_WEEK_DATE'],inplace=True)\n",
    "\n",
    "print('Preprocessing TESTING DATASET COMPLETED!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 240.69 MB\n",
      "Memory usage after optimization is: 103.50 MB\n",
      "Decreased by 57.0%\n"
     ]
    }
   ],
   "source": [
    "df_train = reduce_mem_usage(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 26.54 MB\n",
      "Memory usage after optimization is: 15.33 MB\n",
      "Decreased by 42.3%\n"
     ]
    }
   ],
   "source": [
    "df_test = reduce_mem_usage(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%timeit df_train.to_pickle('../../dataset/train/train_converted_fill_process.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%timeit df_test.to_pickle('../../dataset/test/test_converted_fill_process.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2358650, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Z_MODELO</th>\n",
       "      <th>Z_PUNTO_VENTA</th>\n",
       "      <th>Z_GAMA</th>\n",
       "      <th>ratio</th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>days_from_payday</th>\n",
       "      <th>log_ratio</th>\n",
       "      <th>mean_sales_by_Z_MODELO</th>\n",
       "      <th>mean_sales_by_Z_PUNTO_VENTA</th>\n",
       "      <th>mean_sales_by_Z_GAMA</th>\n",
       "      <th>mean_sales_by_Z_MODELO_Z_GAMA</th>\n",
       "      <th>mean_sales_by_Z_PUNTO_VENTA_Z_GAMA</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>month</th>\n",
       "      <th>dayofyear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>-18.421875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.030289</td>\n",
       "      <td>0.003214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060150</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>-18.421875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.035034</td>\n",
       "      <td>0.003284</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.072327</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Z_MODELO Z_PUNTO_VENTA Z_GAMA  ratio  date_block_num  days_from_payday  \\\n",
       "0    MOD_1       PVENT_1  GAM_1    0.0               0                14   \n",
       "1    MOD_1       PVENT_1  GAM_1    0.0               1                 7   \n",
       "\n",
       "   log_ratio  mean_sales_by_Z_MODELO  mean_sales_by_Z_PUNTO_VENTA  \\\n",
       "0 -18.421875                     0.0                     0.030289   \n",
       "1 -18.421875                     0.0                     0.035034   \n",
       "\n",
       "   mean_sales_by_Z_GAMA  mean_sales_by_Z_MODELO_Z_GAMA  \\\n",
       "0              0.003214                            0.0   \n",
       "1              0.003284                            0.0   \n",
       "\n",
       "   mean_sales_by_Z_PUNTO_VENTA_Z_GAMA dayofweek month dayofyear  \n",
       "0                            0.060150         0     5       137  \n",
       "1                            0.072327         0     5       144  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "#print(list(df_train.columns))\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/cristian/Extreme SSD/Investigacion/DATATHONES/entel-2022/my_3.8_env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/media/cristian/Extreme SSD/Investigacion/DATATHONES/entel-2022/my_3.8_env/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has\n",
      "                not been set for this class (SMAPE). The property determines if `update` by\n",
      "                default needs access to the full metric state. If this is not the case, significant speedups can be\n",
      "                achieved and we recommend setting this to `False`.\n",
      "                We provide an checking function\n",
      "                `from torchmetrics.utilities import check_forward_full_state_property`\n",
      "                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,\n",
      "                default for now) or if `full_state_update=False` can be used safely.\n",
      "                \n",
      "  warnings.warn(*args, **kwargs)\n",
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "import pytorch_forecasting\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer,EncoderNormalizer\n",
    "\n",
    "from pytorch_forecasting.metrics import SMAPE, PoissonLoss, QuantileLoss\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "# configure network and trainer\n",
    "pl.seed_everything(42)\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_steps = df_test['date_block_num'].nunique()\n",
    "prediction_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2358650 entries, 0 to 2358649\n",
      "Data columns (total 15 columns):\n",
      " #   Column                              Dtype   \n",
      "---  ------                              -----   \n",
      " 0   Z_MODELO                            category\n",
      " 1   Z_PUNTO_VENTA                       category\n",
      " 2   Z_GAMA                              category\n",
      " 3   ratio                               float16 \n",
      " 4   date_block_num                      int64   \n",
      " 5   days_from_payday                    int64   \n",
      " 6   log_ratio                           float16 \n",
      " 7   mean_sales_by_Z_MODELO              float16 \n",
      " 8   mean_sales_by_Z_PUNTO_VENTA         float16 \n",
      " 9   mean_sales_by_Z_GAMA                float16 \n",
      " 10  mean_sales_by_Z_MODELO_Z_GAMA       float16 \n",
      " 11  mean_sales_by_Z_PUNTO_VENTA_Z_GAMA  float16 \n",
      " 12  dayofweek                           category\n",
      " 13  month                               category\n",
      " 14  dayofyear                           category\n",
      "dtypes: category(6), float16(7), int64(2)\n",
      "memory usage: 103.5 MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['date_block_num'] = df_train['date_block_num'].astype(int)\n",
    "df_test['date_block_num'] = df_test['date_block_num'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['ratio'] = df_train['ratio'].astype(np.float16)\n",
    "df_test['ratio'] = df_test['ratio'].astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z_MODELO train (0, 15)\n",
      "Z_MODELO test  (0, 9)\n",
      "Z_PUNTO_VENTA train (0, 15)\n",
      "Z_PUNTO_VENTA test  (0, 9)\n",
      "Z_GAMA train (0, 15)\n",
      "Z_GAMA test  (0, 9)\n",
      "ratio train (0, 15)\n",
      "ratio test  (0, 9)\n",
      "date_block_num train (0, 15)\n",
      "date_block_num test  (0, 9)\n",
      "days_from_payday train (0, 15)\n",
      "days_from_payday test  (0, 9)\n",
      "log_ratio train (0, 15)\n",
      "mean_sales_by_Z_MODELO train (0, 15)\n",
      "mean_sales_by_Z_PUNTO_VENTA train (0, 15)\n",
      "mean_sales_by_Z_GAMA train (0, 15)\n",
      "mean_sales_by_Z_MODELO_Z_GAMA train (0, 15)\n",
      "mean_sales_by_Z_PUNTO_VENTA_Z_GAMA train (0, 15)\n",
      "dayofweek train (0, 15)\n",
      "dayofweek test  (0, 9)\n",
      "month train (0, 15)\n",
      "month test  (0, 9)\n",
      "dayofyear train (0, 15)\n",
      "dayofyear test  (0, 9)\n"
     ]
    }
   ],
   "source": [
    "for column in df_train.columns:\n",
    "    print(column,'train',df_train[df_train[column]==358].shape)\n",
    "    if column in df_test.columns:\n",
    "        print(column,'test ',df_test[df_test[column]==358].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Z_MODELO</th>\n",
       "      <th>Z_PUNTO_VENTA</th>\n",
       "      <th>Z_GAMA</th>\n",
       "      <th>ratio</th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>days_from_payday</th>\n",
       "      <th>log_ratio</th>\n",
       "      <th>mean_sales_by_Z_MODELO</th>\n",
       "      <th>mean_sales_by_Z_PUNTO_VENTA</th>\n",
       "      <th>mean_sales_by_Z_GAMA</th>\n",
       "      <th>mean_sales_by_Z_MODELO_Z_GAMA</th>\n",
       "      <th>mean_sales_by_Z_PUNTO_VENTA_Z_GAMA</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>month</th>\n",
       "      <th>dayofyear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>-18.421875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.030289</td>\n",
       "      <td>0.003214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060150</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>-18.421875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.035034</td>\n",
       "      <td>0.003284</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.072327</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Z_MODELO Z_PUNTO_VENTA Z_GAMA  ratio  date_block_num  days_from_payday  \\\n",
       "0    MOD_1       PVENT_1  GAM_1    0.0               0                14   \n",
       "1    MOD_1       PVENT_1  GAM_1    0.0               1                 7   \n",
       "\n",
       "   log_ratio  mean_sales_by_Z_MODELO  mean_sales_by_Z_PUNTO_VENTA  \\\n",
       "0 -18.421875                     0.0                     0.030289   \n",
       "1 -18.421875                     0.0                     0.035034   \n",
       "\n",
       "   mean_sales_by_Z_GAMA  mean_sales_by_Z_MODELO_Z_GAMA  \\\n",
       "0              0.003214                            0.0   \n",
       "1              0.003284                            0.0   \n",
       "\n",
       "   mean_sales_by_Z_PUNTO_VENTA_Z_GAMA dayofweek month dayofyear  \n",
       "0                            0.060150         0     5       137  \n",
       "1                            0.072327         0     5       144  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'day_of_week', 'day', 'is_month_end', 'day_of_year',  'is_quarter_start', 'year', 'month', 'is_year_start', 'is_month_start', \n",
    "# 'I103','S103', 'C101','I100' , 'C100', 'ID', 'I102','S102',, 'S101', 'S100', 'item_id', 'date_block_num', 'I101'\n",
    "max_prediction_length = prediction_steps\n",
    "\n",
    "max_encoder_length = 40\n",
    "\n",
    "training_cutoff = df_train['date_block_num'].max() - max_prediction_length\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    df_train[lambda x: x['date_block_num'] <= training_cutoff],\n",
    "    time_idx='date_block_num',\n",
    "    target=\"ratio\",\n",
    "    group_ids=['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA'],\n",
    "    min_encoder_length= max_encoder_length // 2,   \n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "        \n",
    "    static_categoricals=['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA'],\n",
    "    \n",
    "    time_varying_known_categoricals=[\n",
    "                                     \"month\", \n",
    "                                     \"dayofweek\",\n",
    "                                     \"dayofyear\"],\n",
    "    \n",
    "    time_varying_known_reals=[\"date_block_num\",'days_from_payday'],\n",
    "    time_varying_unknown_categoricals=['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA'],  \n",
    "    time_varying_unknown_reals= statistics_columns+['date_block_num'],#'Demanda',  statistics_columns+['Demanda'],#'date_block_num'],\n",
    "       \n",
    "    #target_normalizer=GroupNormalizer(\n",
    "    #    groups=['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA'], transformation=\"softplus\"\n",
    "    #),  # use softplus and normalize by group    \n",
    "    \n",
    "    categorical_encoders={                          \n",
    "                          \"Z_GAMA\":  pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "                          \"Z_MODELO\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "                          \"Z_PUNTO_VENTA\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "                          \"dayofweek\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "                          \"month\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "                          \"dayofyear\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "                          \"date_block_num\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "                         },\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = TimeSeriesDataSet.from_dataset(training, df_train, predict=True, stop_randomization=True)\n",
    "\n",
    "batch_size = 128\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=3)\n",
    "\n",
    "val_dataloader   = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting.metrics import MultiHorizonMetric\n",
    "from typing import Dict, List, Tuple, Union\n",
    "from pytorch_forecasting.metrics import TweedieLoss,NegativeBinomialDistributionLoss,BetaDistributionLoss\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "'''\n",
    "def tweedieloss(y_true, y_pred):\n",
    "    p=1.5\n",
    "    a = K.pow(y_true, 2-p)/((1-p) * (2-p))\n",
    "    b = K.pow(y_pred, 1-p)/(1-p)\n",
    "    c = K.pow(y_pred, 2-p)/(2-p)\n",
    "    dev = 2 * (a -y_true *b  +c)\n",
    "    return K.mean(dev)\n",
    "'''\n",
    "class new_tweedieloss(MultiHorizonMetric):\n",
    "    def __init__(self, reduction=\"none\", **kwargs):\n",
    "        super().__init__(reduction=reduction, **kwargs)\n",
    "    def loss(self, y_pred: Dict[str, torch.Tensor], target):\n",
    "        p = 1.5#torch.tensor([1.01], dtype=torch.float16)\n",
    "        eps = 1e-10\n",
    "        \n",
    "        factor = 1\n",
    "        \n",
    "        if y_pred.sum() <= eps:\n",
    "            print(\"wtahsd!!\")\n",
    "            factor = 1e19\n",
    "            # y_pred = np.random.rand(len(y_pred))\n",
    "        else:\n",
    "            # y_pred = np.where(y_pred<0, eps, y_pred)  #Filter 0 and negative values \n",
    "            y_pred = torch.abs(y_pred)\n",
    "        #.requires_grad_(True)\n",
    "        preds = self.to_prediction(y_pred) + eps\n",
    "        #'''\n",
    "        \n",
    "        a = target*(torch.pow(preds,1-p))/(1-p)\n",
    "        b = torch.pow(preds,2-p)/(2-p)\n",
    "        tweddie = torch.mean((-a+b)/factor)\n",
    "        '''\n",
    "        a = torch.pow(target, 2-p)/((1-p) * (2-p))\n",
    "        b = torch.pow(preds, 1-p)/(1-p)\n",
    "        c = torch.pow(preds, 2-p)/(2-p)\n",
    "        tweddie = -2 * (a -target *b  +c)\n",
    "        '''\n",
    "        return tweddie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce RTX 2070\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 40.2k\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer = pl.Trainer(gpus=1,gradient_clip_val=0.1)\n",
    "trainer.enforce_positive_output=True\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "\n",
    "    # not meaningful for finding the learning rate but otherwise very important\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=16,  # most important hyperparameter apart from learning rate\n",
    "    # number of attention heads. Set to up to 4 for large datasets\n",
    "    attention_head_size=1,\n",
    "    dropout=0.1,  # between 0.1 and 0.3 are good values\n",
    "    hidden_continuous_size=8,  # set to <= hidden_size\n",
    "    output_size=1,  # 7 quantiles by default\n",
    "    loss = new_tweedieloss(),#.to(device),    \n",
    "    # reduce learning rate if no improvement in validation loss after x epochs\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wtahsd!!\n",
      "wtahsd!!\n",
      "wtahsd!!\n",
      "wtahsd!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding best initial lr:  13%|█████████▊                                                                 | 13/100 [00:02<00:13,  6.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wtahsd!!\n",
      "wtahsd!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding best initial lr: 100%|██████████████████████████████████████████████████████████████████████████| 100/100 [00:15<00:00,  6.69it/s]`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Finding best initial lr: 100%|██████████████████████████████████████████████████████████████████████████| 100/100 [00:15<00:00,  6.33it/s]\n",
      "Restoring states from the checkpoint path at /media/cristian/Extreme SSD/Investigacion/DATATHONES/entel-2022/DATATHON-ENTEL-2022---Reto2/notebooks/cristian/.lr_find_4d84090c-b376-4b7f-89a1-3908d5bf7d08.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suggested learning rate: 4.5708818961487505e-07\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# find optimal learning rate\n",
    "res = trainer.tuner.lr_find(\n",
    "    \n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    "    max_lr=0.1,\n",
    "    min_lr=1e-7,\n",
    ")\n",
    "\n",
    "print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "fig = res.plot(show=True, suggest=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.5708818961487505e-07"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.suggestion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 39.8k\n"
     ]
    }
   ],
   "source": [
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate= res.suggestion(),\n",
    "    hidden_size=16,\n",
    "    attention_head_size=4,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    output_size=1, \n",
    "    loss = new_tweedieloss(),\n",
    "    log_interval=10,  \n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "holas\n"
     ]
    }
   ],
   "source": [
    "#Early Stopping \n",
    "MIN_DELTA  = 1e-7\n",
    "PATIENCE   = 30\n",
    "\n",
    "#PL Trainer\n",
    "MAX_EPOCHS = 2000\n",
    "\n",
    "GPUS = 1\n",
    "\n",
    "\n",
    "\n",
    "# configure network and trainer\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=MIN_DELTA, patience=PATIENCE, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor(logging_interval='epoch')  # log the learning rate\n",
    "logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='../../results/models/tft/',\n",
    "    filename='MODEL_tft-tweedie-loss-epoch_{epoch:02d}-val_loss_{val_loss:.3f}',\n",
    "    auto_insert_metric_name=False,\n",
    "    \n",
    " )\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    gpus=GPUS,\n",
    "    #weights_summary=\"top\",\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=30,#oment in for training, running valiation every 30 batches\n",
    "    #fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "    callbacks=[lr_logger, early_stop_callback,checkpoint_callback],\n",
    "    #log_every_n_steps=10,\n",
    "    logger=logger,\n",
    ")\n",
    "trainer.enforce_positive_output=True\n",
    "\n",
    "\n",
    "print('holas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | new_tweedieloss                 | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 13.5 K\n",
      "3  | prescalers                         | ModuleDict                      | 192   \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 2.3 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 8.0 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 2.2 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K \n",
      "11 | lstm_encoder                       | LSTM                            | 2.2 K \n",
      "12 | lstm_decoder                       | LSTM                            | 2.2 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 676   \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 576   \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 576   \n",
      "20 | output_layer                       | Linear                          | 17    \n",
      "----------------------------------------------------------------------------------------\n",
      "39.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "39.8 K    Total params\n",
      "0.159     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|                                                                                 | 0/2 [00:00<?, ?it/s]wtahsd!!\n",
      "wtahsd!!\n",
      "Epoch 0:  45%|█████████████████████▉                           | 30/67 [00:05<00:06,  5.64it/s, loss=2.23, v_num=1, train_loss_step=2.020]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 0:  46%|██████████████████████▋                          | 31/67 [00:07<00:08,  4.27it/s, loss=2.23, v_num=1, train_loss_step=2.020]\u001b[A\n",
      "Epoch 0:  48%|███████████████████████▍                         | 32/67 [00:07<00:08,  4.22it/s, loss=2.23, v_num=1, train_loss_step=2.020]\u001b[A\n",
      "Epoch 0:  49%|████████████████████████▏                        | 33/67 [00:07<00:08,  4.18it/s, loss=2.23, v_num=1, train_loss_step=2.020]\u001b[A\n",
      "Epoch 0:  51%|████████████████████████▊                        | 34/67 [00:08<00:07,  4.13it/s, loss=2.23, v_num=1, train_loss_step=2.020]\u001b[A\n",
      "Epoch 0:  52%|█████████████████████████▌                       | 35/67 [00:08<00:07,  4.09it/s, loss=2.23, v_num=1, train_loss_step=2.020]\u001b[A\n",
      "Epoch 0:  54%|██████████████████████████▎                      | 36/67 [00:08<00:07,  4.05it/s, loss=2.23, v_num=1, train_loss_step=2.020]\u001b[A\n",
      "Epoch 0:  55%|███████████████████████████                      | 37/67 [00:09<00:07,  4.01it/s, loss=2.23, v_num=1, train_loss_step=2.020]\u001b[A\n",
      "Epoch 0:  57%|███████████████████████████▊                     | 38/67 [00:09<00:07,  3.97it/s, loss=2.23, v_num=1, train_loss_step=2.020]\u001b[A\n",
      "Epoch 0:  58%|████████████████████████████▌                    | 39/67 [00:09<00:07,  3.94it/s, loss=2.23, v_num=1, train_loss_step=2.020]\u001b[A\n",
      "Epoch 0:  60%|█████████████████████████████▎                   | 40/67 [00:10<00:06,  3.90it/s, loss=2.23, v_num=1, train_loss_step=2.020]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 0:  61%|█████████████████████████████▉                   | 41/67 [00:10<00:06,  3.84it/s, loss=2.23, v_num=1, train_loss_step=2.020]\u001b[A\n",
      "Epoch 0:  63%|██████████████████████████████▋                  | 42/67 [00:11<00:06,  3.82it/s, loss=2.23, v_num=1, train_loss_step=2.020]\u001b[A\n",
      "Epoch 0:  64%|███████████████████████████████▍                 | 43/67 [00:11<00:06,  3.79it/s, loss=2.23, v_num=1, train_loss_step=2.020]\u001b[A\n",
      "Epoch 0:  66%|████████████████████████████████▏                | 44/67 [00:11<00:06,  3.77it/s, loss=2.23, v_num=1, train_loss_step=2.020]\u001b[A\n",
      "Epoch 0:  67%|████████████████████████████████▉                | 45/67 [00:11<00:05,  3.75it/s, loss=2.23, v_num=1, train_loss_step=2.020]\u001b[A\n",
      "Epoch 0:  69%|█████████████████████████████████▋               | 46/67 [00:12<00:05,  3.73it/s, loss=2.23, v_num=1, train_loss_step=2.020]\u001b[A\n",
      "Epoch 0:  70%|██████████████████████████████████▎              | 47/67 [00:12<00:05,  3.71it/s, loss=2.23, v_num=1, train_loss_step=2.020]\u001b[A\n",
      "Epoch 0:  72%|███████████████████████████████████              | 48/67 [00:13<00:05,  3.69it/s, loss=2.23, v_num=1, train_loss_step=2.020]\u001b[A\n",
      "Epoch 0:  73%|███████████████████████████████████▊             | 49/67 [00:13<00:04,  3.67it/s, loss=2.23, v_num=1, train_loss_step=2.020]\u001b[A\n",
      "Epoch 0:  75%|████████████████████████████████████▌            | 50/67 [00:13<00:04,  3.66it/s, loss=2.23, v_num=1, train_loss_step=2.020]\u001b[A\n",
      "Epoch 0:  76%|█████████████████████████████████████▎           | 51/67 [00:14<00:04,  3.61it/s, loss=2.23, v_num=1, train_loss_step=2.020]\u001b[A\n",
      "Epoch 0:  78%|██████████████████████████████████████           | 52/67 [00:14<00:04,  3.59it/s, loss=2.23, v_num=1, train_loss_step=2.020]\u001b[A\n",
      "Epoch 0:  79%|██████████████████████████████████████▊          | 53/67 [00:14<00:03,  3.58it/s, loss=2.23, v_num=1, train_loss_step=2.020]\u001b[A\n",
      "Epoch 0:  81%|███████████████████████████████████████▍         | 54/67 [00:15<00:03,  3.57it/s, loss=2.23, v_num=1, train_loss_step=2.020]\u001b[A\n",
      "Epoch 0:  82%|████████████████████████████████████████▏        | 55/67 [00:15<00:03,  3.56it/s, loss=2.23, v_num=1, train_loss_step=2.020]\u001b[A\n",
      "Epoch 0:  84%|████████████████████████████████████████▉        | 56/67 [00:15<00:03,  3.55it/s, loss=2.23, v_num=1, train_loss_step=2.020]\u001b[A\n",
      "Epoch 0:  85%|█████████████████████████████████████████▋       | 57/67 [00:16<00:02,  3.54it/s, loss=2.23, v_num=1, train_loss_step=2.020]\u001b[A\n",
      "Epoch 0:  87%|██████████████████████████████████████████▍      | 58/67 [00:16<00:02,  3.53it/s, loss=2.23, v_num=1, train_loss_step=2.020]\u001b[A\n",
      "Epoch 0:  88%|███████████████████████████████████████████▏     | 59/67 [00:16<00:02,  3.53it/s, loss=2.23, v_num=1, train_loss_step=2.020]\u001b[A\n",
      "Epoch 0:  90%|███████████████████████████████████████████▉     | 60/67 [00:17<00:01,  3.52it/s, loss=2.23, v_num=1, train_loss_step=2.020]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 0:  91%|████████████████████████████████████████████▌    | 61/67 [00:17<00:01,  3.48it/s, loss=2.23, v_num=1, train_loss_step=2.020]\u001b[A\n",
      "Epoch 0:  93%|█████████████████████████████████████████████▎   | 62/67 [00:17<00:01,  3.48it/s, loss=2.23, v_num=1, train_loss_step=2.020]\u001b[A\n",
      "Epoch 0:  94%|██████████████████████████████████████████████   | 63/67 [00:18<00:01,  3.47it/s, loss=2.23, v_num=1, train_loss_step=2.020]\u001b[A\n",
      "Epoch 0:  96%|██████████████████████████████████████████████▊  | 64/67 [00:18<00:00,  3.47it/s, loss=2.23, v_num=1, train_loss_step=2.020]\u001b[A\n",
      "Epoch 0:  97%|███████████████████████████████████████████████▌ | 65/67 [00:18<00:00,  3.47it/s, loss=2.23, v_num=1, train_loss_step=2.020]\u001b[A\n",
      "Epoch 0:  99%|████████████████████████████████████████████████▎| 66/67 [00:19<00:00,  3.46it/s, loss=2.23, v_num=1, train_loss_step=2.020]\u001b[A\n",
      "Epoch 0: 100%|█████████████████████████████████| 67/67 [00:19<00:00,  3.40it/s, loss=2.23, v_num=1, train_loss_step=2.020, val_loss=3.170]\u001b[A\n",
      "Epoch 1:  45%|████     | 30/67 [00:05<00:06,  5.49it/s, loss=1.87, v_num=1, train_loss_step=1.090, val_loss=3.170, train_loss_epoch=2.610]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 1:  46%|████▏    | 31/67 [00:07<00:08,  4.24it/s, loss=1.87, v_num=1, train_loss_step=1.090, val_loss=3.170, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 1:  48%|████▎    | 32/67 [00:07<00:08,  4.17it/s, loss=1.87, v_num=1, train_loss_step=1.090, val_loss=3.170, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 1:  49%|████▍    | 33/67 [00:08<00:08,  4.12it/s, loss=1.87, v_num=1, train_loss_step=1.090, val_loss=3.170, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 1:  51%|████▌    | 34/67 [00:08<00:08,  4.08it/s, loss=1.87, v_num=1, train_loss_step=1.090, val_loss=3.170, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 1:  52%|████▋    | 35/67 [00:08<00:07,  4.03it/s, loss=1.87, v_num=1, train_loss_step=1.090, val_loss=3.170, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 1:  54%|████▊    | 36/67 [00:09<00:07,  3.99it/s, loss=1.87, v_num=1, train_loss_step=1.090, val_loss=3.170, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 1:  55%|████▉    | 37/67 [00:09<00:07,  3.96it/s, loss=1.87, v_num=1, train_loss_step=1.090, val_loss=3.170, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 1:  57%|█████    | 38/67 [00:09<00:07,  3.93it/s, loss=1.87, v_num=1, train_loss_step=1.090, val_loss=3.170, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 1:  58%|█████▏   | 39/67 [00:10<00:07,  3.90it/s, loss=1.87, v_num=1, train_loss_step=1.090, val_loss=3.170, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 1:  60%|█████▎   | 40/67 [00:10<00:06,  3.87it/s, loss=1.87, v_num=1, train_loss_step=1.090, val_loss=3.170, train_loss_epoch=2.610]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 1:  61%|█████▌   | 41/67 [00:10<00:06,  3.80it/s, loss=1.87, v_num=1, train_loss_step=1.090, val_loss=3.170, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 1:  63%|█████▋   | 42/67 [00:11<00:06,  3.78it/s, loss=1.87, v_num=1, train_loss_step=1.090, val_loss=3.170, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 1:  64%|█████▊   | 43/67 [00:11<00:06,  3.75it/s, loss=1.87, v_num=1, train_loss_step=1.090, val_loss=3.170, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 1:  66%|█████▉   | 44/67 [00:11<00:06,  3.73it/s, loss=1.87, v_num=1, train_loss_step=1.090, val_loss=3.170, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 1:  67%|██████   | 45/67 [00:12<00:05,  3.71it/s, loss=1.87, v_num=1, train_loss_step=1.090, val_loss=3.170, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 1:  69%|██████▏  | 46/67 [00:12<00:05,  3.69it/s, loss=1.87, v_num=1, train_loss_step=1.090, val_loss=3.170, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 1:  70%|██████▎  | 47/67 [00:12<00:05,  3.67it/s, loss=1.87, v_num=1, train_loss_step=1.090, val_loss=3.170, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 1:  72%|██████▍  | 48/67 [00:13<00:05,  3.65it/s, loss=1.87, v_num=1, train_loss_step=1.090, val_loss=3.170, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 1:  73%|██████▌  | 49/67 [00:13<00:04,  3.64it/s, loss=1.87, v_num=1, train_loss_step=1.090, val_loss=3.170, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 1:  75%|██████▋  | 50/67 [00:13<00:04,  3.63it/s, loss=1.87, v_num=1, train_loss_step=1.090, val_loss=3.170, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 1:  76%|██████▊  | 51/67 [00:14<00:04,  3.58it/s, loss=1.87, v_num=1, train_loss_step=1.090, val_loss=3.170, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 1:  78%|██████▉  | 52/67 [00:14<00:04,  3.56it/s, loss=1.87, v_num=1, train_loss_step=1.090, val_loss=3.170, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 1:  79%|███████  | 53/67 [00:14<00:03,  3.55it/s, loss=1.87, v_num=1, train_loss_step=1.090, val_loss=3.170, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 1:  81%|███████▎ | 54/67 [00:15<00:03,  3.53it/s, loss=1.87, v_num=1, train_loss_step=1.090, val_loss=3.170, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 1:  82%|███████▍ | 55/67 [00:15<00:03,  3.52it/s, loss=1.87, v_num=1, train_loss_step=1.090, val_loss=3.170, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 1:  84%|███████▌ | 56/67 [00:15<00:03,  3.51it/s, loss=1.87, v_num=1, train_loss_step=1.090, val_loss=3.170, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 1:  85%|███████▋ | 57/67 [00:16<00:02,  3.50it/s, loss=1.87, v_num=1, train_loss_step=1.090, val_loss=3.170, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 1:  87%|███████▊ | 58/67 [00:16<00:02,  3.49it/s, loss=1.87, v_num=1, train_loss_step=1.090, val_loss=3.170, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 1:  88%|███████▉ | 59/67 [00:16<00:02,  3.48it/s, loss=1.87, v_num=1, train_loss_step=1.090, val_loss=3.170, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 1:  90%|████████ | 60/67 [00:17<00:02,  3.47it/s, loss=1.87, v_num=1, train_loss_step=1.090, val_loss=3.170, train_loss_epoch=2.610]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 1:  91%|████████▏| 61/67 [00:17<00:01,  3.44it/s, loss=1.87, v_num=1, train_loss_step=1.090, val_loss=3.170, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 1:  93%|████████▎| 62/67 [00:18<00:01,  3.43it/s, loss=1.87, v_num=1, train_loss_step=1.090, val_loss=3.170, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 63/67 [00:18<00:01,  3.42it/s, loss=1.87, v_num=1, train_loss_step=1.090, val_loss=3.170, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 1:  96%|████████▌| 64/67 [00:18<00:00,  3.41it/s, loss=1.87, v_num=1, train_loss_step=1.090, val_loss=3.170, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 1:  97%|████████▋| 65/67 [00:19<00:00,  3.41it/s, loss=1.87, v_num=1, train_loss_step=1.090, val_loss=3.170, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 1:  99%|████████▊| 66/67 [00:19<00:00,  3.40it/s, loss=1.87, v_num=1, train_loss_step=1.090, val_loss=3.170, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 1: 100%|█████████| 67/67 [00:20<00:00,  3.34it/s, loss=1.87, v_num=1, train_loss_step=1.090, val_loss=3.170, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 2:  45%|████     | 30/67 [00:05<00:06,  5.49it/s, loss=2.63, v_num=1, train_loss_step=4.780, val_loss=3.170, train_loss_epoch=1.970]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 2:  46%|████▏    | 31/67 [00:07<00:08,  4.22it/s, loss=2.63, v_num=1, train_loss_step=4.780, val_loss=3.170, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 2:  48%|████▎    | 32/67 [00:07<00:08,  4.17it/s, loss=2.63, v_num=1, train_loss_step=4.780, val_loss=3.170, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 2:  49%|████▍    | 33/67 [00:08<00:08,  4.12it/s, loss=2.63, v_num=1, train_loss_step=4.780, val_loss=3.170, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 2:  51%|████▌    | 34/67 [00:08<00:08,  4.07it/s, loss=2.63, v_num=1, train_loss_step=4.780, val_loss=3.170, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 2:  52%|████▋    | 35/67 [00:08<00:07,  4.02it/s, loss=2.63, v_num=1, train_loss_step=4.780, val_loss=3.170, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 2:  54%|████▊    | 36/67 [00:09<00:07,  3.98it/s, loss=2.63, v_num=1, train_loss_step=4.780, val_loss=3.170, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 2:  55%|████▉    | 37/67 [00:09<00:07,  3.95it/s, loss=2.63, v_num=1, train_loss_step=4.780, val_loss=3.170, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 2:  57%|█████    | 38/67 [00:09<00:07,  3.91it/s, loss=2.63, v_num=1, train_loss_step=4.780, val_loss=3.170, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 2:  58%|█████▏   | 39/67 [00:10<00:07,  3.87it/s, loss=2.63, v_num=1, train_loss_step=4.780, val_loss=3.170, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 2:  60%|█████▎   | 40/67 [00:10<00:07,  3.83it/s, loss=2.63, v_num=1, train_loss_step=4.780, val_loss=3.170, train_loss_epoch=1.970]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 2:  61%|█████▌   | 41/67 [00:10<00:06,  3.76it/s, loss=2.63, v_num=1, train_loss_step=4.780, val_loss=3.170, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 2:  63%|█████▋   | 42/67 [00:11<00:06,  3.74it/s, loss=2.63, v_num=1, train_loss_step=4.780, val_loss=3.170, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 2:  64%|█████▊   | 43/67 [00:11<00:06,  3.71it/s, loss=2.63, v_num=1, train_loss_step=4.780, val_loss=3.170, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 2:  66%|█████▉   | 44/67 [00:11<00:06,  3.70it/s, loss=2.63, v_num=1, train_loss_step=4.780, val_loss=3.170, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 2:  67%|██████   | 45/67 [00:12<00:05,  3.68it/s, loss=2.63, v_num=1, train_loss_step=4.780, val_loss=3.170, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 2:  69%|██████▏  | 46/67 [00:12<00:05,  3.66it/s, loss=2.63, v_num=1, train_loss_step=4.780, val_loss=3.170, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 2:  70%|██████▎  | 47/67 [00:12<00:05,  3.64it/s, loss=2.63, v_num=1, train_loss_step=4.780, val_loss=3.170, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 2:  72%|██████▍  | 48/67 [00:13<00:05,  3.62it/s, loss=2.63, v_num=1, train_loss_step=4.780, val_loss=3.170, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 2:  73%|██████▌  | 49/67 [00:13<00:04,  3.61it/s, loss=2.63, v_num=1, train_loss_step=4.780, val_loss=3.170, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 2:  75%|██████▋  | 50/67 [00:13<00:04,  3.59it/s, loss=2.63, v_num=1, train_loss_step=4.780, val_loss=3.170, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 2:  76%|██████▊  | 51/67 [00:14<00:04,  3.56it/s, loss=2.63, v_num=1, train_loss_step=4.780, val_loss=3.170, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 2:  78%|██████▉  | 52/67 [00:14<00:04,  3.55it/s, loss=2.63, v_num=1, train_loss_step=4.780, val_loss=3.170, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 2:  79%|███████  | 53/67 [00:14<00:03,  3.54it/s, loss=2.63, v_num=1, train_loss_step=4.780, val_loss=3.170, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 2:  81%|███████▎ | 54/67 [00:15<00:03,  3.53it/s, loss=2.63, v_num=1, train_loss_step=4.780, val_loss=3.170, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 2:  82%|███████▍ | 55/67 [00:15<00:03,  3.51it/s, loss=2.63, v_num=1, train_loss_step=4.780, val_loss=3.170, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 2:  84%|███████▌ | 56/67 [00:16<00:03,  3.46it/s, loss=2.63, v_num=1, train_loss_step=4.780, val_loss=3.170, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 2:  85%|███████▋ | 57/67 [00:16<00:02,  3.45it/s, loss=2.63, v_num=1, train_loss_step=4.780, val_loss=3.170, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 2:  87%|███████▊ | 58/67 [00:16<00:02,  3.44it/s, loss=2.63, v_num=1, train_loss_step=4.780, val_loss=3.170, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 2:  88%|███████▉ | 59/67 [00:17<00:02,  3.44it/s, loss=2.63, v_num=1, train_loss_step=4.780, val_loss=3.170, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 2:  90%|████████ | 60/67 [00:17<00:02,  3.43it/s, loss=2.63, v_num=1, train_loss_step=4.780, val_loss=3.170, train_loss_epoch=1.970]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 2:  91%|████████▏| 61/67 [00:17<00:01,  3.40it/s, loss=2.63, v_num=1, train_loss_step=4.780, val_loss=3.170, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 2:  93%|████████▎| 62/67 [00:18<00:01,  3.40it/s, loss=2.63, v_num=1, train_loss_step=4.780, val_loss=3.170, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 2:  94%|████████▍| 63/67 [00:18<00:01,  3.39it/s, loss=2.63, v_num=1, train_loss_step=4.780, val_loss=3.170, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 2:  96%|████████▌| 64/67 [00:18<00:00,  3.39it/s, loss=2.63, v_num=1, train_loss_step=4.780, val_loss=3.170, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 2:  97%|████████▋| 65/67 [00:19<00:00,  3.38it/s, loss=2.63, v_num=1, train_loss_step=4.780, val_loss=3.170, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 2:  99%|████████▊| 66/67 [00:19<00:00,  3.38it/s, loss=2.63, v_num=1, train_loss_step=4.780, val_loss=3.170, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 2: 100%|█████████| 67/67 [00:20<00:00,  3.32it/s, loss=2.63, v_num=1, train_loss_step=4.780, val_loss=3.180, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 3:  45%|████     | 30/67 [00:05<00:06,  5.67it/s, loss=2.51, v_num=1, train_loss_step=3.800, val_loss=3.180, train_loss_epoch=2.410]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 3:  46%|████▏    | 31/67 [00:07<00:08,  4.35it/s, loss=2.51, v_num=1, train_loss_step=3.800, val_loss=3.180, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 3:  48%|████▎    | 32/67 [00:07<00:08,  4.29it/s, loss=2.51, v_num=1, train_loss_step=3.800, val_loss=3.180, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 3:  49%|████▍    | 33/67 [00:07<00:08,  4.25it/s, loss=2.51, v_num=1, train_loss_step=3.800, val_loss=3.180, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 3:  51%|████▌    | 34/67 [00:08<00:07,  4.19it/s, loss=2.51, v_num=1, train_loss_step=3.800, val_loss=3.180, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 3:  52%|████▋    | 35/67 [00:08<00:07,  4.15it/s, loss=2.51, v_num=1, train_loss_step=3.800, val_loss=3.180, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 3:  54%|████▊    | 36/67 [00:08<00:07,  4.12it/s, loss=2.51, v_num=1, train_loss_step=3.800, val_loss=3.180, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 3:  55%|████▉    | 37/67 [00:09<00:07,  4.08it/s, loss=2.51, v_num=1, train_loss_step=3.800, val_loss=3.180, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 3:  57%|█████    | 38/67 [00:09<00:07,  4.04it/s, loss=2.51, v_num=1, train_loss_step=3.800, val_loss=3.180, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 3:  58%|█████▏   | 39/67 [00:09<00:06,  4.01it/s, loss=2.51, v_num=1, train_loss_step=3.800, val_loss=3.180, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 3:  60%|█████▎   | 40/67 [00:10<00:06,  3.98it/s, loss=2.51, v_num=1, train_loss_step=3.800, val_loss=3.180, train_loss_epoch=2.410]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 3:  61%|█████▌   | 41/67 [00:10<00:06,  3.91it/s, loss=2.51, v_num=1, train_loss_step=3.800, val_loss=3.180, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 3:  63%|█████▋   | 42/67 [00:10<00:06,  3.88it/s, loss=2.51, v_num=1, train_loss_step=3.800, val_loss=3.180, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 3:  64%|█████▊   | 43/67 [00:11<00:06,  3.86it/s, loss=2.51, v_num=1, train_loss_step=3.800, val_loss=3.180, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 3:  66%|█████▉   | 44/67 [00:11<00:05,  3.84it/s, loss=2.51, v_num=1, train_loss_step=3.800, val_loss=3.180, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 3:  67%|██████   | 45/67 [00:11<00:05,  3.82it/s, loss=2.51, v_num=1, train_loss_step=3.800, val_loss=3.180, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 3:  69%|██████▏  | 46/67 [00:12<00:05,  3.80it/s, loss=2.51, v_num=1, train_loss_step=3.800, val_loss=3.180, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 3:  70%|██████▎  | 47/67 [00:12<00:05,  3.78it/s, loss=2.51, v_num=1, train_loss_step=3.800, val_loss=3.180, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 3:  72%|██████▍  | 48/67 [00:12<00:05,  3.70it/s, loss=2.51, v_num=1, train_loss_step=3.800, val_loss=3.180, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 3:  73%|██████▌  | 49/67 [00:13<00:04,  3.69it/s, loss=2.51, v_num=1, train_loss_step=3.800, val_loss=3.180, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 3:  75%|██████▋  | 50/67 [00:13<00:04,  3.67it/s, loss=2.51, v_num=1, train_loss_step=3.800, val_loss=3.180, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 3:  76%|██████▊  | 51/67 [00:14<00:04,  3.62it/s, loss=2.51, v_num=1, train_loss_step=3.800, val_loss=3.180, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 3:  78%|██████▉  | 52/67 [00:14<00:04,  3.61it/s, loss=2.51, v_num=1, train_loss_step=3.800, val_loss=3.180, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 3:  79%|███████  | 53/67 [00:14<00:03,  3.59it/s, loss=2.51, v_num=1, train_loss_step=3.800, val_loss=3.180, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 3:  81%|███████▎ | 54/67 [00:15<00:03,  3.58it/s, loss=2.51, v_num=1, train_loss_step=3.800, val_loss=3.180, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 3:  82%|███████▍ | 55/67 [00:15<00:03,  3.56it/s, loss=2.51, v_num=1, train_loss_step=3.800, val_loss=3.180, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 3:  84%|███████▌ | 56/67 [00:15<00:03,  3.56it/s, loss=2.51, v_num=1, train_loss_step=3.800, val_loss=3.180, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 3:  85%|███████▋ | 57/67 [00:16<00:02,  3.55it/s, loss=2.51, v_num=1, train_loss_step=3.800, val_loss=3.180, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 3:  87%|███████▊ | 58/67 [00:16<00:02,  3.54it/s, loss=2.51, v_num=1, train_loss_step=3.800, val_loss=3.180, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 3:  88%|███████▉ | 59/67 [00:16<00:02,  3.53it/s, loss=2.51, v_num=1, train_loss_step=3.800, val_loss=3.180, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 3:  90%|████████ | 60/67 [00:17<00:01,  3.52it/s, loss=2.51, v_num=1, train_loss_step=3.800, val_loss=3.180, train_loss_epoch=2.410]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 3:  91%|████████▏| 61/67 [00:17<00:01,  3.48it/s, loss=2.51, v_num=1, train_loss_step=3.800, val_loss=3.180, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 3:  93%|████████▎| 62/67 [00:17<00:01,  3.47it/s, loss=2.51, v_num=1, train_loss_step=3.800, val_loss=3.180, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 3:  94%|████████▍| 63/67 [00:18<00:01,  3.46it/s, loss=2.51, v_num=1, train_loss_step=3.800, val_loss=3.180, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 3:  96%|████████▌| 64/67 [00:18<00:00,  3.45it/s, loss=2.51, v_num=1, train_loss_step=3.800, val_loss=3.180, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 3:  97%|████████▋| 65/67 [00:18<00:00,  3.44it/s, loss=2.51, v_num=1, train_loss_step=3.800, val_loss=3.180, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 3:  99%|████████▊| 66/67 [00:19<00:00,  3.43it/s, loss=2.51, v_num=1, train_loss_step=3.800, val_loss=3.180, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 3: 100%|█████████| 67/67 [00:19<00:00,  3.37it/s, loss=2.51, v_num=1, train_loss_step=3.800, val_loss=3.170, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 4:  45%|████     | 30/67 [00:05<00:06,  5.30it/s, loss=2.33, v_num=1, train_loss_step=2.420, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 4:  46%|████▏    | 31/67 [00:07<00:08,  4.11it/s, loss=2.33, v_num=1, train_loss_step=2.420, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 4:  48%|████▎    | 32/67 [00:07<00:08,  4.07it/s, loss=2.33, v_num=1, train_loss_step=2.420, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 4:  49%|████▍    | 33/67 [00:08<00:08,  4.04it/s, loss=2.33, v_num=1, train_loss_step=2.420, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 4:  51%|████▌    | 34/67 [00:08<00:08,  4.00it/s, loss=2.33, v_num=1, train_loss_step=2.420, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 4:  52%|████▋    | 35/67 [00:08<00:08,  3.97it/s, loss=2.33, v_num=1, train_loss_step=2.420, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 4:  54%|████▊    | 36/67 [00:09<00:07,  3.93it/s, loss=2.33, v_num=1, train_loss_step=2.420, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 4:  55%|████▉    | 37/67 [00:09<00:07,  3.89it/s, loss=2.33, v_num=1, train_loss_step=2.420, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 4:  57%|█████    | 38/67 [00:09<00:07,  3.86it/s, loss=2.33, v_num=1, train_loss_step=2.420, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 4:  58%|█████▏   | 39/67 [00:10<00:07,  3.74it/s, loss=2.33, v_num=1, train_loss_step=2.420, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 4:  60%|█████▎   | 40/67 [00:10<00:07,  3.72it/s, loss=2.33, v_num=1, train_loss_step=2.420, val_loss=3.170, train_loss_epoch=2.390]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 4:  61%|█████▌   | 41/67 [00:11<00:07,  3.66it/s, loss=2.33, v_num=1, train_loss_step=2.420, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 4:  63%|█████▋   | 42/67 [00:11<00:06,  3.64it/s, loss=2.33, v_num=1, train_loss_step=2.420, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 4:  64%|█████▊   | 43/67 [00:11<00:06,  3.62it/s, loss=2.33, v_num=1, train_loss_step=2.420, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 4:  66%|█████▉   | 44/67 [00:12<00:06,  3.61it/s, loss=2.33, v_num=1, train_loss_step=2.420, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 4:  67%|██████   | 45/67 [00:12<00:06,  3.59it/s, loss=2.33, v_num=1, train_loss_step=2.420, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 4:  69%|██████▏  | 46/67 [00:12<00:05,  3.58it/s, loss=2.33, v_num=1, train_loss_step=2.420, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 4:  70%|██████▎  | 47/67 [00:13<00:05,  3.57it/s, loss=2.33, v_num=1, train_loss_step=2.420, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 4:  72%|██████▍  | 48/67 [00:13<00:05,  3.56it/s, loss=2.33, v_num=1, train_loss_step=2.420, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 4:  73%|██████▌  | 49/67 [00:13<00:05,  3.55it/s, loss=2.33, v_num=1, train_loss_step=2.420, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 4:  75%|██████▋  | 50/67 [00:14<00:04,  3.54it/s, loss=2.33, v_num=1, train_loss_step=2.420, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 4:  76%|██████▊  | 51/67 [00:14<00:04,  3.50it/s, loss=2.33, v_num=1, train_loss_step=2.420, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 4:  78%|██████▉  | 52/67 [00:14<00:04,  3.49it/s, loss=2.33, v_num=1, train_loss_step=2.420, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 4:  79%|███████  | 53/67 [00:15<00:04,  3.49it/s, loss=2.33, v_num=1, train_loss_step=2.420, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 4:  81%|███████▎ | 54/67 [00:15<00:03,  3.48it/s, loss=2.33, v_num=1, train_loss_step=2.420, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 4:  82%|███████▍ | 55/67 [00:15<00:03,  3.47it/s, loss=2.33, v_num=1, train_loss_step=2.420, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 4:  84%|███████▌ | 56/67 [00:16<00:03,  3.47it/s, loss=2.33, v_num=1, train_loss_step=2.420, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 4:  85%|███████▋ | 57/67 [00:16<00:02,  3.46it/s, loss=2.33, v_num=1, train_loss_step=2.420, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 4:  87%|███████▊ | 58/67 [00:16<00:02,  3.45it/s, loss=2.33, v_num=1, train_loss_step=2.420, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 4:  88%|███████▉ | 59/67 [00:17<00:02,  3.45it/s, loss=2.33, v_num=1, train_loss_step=2.420, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 4:  90%|████████ | 60/67 [00:17<00:02,  3.44it/s, loss=2.33, v_num=1, train_loss_step=2.420, val_loss=3.170, train_loss_epoch=2.390]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 4:  91%|████████▏| 61/67 [00:17<00:01,  3.42it/s, loss=2.33, v_num=1, train_loss_step=2.420, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 4:  93%|████████▎| 62/67 [00:18<00:01,  3.41it/s, loss=2.33, v_num=1, train_loss_step=2.420, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 4:  94%|████████▍| 63/67 [00:18<00:01,  3.41it/s, loss=2.33, v_num=1, train_loss_step=2.420, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 4:  96%|████████▌| 64/67 [00:18<00:00,  3.41it/s, loss=2.33, v_num=1, train_loss_step=2.420, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 4:  97%|████████▋| 65/67 [00:19<00:00,  3.40it/s, loss=2.33, v_num=1, train_loss_step=2.420, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 4:  99%|████████▊| 66/67 [00:19<00:00,  3.40it/s, loss=2.33, v_num=1, train_loss_step=2.420, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 4: 100%|█████████| 67/67 [00:20<00:00,  3.34it/s, loss=2.33, v_num=1, train_loss_step=2.420, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 5:  45%|████     | 30/67 [00:05<00:06,  5.81it/s, loss=1.88, v_num=1, train_loss_step=1.190, val_loss=3.170, train_loss_epoch=2.270]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 5:  46%|████▏    | 31/67 [00:07<00:08,  4.42it/s, loss=1.88, v_num=1, train_loss_step=1.190, val_loss=3.170, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 5:  48%|████▎    | 32/67 [00:07<00:08,  4.37it/s, loss=1.88, v_num=1, train_loss_step=1.190, val_loss=3.170, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 5:  49%|████▍    | 33/67 [00:07<00:08,  4.22it/s, loss=1.88, v_num=1, train_loss_step=1.190, val_loss=3.170, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 5:  51%|████▌    | 34/67 [00:08<00:07,  4.17it/s, loss=1.88, v_num=1, train_loss_step=1.190, val_loss=3.170, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 5:  52%|████▋    | 35/67 [00:08<00:07,  4.13it/s, loss=1.88, v_num=1, train_loss_step=1.190, val_loss=3.170, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 5:  54%|████▊    | 36/67 [00:08<00:07,  4.10it/s, loss=1.88, v_num=1, train_loss_step=1.190, val_loss=3.170, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 5:  55%|████▉    | 37/67 [00:09<00:07,  4.06it/s, loss=1.88, v_num=1, train_loss_step=1.190, val_loss=3.170, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 5:  57%|█████    | 38/67 [00:09<00:07,  4.03it/s, loss=1.88, v_num=1, train_loss_step=1.190, val_loss=3.170, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 5:  58%|█████▏   | 39/67 [00:09<00:07,  4.00it/s, loss=1.88, v_num=1, train_loss_step=1.190, val_loss=3.170, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 5:  60%|█████▎   | 40/67 [00:10<00:06,  3.97it/s, loss=1.88, v_num=1, train_loss_step=1.190, val_loss=3.170, train_loss_epoch=2.270]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 5:  61%|█████▌   | 41/67 [00:10<00:06,  3.89it/s, loss=1.88, v_num=1, train_loss_step=1.190, val_loss=3.170, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 5:  63%|█████▋   | 42/67 [00:10<00:06,  3.87it/s, loss=1.88, v_num=1, train_loss_step=1.190, val_loss=3.170, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 5:  64%|█████▊   | 43/67 [00:11<00:06,  3.84it/s, loss=1.88, v_num=1, train_loss_step=1.190, val_loss=3.170, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 5:  66%|█████▉   | 44/67 [00:11<00:06,  3.83it/s, loss=1.88, v_num=1, train_loss_step=1.190, val_loss=3.170, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 5:  67%|██████   | 45/67 [00:11<00:05,  3.81it/s, loss=1.88, v_num=1, train_loss_step=1.190, val_loss=3.170, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 5:  69%|██████▏  | 46/67 [00:12<00:05,  3.79it/s, loss=1.88, v_num=1, train_loss_step=1.190, val_loss=3.170, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 5:  70%|██████▎  | 47/67 [00:12<00:05,  3.77it/s, loss=1.88, v_num=1, train_loss_step=1.190, val_loss=3.170, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 5:  72%|██████▍  | 48/67 [00:12<00:05,  3.76it/s, loss=1.88, v_num=1, train_loss_step=1.190, val_loss=3.170, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 5:  73%|██████▌  | 49/67 [00:13<00:04,  3.74it/s, loss=1.88, v_num=1, train_loss_step=1.190, val_loss=3.170, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 5:  75%|██████▋  | 50/67 [00:13<00:04,  3.72it/s, loss=1.88, v_num=1, train_loss_step=1.190, val_loss=3.170, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 5:  76%|██████▊  | 51/67 [00:13<00:04,  3.68it/s, loss=1.88, v_num=1, train_loss_step=1.190, val_loss=3.170, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 5:  78%|██████▉  | 52/67 [00:14<00:04,  3.67it/s, loss=1.88, v_num=1, train_loss_step=1.190, val_loss=3.170, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 5:  79%|███████  | 53/67 [00:14<00:03,  3.65it/s, loss=1.88, v_num=1, train_loss_step=1.190, val_loss=3.170, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 5:  81%|███████▎ | 54/67 [00:14<00:03,  3.64it/s, loss=1.88, v_num=1, train_loss_step=1.190, val_loss=3.170, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 5:  82%|███████▍ | 55/67 [00:15<00:03,  3.62it/s, loss=1.88, v_num=1, train_loss_step=1.190, val_loss=3.170, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 5:  84%|███████▌ | 56/67 [00:15<00:03,  3.61it/s, loss=1.88, v_num=1, train_loss_step=1.190, val_loss=3.170, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 5:  85%|███████▋ | 57/67 [00:15<00:02,  3.60it/s, loss=1.88, v_num=1, train_loss_step=1.190, val_loss=3.170, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 5:  87%|███████▊ | 58/67 [00:16<00:02,  3.58it/s, loss=1.88, v_num=1, train_loss_step=1.190, val_loss=3.170, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 5:  88%|███████▉ | 59/67 [00:16<00:02,  3.57it/s, loss=1.88, v_num=1, train_loss_step=1.190, val_loss=3.170, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 5:  90%|████████ | 60/67 [00:16<00:01,  3.56it/s, loss=1.88, v_num=1, train_loss_step=1.190, val_loss=3.170, train_loss_epoch=2.270]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 5:  91%|████████▏| 61/67 [00:17<00:01,  3.46it/s, loss=1.88, v_num=1, train_loss_step=1.190, val_loss=3.170, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 5:  93%|████████▎| 62/67 [00:17<00:01,  3.46it/s, loss=1.88, v_num=1, train_loss_step=1.190, val_loss=3.170, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 5:  94%|████████▍| 63/67 [00:18<00:01,  3.45it/s, loss=1.88, v_num=1, train_loss_step=1.190, val_loss=3.170, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 5:  96%|████████▌| 64/67 [00:18<00:00,  3.44it/s, loss=1.88, v_num=1, train_loss_step=1.190, val_loss=3.170, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 5:  97%|████████▋| 65/67 [00:18<00:00,  3.44it/s, loss=1.88, v_num=1, train_loss_step=1.190, val_loss=3.170, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 5:  99%|████████▊| 66/67 [00:19<00:00,  3.43it/s, loss=1.88, v_num=1, train_loss_step=1.190, val_loss=3.170, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 5: 100%|█████████| 67/67 [00:19<00:00,  3.37it/s, loss=1.88, v_num=1, train_loss_step=1.190, val_loss=3.170, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 6:  45%|████▍     | 30/67 [00:05<00:06,  5.72it/s, loss=2.1, v_num=1, train_loss_step=1.180, val_loss=3.170, train_loss_epoch=2.470]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 6:  46%|████▋     | 31/67 [00:07<00:08,  4.38it/s, loss=2.1, v_num=1, train_loss_step=1.180, val_loss=3.170, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 6:  48%|████▊     | 32/67 [00:07<00:08,  4.32it/s, loss=2.1, v_num=1, train_loss_step=1.180, val_loss=3.170, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 6:  49%|████▉     | 33/67 [00:07<00:07,  4.27it/s, loss=2.1, v_num=1, train_loss_step=1.180, val_loss=3.170, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 6:  51%|█████     | 34/67 [00:08<00:07,  4.22it/s, loss=2.1, v_num=1, train_loss_step=1.180, val_loss=3.170, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 6:  52%|█████▏    | 35/67 [00:08<00:07,  4.17it/s, loss=2.1, v_num=1, train_loss_step=1.180, val_loss=3.170, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 6:  54%|█████▎    | 36/67 [00:08<00:07,  4.13it/s, loss=2.1, v_num=1, train_loss_step=1.180, val_loss=3.170, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 6:  55%|█████▌    | 37/67 [00:09<00:07,  4.09it/s, loss=2.1, v_num=1, train_loss_step=1.180, val_loss=3.170, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 6:  57%|█████▋    | 38/67 [00:09<00:07,  4.05it/s, loss=2.1, v_num=1, train_loss_step=1.180, val_loss=3.170, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 6:  58%|█████▊    | 39/67 [00:09<00:06,  4.02it/s, loss=2.1, v_num=1, train_loss_step=1.180, val_loss=3.170, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 6:  60%|█████▉    | 40/67 [00:10<00:06,  3.99it/s, loss=2.1, v_num=1, train_loss_step=1.180, val_loss=3.170, train_loss_epoch=2.470]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 6:  61%|██████    | 41/67 [00:10<00:06,  3.92it/s, loss=2.1, v_num=1, train_loss_step=1.180, val_loss=3.170, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 6:  63%|██████▎   | 42/67 [00:10<00:06,  3.90it/s, loss=2.1, v_num=1, train_loss_step=1.180, val_loss=3.170, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 6:  64%|██████▍   | 43/67 [00:11<00:06,  3.87it/s, loss=2.1, v_num=1, train_loss_step=1.180, val_loss=3.170, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 6:  66%|██████▌   | 44/67 [00:11<00:05,  3.85it/s, loss=2.1, v_num=1, train_loss_step=1.180, val_loss=3.170, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 6:  67%|██████▋   | 45/67 [00:11<00:05,  3.82it/s, loss=2.1, v_num=1, train_loss_step=1.180, val_loss=3.170, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 6:  69%|██████▊   | 46/67 [00:12<00:05,  3.80it/s, loss=2.1, v_num=1, train_loss_step=1.180, val_loss=3.170, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 6:  70%|███████   | 47/67 [00:12<00:05,  3.78it/s, loss=2.1, v_num=1, train_loss_step=1.180, val_loss=3.170, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 6:  72%|███████▏  | 48/67 [00:12<00:05,  3.76it/s, loss=2.1, v_num=1, train_loss_step=1.180, val_loss=3.170, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 6:  73%|███████▎  | 49/67 [00:13<00:04,  3.75it/s, loss=2.1, v_num=1, train_loss_step=1.180, val_loss=3.170, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 6:  75%|███████▍  | 50/67 [00:13<00:04,  3.73it/s, loss=2.1, v_num=1, train_loss_step=1.180, val_loss=3.170, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 6:  76%|███████▌  | 51/67 [00:13<00:04,  3.69it/s, loss=2.1, v_num=1, train_loss_step=1.180, val_loss=3.170, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 6:  78%|███████▊  | 52/67 [00:14<00:04,  3.67it/s, loss=2.1, v_num=1, train_loss_step=1.180, val_loss=3.170, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 6:  79%|███████▉  | 53/67 [00:14<00:03,  3.66it/s, loss=2.1, v_num=1, train_loss_step=1.180, val_loss=3.170, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 6:  81%|████████  | 54/67 [00:14<00:03,  3.65it/s, loss=2.1, v_num=1, train_loss_step=1.180, val_loss=3.170, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 6:  82%|████████▏ | 55/67 [00:15<00:03,  3.64it/s, loss=2.1, v_num=1, train_loss_step=1.180, val_loss=3.170, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 6:  84%|████████▎ | 56/67 [00:15<00:03,  3.63it/s, loss=2.1, v_num=1, train_loss_step=1.180, val_loss=3.170, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 6:  85%|████████▌ | 57/67 [00:15<00:02,  3.62it/s, loss=2.1, v_num=1, train_loss_step=1.180, val_loss=3.170, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 6:  87%|████████▋ | 58/67 [00:16<00:02,  3.61it/s, loss=2.1, v_num=1, train_loss_step=1.180, val_loss=3.170, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 6:  88%|████████▊ | 59/67 [00:16<00:02,  3.60it/s, loss=2.1, v_num=1, train_loss_step=1.180, val_loss=3.170, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 6:  90%|████████▉ | 60/67 [00:16<00:01,  3.59it/s, loss=2.1, v_num=1, train_loss_step=1.180, val_loss=3.170, train_loss_epoch=2.470]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 6:  91%|█████████ | 61/67 [00:17<00:01,  3.56it/s, loss=2.1, v_num=1, train_loss_step=1.180, val_loss=3.170, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 6:  93%|█████████▎| 62/67 [00:17<00:01,  3.55it/s, loss=2.1, v_num=1, train_loss_step=1.180, val_loss=3.170, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 6:  94%|█████████▍| 63/67 [00:17<00:01,  3.54it/s, loss=2.1, v_num=1, train_loss_step=1.180, val_loss=3.170, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 6:  96%|█████████▌| 64/67 [00:18<00:00,  3.54it/s, loss=2.1, v_num=1, train_loss_step=1.180, val_loss=3.170, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 6:  97%|█████████▋| 65/67 [00:18<00:00,  3.53it/s, loss=2.1, v_num=1, train_loss_step=1.180, val_loss=3.170, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 6:  99%|█████████▊| 66/67 [00:18<00:00,  3.52it/s, loss=2.1, v_num=1, train_loss_step=1.180, val_loss=3.170, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 6: 100%|██████████| 67/67 [00:19<00:00,  3.46it/s, loss=2.1, v_num=1, train_loss_step=1.180, val_loss=3.170, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 7:  45%|████     | 30/67 [00:05<00:06,  5.68it/s, loss=1.89, v_num=1, train_loss_step=1.940, val_loss=3.170, train_loss_epoch=2.660]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 7:  46%|████▏    | 31/67 [00:07<00:08,  4.40it/s, loss=1.89, v_num=1, train_loss_step=1.940, val_loss=3.170, train_loss_epoch=2.660]\u001b[A\n",
      "Epoch 7:  48%|████▎    | 32/67 [00:07<00:08,  4.34it/s, loss=1.89, v_num=1, train_loss_step=1.940, val_loss=3.170, train_loss_epoch=2.660]\u001b[A\n",
      "Epoch 7:  49%|████▍    | 33/67 [00:07<00:07,  4.28it/s, loss=1.89, v_num=1, train_loss_step=1.940, val_loss=3.170, train_loss_epoch=2.660]\u001b[A\n",
      "Epoch 7:  51%|████▌    | 34/67 [00:08<00:07,  4.23it/s, loss=1.89, v_num=1, train_loss_step=1.940, val_loss=3.170, train_loss_epoch=2.660]\u001b[A\n",
      "Epoch 7:  52%|████▋    | 35/67 [00:08<00:07,  4.19it/s, loss=1.89, v_num=1, train_loss_step=1.940, val_loss=3.170, train_loss_epoch=2.660]\u001b[A\n",
      "Epoch 7:  54%|████▊    | 36/67 [00:08<00:07,  4.15it/s, loss=1.89, v_num=1, train_loss_step=1.940, val_loss=3.170, train_loss_epoch=2.660]\u001b[A\n",
      "Epoch 7:  55%|████▉    | 37/67 [00:08<00:07,  4.11it/s, loss=1.89, v_num=1, train_loss_step=1.940, val_loss=3.170, train_loss_epoch=2.660]\u001b[A\n",
      "Epoch 7:  57%|█████    | 38/67 [00:09<00:07,  4.08it/s, loss=1.89, v_num=1, train_loss_step=1.940, val_loss=3.170, train_loss_epoch=2.660]\u001b[A\n",
      "Epoch 7:  58%|█████▏   | 39/67 [00:09<00:06,  4.03it/s, loss=1.89, v_num=1, train_loss_step=1.940, val_loss=3.170, train_loss_epoch=2.660]\u001b[A\n",
      "Epoch 7:  60%|█████▎   | 40/67 [00:10<00:06,  4.00it/s, loss=1.89, v_num=1, train_loss_step=1.940, val_loss=3.170, train_loss_epoch=2.660]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 7:  61%|█████▌   | 41/67 [00:10<00:06,  3.93it/s, loss=1.89, v_num=1, train_loss_step=1.940, val_loss=3.170, train_loss_epoch=2.660]\u001b[A\n",
      "Epoch 7:  63%|█████▋   | 42/67 [00:10<00:06,  3.90it/s, loss=1.89, v_num=1, train_loss_step=1.940, val_loss=3.170, train_loss_epoch=2.660]\u001b[A\n",
      "Epoch 7:  64%|█████▊   | 43/67 [00:11<00:06,  3.87it/s, loss=1.89, v_num=1, train_loss_step=1.940, val_loss=3.170, train_loss_epoch=2.660]\u001b[A\n",
      "Epoch 7:  66%|█████▉   | 44/67 [00:11<00:05,  3.85it/s, loss=1.89, v_num=1, train_loss_step=1.940, val_loss=3.170, train_loss_epoch=2.660]\u001b[A\n",
      "Epoch 7:  67%|██████   | 45/67 [00:11<00:05,  3.82it/s, loss=1.89, v_num=1, train_loss_step=1.940, val_loss=3.170, train_loss_epoch=2.660]\u001b[A\n",
      "Epoch 7:  69%|██████▏  | 46/67 [00:12<00:05,  3.80it/s, loss=1.89, v_num=1, train_loss_step=1.940, val_loss=3.170, train_loss_epoch=2.660]\u001b[A\n",
      "Epoch 7:  70%|██████▎  | 47/67 [00:12<00:05,  3.78it/s, loss=1.89, v_num=1, train_loss_step=1.940, val_loss=3.170, train_loss_epoch=2.660]\u001b[A\n",
      "Epoch 7:  72%|██████▍  | 48/67 [00:12<00:05,  3.76it/s, loss=1.89, v_num=1, train_loss_step=1.940, val_loss=3.170, train_loss_epoch=2.660]\u001b[A\n",
      "Epoch 7:  73%|██████▌  | 49/67 [00:13<00:04,  3.74it/s, loss=1.89, v_num=1, train_loss_step=1.940, val_loss=3.170, train_loss_epoch=2.660]\u001b[A\n",
      "Epoch 7:  75%|██████▋  | 50/67 [00:13<00:04,  3.72it/s, loss=1.89, v_num=1, train_loss_step=1.940, val_loss=3.170, train_loss_epoch=2.660]\u001b[A\n",
      "Epoch 7:  76%|██████▊  | 51/67 [00:13<00:04,  3.68it/s, loss=1.89, v_num=1, train_loss_step=1.940, val_loss=3.170, train_loss_epoch=2.660]\u001b[A\n",
      "Epoch 7:  78%|██████▉  | 52/67 [00:14<00:04,  3.58it/s, loss=1.89, v_num=1, train_loss_step=1.940, val_loss=3.170, train_loss_epoch=2.660]\u001b[A\n",
      "Epoch 7:  79%|███████  | 53/67 [00:14<00:03,  3.57it/s, loss=1.89, v_num=1, train_loss_step=1.940, val_loss=3.170, train_loss_epoch=2.660]\u001b[A\n",
      "Epoch 7:  81%|███████▎ | 54/67 [00:15<00:03,  3.56it/s, loss=1.89, v_num=1, train_loss_step=1.940, val_loss=3.170, train_loss_epoch=2.660]\u001b[A\n",
      "Epoch 7:  82%|███████▍ | 55/67 [00:15<00:03,  3.55it/s, loss=1.89, v_num=1, train_loss_step=1.940, val_loss=3.170, train_loss_epoch=2.660]\u001b[A\n",
      "Epoch 7:  84%|███████▌ | 56/67 [00:15<00:03,  3.54it/s, loss=1.89, v_num=1, train_loss_step=1.940, val_loss=3.170, train_loss_epoch=2.660]\u001b[A\n",
      "Epoch 7:  85%|███████▋ | 57/67 [00:16<00:02,  3.53it/s, loss=1.89, v_num=1, train_loss_step=1.940, val_loss=3.170, train_loss_epoch=2.660]\u001b[A\n",
      "Epoch 7:  87%|███████▊ | 58/67 [00:16<00:02,  3.52it/s, loss=1.89, v_num=1, train_loss_step=1.940, val_loss=3.170, train_loss_epoch=2.660]\u001b[A\n",
      "Epoch 7:  88%|███████▉ | 59/67 [00:16<00:02,  3.51it/s, loss=1.89, v_num=1, train_loss_step=1.940, val_loss=3.170, train_loss_epoch=2.660]\u001b[A\n",
      "Epoch 7:  90%|████████ | 60/67 [00:17<00:01,  3.51it/s, loss=1.89, v_num=1, train_loss_step=1.940, val_loss=3.170, train_loss_epoch=2.660]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 7:  91%|████████▏| 61/67 [00:17<00:01,  3.48it/s, loss=1.89, v_num=1, train_loss_step=1.940, val_loss=3.170, train_loss_epoch=2.660]\u001b[A\n",
      "Epoch 7:  93%|████████▎| 62/67 [00:17<00:01,  3.47it/s, loss=1.89, v_num=1, train_loss_step=1.940, val_loss=3.170, train_loss_epoch=2.660]\u001b[A\n",
      "Epoch 7:  94%|████████▍| 63/67 [00:18<00:01,  3.46it/s, loss=1.89, v_num=1, train_loss_step=1.940, val_loss=3.170, train_loss_epoch=2.660]\u001b[A\n",
      "Epoch 7:  96%|████████▌| 64/67 [00:18<00:00,  3.45it/s, loss=1.89, v_num=1, train_loss_step=1.940, val_loss=3.170, train_loss_epoch=2.660]\u001b[A\n",
      "Epoch 7:  97%|████████▋| 65/67 [00:18<00:00,  3.45it/s, loss=1.89, v_num=1, train_loss_step=1.940, val_loss=3.170, train_loss_epoch=2.660]\u001b[A\n",
      "Epoch 7:  99%|████████▊| 66/67 [00:19<00:00,  3.44it/s, loss=1.89, v_num=1, train_loss_step=1.940, val_loss=3.170, train_loss_epoch=2.660]\u001b[A\n",
      "Epoch 7: 100%|█████████| 67/67 [00:19<00:00,  3.37it/s, loss=1.89, v_num=1, train_loss_step=1.940, val_loss=3.170, train_loss_epoch=2.660]\u001b[A\n",
      "Epoch 8:  45%|████     | 30/67 [00:05<00:06,  5.49it/s, loss=2.82, v_num=1, train_loss_step=4.670, val_loss=3.170, train_loss_epoch=1.940]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 8:  46%|████▏    | 31/67 [00:07<00:08,  4.25it/s, loss=2.82, v_num=1, train_loss_step=4.670, val_loss=3.170, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 8:  48%|████▎    | 32/67 [00:07<00:08,  4.20it/s, loss=2.82, v_num=1, train_loss_step=4.670, val_loss=3.170, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 8:  49%|████▍    | 33/67 [00:07<00:08,  4.16it/s, loss=2.82, v_num=1, train_loss_step=4.670, val_loss=3.170, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 8:  51%|████▌    | 34/67 [00:08<00:08,  4.11it/s, loss=2.82, v_num=1, train_loss_step=4.670, val_loss=3.170, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 8:  52%|████▋    | 35/67 [00:08<00:07,  4.07it/s, loss=2.82, v_num=1, train_loss_step=4.670, val_loss=3.170, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 8:  54%|████▊    | 36/67 [00:08<00:07,  4.03it/s, loss=2.82, v_num=1, train_loss_step=4.670, val_loss=3.170, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 8:  55%|████▉    | 37/67 [00:09<00:07,  4.00it/s, loss=2.82, v_num=1, train_loss_step=4.670, val_loss=3.170, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 8:  57%|█████    | 38/67 [00:09<00:07,  3.97it/s, loss=2.82, v_num=1, train_loss_step=4.670, val_loss=3.170, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 8:  58%|█████▏   | 39/67 [00:09<00:07,  3.94it/s, loss=2.82, v_num=1, train_loss_step=4.670, val_loss=3.170, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 8:  60%|█████▎   | 40/67 [00:10<00:06,  3.91it/s, loss=2.82, v_num=1, train_loss_step=4.670, val_loss=3.170, train_loss_epoch=1.940]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 8:  61%|█████▌   | 41/67 [00:10<00:06,  3.85it/s, loss=2.82, v_num=1, train_loss_step=4.670, val_loss=3.170, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 8:  63%|█████▋   | 42/67 [00:10<00:06,  3.83it/s, loss=2.82, v_num=1, train_loss_step=4.670, val_loss=3.170, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 8:  64%|█████▊   | 43/67 [00:11<00:06,  3.81it/s, loss=2.82, v_num=1, train_loss_step=4.670, val_loss=3.170, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 8:  66%|█████▉   | 44/67 [00:11<00:06,  3.79it/s, loss=2.82, v_num=1, train_loss_step=4.670, val_loss=3.170, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 8:  67%|██████   | 45/67 [00:12<00:05,  3.71it/s, loss=2.82, v_num=1, train_loss_step=4.670, val_loss=3.170, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 8:  69%|██████▏  | 46/67 [00:12<00:05,  3.69it/s, loss=2.82, v_num=1, train_loss_step=4.670, val_loss=3.170, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 8:  70%|██████▎  | 47/67 [00:12<00:05,  3.68it/s, loss=2.82, v_num=1, train_loss_step=4.670, val_loss=3.170, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 8:  72%|██████▍  | 48/67 [00:13<00:05,  3.67it/s, loss=2.82, v_num=1, train_loss_step=4.670, val_loss=3.170, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 8:  73%|██████▌  | 49/67 [00:13<00:04,  3.65it/s, loss=2.82, v_num=1, train_loss_step=4.670, val_loss=3.170, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 8:  75%|██████▋  | 50/67 [00:13<00:04,  3.64it/s, loss=2.82, v_num=1, train_loss_step=4.670, val_loss=3.170, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 8:  76%|██████▊  | 51/67 [00:14<00:04,  3.59it/s, loss=2.82, v_num=1, train_loss_step=4.670, val_loss=3.170, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 8:  78%|██████▉  | 52/67 [00:14<00:04,  3.58it/s, loss=2.82, v_num=1, train_loss_step=4.670, val_loss=3.170, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 8:  79%|███████  | 53/67 [00:14<00:03,  3.57it/s, loss=2.82, v_num=1, train_loss_step=4.670, val_loss=3.170, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 8:  81%|███████▎ | 54/67 [00:15<00:03,  3.55it/s, loss=2.82, v_num=1, train_loss_step=4.670, val_loss=3.170, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 8:  82%|███████▍ | 55/67 [00:15<00:03,  3.54it/s, loss=2.82, v_num=1, train_loss_step=4.670, val_loss=3.170, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 8:  84%|███████▌ | 56/67 [00:15<00:03,  3.53it/s, loss=2.82, v_num=1, train_loss_step=4.670, val_loss=3.170, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 8:  85%|███████▋ | 57/67 [00:16<00:02,  3.52it/s, loss=2.82, v_num=1, train_loss_step=4.670, val_loss=3.170, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 8:  87%|███████▊ | 58/67 [00:16<00:02,  3.51it/s, loss=2.82, v_num=1, train_loss_step=4.670, val_loss=3.170, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 8:  88%|███████▉ | 59/67 [00:16<00:02,  3.50it/s, loss=2.82, v_num=1, train_loss_step=4.670, val_loss=3.170, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 8:  90%|████████ | 60/67 [00:17<00:02,  3.49it/s, loss=2.82, v_num=1, train_loss_step=4.670, val_loss=3.170, train_loss_epoch=1.940]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 8:  91%|████████▏| 61/67 [00:17<00:01,  3.46it/s, loss=2.82, v_num=1, train_loss_step=4.670, val_loss=3.170, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 8:  93%|████████▎| 62/67 [00:17<00:01,  3.45it/s, loss=2.82, v_num=1, train_loss_step=4.670, val_loss=3.170, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 8:  94%|████████▍| 63/67 [00:18<00:01,  3.44it/s, loss=2.82, v_num=1, train_loss_step=4.670, val_loss=3.170, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 8:  96%|████████▌| 64/67 [00:18<00:00,  3.43it/s, loss=2.82, v_num=1, train_loss_step=4.670, val_loss=3.170, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 8:  97%|████████▋| 65/67 [00:18<00:00,  3.42it/s, loss=2.82, v_num=1, train_loss_step=4.670, val_loss=3.170, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 8:  99%|████████▊| 66/67 [00:19<00:00,  3.42it/s, loss=2.82, v_num=1, train_loss_step=4.670, val_loss=3.170, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 8: 100%|█████████| 67/67 [00:19<00:00,  3.35it/s, loss=2.82, v_num=1, train_loss_step=4.670, val_loss=3.170, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 9:  45%|████     | 30/67 [00:05<00:06,  5.61it/s, loss=2.26, v_num=1, train_loss_step=1.660, val_loss=3.170, train_loss_epoch=2.550]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 9:  46%|████▏    | 31/67 [00:07<00:08,  4.31it/s, loss=2.26, v_num=1, train_loss_step=1.660, val_loss=3.170, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 9:  48%|████▎    | 32/67 [00:07<00:08,  4.26it/s, loss=2.26, v_num=1, train_loss_step=1.660, val_loss=3.170, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 9:  49%|████▍    | 33/67 [00:07<00:08,  4.21it/s, loss=2.26, v_num=1, train_loss_step=1.660, val_loss=3.170, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 9:  51%|████▌    | 34/67 [00:08<00:07,  4.17it/s, loss=2.26, v_num=1, train_loss_step=1.660, val_loss=3.170, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 9:  52%|████▋    | 35/67 [00:08<00:07,  4.12it/s, loss=2.26, v_num=1, train_loss_step=1.660, val_loss=3.170, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 9:  54%|████▊    | 36/67 [00:09<00:07,  3.94it/s, loss=2.26, v_num=1, train_loss_step=1.660, val_loss=3.170, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 9:  55%|████▉    | 37/67 [00:09<00:07,  3.90it/s, loss=2.26, v_num=1, train_loss_step=1.660, val_loss=3.170, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 9:  57%|█████    | 38/67 [00:09<00:07,  3.88it/s, loss=2.26, v_num=1, train_loss_step=1.660, val_loss=3.170, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 9:  58%|█████▏   | 39/67 [00:10<00:07,  3.85it/s, loss=2.26, v_num=1, train_loss_step=1.660, val_loss=3.170, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 9:  60%|█████▎   | 40/67 [00:10<00:07,  3.83it/s, loss=2.26, v_num=1, train_loss_step=1.660, val_loss=3.170, train_loss_epoch=2.550]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 9:  61%|█████▌   | 41/67 [00:10<00:06,  3.77it/s, loss=2.26, v_num=1, train_loss_step=1.660, val_loss=3.170, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 9:  63%|█████▋   | 42/67 [00:11<00:06,  3.75it/s, loss=2.26, v_num=1, train_loss_step=1.660, val_loss=3.170, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 9:  64%|█████▊   | 43/67 [00:11<00:06,  3.74it/s, loss=2.26, v_num=1, train_loss_step=1.660, val_loss=3.170, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 9:  66%|█████▉   | 44/67 [00:11<00:06,  3.72it/s, loss=2.26, v_num=1, train_loss_step=1.660, val_loss=3.170, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 9:  67%|██████   | 45/67 [00:12<00:05,  3.70it/s, loss=2.26, v_num=1, train_loss_step=1.660, val_loss=3.170, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 9:  69%|██████▏  | 46/67 [00:12<00:05,  3.68it/s, loss=2.26, v_num=1, train_loss_step=1.660, val_loss=3.170, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 9:  70%|██████▎  | 47/67 [00:12<00:05,  3.67it/s, loss=2.26, v_num=1, train_loss_step=1.660, val_loss=3.170, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 9:  72%|██████▍  | 48/67 [00:13<00:05,  3.65it/s, loss=2.26, v_num=1, train_loss_step=1.660, val_loss=3.170, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 9:  73%|██████▌  | 49/67 [00:13<00:04,  3.63it/s, loss=2.26, v_num=1, train_loss_step=1.660, val_loss=3.170, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 9:  75%|██████▋  | 50/67 [00:13<00:04,  3.62it/s, loss=2.26, v_num=1, train_loss_step=1.660, val_loss=3.170, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 9:  76%|██████▊  | 51/67 [00:14<00:04,  3.58it/s, loss=2.26, v_num=1, train_loss_step=1.660, val_loss=3.170, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 9:  78%|██████▉  | 52/67 [00:14<00:04,  3.56it/s, loss=2.26, v_num=1, train_loss_step=1.660, val_loss=3.170, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 9:  79%|███████  | 53/67 [00:14<00:03,  3.55it/s, loss=2.26, v_num=1, train_loss_step=1.660, val_loss=3.170, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 9:  81%|███████▎ | 54/67 [00:15<00:03,  3.54it/s, loss=2.26, v_num=1, train_loss_step=1.660, val_loss=3.170, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 9:  82%|███████▍ | 55/67 [00:15<00:03,  3.53it/s, loss=2.26, v_num=1, train_loss_step=1.660, val_loss=3.170, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 9:  84%|███████▌ | 56/67 [00:15<00:03,  3.53it/s, loss=2.26, v_num=1, train_loss_step=1.660, val_loss=3.170, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 9:  85%|███████▋ | 57/67 [00:16<00:02,  3.52it/s, loss=2.26, v_num=1, train_loss_step=1.660, val_loss=3.170, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 9:  87%|███████▊ | 58/67 [00:16<00:02,  3.51it/s, loss=2.26, v_num=1, train_loss_step=1.660, val_loss=3.170, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 9:  88%|███████▉ | 59/67 [00:16<00:02,  3.50it/s, loss=2.26, v_num=1, train_loss_step=1.660, val_loss=3.170, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 9:  90%|████████ | 60/67 [00:17<00:02,  3.50it/s, loss=2.26, v_num=1, train_loss_step=1.660, val_loss=3.170, train_loss_epoch=2.550]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 9:  91%|████████▏| 61/67 [00:17<00:01,  3.47it/s, loss=2.26, v_num=1, train_loss_step=1.660, val_loss=3.170, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 9:  93%|████████▎| 62/67 [00:17<00:01,  3.46it/s, loss=2.26, v_num=1, train_loss_step=1.660, val_loss=3.170, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 9:  94%|████████▍| 63/67 [00:18<00:01,  3.46it/s, loss=2.26, v_num=1, train_loss_step=1.660, val_loss=3.170, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 9:  96%|████████▌| 64/67 [00:18<00:00,  3.45it/s, loss=2.26, v_num=1, train_loss_step=1.660, val_loss=3.170, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 9:  97%|████████▋| 65/67 [00:18<00:00,  3.45it/s, loss=2.26, v_num=1, train_loss_step=1.660, val_loss=3.170, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 9:  99%|████████▊| 66/67 [00:19<00:00,  3.44it/s, loss=2.26, v_num=1, train_loss_step=1.660, val_loss=3.170, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 9: 100%|█████████| 67/67 [00:19<00:00,  3.38it/s, loss=2.26, v_num=1, train_loss_step=1.660, val_loss=3.170, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 10:  45%|███▌    | 30/67 [00:05<00:06,  5.49it/s, loss=2.79, v_num=1, train_loss_step=10.90, val_loss=3.170, train_loss_epoch=2.280]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 10:  46%|███▋    | 31/67 [00:07<00:08,  4.22it/s, loss=2.79, v_num=1, train_loss_step=10.90, val_loss=3.170, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 10:  48%|███▊    | 32/67 [00:07<00:08,  4.17it/s, loss=2.79, v_num=1, train_loss_step=10.90, val_loss=3.170, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 10:  49%|███▉    | 33/67 [00:08<00:08,  4.12it/s, loss=2.79, v_num=1, train_loss_step=10.90, val_loss=3.170, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 10:  51%|████    | 34/67 [00:08<00:08,  4.08it/s, loss=2.79, v_num=1, train_loss_step=10.90, val_loss=3.170, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 10:  52%|████▏   | 35/67 [00:08<00:07,  4.04it/s, loss=2.79, v_num=1, train_loss_step=10.90, val_loss=3.170, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 10:  54%|████▎   | 36/67 [00:08<00:07,  4.01it/s, loss=2.79, v_num=1, train_loss_step=10.90, val_loss=3.170, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 10:  55%|████▍   | 37/67 [00:09<00:07,  3.97it/s, loss=2.79, v_num=1, train_loss_step=10.90, val_loss=3.170, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 10:  57%|████▌   | 38/67 [00:09<00:07,  3.94it/s, loss=2.79, v_num=1, train_loss_step=10.90, val_loss=3.170, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 10:  58%|████▋   | 39/67 [00:09<00:07,  3.91it/s, loss=2.79, v_num=1, train_loss_step=10.90, val_loss=3.170, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 10:  60%|████▊   | 40/67 [00:10<00:06,  3.88it/s, loss=2.79, v_num=1, train_loss_step=10.90, val_loss=3.170, train_loss_epoch=2.280]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 10:  61%|████▉   | 41/67 [00:10<00:06,  3.81it/s, loss=2.79, v_num=1, train_loss_step=10.90, val_loss=3.170, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 10:  63%|█████   | 42/67 [00:11<00:06,  3.79it/s, loss=2.79, v_num=1, train_loss_step=10.90, val_loss=3.170, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 10:  64%|█████▏  | 43/67 [00:11<00:06,  3.77it/s, loss=2.79, v_num=1, train_loss_step=10.90, val_loss=3.170, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 10:  66%|█████▎  | 44/67 [00:11<00:06,  3.74it/s, loss=2.79, v_num=1, train_loss_step=10.90, val_loss=3.170, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 10:  67%|█████▎  | 45/67 [00:12<00:05,  3.72it/s, loss=2.79, v_num=1, train_loss_step=10.90, val_loss=3.170, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 10:  69%|█████▍  | 46/67 [00:12<00:05,  3.70it/s, loss=2.79, v_num=1, train_loss_step=10.90, val_loss=3.170, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 10:  70%|█████▌  | 47/67 [00:12<00:05,  3.69it/s, loss=2.79, v_num=1, train_loss_step=10.90, val_loss=3.170, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 10:  72%|█████▋  | 48/67 [00:13<00:05,  3.67it/s, loss=2.79, v_num=1, train_loss_step=10.90, val_loss=3.170, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 10:  73%|█████▊  | 49/67 [00:13<00:04,  3.66it/s, loss=2.79, v_num=1, train_loss_step=10.90, val_loss=3.170, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 10:  75%|█████▉  | 50/67 [00:13<00:04,  3.64it/s, loss=2.79, v_num=1, train_loss_step=10.90, val_loss=3.170, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 10:  76%|██████  | 51/67 [00:14<00:04,  3.60it/s, loss=2.79, v_num=1, train_loss_step=10.90, val_loss=3.170, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 10:  78%|██████▏ | 52/67 [00:14<00:04,  3.59it/s, loss=2.79, v_num=1, train_loss_step=10.90, val_loss=3.170, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 10:  79%|██████▎ | 53/67 [00:14<00:03,  3.58it/s, loss=2.79, v_num=1, train_loss_step=10.90, val_loss=3.170, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 10:  81%|██████▍ | 54/67 [00:15<00:03,  3.57it/s, loss=2.79, v_num=1, train_loss_step=10.90, val_loss=3.170, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 10:  82%|██████▌ | 55/67 [00:15<00:03,  3.56it/s, loss=2.79, v_num=1, train_loss_step=10.90, val_loss=3.170, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 10:  84%|██████▋ | 56/67 [00:15<00:03,  3.55it/s, loss=2.79, v_num=1, train_loss_step=10.90, val_loss=3.170, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 10:  85%|██████▊ | 57/67 [00:16<00:02,  3.54it/s, loss=2.79, v_num=1, train_loss_step=10.90, val_loss=3.170, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 10:  87%|██████▉ | 58/67 [00:16<00:02,  3.53it/s, loss=2.79, v_num=1, train_loss_step=10.90, val_loss=3.170, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 10:  88%|███████ | 59/67 [00:16<00:02,  3.52it/s, loss=2.79, v_num=1, train_loss_step=10.90, val_loss=3.170, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 10:  90%|███████▏| 60/67 [00:17<00:01,  3.51it/s, loss=2.79, v_num=1, train_loss_step=10.90, val_loss=3.170, train_loss_epoch=2.280]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 10:  91%|███████▎| 61/67 [00:17<00:01,  3.48it/s, loss=2.79, v_num=1, train_loss_step=10.90, val_loss=3.170, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 10:  93%|███████▍| 62/67 [00:17<00:01,  3.48it/s, loss=2.79, v_num=1, train_loss_step=10.90, val_loss=3.170, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 10:  94%|███████▌| 63/67 [00:18<00:01,  3.47it/s, loss=2.79, v_num=1, train_loss_step=10.90, val_loss=3.170, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 10:  96%|███████▋| 64/67 [00:18<00:00,  3.46it/s, loss=2.79, v_num=1, train_loss_step=10.90, val_loss=3.170, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 10:  97%|███████▊| 65/67 [00:18<00:00,  3.46it/s, loss=2.79, v_num=1, train_loss_step=10.90, val_loss=3.170, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 10:  99%|███████▉| 66/67 [00:19<00:00,  3.45it/s, loss=2.79, v_num=1, train_loss_step=10.90, val_loss=3.170, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 10: 100%|████████| 67/67 [00:19<00:00,  3.39it/s, loss=2.79, v_num=1, train_loss_step=10.90, val_loss=3.170, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 11:  45%|███▌    | 30/67 [00:05<00:06,  5.49it/s, loss=2.38, v_num=1, train_loss_step=3.090, val_loss=3.170, train_loss_epoch=2.500]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 11:  46%|███▋    | 31/67 [00:07<00:08,  4.25it/s, loss=2.38, v_num=1, train_loss_step=3.090, val_loss=3.170, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 11:  48%|███▊    | 32/67 [00:07<00:08,  4.20it/s, loss=2.38, v_num=1, train_loss_step=3.090, val_loss=3.170, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 11:  49%|███▉    | 33/67 [00:07<00:08,  4.16it/s, loss=2.38, v_num=1, train_loss_step=3.090, val_loss=3.170, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 11:  51%|████    | 34/67 [00:08<00:08,  4.11it/s, loss=2.38, v_num=1, train_loss_step=3.090, val_loss=3.170, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 11:  52%|████▏   | 35/67 [00:08<00:07,  4.07it/s, loss=2.38, v_num=1, train_loss_step=3.090, val_loss=3.170, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 11:  54%|████▎   | 36/67 [00:08<00:07,  4.03it/s, loss=2.38, v_num=1, train_loss_step=3.090, val_loss=3.170, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 11:  55%|████▍   | 37/67 [00:09<00:07,  4.00it/s, loss=2.38, v_num=1, train_loss_step=3.090, val_loss=3.170, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 11:  57%|████▌   | 38/67 [00:09<00:07,  3.96it/s, loss=2.38, v_num=1, train_loss_step=3.090, val_loss=3.170, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 11:  58%|████▋   | 39/67 [00:09<00:07,  3.93it/s, loss=2.38, v_num=1, train_loss_step=3.090, val_loss=3.170, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 11:  60%|████▊   | 40/67 [00:10<00:06,  3.90it/s, loss=2.38, v_num=1, train_loss_step=3.090, val_loss=3.170, train_loss_epoch=2.500]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 11:  61%|████▉   | 41/67 [00:10<00:06,  3.84it/s, loss=2.38, v_num=1, train_loss_step=3.090, val_loss=3.170, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 11:  63%|█████   | 42/67 [00:11<00:06,  3.81it/s, loss=2.38, v_num=1, train_loss_step=3.090, val_loss=3.170, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 11:  64%|█████▏  | 43/67 [00:11<00:06,  3.78it/s, loss=2.38, v_num=1, train_loss_step=3.090, val_loss=3.170, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 11:  66%|█████▎  | 44/67 [00:11<00:06,  3.76it/s, loss=2.38, v_num=1, train_loss_step=3.090, val_loss=3.170, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 11:  67%|█████▎  | 45/67 [00:12<00:05,  3.74it/s, loss=2.38, v_num=1, train_loss_step=3.090, val_loss=3.170, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 11:  69%|█████▍  | 46/67 [00:12<00:05,  3.72it/s, loss=2.38, v_num=1, train_loss_step=3.090, val_loss=3.170, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 11:  70%|█████▌  | 47/67 [00:12<00:05,  3.70it/s, loss=2.38, v_num=1, train_loss_step=3.090, val_loss=3.170, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 11:  72%|█████▋  | 48/67 [00:13<00:05,  3.69it/s, loss=2.38, v_num=1, train_loss_step=3.090, val_loss=3.170, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 11:  73%|█████▊  | 49/67 [00:13<00:04,  3.67it/s, loss=2.38, v_num=1, train_loss_step=3.090, val_loss=3.170, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 11:  75%|█████▉  | 50/67 [00:13<00:04,  3.66it/s, loss=2.38, v_num=1, train_loss_step=3.090, val_loss=3.170, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 11:  76%|██████  | 51/67 [00:14<00:04,  3.61it/s, loss=2.38, v_num=1, train_loss_step=3.090, val_loss=3.170, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 11:  78%|██████▏ | 52/67 [00:14<00:04,  3.59it/s, loss=2.38, v_num=1, train_loss_step=3.090, val_loss=3.170, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 11:  79%|██████▎ | 53/67 [00:14<00:03,  3.58it/s, loss=2.38, v_num=1, train_loss_step=3.090, val_loss=3.170, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 11:  81%|██████▍ | 54/67 [00:15<00:03,  3.56it/s, loss=2.38, v_num=1, train_loss_step=3.090, val_loss=3.170, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 11:  82%|██████▌ | 55/67 [00:15<00:03,  3.55it/s, loss=2.38, v_num=1, train_loss_step=3.090, val_loss=3.170, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 11:  84%|██████▋ | 56/67 [00:15<00:03,  3.54it/s, loss=2.38, v_num=1, train_loss_step=3.090, val_loss=3.170, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 11:  85%|██████▊ | 57/67 [00:16<00:02,  3.53it/s, loss=2.38, v_num=1, train_loss_step=3.090, val_loss=3.170, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 11:  87%|██████▉ | 58/67 [00:16<00:02,  3.52it/s, loss=2.38, v_num=1, train_loss_step=3.090, val_loss=3.170, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 11:  88%|███████ | 59/67 [00:16<00:02,  3.51it/s, loss=2.38, v_num=1, train_loss_step=3.090, val_loss=3.170, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 11:  90%|███████▏| 60/67 [00:17<00:01,  3.50it/s, loss=2.38, v_num=1, train_loss_step=3.090, val_loss=3.170, train_loss_epoch=2.500]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 11:  91%|███████▎| 61/67 [00:17<00:01,  3.48it/s, loss=2.38, v_num=1, train_loss_step=3.090, val_loss=3.170, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 11:  93%|███████▍| 62/67 [00:17<00:01,  3.47it/s, loss=2.38, v_num=1, train_loss_step=3.090, val_loss=3.170, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 11:  94%|███████▌| 63/67 [00:18<00:01,  3.46it/s, loss=2.38, v_num=1, train_loss_step=3.090, val_loss=3.170, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 11:  96%|███████▋| 64/67 [00:18<00:00,  3.46it/s, loss=2.38, v_num=1, train_loss_step=3.090, val_loss=3.170, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 11:  97%|███████▊| 65/67 [00:18<00:00,  3.45it/s, loss=2.38, v_num=1, train_loss_step=3.090, val_loss=3.170, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 11:  99%|███████▉| 66/67 [00:19<00:00,  3.45it/s, loss=2.38, v_num=1, train_loss_step=3.090, val_loss=3.170, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 11: 100%|████████| 67/67 [00:19<00:00,  3.38it/s, loss=2.38, v_num=1, train_loss_step=3.090, val_loss=3.170, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 12:  45%|███▌    | 30/67 [00:05<00:06,  5.62it/s, loss=2.17, v_num=1, train_loss_step=1.920, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 12:  46%|███▋    | 31/67 [00:07<00:08,  4.29it/s, loss=2.17, v_num=1, train_loss_step=1.920, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 12:  48%|███▊    | 32/67 [00:07<00:08,  4.24it/s, loss=2.17, v_num=1, train_loss_step=1.920, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 12:  49%|███▉    | 33/67 [00:07<00:08,  4.19it/s, loss=2.17, v_num=1, train_loss_step=1.920, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 12:  51%|████    | 34/67 [00:08<00:07,  4.14it/s, loss=2.17, v_num=1, train_loss_step=1.920, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 12:  52%|████▏   | 35/67 [00:08<00:07,  4.10it/s, loss=2.17, v_num=1, train_loss_step=1.920, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 12:  54%|████▎   | 36/67 [00:08<00:07,  4.06it/s, loss=2.17, v_num=1, train_loss_step=1.920, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 12:  55%|████▍   | 37/67 [00:09<00:07,  4.01it/s, loss=2.17, v_num=1, train_loss_step=1.920, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 12:  57%|████▌   | 38/67 [00:09<00:07,  3.97it/s, loss=2.17, v_num=1, train_loss_step=1.920, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 12:  58%|████▋   | 39/67 [00:09<00:07,  3.94it/s, loss=2.17, v_num=1, train_loss_step=1.920, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 12:  60%|████▊   | 40/67 [00:10<00:06,  3.91it/s, loss=2.17, v_num=1, train_loss_step=1.920, val_loss=3.170, train_loss_epoch=2.390]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 12:  61%|████▉   | 41/67 [00:10<00:06,  3.85it/s, loss=2.17, v_num=1, train_loss_step=1.920, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 12:  63%|█████   | 42/67 [00:10<00:06,  3.82it/s, loss=2.17, v_num=1, train_loss_step=1.920, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 12:  64%|█████▏  | 43/67 [00:11<00:06,  3.80it/s, loss=2.17, v_num=1, train_loss_step=1.920, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 12:  66%|█████▎  | 44/67 [00:11<00:06,  3.78it/s, loss=2.17, v_num=1, train_loss_step=1.920, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 12:  67%|█████▎  | 45/67 [00:11<00:05,  3.76it/s, loss=2.17, v_num=1, train_loss_step=1.920, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 12:  69%|█████▍  | 46/67 [00:12<00:05,  3.74it/s, loss=2.17, v_num=1, train_loss_step=1.920, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 12:  70%|█████▌  | 47/67 [00:12<00:05,  3.72it/s, loss=2.17, v_num=1, train_loss_step=1.920, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 12:  72%|█████▋  | 48/67 [00:12<00:05,  3.70it/s, loss=2.17, v_num=1, train_loss_step=1.920, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 12:  73%|█████▊  | 49/67 [00:13<00:04,  3.68it/s, loss=2.17, v_num=1, train_loss_step=1.920, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 12:  75%|█████▉  | 50/67 [00:13<00:04,  3.66it/s, loss=2.17, v_num=1, train_loss_step=1.920, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 12:  76%|██████  | 51/67 [00:14<00:04,  3.62it/s, loss=2.17, v_num=1, train_loss_step=1.920, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 12:  78%|██████▏ | 52/67 [00:14<00:04,  3.61it/s, loss=2.17, v_num=1, train_loss_step=1.920, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 12:  79%|██████▎ | 53/67 [00:14<00:03,  3.59it/s, loss=2.17, v_num=1, train_loss_step=1.920, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 12:  81%|██████▍ | 54/67 [00:15<00:03,  3.58it/s, loss=2.17, v_num=1, train_loss_step=1.920, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 12:  82%|██████▌ | 55/67 [00:15<00:03,  3.57it/s, loss=2.17, v_num=1, train_loss_step=1.920, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 12:  84%|██████▋ | 56/67 [00:15<00:03,  3.56it/s, loss=2.17, v_num=1, train_loss_step=1.920, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 12:  85%|██████▊ | 57/67 [00:16<00:02,  3.55it/s, loss=2.17, v_num=1, train_loss_step=1.920, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 12:  87%|██████▉ | 58/67 [00:16<00:02,  3.54it/s, loss=2.17, v_num=1, train_loss_step=1.920, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 12:  88%|███████ | 59/67 [00:16<00:02,  3.54it/s, loss=2.17, v_num=1, train_loss_step=1.920, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 12:  90%|███████▏| 60/67 [00:17<00:01,  3.53it/s, loss=2.17, v_num=1, train_loss_step=1.920, val_loss=3.170, train_loss_epoch=2.390]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 12:  91%|███████▎| 61/67 [00:17<00:01,  3.48it/s, loss=2.17, v_num=1, train_loss_step=1.920, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 12:  93%|███████▍| 62/67 [00:17<00:01,  3.47it/s, loss=2.17, v_num=1, train_loss_step=1.920, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 12:  94%|███████▌| 63/67 [00:18<00:01,  3.46it/s, loss=2.17, v_num=1, train_loss_step=1.920, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 12:  96%|███████▋| 64/67 [00:18<00:00,  3.45it/s, loss=2.17, v_num=1, train_loss_step=1.920, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 12:  97%|███████▊| 65/67 [00:18<00:00,  3.45it/s, loss=2.17, v_num=1, train_loss_step=1.920, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 12:  99%|███████▉| 66/67 [00:19<00:00,  3.44it/s, loss=2.17, v_num=1, train_loss_step=1.920, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 12: 100%|████████| 67/67 [00:19<00:00,  3.38it/s, loss=2.17, v_num=1, train_loss_step=1.920, val_loss=3.170, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 13:  45%|███▌    | 30/67 [00:05<00:06,  5.55it/s, loss=2.04, v_num=1, train_loss_step=5.360, val_loss=3.170, train_loss_epoch=2.010]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 13:  46%|███▋    | 31/67 [00:07<00:08,  4.28it/s, loss=2.04, v_num=1, train_loss_step=5.360, val_loss=3.170, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 13:  48%|███▊    | 32/67 [00:07<00:08,  4.23it/s, loss=2.04, v_num=1, train_loss_step=5.360, val_loss=3.170, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 13:  49%|███▉    | 33/67 [00:07<00:08,  4.18it/s, loss=2.04, v_num=1, train_loss_step=5.360, val_loss=3.170, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 13:  51%|████    | 34/67 [00:08<00:07,  4.13it/s, loss=2.04, v_num=1, train_loss_step=5.360, val_loss=3.170, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 13:  52%|████▏   | 35/67 [00:08<00:07,  4.09it/s, loss=2.04, v_num=1, train_loss_step=5.360, val_loss=3.170, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 13:  54%|████▎   | 36/67 [00:08<00:07,  4.05it/s, loss=2.04, v_num=1, train_loss_step=5.360, val_loss=3.170, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 13:  55%|████▍   | 37/67 [00:09<00:07,  4.02it/s, loss=2.04, v_num=1, train_loss_step=5.360, val_loss=3.170, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 13:  57%|████▌   | 38/67 [00:09<00:07,  3.98it/s, loss=2.04, v_num=1, train_loss_step=5.360, val_loss=3.170, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 13:  58%|████▋   | 39/67 [00:09<00:07,  3.95it/s, loss=2.04, v_num=1, train_loss_step=5.360, val_loss=3.170, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 13:  60%|████▊   | 40/67 [00:10<00:06,  3.92it/s, loss=2.04, v_num=1, train_loss_step=5.360, val_loss=3.170, train_loss_epoch=2.010]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 13:  61%|████▉   | 41/67 [00:10<00:06,  3.85it/s, loss=2.04, v_num=1, train_loss_step=5.360, val_loss=3.170, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 13:  63%|█████   | 42/67 [00:10<00:06,  3.83it/s, loss=2.04, v_num=1, train_loss_step=5.360, val_loss=3.170, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 13:  64%|█████▏  | 43/67 [00:11<00:06,  3.81it/s, loss=2.04, v_num=1, train_loss_step=5.360, val_loss=3.170, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 13:  66%|█████▎  | 44/67 [00:11<00:06,  3.78it/s, loss=2.04, v_num=1, train_loss_step=5.360, val_loss=3.170, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 13:  67%|█████▎  | 45/67 [00:11<00:05,  3.76it/s, loss=2.04, v_num=1, train_loss_step=5.360, val_loss=3.170, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 13:  69%|█████▍  | 46/67 [00:12<00:05,  3.74it/s, loss=2.04, v_num=1, train_loss_step=5.360, val_loss=3.170, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 13:  70%|█████▌  | 47/67 [00:12<00:05,  3.72it/s, loss=2.04, v_num=1, train_loss_step=5.360, val_loss=3.170, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 13:  72%|█████▋  | 48/67 [00:12<00:05,  3.71it/s, loss=2.04, v_num=1, train_loss_step=5.360, val_loss=3.170, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 13:  73%|█████▊  | 49/67 [00:13<00:04,  3.69it/s, loss=2.04, v_num=1, train_loss_step=5.360, val_loss=3.170, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 13:  75%|█████▉  | 50/67 [00:13<00:04,  3.58it/s, loss=2.04, v_num=1, train_loss_step=5.360, val_loss=3.170, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 13:  76%|██████  | 51/67 [00:14<00:04,  3.54it/s, loss=2.04, v_num=1, train_loss_step=5.360, val_loss=3.170, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 13:  78%|██████▏ | 52/67 [00:14<00:04,  3.53it/s, loss=2.04, v_num=1, train_loss_step=5.360, val_loss=3.170, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 13:  79%|██████▎ | 53/67 [00:15<00:03,  3.52it/s, loss=2.04, v_num=1, train_loss_step=5.360, val_loss=3.170, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 13:  81%|██████▍ | 54/67 [00:15<00:03,  3.51it/s, loss=2.04, v_num=1, train_loss_step=5.360, val_loss=3.170, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 13:  82%|██████▌ | 55/67 [00:15<00:03,  3.50it/s, loss=2.04, v_num=1, train_loss_step=5.360, val_loss=3.170, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 13:  84%|██████▋ | 56/67 [00:16<00:03,  3.49it/s, loss=2.04, v_num=1, train_loss_step=5.360, val_loss=3.170, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 13:  85%|██████▊ | 57/67 [00:16<00:02,  3.48it/s, loss=2.04, v_num=1, train_loss_step=5.360, val_loss=3.170, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 13:  87%|██████▉ | 58/67 [00:16<00:02,  3.47it/s, loss=2.04, v_num=1, train_loss_step=5.360, val_loss=3.170, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 13:  88%|███████ | 59/67 [00:17<00:02,  3.47it/s, loss=2.04, v_num=1, train_loss_step=5.360, val_loss=3.170, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 13:  90%|███████▏| 60/67 [00:17<00:02,  3.46it/s, loss=2.04, v_num=1, train_loss_step=5.360, val_loss=3.170, train_loss_epoch=2.010]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 13:  91%|███████▎| 61/67 [00:17<00:01,  3.43it/s, loss=2.04, v_num=1, train_loss_step=5.360, val_loss=3.170, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 13:  93%|███████▍| 62/67 [00:18<00:01,  3.43it/s, loss=2.04, v_num=1, train_loss_step=5.360, val_loss=3.170, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 13:  94%|███████▌| 63/67 [00:18<00:01,  3.42it/s, loss=2.04, v_num=1, train_loss_step=5.360, val_loss=3.170, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 13:  96%|███████▋| 64/67 [00:18<00:00,  3.42it/s, loss=2.04, v_num=1, train_loss_step=5.360, val_loss=3.170, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 13:  97%|███████▊| 65/67 [00:19<00:00,  3.41it/s, loss=2.04, v_num=1, train_loss_step=5.360, val_loss=3.170, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 13:  99%|███████▉| 66/67 [00:19<00:00,  3.41it/s, loss=2.04, v_num=1, train_loss_step=5.360, val_loss=3.170, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 13: 100%|████████| 67/67 [00:20<00:00,  3.34it/s, loss=2.04, v_num=1, train_loss_step=5.360, val_loss=3.170, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 14:  45%|███▌    | 30/67 [00:05<00:06,  5.60it/s, loss=2.41, v_num=1, train_loss_step=2.640, val_loss=3.170, train_loss_epoch=1.750]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 14:  46%|███▋    | 31/67 [00:07<00:08,  4.28it/s, loss=2.41, v_num=1, train_loss_step=2.640, val_loss=3.170, train_loss_epoch=1.750]\u001b[A\n",
      "Epoch 14:  48%|███▊    | 32/67 [00:07<00:08,  4.23it/s, loss=2.41, v_num=1, train_loss_step=2.640, val_loss=3.170, train_loss_epoch=1.750]\u001b[A\n",
      "Epoch 14:  49%|███▉    | 33/67 [00:07<00:08,  4.17it/s, loss=2.41, v_num=1, train_loss_step=2.640, val_loss=3.170, train_loss_epoch=1.750]\u001b[A\n",
      "Epoch 14:  51%|████    | 34/67 [00:08<00:08,  4.12it/s, loss=2.41, v_num=1, train_loss_step=2.640, val_loss=3.170, train_loss_epoch=1.750]\u001b[A\n",
      "Epoch 14:  52%|████▏   | 35/67 [00:08<00:07,  4.08it/s, loss=2.41, v_num=1, train_loss_step=2.640, val_loss=3.170, train_loss_epoch=1.750]\u001b[A\n",
      "Epoch 14:  54%|████▎   | 36/67 [00:08<00:07,  4.05it/s, loss=2.41, v_num=1, train_loss_step=2.640, val_loss=3.170, train_loss_epoch=1.750]\u001b[A\n",
      "Epoch 14:  55%|████▍   | 37/67 [00:09<00:07,  4.01it/s, loss=2.41, v_num=1, train_loss_step=2.640, val_loss=3.170, train_loss_epoch=1.750]\u001b[A\n",
      "Epoch 14:  57%|████▌   | 38/67 [00:09<00:07,  3.98it/s, loss=2.41, v_num=1, train_loss_step=2.640, val_loss=3.170, train_loss_epoch=1.750]\u001b[A\n",
      "Epoch 14:  58%|████▋   | 39/67 [00:09<00:07,  3.95it/s, loss=2.41, v_num=1, train_loss_step=2.640, val_loss=3.170, train_loss_epoch=1.750]\u001b[A\n",
      "Epoch 14:  60%|████▊   | 40/67 [00:10<00:06,  3.92it/s, loss=2.41, v_num=1, train_loss_step=2.640, val_loss=3.170, train_loss_epoch=1.750]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 14:  61%|████▉   | 41/67 [00:10<00:06,  3.84it/s, loss=2.41, v_num=1, train_loss_step=2.640, val_loss=3.170, train_loss_epoch=1.750]\u001b[A\n",
      "Epoch 14:  63%|█████   | 42/67 [00:10<00:06,  3.82it/s, loss=2.41, v_num=1, train_loss_step=2.640, val_loss=3.170, train_loss_epoch=1.750]\u001b[A\n",
      "Epoch 14:  64%|█████▏  | 43/67 [00:11<00:06,  3.78it/s, loss=2.41, v_num=1, train_loss_step=2.640, val_loss=3.170, train_loss_epoch=1.750]\u001b[A\n",
      "Epoch 14:  66%|█████▎  | 44/67 [00:11<00:06,  3.76it/s, loss=2.41, v_num=1, train_loss_step=2.640, val_loss=3.170, train_loss_epoch=1.750]\u001b[A\n",
      "Epoch 14:  67%|█████▎  | 45/67 [00:12<00:05,  3.74it/s, loss=2.41, v_num=1, train_loss_step=2.640, val_loss=3.170, train_loss_epoch=1.750]\u001b[A\n",
      "Epoch 14:  69%|█████▍  | 46/67 [00:12<00:05,  3.72it/s, loss=2.41, v_num=1, train_loss_step=2.640, val_loss=3.170, train_loss_epoch=1.750]\u001b[A\n",
      "Epoch 14:  70%|█████▌  | 47/67 [00:12<00:05,  3.70it/s, loss=2.41, v_num=1, train_loss_step=2.640, val_loss=3.170, train_loss_epoch=1.750]\u001b[A\n",
      "Epoch 14:  72%|█████▋  | 48/67 [00:13<00:05,  3.69it/s, loss=2.41, v_num=1, train_loss_step=2.640, val_loss=3.170, train_loss_epoch=1.750]\u001b[A\n",
      "Epoch 14:  73%|█████▊  | 49/67 [00:13<00:04,  3.67it/s, loss=2.41, v_num=1, train_loss_step=2.640, val_loss=3.170, train_loss_epoch=1.750]\u001b[A\n",
      "Epoch 14:  75%|█████▉  | 50/67 [00:13<00:04,  3.66it/s, loss=2.41, v_num=1, train_loss_step=2.640, val_loss=3.170, train_loss_epoch=1.750]\u001b[A\n",
      "Epoch 14:  76%|██████  | 51/67 [00:14<00:04,  3.62it/s, loss=2.41, v_num=1, train_loss_step=2.640, val_loss=3.170, train_loss_epoch=1.750]\u001b[A\n",
      "Epoch 14:  78%|██████▏ | 52/67 [00:14<00:04,  3.60it/s, loss=2.41, v_num=1, train_loss_step=2.640, val_loss=3.170, train_loss_epoch=1.750]\u001b[A\n",
      "Epoch 14:  79%|██████▎ | 53/67 [00:14<00:03,  3.59it/s, loss=2.41, v_num=1, train_loss_step=2.640, val_loss=3.170, train_loss_epoch=1.750]\u001b[A\n",
      "Epoch 14:  81%|██████▍ | 54/67 [00:15<00:03,  3.58it/s, loss=2.41, v_num=1, train_loss_step=2.640, val_loss=3.170, train_loss_epoch=1.750]\u001b[A\n",
      "Epoch 14:  82%|██████▌ | 55/67 [00:15<00:03,  3.57it/s, loss=2.41, v_num=1, train_loss_step=2.640, val_loss=3.170, train_loss_epoch=1.750]\u001b[A\n",
      "Epoch 14:  84%|██████▋ | 56/67 [00:15<00:03,  3.56it/s, loss=2.41, v_num=1, train_loss_step=2.640, val_loss=3.170, train_loss_epoch=1.750]\u001b[A\n",
      "Epoch 14:  85%|██████▊ | 57/67 [00:16<00:02,  3.55it/s, loss=2.41, v_num=1, train_loss_step=2.640, val_loss=3.170, train_loss_epoch=1.750]\u001b[A\n",
      "Epoch 14:  87%|██████▉ | 58/67 [00:16<00:02,  3.54it/s, loss=2.41, v_num=1, train_loss_step=2.640, val_loss=3.170, train_loss_epoch=1.750]\u001b[A\n",
      "Epoch 14:  88%|███████ | 59/67 [00:16<00:02,  3.53it/s, loss=2.41, v_num=1, train_loss_step=2.640, val_loss=3.170, train_loss_epoch=1.750]\u001b[A\n",
      "Epoch 14:  90%|███████▏| 60/67 [00:17<00:01,  3.52it/s, loss=2.41, v_num=1, train_loss_step=2.640, val_loss=3.170, train_loss_epoch=1.750]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 14:  91%|███████▎| 61/67 [00:17<00:01,  3.49it/s, loss=2.41, v_num=1, train_loss_step=2.640, val_loss=3.170, train_loss_epoch=1.750]\u001b[A\n",
      "Epoch 14:  93%|███████▍| 62/67 [00:17<00:01,  3.48it/s, loss=2.41, v_num=1, train_loss_step=2.640, val_loss=3.170, train_loss_epoch=1.750]\u001b[A\n",
      "Epoch 14:  94%|███████▌| 63/67 [00:18<00:01,  3.48it/s, loss=2.41, v_num=1, train_loss_step=2.640, val_loss=3.170, train_loss_epoch=1.750]\u001b[A\n",
      "Epoch 14:  96%|███████▋| 64/67 [00:18<00:00,  3.47it/s, loss=2.41, v_num=1, train_loss_step=2.640, val_loss=3.170, train_loss_epoch=1.750]\u001b[A\n",
      "Epoch 14:  97%|███████▊| 65/67 [00:18<00:00,  3.46it/s, loss=2.41, v_num=1, train_loss_step=2.640, val_loss=3.170, train_loss_epoch=1.750]\u001b[A\n",
      "Epoch 14:  99%|███████▉| 66/67 [00:19<00:00,  3.46it/s, loss=2.41, v_num=1, train_loss_step=2.640, val_loss=3.170, train_loss_epoch=1.750]\u001b[A\n",
      "Epoch 14: 100%|████████| 67/67 [00:19<00:00,  3.40it/s, loss=2.41, v_num=1, train_loss_step=2.640, val_loss=3.170, train_loss_epoch=1.750]\u001b[A\n",
      "Epoch 15:  45%|███▌    | 30/67 [00:05<00:06,  5.63it/s, loss=2.04, v_num=1, train_loss_step=0.823, val_loss=3.170, train_loss_epoch=2.440]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 15:  46%|███▋    | 31/67 [00:07<00:08,  4.36it/s, loss=2.04, v_num=1, train_loss_step=0.823, val_loss=3.170, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 15:  48%|███▊    | 32/67 [00:07<00:08,  4.30it/s, loss=2.04, v_num=1, train_loss_step=0.823, val_loss=3.170, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 15:  49%|███▉    | 33/67 [00:08<00:08,  4.09it/s, loss=2.04, v_num=1, train_loss_step=0.823, val_loss=3.170, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 15:  51%|████    | 34/67 [00:08<00:08,  4.05it/s, loss=2.04, v_num=1, train_loss_step=0.823, val_loss=3.170, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 15:  52%|████▏   | 35/67 [00:08<00:07,  4.01it/s, loss=2.04, v_num=1, train_loss_step=0.823, val_loss=3.170, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 15:  54%|████▎   | 36/67 [00:09<00:07,  3.98it/s, loss=2.04, v_num=1, train_loss_step=0.823, val_loss=3.170, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 15:  55%|████▍   | 37/67 [00:09<00:07,  3.94it/s, loss=2.04, v_num=1, train_loss_step=0.823, val_loss=3.170, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 15:  57%|████▌   | 38/67 [00:09<00:07,  3.91it/s, loss=2.04, v_num=1, train_loss_step=0.823, val_loss=3.170, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 15:  58%|████▋   | 39/67 [00:10<00:07,  3.88it/s, loss=2.04, v_num=1, train_loss_step=0.823, val_loss=3.170, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 15:  60%|████▊   | 40/67 [00:10<00:07,  3.85it/s, loss=2.04, v_num=1, train_loss_step=0.823, val_loss=3.170, train_loss_epoch=2.440]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 15:  61%|████▉   | 41/67 [00:10<00:06,  3.79it/s, loss=2.04, v_num=1, train_loss_step=0.823, val_loss=3.170, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 15:  63%|█████   | 42/67 [00:11<00:06,  3.77it/s, loss=2.04, v_num=1, train_loss_step=0.823, val_loss=3.170, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 15:  64%|█████▏  | 43/67 [00:11<00:06,  3.74it/s, loss=2.04, v_num=1, train_loss_step=0.823, val_loss=3.170, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 15:  66%|█████▎  | 44/67 [00:11<00:06,  3.73it/s, loss=2.04, v_num=1, train_loss_step=0.823, val_loss=3.170, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 15:  67%|█████▎  | 45/67 [00:12<00:05,  3.71it/s, loss=2.04, v_num=1, train_loss_step=0.823, val_loss=3.170, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 15:  69%|█████▍  | 46/67 [00:12<00:05,  3.69it/s, loss=2.04, v_num=1, train_loss_step=0.823, val_loss=3.170, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 15:  70%|█████▌  | 47/67 [00:12<00:05,  3.68it/s, loss=2.04, v_num=1, train_loss_step=0.823, val_loss=3.170, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 15:  72%|█████▋  | 48/67 [00:13<00:05,  3.66it/s, loss=2.04, v_num=1, train_loss_step=0.823, val_loss=3.170, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 15:  73%|█████▊  | 49/67 [00:13<00:04,  3.64it/s, loss=2.04, v_num=1, train_loss_step=0.823, val_loss=3.170, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 15:  75%|█████▉  | 50/67 [00:13<00:04,  3.63it/s, loss=2.04, v_num=1, train_loss_step=0.823, val_loss=3.170, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 15:  76%|██████  | 51/67 [00:14<00:04,  3.59it/s, loss=2.04, v_num=1, train_loss_step=0.823, val_loss=3.170, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 15:  78%|██████▏ | 52/67 [00:14<00:04,  3.57it/s, loss=2.04, v_num=1, train_loss_step=0.823, val_loss=3.170, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 15:  79%|██████▎ | 53/67 [00:14<00:03,  3.56it/s, loss=2.04, v_num=1, train_loss_step=0.823, val_loss=3.170, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 15:  81%|██████▍ | 54/67 [00:15<00:03,  3.55it/s, loss=2.04, v_num=1, train_loss_step=0.823, val_loss=3.170, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 15:  82%|██████▌ | 55/67 [00:15<00:03,  3.54it/s, loss=2.04, v_num=1, train_loss_step=0.823, val_loss=3.170, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 15:  84%|██████▋ | 56/67 [00:15<00:03,  3.53it/s, loss=2.04, v_num=1, train_loss_step=0.823, val_loss=3.170, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 15:  85%|██████▊ | 57/67 [00:16<00:02,  3.53it/s, loss=2.04, v_num=1, train_loss_step=0.823, val_loss=3.170, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 15:  87%|██████▉ | 58/67 [00:16<00:02,  3.52it/s, loss=2.04, v_num=1, train_loss_step=0.823, val_loss=3.170, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 15:  88%|███████ | 59/67 [00:16<00:02,  3.51it/s, loss=2.04, v_num=1, train_loss_step=0.823, val_loss=3.170, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 15:  90%|███████▏| 60/67 [00:17<00:02,  3.50it/s, loss=2.04, v_num=1, train_loss_step=0.823, val_loss=3.170, train_loss_epoch=2.440]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 15:  91%|███████▎| 61/67 [00:18<00:01,  3.37it/s, loss=2.04, v_num=1, train_loss_step=0.823, val_loss=3.170, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 15:  93%|███████▍| 62/67 [00:18<00:01,  3.37it/s, loss=2.04, v_num=1, train_loss_step=0.823, val_loss=3.170, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 15:  94%|███████▌| 63/67 [00:18<00:01,  3.37it/s, loss=2.04, v_num=1, train_loss_step=0.823, val_loss=3.170, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 15:  96%|███████▋| 64/67 [00:19<00:00,  3.36it/s, loss=2.04, v_num=1, train_loss_step=0.823, val_loss=3.170, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 15:  97%|███████▊| 65/67 [00:19<00:00,  3.36it/s, loss=2.04, v_num=1, train_loss_step=0.823, val_loss=3.170, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 15:  99%|███████▉| 66/67 [00:19<00:00,  3.35it/s, loss=2.04, v_num=1, train_loss_step=0.823, val_loss=3.170, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 15: 100%|████████| 67/67 [00:20<00:00,  3.30it/s, loss=2.04, v_num=1, train_loss_step=0.823, val_loss=3.170, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 16:  45%|███▌    | 30/67 [00:05<00:06,  5.65it/s, loss=2.05, v_num=1, train_loss_step=2.760, val_loss=3.170, train_loss_epoch=1.930]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 16:  46%|███▋    | 31/67 [00:07<00:08,  4.36it/s, loss=2.05, v_num=1, train_loss_step=2.760, val_loss=3.170, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 16:  48%|███▊    | 32/67 [00:07<00:08,  4.30it/s, loss=2.05, v_num=1, train_loss_step=2.760, val_loss=3.170, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 16:  49%|███▉    | 33/67 [00:07<00:08,  4.25it/s, loss=2.05, v_num=1, train_loss_step=2.760, val_loss=3.170, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 16:  51%|████    | 34/67 [00:08<00:07,  4.19it/s, loss=2.05, v_num=1, train_loss_step=2.760, val_loss=3.170, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 16:  52%|████▏   | 35/67 [00:08<00:07,  4.15it/s, loss=2.05, v_num=1, train_loss_step=2.760, val_loss=3.170, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 16:  54%|████▎   | 36/67 [00:08<00:07,  4.10it/s, loss=2.05, v_num=1, train_loss_step=2.760, val_loss=3.170, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 16:  55%|████▍   | 37/67 [00:09<00:07,  4.06it/s, loss=2.05, v_num=1, train_loss_step=2.760, val_loss=3.170, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 16:  57%|████▌   | 38/67 [00:09<00:07,  4.01it/s, loss=2.05, v_num=1, train_loss_step=2.760, val_loss=3.170, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 16:  58%|████▋   | 39/67 [00:09<00:07,  3.98it/s, loss=2.05, v_num=1, train_loss_step=2.760, val_loss=3.170, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 16:  60%|████▊   | 40/67 [00:10<00:06,  3.94it/s, loss=2.05, v_num=1, train_loss_step=2.760, val_loss=3.170, train_loss_epoch=1.930]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 16:  61%|████▉   | 41/67 [00:10<00:06,  3.88it/s, loss=2.05, v_num=1, train_loss_step=2.760, val_loss=3.170, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 16:  63%|█████   | 42/67 [00:10<00:06,  3.85it/s, loss=2.05, v_num=1, train_loss_step=2.760, val_loss=3.170, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 16:  64%|█████▏  | 43/67 [00:11<00:06,  3.83it/s, loss=2.05, v_num=1, train_loss_step=2.760, val_loss=3.170, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 16:  66%|█████▎  | 44/67 [00:11<00:06,  3.80it/s, loss=2.05, v_num=1, train_loss_step=2.760, val_loss=3.170, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 16:  67%|█████▎  | 45/67 [00:11<00:05,  3.79it/s, loss=2.05, v_num=1, train_loss_step=2.760, val_loss=3.170, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 16:  69%|█████▍  | 46/67 [00:12<00:05,  3.77it/s, loss=2.05, v_num=1, train_loss_step=2.760, val_loss=3.170, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 16:  70%|█████▌  | 47/67 [00:12<00:05,  3.75it/s, loss=2.05, v_num=1, train_loss_step=2.760, val_loss=3.170, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 16:  72%|█████▋  | 48/67 [00:12<00:05,  3.73it/s, loss=2.05, v_num=1, train_loss_step=2.760, val_loss=3.170, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 16:  73%|█████▊  | 49/67 [00:13<00:04,  3.71it/s, loss=2.05, v_num=1, train_loss_step=2.760, val_loss=3.170, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 16:  75%|█████▉  | 50/67 [00:13<00:04,  3.69it/s, loss=2.05, v_num=1, train_loss_step=2.760, val_loss=3.170, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 16:  76%|██████  | 51/67 [00:14<00:04,  3.64it/s, loss=2.05, v_num=1, train_loss_step=2.760, val_loss=3.170, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 16:  78%|██████▏ | 52/67 [00:14<00:04,  3.63it/s, loss=2.05, v_num=1, train_loss_step=2.760, val_loss=3.170, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 16:  79%|██████▎ | 53/67 [00:14<00:03,  3.62it/s, loss=2.05, v_num=1, train_loss_step=2.760, val_loss=3.170, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 16:  81%|██████▍ | 54/67 [00:14<00:03,  3.61it/s, loss=2.05, v_num=1, train_loss_step=2.760, val_loss=3.170, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 16:  82%|██████▌ | 55/67 [00:15<00:03,  3.59it/s, loss=2.05, v_num=1, train_loss_step=2.760, val_loss=3.170, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 16:  84%|██████▋ | 56/67 [00:15<00:03,  3.58it/s, loss=2.05, v_num=1, train_loss_step=2.760, val_loss=3.170, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 16:  85%|██████▊ | 57/67 [00:15<00:02,  3.57it/s, loss=2.05, v_num=1, train_loss_step=2.760, val_loss=3.170, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 16:  87%|██████▉ | 58/67 [00:16<00:02,  3.56it/s, loss=2.05, v_num=1, train_loss_step=2.760, val_loss=3.170, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 16:  88%|███████ | 59/67 [00:16<00:02,  3.55it/s, loss=2.05, v_num=1, train_loss_step=2.760, val_loss=3.170, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 16:  90%|███████▏| 60/67 [00:16<00:01,  3.55it/s, loss=2.05, v_num=1, train_loss_step=2.760, val_loss=3.170, train_loss_epoch=1.930]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 16:  91%|███████▎| 61/67 [00:17<00:01,  3.51it/s, loss=2.05, v_num=1, train_loss_step=2.760, val_loss=3.170, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 16:  93%|███████▍| 62/67 [00:17<00:01,  3.51it/s, loss=2.05, v_num=1, train_loss_step=2.760, val_loss=3.170, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 16:  94%|███████▌| 63/67 [00:17<00:01,  3.50it/s, loss=2.05, v_num=1, train_loss_step=2.760, val_loss=3.170, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 16:  96%|███████▋| 64/67 [00:18<00:00,  3.49it/s, loss=2.05, v_num=1, train_loss_step=2.760, val_loss=3.170, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 16:  97%|███████▊| 65/67 [00:18<00:00,  3.49it/s, loss=2.05, v_num=1, train_loss_step=2.760, val_loss=3.170, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 16:  99%|███████▉| 66/67 [00:18<00:00,  3.48it/s, loss=2.05, v_num=1, train_loss_step=2.760, val_loss=3.170, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 16: 100%|████████| 67/67 [00:19<00:00,  3.42it/s, loss=2.05, v_num=1, train_loss_step=2.760, val_loss=3.170, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 17:  45%|███▌    | 30/67 [00:05<00:06,  5.60it/s, loss=2.35, v_num=1, train_loss_step=0.719, val_loss=3.170, train_loss_epoch=2.000]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 17:  46%|███▋    | 31/67 [00:07<00:08,  4.32it/s, loss=2.35, v_num=1, train_loss_step=0.719, val_loss=3.170, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 17:  48%|███▊    | 32/67 [00:07<00:08,  4.26it/s, loss=2.35, v_num=1, train_loss_step=0.719, val_loss=3.170, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 17:  49%|███▉    | 33/67 [00:07<00:08,  4.22it/s, loss=2.35, v_num=1, train_loss_step=0.719, val_loss=3.170, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 17:  51%|████    | 34/67 [00:08<00:07,  4.16it/s, loss=2.35, v_num=1, train_loss_step=0.719, val_loss=3.170, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 17:  52%|████▏   | 35/67 [00:08<00:07,  4.12it/s, loss=2.35, v_num=1, train_loss_step=0.719, val_loss=3.170, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 17:  54%|████▎   | 36/67 [00:08<00:07,  4.08it/s, loss=2.35, v_num=1, train_loss_step=0.719, val_loss=3.170, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 17:  55%|████▍   | 37/67 [00:09<00:07,  4.04it/s, loss=2.35, v_num=1, train_loss_step=0.719, val_loss=3.170, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 17:  57%|████▌   | 38/67 [00:09<00:07,  4.01it/s, loss=2.35, v_num=1, train_loss_step=0.719, val_loss=3.170, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 17:  58%|████▋   | 39/67 [00:09<00:07,  3.98it/s, loss=2.35, v_num=1, train_loss_step=0.719, val_loss=3.170, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 17:  60%|████▊   | 40/67 [00:10<00:06,  3.95it/s, loss=2.35, v_num=1, train_loss_step=0.719, val_loss=3.170, train_loss_epoch=2.000]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 17:  61%|████▉   | 41/67 [00:10<00:06,  3.88it/s, loss=2.35, v_num=1, train_loss_step=0.719, val_loss=3.170, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 17:  63%|█████   | 42/67 [00:10<00:06,  3.86it/s, loss=2.35, v_num=1, train_loss_step=0.719, val_loss=3.170, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 17:  64%|█████▏  | 43/67 [00:11<00:06,  3.83it/s, loss=2.35, v_num=1, train_loss_step=0.719, val_loss=3.170, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 17:  66%|█████▎  | 44/67 [00:11<00:06,  3.81it/s, loss=2.35, v_num=1, train_loss_step=0.719, val_loss=3.170, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 17:  67%|█████▎  | 45/67 [00:11<00:05,  3.79it/s, loss=2.35, v_num=1, train_loss_step=0.719, val_loss=3.170, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 17:  69%|█████▍  | 46/67 [00:12<00:05,  3.77it/s, loss=2.35, v_num=1, train_loss_step=0.719, val_loss=3.170, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 17:  70%|█████▌  | 47/67 [00:12<00:05,  3.75it/s, loss=2.35, v_num=1, train_loss_step=0.719, val_loss=3.170, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 17:  72%|█████▋  | 48/67 [00:12<00:05,  3.74it/s, loss=2.35, v_num=1, train_loss_step=0.719, val_loss=3.170, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 17:  73%|█████▊  | 49/67 [00:13<00:04,  3.72it/s, loss=2.35, v_num=1, train_loss_step=0.719, val_loss=3.170, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 17:  75%|█████▉  | 50/67 [00:13<00:04,  3.71it/s, loss=2.35, v_num=1, train_loss_step=0.719, val_loss=3.170, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 17:  76%|██████  | 51/67 [00:13<00:04,  3.67it/s, loss=2.35, v_num=1, train_loss_step=0.719, val_loss=3.170, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 17:  78%|██████▏ | 52/67 [00:14<00:04,  3.65it/s, loss=2.35, v_num=1, train_loss_step=0.719, val_loss=3.170, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 17:  79%|██████▎ | 53/67 [00:14<00:03,  3.64it/s, loss=2.35, v_num=1, train_loss_step=0.719, val_loss=3.170, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 17:  81%|██████▍ | 54/67 [00:14<00:03,  3.63it/s, loss=2.35, v_num=1, train_loss_step=0.719, val_loss=3.170, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 17:  82%|██████▌ | 55/67 [00:15<00:03,  3.61it/s, loss=2.35, v_num=1, train_loss_step=0.719, val_loss=3.170, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 17:  84%|██████▋ | 56/67 [00:15<00:03,  3.60it/s, loss=2.35, v_num=1, train_loss_step=0.719, val_loss=3.170, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 17:  85%|██████▊ | 57/67 [00:15<00:02,  3.59it/s, loss=2.35, v_num=1, train_loss_step=0.719, val_loss=3.170, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 17:  87%|██████▉ | 58/67 [00:16<00:02,  3.58it/s, loss=2.35, v_num=1, train_loss_step=0.719, val_loss=3.170, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 17:  88%|███████ | 59/67 [00:16<00:02,  3.57it/s, loss=2.35, v_num=1, train_loss_step=0.719, val_loss=3.170, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 17:  90%|███████▏| 60/67 [00:16<00:01,  3.56it/s, loss=2.35, v_num=1, train_loss_step=0.719, val_loss=3.170, train_loss_epoch=2.000]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 17:  91%|███████▎| 61/67 [00:17<00:01,  3.53it/s, loss=2.35, v_num=1, train_loss_step=0.719, val_loss=3.170, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 17:  93%|███████▍| 62/67 [00:17<00:01,  3.52it/s, loss=2.35, v_num=1, train_loss_step=0.719, val_loss=3.170, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 17:  94%|███████▌| 63/67 [00:17<00:01,  3.52it/s, loss=2.35, v_num=1, train_loss_step=0.719, val_loss=3.170, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 17:  96%|███████▋| 64/67 [00:18<00:00,  3.51it/s, loss=2.35, v_num=1, train_loss_step=0.719, val_loss=3.170, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 17:  97%|███████▊| 65/67 [00:18<00:00,  3.50it/s, loss=2.35, v_num=1, train_loss_step=0.719, val_loss=3.170, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 17:  99%|███████▉| 66/67 [00:18<00:00,  3.50it/s, loss=2.35, v_num=1, train_loss_step=0.719, val_loss=3.170, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 17: 100%|████████| 67/67 [00:19<00:00,  3.43it/s, loss=2.35, v_num=1, train_loss_step=0.719, val_loss=3.170, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 18:  45%|███▌    | 30/67 [00:05<00:06,  5.53it/s, loss=2.29, v_num=1, train_loss_step=3.610, val_loss=3.170, train_loss_epoch=2.200]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 18:  46%|███▋    | 31/67 [00:07<00:08,  4.23it/s, loss=2.29, v_num=1, train_loss_step=3.610, val_loss=3.170, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 18:  48%|███▊    | 32/67 [00:07<00:08,  4.18it/s, loss=2.29, v_num=1, train_loss_step=3.610, val_loss=3.170, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 18:  49%|███▉    | 33/67 [00:07<00:08,  4.13it/s, loss=2.29, v_num=1, train_loss_step=3.610, val_loss=3.170, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 18:  51%|████    | 34/67 [00:08<00:08,  4.08it/s, loss=2.29, v_num=1, train_loss_step=3.610, val_loss=3.170, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 18:  52%|████▏   | 35/67 [00:08<00:07,  4.04it/s, loss=2.29, v_num=1, train_loss_step=3.610, val_loss=3.170, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 18:  54%|████▎   | 36/67 [00:09<00:07,  4.00it/s, loss=2.29, v_num=1, train_loss_step=3.610, val_loss=3.170, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 18:  55%|████▍   | 37/67 [00:09<00:07,  3.96it/s, loss=2.29, v_num=1, train_loss_step=3.610, val_loss=3.170, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 18:  57%|████▌   | 38/67 [00:09<00:07,  3.93it/s, loss=2.29, v_num=1, train_loss_step=3.610, val_loss=3.170, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 18:  58%|████▋   | 39/67 [00:09<00:07,  3.91it/s, loss=2.29, v_num=1, train_loss_step=3.610, val_loss=3.170, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 18:  60%|████▊   | 40/67 [00:10<00:06,  3.88it/s, loss=2.29, v_num=1, train_loss_step=3.610, val_loss=3.170, train_loss_epoch=2.200]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 18:  61%|████▉   | 41/67 [00:10<00:06,  3.81it/s, loss=2.29, v_num=1, train_loss_step=3.610, val_loss=3.170, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 18:  63%|█████   | 42/67 [00:11<00:06,  3.79it/s, loss=2.29, v_num=1, train_loss_step=3.610, val_loss=3.170, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 18:  64%|█████▏  | 43/67 [00:11<00:06,  3.76it/s, loss=2.29, v_num=1, train_loss_step=3.610, val_loss=3.170, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 18:  66%|█████▎  | 44/67 [00:11<00:06,  3.74it/s, loss=2.29, v_num=1, train_loss_step=3.610, val_loss=3.170, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 18:  67%|█████▎  | 45/67 [00:12<00:05,  3.72it/s, loss=2.29, v_num=1, train_loss_step=3.610, val_loss=3.170, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 18:  69%|█████▍  | 46/67 [00:12<00:05,  3.70it/s, loss=2.29, v_num=1, train_loss_step=3.610, val_loss=3.170, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 18:  70%|█████▌  | 47/67 [00:12<00:05,  3.68it/s, loss=2.29, v_num=1, train_loss_step=3.610, val_loss=3.170, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 18:  72%|█████▋  | 48/67 [00:13<00:05,  3.67it/s, loss=2.29, v_num=1, train_loss_step=3.610, val_loss=3.170, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 18:  73%|█████▊  | 49/67 [00:13<00:04,  3.65it/s, loss=2.29, v_num=1, train_loss_step=3.610, val_loss=3.170, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 18:  75%|█████▉  | 50/67 [00:13<00:04,  3.63it/s, loss=2.29, v_num=1, train_loss_step=3.610, val_loss=3.170, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 18:  76%|██████  | 51/67 [00:14<00:04,  3.59it/s, loss=2.29, v_num=1, train_loss_step=3.610, val_loss=3.170, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 18:  78%|██████▏ | 52/67 [00:14<00:04,  3.57it/s, loss=2.29, v_num=1, train_loss_step=3.610, val_loss=3.170, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 18:  79%|██████▎ | 53/67 [00:14<00:03,  3.56it/s, loss=2.29, v_num=1, train_loss_step=3.610, val_loss=3.170, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 18:  81%|██████▍ | 54/67 [00:15<00:03,  3.55it/s, loss=2.29, v_num=1, train_loss_step=3.610, val_loss=3.170, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 18:  82%|██████▌ | 55/67 [00:15<00:03,  3.54it/s, loss=2.29, v_num=1, train_loss_step=3.610, val_loss=3.170, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 18:  84%|██████▋ | 56/67 [00:15<00:03,  3.53it/s, loss=2.29, v_num=1, train_loss_step=3.610, val_loss=3.170, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 18:  85%|██████▊ | 57/67 [00:16<00:02,  3.52it/s, loss=2.29, v_num=1, train_loss_step=3.610, val_loss=3.170, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 18:  87%|██████▉ | 58/67 [00:16<00:02,  3.51it/s, loss=2.29, v_num=1, train_loss_step=3.610, val_loss=3.170, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 18:  88%|███████ | 59/67 [00:16<00:02,  3.50it/s, loss=2.29, v_num=1, train_loss_step=3.610, val_loss=3.170, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 18:  90%|███████▏| 60/67 [00:17<00:02,  3.49it/s, loss=2.29, v_num=1, train_loss_step=3.610, val_loss=3.170, train_loss_epoch=2.200]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 18:  91%|███████▎| 61/67 [00:17<00:01,  3.46it/s, loss=2.29, v_num=1, train_loss_step=3.610, val_loss=3.170, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 18:  93%|███████▍| 62/67 [00:17<00:01,  3.45it/s, loss=2.29, v_num=1, train_loss_step=3.610, val_loss=3.170, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 18:  94%|███████▌| 63/67 [00:18<00:01,  3.45it/s, loss=2.29, v_num=1, train_loss_step=3.610, val_loss=3.170, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 18:  96%|███████▋| 64/67 [00:18<00:00,  3.44it/s, loss=2.29, v_num=1, train_loss_step=3.610, val_loss=3.170, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 18:  97%|███████▊| 65/67 [00:18<00:00,  3.43it/s, loss=2.29, v_num=1, train_loss_step=3.610, val_loss=3.170, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 18:  99%|███████▉| 66/67 [00:19<00:00,  3.43it/s, loss=2.29, v_num=1, train_loss_step=3.610, val_loss=3.170, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 18: 100%|████████| 67/67 [00:19<00:00,  3.37it/s, loss=2.29, v_num=1, train_loss_step=3.610, val_loss=3.170, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 19:  45%|███▌    | 30/67 [00:05<00:06,  5.56it/s, loss=2.07, v_num=1, train_loss_step=1.740, val_loss=3.170, train_loss_epoch=2.230]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 19:  46%|███▋    | 31/67 [00:07<00:08,  4.28it/s, loss=2.07, v_num=1, train_loss_step=1.740, val_loss=3.170, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 19:  48%|███▊    | 32/67 [00:07<00:08,  4.23it/s, loss=2.07, v_num=1, train_loss_step=1.740, val_loss=3.170, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 19:  49%|███▉    | 33/67 [00:07<00:08,  4.18it/s, loss=2.07, v_num=1, train_loss_step=1.740, val_loss=3.170, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 19:  51%|████    | 34/67 [00:08<00:07,  4.13it/s, loss=2.07, v_num=1, train_loss_step=1.740, val_loss=3.170, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 19:  52%|████▏   | 35/67 [00:08<00:07,  4.09it/s, loss=2.07, v_num=1, train_loss_step=1.740, val_loss=3.170, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 19:  54%|████▎   | 36/67 [00:08<00:07,  4.06it/s, loss=2.07, v_num=1, train_loss_step=1.740, val_loss=3.170, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 19:  55%|████▍   | 37/67 [00:09<00:07,  4.01it/s, loss=2.07, v_num=1, train_loss_step=1.740, val_loss=3.170, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 19:  57%|████▌   | 38/67 [00:09<00:07,  3.98it/s, loss=2.07, v_num=1, train_loss_step=1.740, val_loss=3.170, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 19:  58%|████▋   | 39/67 [00:09<00:07,  3.96it/s, loss=2.07, v_num=1, train_loss_step=1.740, val_loss=3.170, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 19:  60%|████▊   | 40/67 [00:10<00:06,  3.92it/s, loss=2.07, v_num=1, train_loss_step=1.740, val_loss=3.170, train_loss_epoch=2.230]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 19:  61%|████▉   | 41/67 [00:10<00:06,  3.85it/s, loss=2.07, v_num=1, train_loss_step=1.740, val_loss=3.170, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 19:  63%|█████   | 42/67 [00:10<00:06,  3.83it/s, loss=2.07, v_num=1, train_loss_step=1.740, val_loss=3.170, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 19:  64%|█████▏  | 43/67 [00:11<00:06,  3.81it/s, loss=2.07, v_num=1, train_loss_step=1.740, val_loss=3.170, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 19:  66%|█████▎  | 44/67 [00:11<00:06,  3.79it/s, loss=2.07, v_num=1, train_loss_step=1.740, val_loss=3.170, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 19:  67%|█████▎  | 45/67 [00:11<00:05,  3.77it/s, loss=2.07, v_num=1, train_loss_step=1.740, val_loss=3.170, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 19:  69%|█████▍  | 46/67 [00:12<00:05,  3.75it/s, loss=2.07, v_num=1, train_loss_step=1.740, val_loss=3.170, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 19:  70%|█████▌  | 47/67 [00:12<00:05,  3.72it/s, loss=2.07, v_num=1, train_loss_step=1.740, val_loss=3.170, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 19:  72%|█████▋  | 48/67 [00:12<00:05,  3.71it/s, loss=2.07, v_num=1, train_loss_step=1.740, val_loss=3.170, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 19:  73%|█████▊  | 49/67 [00:13<00:04,  3.69it/s, loss=2.07, v_num=1, train_loss_step=1.740, val_loss=3.170, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 19:  75%|█████▉  | 50/67 [00:13<00:04,  3.67it/s, loss=2.07, v_num=1, train_loss_step=1.740, val_loss=3.170, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 19:  76%|██████  | 51/67 [00:14<00:04,  3.63it/s, loss=2.07, v_num=1, train_loss_step=1.740, val_loss=3.170, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 19:  78%|██████▏ | 52/67 [00:14<00:04,  3.61it/s, loss=2.07, v_num=1, train_loss_step=1.740, val_loss=3.170, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 19:  79%|██████▎ | 53/67 [00:14<00:03,  3.60it/s, loss=2.07, v_num=1, train_loss_step=1.740, val_loss=3.170, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 19:  81%|██████▍ | 54/67 [00:15<00:03,  3.59it/s, loss=2.07, v_num=1, train_loss_step=1.740, val_loss=3.170, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 19:  82%|██████▌ | 55/67 [00:15<00:03,  3.57it/s, loss=2.07, v_num=1, train_loss_step=1.740, val_loss=3.170, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 19:  84%|██████▋ | 56/67 [00:16<00:03,  3.47it/s, loss=2.07, v_num=1, train_loss_step=1.740, val_loss=3.170, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 19:  85%|██████▊ | 57/67 [00:16<00:02,  3.47it/s, loss=2.07, v_num=1, train_loss_step=1.740, val_loss=3.170, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 19:  87%|██████▉ | 58/67 [00:16<00:02,  3.46it/s, loss=2.07, v_num=1, train_loss_step=1.740, val_loss=3.170, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 19:  88%|███████ | 59/67 [00:17<00:02,  3.45it/s, loss=2.07, v_num=1, train_loss_step=1.740, val_loss=3.170, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 19:  90%|███████▏| 60/67 [00:17<00:02,  3.44it/s, loss=2.07, v_num=1, train_loss_step=1.740, val_loss=3.170, train_loss_epoch=2.230]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 19:  91%|███████▎| 61/67 [00:17<00:01,  3.41it/s, loss=2.07, v_num=1, train_loss_step=1.740, val_loss=3.170, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 19:  93%|███████▍| 62/67 [00:18<00:01,  3.41it/s, loss=2.07, v_num=1, train_loss_step=1.740, val_loss=3.170, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 19:  94%|███████▌| 63/67 [00:18<00:01,  3.40it/s, loss=2.07, v_num=1, train_loss_step=1.740, val_loss=3.170, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 19:  96%|███████▋| 64/67 [00:18<00:00,  3.40it/s, loss=2.07, v_num=1, train_loss_step=1.740, val_loss=3.170, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 19:  97%|███████▊| 65/67 [00:19<00:00,  3.39it/s, loss=2.07, v_num=1, train_loss_step=1.740, val_loss=3.170, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 19:  99%|███████▉| 66/67 [00:19<00:00,  3.39it/s, loss=2.07, v_num=1, train_loss_step=1.740, val_loss=3.170, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 19: 100%|████████| 67/67 [00:20<00:00,  3.33it/s, loss=2.07, v_num=1, train_loss_step=1.740, val_loss=3.170, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 20:  45%|███▌    | 30/67 [00:05<00:06,  5.55it/s, loss=2.08, v_num=1, train_loss_step=2.620, val_loss=3.170, train_loss_epoch=2.140]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 20:  46%|███▋    | 31/67 [00:07<00:08,  4.25it/s, loss=2.08, v_num=1, train_loss_step=2.620, val_loss=3.170, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 20:  48%|███▊    | 32/67 [00:07<00:08,  4.20it/s, loss=2.08, v_num=1, train_loss_step=2.620, val_loss=3.170, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 20:  49%|███▉    | 33/67 [00:07<00:08,  4.15it/s, loss=2.08, v_num=1, train_loss_step=2.620, val_loss=3.170, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 20:  51%|████    | 34/67 [00:08<00:08,  4.10it/s, loss=2.08, v_num=1, train_loss_step=2.620, val_loss=3.170, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 20:  52%|████▏   | 35/67 [00:08<00:07,  4.06it/s, loss=2.08, v_num=1, train_loss_step=2.620, val_loss=3.170, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 20:  54%|████▎   | 36/67 [00:08<00:07,  4.03it/s, loss=2.08, v_num=1, train_loss_step=2.620, val_loss=3.170, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 20:  55%|████▍   | 37/67 [00:09<00:07,  3.98it/s, loss=2.08, v_num=1, train_loss_step=2.620, val_loss=3.170, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 20:  57%|████▌   | 38/67 [00:09<00:07,  3.95it/s, loss=2.08, v_num=1, train_loss_step=2.620, val_loss=3.170, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 20:  58%|████▋   | 39/67 [00:09<00:07,  3.92it/s, loss=2.08, v_num=1, train_loss_step=2.620, val_loss=3.170, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 20:  60%|████▊   | 40/67 [00:10<00:06,  3.89it/s, loss=2.08, v_num=1, train_loss_step=2.620, val_loss=3.170, train_loss_epoch=2.140]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 20:  61%|████▉   | 41/67 [00:10<00:06,  3.83it/s, loss=2.08, v_num=1, train_loss_step=2.620, val_loss=3.170, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 20:  63%|█████   | 42/67 [00:11<00:06,  3.81it/s, loss=2.08, v_num=1, train_loss_step=2.620, val_loss=3.170, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 20:  64%|█████▏  | 43/67 [00:11<00:06,  3.78it/s, loss=2.08, v_num=1, train_loss_step=2.620, val_loss=3.170, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 20:  66%|█████▎  | 44/67 [00:11<00:06,  3.76it/s, loss=2.08, v_num=1, train_loss_step=2.620, val_loss=3.170, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 20:  67%|█████▎  | 45/67 [00:12<00:05,  3.74it/s, loss=2.08, v_num=1, train_loss_step=2.620, val_loss=3.170, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 20:  69%|█████▍  | 46/67 [00:12<00:05,  3.72it/s, loss=2.08, v_num=1, train_loss_step=2.620, val_loss=3.170, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 20:  70%|█████▌  | 47/67 [00:12<00:05,  3.70it/s, loss=2.08, v_num=1, train_loss_step=2.620, val_loss=3.170, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 20:  72%|█████▋  | 48/67 [00:13<00:05,  3.59it/s, loss=2.08, v_num=1, train_loss_step=2.620, val_loss=3.170, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 20:  73%|█████▊  | 49/67 [00:13<00:05,  3.57it/s, loss=2.08, v_num=1, train_loss_step=2.620, val_loss=3.170, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 20:  75%|█████▉  | 50/67 [00:14<00:04,  3.56it/s, loss=2.08, v_num=1, train_loss_step=2.620, val_loss=3.170, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 20:  76%|██████  | 51/67 [00:14<00:04,  3.52it/s, loss=2.08, v_num=1, train_loss_step=2.620, val_loss=3.170, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 20:  78%|██████▏ | 52/67 [00:14<00:04,  3.51it/s, loss=2.08, v_num=1, train_loss_step=2.620, val_loss=3.170, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 20:  79%|██████▎ | 53/67 [00:15<00:04,  3.50it/s, loss=2.08, v_num=1, train_loss_step=2.620, val_loss=3.170, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 20:  81%|██████▍ | 54/67 [00:15<00:03,  3.49it/s, loss=2.08, v_num=1, train_loss_step=2.620, val_loss=3.170, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 20:  82%|██████▌ | 55/67 [00:15<00:03,  3.48it/s, loss=2.08, v_num=1, train_loss_step=2.620, val_loss=3.170, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 20:  84%|██████▋ | 56/67 [00:16<00:03,  3.47it/s, loss=2.08, v_num=1, train_loss_step=2.620, val_loss=3.170, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 20:  85%|██████▊ | 57/67 [00:16<00:02,  3.46it/s, loss=2.08, v_num=1, train_loss_step=2.620, val_loss=3.170, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 20:  87%|██████▉ | 58/67 [00:16<00:02,  3.45it/s, loss=2.08, v_num=1, train_loss_step=2.620, val_loss=3.170, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 20:  88%|███████ | 59/67 [00:17<00:02,  3.45it/s, loss=2.08, v_num=1, train_loss_step=2.620, val_loss=3.170, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 20:  90%|███████▏| 60/67 [00:17<00:02,  3.44it/s, loss=2.08, v_num=1, train_loss_step=2.620, val_loss=3.170, train_loss_epoch=2.140]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 20:  91%|███████▎| 61/67 [00:17<00:01,  3.41it/s, loss=2.08, v_num=1, train_loss_step=2.620, val_loss=3.170, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 20:  93%|███████▍| 62/67 [00:18<00:01,  3.40it/s, loss=2.08, v_num=1, train_loss_step=2.620, val_loss=3.170, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 20:  94%|███████▌| 63/67 [00:18<00:01,  3.39it/s, loss=2.08, v_num=1, train_loss_step=2.620, val_loss=3.170, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 20:  96%|███████▋| 64/67 [00:18<00:00,  3.39it/s, loss=2.08, v_num=1, train_loss_step=2.620, val_loss=3.170, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 20:  97%|███████▊| 65/67 [00:19<00:00,  3.38it/s, loss=2.08, v_num=1, train_loss_step=2.620, val_loss=3.170, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 20:  99%|███████▉| 66/67 [00:19<00:00,  3.38it/s, loss=2.08, v_num=1, train_loss_step=2.620, val_loss=3.170, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 20: 100%|████████| 67/67 [00:20<00:00,  3.32it/s, loss=2.08, v_num=1, train_loss_step=2.620, val_loss=3.170, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 21:  45%|███▌    | 30/67 [00:05<00:06,  5.46it/s, loss=2.58, v_num=1, train_loss_step=2.020, val_loss=3.170, train_loss_epoch=2.030]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 21:  46%|███▋    | 31/67 [00:07<00:08,  4.19it/s, loss=2.58, v_num=1, train_loss_step=2.020, val_loss=3.170, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 21:  48%|███▊    | 32/67 [00:07<00:08,  4.14it/s, loss=2.58, v_num=1, train_loss_step=2.020, val_loss=3.170, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 21:  49%|███▉    | 33/67 [00:08<00:08,  4.09it/s, loss=2.58, v_num=1, train_loss_step=2.020, val_loss=3.170, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 21:  51%|████    | 34/67 [00:08<00:08,  4.05it/s, loss=2.58, v_num=1, train_loss_step=2.020, val_loss=3.170, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 21:  52%|████▏   | 35/67 [00:08<00:07,  4.01it/s, loss=2.58, v_num=1, train_loss_step=2.020, val_loss=3.170, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 21:  54%|████▎   | 36/67 [00:09<00:07,  3.98it/s, loss=2.58, v_num=1, train_loss_step=2.020, val_loss=3.170, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 21:  55%|████▍   | 37/67 [00:09<00:07,  3.94it/s, loss=2.58, v_num=1, train_loss_step=2.020, val_loss=3.170, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 21:  57%|████▌   | 38/67 [00:09<00:07,  3.92it/s, loss=2.58, v_num=1, train_loss_step=2.020, val_loss=3.170, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 21:  58%|████▋   | 39/67 [00:10<00:07,  3.89it/s, loss=2.58, v_num=1, train_loss_step=2.020, val_loss=3.170, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 21:  60%|████▊   | 40/67 [00:10<00:07,  3.70it/s, loss=2.58, v_num=1, train_loss_step=2.020, val_loss=3.170, train_loss_epoch=2.030]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 21:  61%|████▉   | 41/67 [00:11<00:07,  3.64it/s, loss=2.58, v_num=1, train_loss_step=2.020, val_loss=3.170, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 21:  63%|█████   | 42/67 [00:11<00:06,  3.63it/s, loss=2.58, v_num=1, train_loss_step=2.020, val_loss=3.170, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 21:  64%|█████▏  | 43/67 [00:11<00:06,  3.61it/s, loss=2.58, v_num=1, train_loss_step=2.020, val_loss=3.170, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 21:  66%|█████▎  | 44/67 [00:12<00:06,  3.59it/s, loss=2.58, v_num=1, train_loss_step=2.020, val_loss=3.170, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 21:  67%|█████▎  | 45/67 [00:12<00:06,  3.58it/s, loss=2.58, v_num=1, train_loss_step=2.020, val_loss=3.170, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 21:  69%|█████▍  | 46/67 [00:12<00:05,  3.56it/s, loss=2.58, v_num=1, train_loss_step=2.020, val_loss=3.170, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 21:  70%|█████▌  | 47/67 [00:13<00:05,  3.55it/s, loss=2.58, v_num=1, train_loss_step=2.020, val_loss=3.170, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 21:  72%|█████▋  | 48/67 [00:13<00:05,  3.54it/s, loss=2.58, v_num=1, train_loss_step=2.020, val_loss=3.170, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 21:  73%|█████▊  | 49/67 [00:13<00:05,  3.53it/s, loss=2.58, v_num=1, train_loss_step=2.020, val_loss=3.170, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 21:  75%|█████▉  | 50/67 [00:14<00:04,  3.52it/s, loss=2.58, v_num=1, train_loss_step=2.020, val_loss=3.170, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 21:  76%|██████  | 51/67 [00:14<00:04,  3.48it/s, loss=2.58, v_num=1, train_loss_step=2.020, val_loss=3.170, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 21:  78%|██████▏ | 52/67 [00:14<00:04,  3.47it/s, loss=2.58, v_num=1, train_loss_step=2.020, val_loss=3.170, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 21:  79%|██████▎ | 53/67 [00:15<00:04,  3.46it/s, loss=2.58, v_num=1, train_loss_step=2.020, val_loss=3.170, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 21:  81%|██████▍ | 54/67 [00:15<00:03,  3.46it/s, loss=2.58, v_num=1, train_loss_step=2.020, val_loss=3.170, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 21:  82%|██████▌ | 55/67 [00:15<00:03,  3.45it/s, loss=2.58, v_num=1, train_loss_step=2.020, val_loss=3.170, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 21:  84%|██████▋ | 56/67 [00:16<00:03,  3.44it/s, loss=2.58, v_num=1, train_loss_step=2.020, val_loss=3.170, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 21:  85%|██████▊ | 57/67 [00:16<00:02,  3.44it/s, loss=2.58, v_num=1, train_loss_step=2.020, val_loss=3.170, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 21:  87%|██████▉ | 58/67 [00:16<00:02,  3.43it/s, loss=2.58, v_num=1, train_loss_step=2.020, val_loss=3.170, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 21:  88%|███████ | 59/67 [00:17<00:02,  3.42it/s, loss=2.58, v_num=1, train_loss_step=2.020, val_loss=3.170, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 21:  90%|███████▏| 60/67 [00:17<00:02,  3.42it/s, loss=2.58, v_num=1, train_loss_step=2.020, val_loss=3.170, train_loss_epoch=2.030]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 21:  91%|███████▎| 61/67 [00:17<00:01,  3.39it/s, loss=2.58, v_num=1, train_loss_step=2.020, val_loss=3.170, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 21:  93%|███████▍| 62/67 [00:18<00:01,  3.38it/s, loss=2.58, v_num=1, train_loss_step=2.020, val_loss=3.170, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 21:  94%|███████▌| 63/67 [00:18<00:01,  3.38it/s, loss=2.58, v_num=1, train_loss_step=2.020, val_loss=3.170, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 21:  96%|███████▋| 64/67 [00:18<00:00,  3.37it/s, loss=2.58, v_num=1, train_loss_step=2.020, val_loss=3.170, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 21:  97%|███████▊| 65/67 [00:19<00:00,  3.37it/s, loss=2.58, v_num=1, train_loss_step=2.020, val_loss=3.170, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 21:  99%|███████▉| 66/67 [00:19<00:00,  3.36it/s, loss=2.58, v_num=1, train_loss_step=2.020, val_loss=3.170, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 21: 100%|████████| 67/67 [00:20<00:00,  3.31it/s, loss=2.58, v_num=1, train_loss_step=2.020, val_loss=3.170, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 22:  45%|████▉      | 30/67 [00:05<00:06,  5.42it/s, loss=2, v_num=1, train_loss_step=3.430, val_loss=3.170, train_loss_epoch=2.630]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 22:  46%|█████      | 31/67 [00:07<00:08,  4.17it/s, loss=2, v_num=1, train_loss_step=3.430, val_loss=3.170, train_loss_epoch=2.630]\u001b[A\n",
      "Epoch 22:  48%|█████▎     | 32/67 [00:07<00:08,  4.12it/s, loss=2, v_num=1, train_loss_step=3.430, val_loss=3.170, train_loss_epoch=2.630]\u001b[A\n",
      "Epoch 22:  49%|█████▍     | 33/67 [00:08<00:08,  3.88it/s, loss=2, v_num=1, train_loss_step=3.430, val_loss=3.170, train_loss_epoch=2.630]\u001b[A\n",
      "Epoch 22:  51%|█████▌     | 34/67 [00:08<00:08,  3.84it/s, loss=2, v_num=1, train_loss_step=3.430, val_loss=3.170, train_loss_epoch=2.630]\u001b[A\n",
      "Epoch 22:  52%|█████▋     | 35/67 [00:09<00:08,  3.81it/s, loss=2, v_num=1, train_loss_step=3.430, val_loss=3.170, train_loss_epoch=2.630]\u001b[A\n",
      "Epoch 22:  54%|█████▉     | 36/67 [00:09<00:08,  3.79it/s, loss=2, v_num=1, train_loss_step=3.430, val_loss=3.170, train_loss_epoch=2.630]\u001b[A\n",
      "Epoch 22:  55%|██████     | 37/67 [00:09<00:07,  3.76it/s, loss=2, v_num=1, train_loss_step=3.430, val_loss=3.170, train_loss_epoch=2.630]\u001b[A\n",
      "Epoch 22:  57%|██████▏    | 38/67 [00:10<00:07,  3.74it/s, loss=2, v_num=1, train_loss_step=3.430, val_loss=3.170, train_loss_epoch=2.630]\u001b[A\n",
      "Epoch 22:  58%|██████▍    | 39/67 [00:10<00:07,  3.72it/s, loss=2, v_num=1, train_loss_step=3.430, val_loss=3.170, train_loss_epoch=2.630]\u001b[A\n",
      "Epoch 22:  60%|██████▌    | 40/67 [00:10<00:07,  3.70it/s, loss=2, v_num=1, train_loss_step=3.430, val_loss=3.170, train_loss_epoch=2.630]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 22:  61%|██████▋    | 41/67 [00:11<00:07,  3.65it/s, loss=2, v_num=1, train_loss_step=3.430, val_loss=3.170, train_loss_epoch=2.630]\u001b[A\n",
      "Epoch 22:  63%|██████▉    | 42/67 [00:11<00:06,  3.63it/s, loss=2, v_num=1, train_loss_step=3.430, val_loss=3.170, train_loss_epoch=2.630]\u001b[A\n",
      "Epoch 22:  64%|███████    | 43/67 [00:11<00:06,  3.62it/s, loss=2, v_num=1, train_loss_step=3.430, val_loss=3.170, train_loss_epoch=2.630]\u001b[A\n",
      "Epoch 22:  66%|███████▏   | 44/67 [00:12<00:06,  3.60it/s, loss=2, v_num=1, train_loss_step=3.430, val_loss=3.170, train_loss_epoch=2.630]\u001b[A\n",
      "Epoch 22:  67%|███████▍   | 45/67 [00:12<00:06,  3.59it/s, loss=2, v_num=1, train_loss_step=3.430, val_loss=3.170, train_loss_epoch=2.630]\u001b[A\n",
      "Epoch 22:  69%|███████▌   | 46/67 [00:12<00:05,  3.58it/s, loss=2, v_num=1, train_loss_step=3.430, val_loss=3.170, train_loss_epoch=2.630]\u001b[A\n",
      "Epoch 22:  70%|███████▋   | 47/67 [00:13<00:05,  3.56it/s, loss=2, v_num=1, train_loss_step=3.430, val_loss=3.170, train_loss_epoch=2.630]\u001b[A\n",
      "Epoch 22:  72%|███████▉   | 48/67 [00:13<00:05,  3.55it/s, loss=2, v_num=1, train_loss_step=3.430, val_loss=3.170, train_loss_epoch=2.630]\u001b[A\n",
      "Epoch 22:  73%|████████   | 49/67 [00:13<00:05,  3.54it/s, loss=2, v_num=1, train_loss_step=3.430, val_loss=3.170, train_loss_epoch=2.630]\u001b[A\n",
      "Epoch 22:  75%|████████▏  | 50/67 [00:14<00:04,  3.53it/s, loss=2, v_num=1, train_loss_step=3.430, val_loss=3.170, train_loss_epoch=2.630]\u001b[A\n",
      "Epoch 22:  76%|████████▎  | 51/67 [00:14<00:04,  3.49it/s, loss=2, v_num=1, train_loss_step=3.430, val_loss=3.170, train_loss_epoch=2.630]\u001b[A\n",
      "Epoch 22:  78%|████████▌  | 52/67 [00:14<00:04,  3.48it/s, loss=2, v_num=1, train_loss_step=3.430, val_loss=3.170, train_loss_epoch=2.630]\u001b[A\n",
      "Epoch 22:  79%|████████▋  | 53/67 [00:15<00:04,  3.47it/s, loss=2, v_num=1, train_loss_step=3.430, val_loss=3.170, train_loss_epoch=2.630]\u001b[A\n",
      "Epoch 22:  81%|████████▊  | 54/67 [00:15<00:03,  3.46it/s, loss=2, v_num=1, train_loss_step=3.430, val_loss=3.170, train_loss_epoch=2.630]\u001b[A\n",
      "Epoch 22:  82%|█████████  | 55/67 [00:15<00:03,  3.45it/s, loss=2, v_num=1, train_loss_step=3.430, val_loss=3.170, train_loss_epoch=2.630]\u001b[A\n",
      "Epoch 22:  84%|█████████▏ | 56/67 [00:16<00:03,  3.44it/s, loss=2, v_num=1, train_loss_step=3.430, val_loss=3.170, train_loss_epoch=2.630]\u001b[A\n",
      "Epoch 22:  85%|█████████▎ | 57/67 [00:16<00:02,  3.44it/s, loss=2, v_num=1, train_loss_step=3.430, val_loss=3.170, train_loss_epoch=2.630]\u001b[A\n",
      "Epoch 22:  87%|█████████▌ | 58/67 [00:16<00:02,  3.43it/s, loss=2, v_num=1, train_loss_step=3.430, val_loss=3.170, train_loss_epoch=2.630]\u001b[A\n",
      "Epoch 22:  88%|█████████▋ | 59/67 [00:17<00:02,  3.42it/s, loss=2, v_num=1, train_loss_step=3.430, val_loss=3.170, train_loss_epoch=2.630]\u001b[A\n",
      "Epoch 22:  90%|█████████▊ | 60/67 [00:17<00:02,  3.42it/s, loss=2, v_num=1, train_loss_step=3.430, val_loss=3.170, train_loss_epoch=2.630]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 22:  91%|██████████ | 61/67 [00:18<00:01,  3.26it/s, loss=2, v_num=1, train_loss_step=3.430, val_loss=3.170, train_loss_epoch=2.630]\u001b[A\n",
      "Epoch 22:  93%|██████████▏| 62/67 [00:19<00:01,  3.26it/s, loss=2, v_num=1, train_loss_step=3.430, val_loss=3.170, train_loss_epoch=2.630]\u001b[A\n",
      "Epoch 22:  94%|██████████▎| 63/67 [00:19<00:01,  3.26it/s, loss=2, v_num=1, train_loss_step=3.430, val_loss=3.170, train_loss_epoch=2.630]\u001b[A\n",
      "Epoch 22:  96%|██████████▌| 64/67 [00:19<00:00,  3.25it/s, loss=2, v_num=1, train_loss_step=3.430, val_loss=3.170, train_loss_epoch=2.630]\u001b[A\n",
      "Epoch 22:  97%|██████████▋| 65/67 [00:19<00:00,  3.25it/s, loss=2, v_num=1, train_loss_step=3.430, val_loss=3.170, train_loss_epoch=2.630]\u001b[A\n",
      "Epoch 22:  99%|██████████▊| 66/67 [00:20<00:00,  3.25it/s, loss=2, v_num=1, train_loss_step=3.430, val_loss=3.170, train_loss_epoch=2.630]\u001b[A\n",
      "Epoch 22: 100%|███████████| 67/67 [00:20<00:00,  3.19it/s, loss=2, v_num=1, train_loss_step=3.430, val_loss=3.160, train_loss_epoch=2.630]\u001b[A\n",
      "Epoch 23:  45%|███▌    | 30/67 [00:05<00:06,  5.53it/s, loss=2.16, v_num=1, train_loss_step=1.390, val_loss=3.160, train_loss_epoch=2.130]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 23:  46%|███▋    | 31/67 [00:07<00:08,  4.25it/s, loss=2.16, v_num=1, train_loss_step=1.390, val_loss=3.160, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 23:  48%|███▊    | 32/67 [00:07<00:08,  4.19it/s, loss=2.16, v_num=1, train_loss_step=1.390, val_loss=3.160, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 23:  49%|███▉    | 33/67 [00:07<00:08,  4.15it/s, loss=2.16, v_num=1, train_loss_step=1.390, val_loss=3.160, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 23:  51%|████    | 34/67 [00:08<00:08,  4.10it/s, loss=2.16, v_num=1, train_loss_step=1.390, val_loss=3.160, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 23:  52%|████▏   | 35/67 [00:08<00:07,  4.06it/s, loss=2.16, v_num=1, train_loss_step=1.390, val_loss=3.160, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 23:  54%|████▎   | 36/67 [00:08<00:07,  4.03it/s, loss=2.16, v_num=1, train_loss_step=1.390, val_loss=3.160, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 23:  55%|████▍   | 37/67 [00:09<00:07,  3.99it/s, loss=2.16, v_num=1, train_loss_step=1.390, val_loss=3.160, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 23:  57%|████▌   | 38/67 [00:09<00:07,  3.96it/s, loss=2.16, v_num=1, train_loss_step=1.390, val_loss=3.160, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 23:  58%|████▋   | 39/67 [00:09<00:07,  3.93it/s, loss=2.16, v_num=1, train_loss_step=1.390, val_loss=3.160, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 23:  60%|████▊   | 40/67 [00:10<00:06,  3.90it/s, loss=2.16, v_num=1, train_loss_step=1.390, val_loss=3.160, train_loss_epoch=2.130]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 23:  61%|████▉   | 41/67 [00:10<00:06,  3.83it/s, loss=2.16, v_num=1, train_loss_step=1.390, val_loss=3.160, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 23:  63%|█████   | 42/67 [00:11<00:06,  3.81it/s, loss=2.16, v_num=1, train_loss_step=1.390, val_loss=3.160, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 23:  64%|█████▏  | 43/67 [00:11<00:06,  3.79it/s, loss=2.16, v_num=1, train_loss_step=1.390, val_loss=3.160, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 23:  66%|█████▎  | 44/67 [00:11<00:06,  3.77it/s, loss=2.16, v_num=1, train_loss_step=1.390, val_loss=3.160, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 23:  67%|█████▎  | 45/67 [00:12<00:05,  3.75it/s, loss=2.16, v_num=1, train_loss_step=1.390, val_loss=3.160, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 23:  69%|█████▍  | 46/67 [00:12<00:05,  3.73it/s, loss=2.16, v_num=1, train_loss_step=1.390, val_loss=3.160, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 23:  70%|█████▌  | 47/67 [00:12<00:05,  3.71it/s, loss=2.16, v_num=1, train_loss_step=1.390, val_loss=3.160, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 23:  72%|█████▋  | 48/67 [00:13<00:05,  3.69it/s, loss=2.16, v_num=1, train_loss_step=1.390, val_loss=3.160, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 23:  73%|█████▊  | 49/67 [00:13<00:04,  3.68it/s, loss=2.16, v_num=1, train_loss_step=1.390, val_loss=3.160, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 23:  75%|█████▉  | 50/67 [00:13<00:04,  3.66it/s, loss=2.16, v_num=1, train_loss_step=1.390, val_loss=3.160, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 23:  76%|██████  | 51/67 [00:14<00:04,  3.62it/s, loss=2.16, v_num=1, train_loss_step=1.390, val_loss=3.160, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 23:  78%|██████▏ | 52/67 [00:14<00:04,  3.61it/s, loss=2.16, v_num=1, train_loss_step=1.390, val_loss=3.160, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 23:  79%|██████▎ | 53/67 [00:14<00:03,  3.59it/s, loss=2.16, v_num=1, train_loss_step=1.390, val_loss=3.160, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 23:  81%|██████▍ | 54/67 [00:15<00:03,  3.58it/s, loss=2.16, v_num=1, train_loss_step=1.390, val_loss=3.160, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 23:  82%|██████▌ | 55/67 [00:15<00:03,  3.56it/s, loss=2.16, v_num=1, train_loss_step=1.390, val_loss=3.160, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 23:  84%|██████▋ | 56/67 [00:15<00:03,  3.55it/s, loss=2.16, v_num=1, train_loss_step=1.390, val_loss=3.160, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 23:  85%|██████▊ | 57/67 [00:16<00:02,  3.54it/s, loss=2.16, v_num=1, train_loss_step=1.390, val_loss=3.160, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 23:  87%|██████▉ | 58/67 [00:16<00:02,  3.53it/s, loss=2.16, v_num=1, train_loss_step=1.390, val_loss=3.160, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 23:  88%|███████ | 59/67 [00:16<00:02,  3.52it/s, loss=2.16, v_num=1, train_loss_step=1.390, val_loss=3.160, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 23:  90%|███████▏| 60/67 [00:17<00:01,  3.52it/s, loss=2.16, v_num=1, train_loss_step=1.390, val_loss=3.160, train_loss_epoch=2.130]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 23:  91%|███████▎| 61/67 [00:17<00:01,  3.48it/s, loss=2.16, v_num=1, train_loss_step=1.390, val_loss=3.160, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 23:  93%|███████▍| 62/67 [00:17<00:01,  3.48it/s, loss=2.16, v_num=1, train_loss_step=1.390, val_loss=3.160, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 23:  94%|███████▌| 63/67 [00:18<00:01,  3.47it/s, loss=2.16, v_num=1, train_loss_step=1.390, val_loss=3.160, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 23:  96%|███████▋| 64/67 [00:18<00:00,  3.46it/s, loss=2.16, v_num=1, train_loss_step=1.390, val_loss=3.160, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 23:  97%|███████▊| 65/67 [00:18<00:00,  3.46it/s, loss=2.16, v_num=1, train_loss_step=1.390, val_loss=3.160, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 23:  99%|███████▉| 66/67 [00:19<00:00,  3.45it/s, loss=2.16, v_num=1, train_loss_step=1.390, val_loss=3.160, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 23: 100%|████████| 67/67 [00:19<00:00,  3.38it/s, loss=2.16, v_num=1, train_loss_step=1.390, val_loss=3.160, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 24:  45%|███▌    | 30/67 [00:05<00:06,  5.58it/s, loss=1.67, v_num=1, train_loss_step=1.770, val_loss=3.160, train_loss_epoch=2.330]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 24:  46%|███▋    | 31/67 [00:07<00:08,  4.29it/s, loss=1.67, v_num=1, train_loss_step=1.770, val_loss=3.160, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 24:  48%|███▊    | 32/67 [00:07<00:08,  4.24it/s, loss=1.67, v_num=1, train_loss_step=1.770, val_loss=3.160, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 24:  49%|███▉    | 33/67 [00:07<00:08,  4.19it/s, loss=1.67, v_num=1, train_loss_step=1.770, val_loss=3.160, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 24:  51%|████    | 34/67 [00:08<00:07,  4.14it/s, loss=1.67, v_num=1, train_loss_step=1.770, val_loss=3.160, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 24:  52%|████▏   | 35/67 [00:08<00:07,  4.10it/s, loss=1.67, v_num=1, train_loss_step=1.770, val_loss=3.160, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 24:  54%|████▎   | 36/67 [00:08<00:07,  4.06it/s, loss=1.67, v_num=1, train_loss_step=1.770, val_loss=3.160, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 24:  55%|████▍   | 37/67 [00:09<00:07,  4.02it/s, loss=1.67, v_num=1, train_loss_step=1.770, val_loss=3.160, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 24:  57%|████▌   | 38/67 [00:09<00:07,  3.99it/s, loss=1.67, v_num=1, train_loss_step=1.770, val_loss=3.160, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 24:  58%|████▋   | 39/67 [00:09<00:07,  3.96it/s, loss=1.67, v_num=1, train_loss_step=1.770, val_loss=3.160, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 24:  60%|████▊   | 40/67 [00:10<00:06,  3.93it/s, loss=1.67, v_num=1, train_loss_step=1.770, val_loss=3.160, train_loss_epoch=2.330]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 24:  61%|████▉   | 41/67 [00:10<00:06,  3.86it/s, loss=1.67, v_num=1, train_loss_step=1.770, val_loss=3.160, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 24:  63%|█████   | 42/67 [00:10<00:06,  3.83it/s, loss=1.67, v_num=1, train_loss_step=1.770, val_loss=3.160, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 24:  64%|█████▏  | 43/67 [00:11<00:06,  3.81it/s, loss=1.67, v_num=1, train_loss_step=1.770, val_loss=3.160, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 24:  66%|█████▎  | 44/67 [00:11<00:06,  3.78it/s, loss=1.67, v_num=1, train_loss_step=1.770, val_loss=3.160, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 24:  67%|█████▎  | 45/67 [00:11<00:05,  3.76it/s, loss=1.67, v_num=1, train_loss_step=1.770, val_loss=3.160, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 24:  69%|█████▍  | 46/67 [00:12<00:05,  3.74it/s, loss=1.67, v_num=1, train_loss_step=1.770, val_loss=3.160, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 24:  70%|█████▌  | 47/67 [00:12<00:05,  3.73it/s, loss=1.67, v_num=1, train_loss_step=1.770, val_loss=3.160, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 24:  72%|█████▋  | 48/67 [00:12<00:05,  3.71it/s, loss=1.67, v_num=1, train_loss_step=1.770, val_loss=3.160, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 24:  73%|█████▊  | 49/67 [00:13<00:04,  3.69it/s, loss=1.67, v_num=1, train_loss_step=1.770, val_loss=3.160, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 24:  75%|█████▉  | 50/67 [00:13<00:04,  3.68it/s, loss=1.67, v_num=1, train_loss_step=1.770, val_loss=3.160, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 24:  76%|██████  | 51/67 [00:14<00:04,  3.63it/s, loss=1.67, v_num=1, train_loss_step=1.770, val_loss=3.160, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 24:  78%|██████▏ | 52/67 [00:14<00:04,  3.62it/s, loss=1.67, v_num=1, train_loss_step=1.770, val_loss=3.160, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 24:  79%|██████▎ | 53/67 [00:14<00:03,  3.60it/s, loss=1.67, v_num=1, train_loss_step=1.770, val_loss=3.160, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 24:  81%|██████▍ | 54/67 [00:15<00:03,  3.59it/s, loss=1.67, v_num=1, train_loss_step=1.770, val_loss=3.160, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 24:  82%|██████▌ | 55/67 [00:15<00:03,  3.58it/s, loss=1.67, v_num=1, train_loss_step=1.770, val_loss=3.160, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 24:  84%|██████▋ | 56/67 [00:15<00:03,  3.57it/s, loss=1.67, v_num=1, train_loss_step=1.770, val_loss=3.160, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 24:  85%|██████▊ | 57/67 [00:16<00:02,  3.56it/s, loss=1.67, v_num=1, train_loss_step=1.770, val_loss=3.160, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 24:  87%|██████▉ | 58/67 [00:16<00:02,  3.55it/s, loss=1.67, v_num=1, train_loss_step=1.770, val_loss=3.160, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 24:  88%|███████ | 59/67 [00:16<00:02,  3.54it/s, loss=1.67, v_num=1, train_loss_step=1.770, val_loss=3.160, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 24:  90%|███████▏| 60/67 [00:16<00:01,  3.53it/s, loss=1.67, v_num=1, train_loss_step=1.770, val_loss=3.160, train_loss_epoch=2.330]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 24:  91%|███████▎| 61/67 [00:17<00:01,  3.50it/s, loss=1.67, v_num=1, train_loss_step=1.770, val_loss=3.160, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 24:  93%|███████▍| 62/67 [00:17<00:01,  3.49it/s, loss=1.67, v_num=1, train_loss_step=1.770, val_loss=3.160, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 24:  94%|███████▌| 63/67 [00:18<00:01,  3.48it/s, loss=1.67, v_num=1, train_loss_step=1.770, val_loss=3.160, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 24:  96%|███████▋| 64/67 [00:18<00:00,  3.48it/s, loss=1.67, v_num=1, train_loss_step=1.770, val_loss=3.160, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 24:  97%|███████▊| 65/67 [00:18<00:00,  3.47it/s, loss=1.67, v_num=1, train_loss_step=1.770, val_loss=3.160, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 24:  99%|███████▉| 66/67 [00:19<00:00,  3.46it/s, loss=1.67, v_num=1, train_loss_step=1.770, val_loss=3.160, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 24: 100%|████████| 67/67 [00:19<00:00,  3.40it/s, loss=1.67, v_num=1, train_loss_step=1.770, val_loss=3.160, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 25:  45%|████     | 30/67 [00:05<00:06,  5.40it/s, loss=2.1, v_num=1, train_loss_step=1.450, val_loss=3.160, train_loss_epoch=1.580]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 25:  46%|████▏    | 31/67 [00:07<00:08,  4.16it/s, loss=2.1, v_num=1, train_loss_step=1.450, val_loss=3.160, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 25:  48%|████▎    | 32/67 [00:07<00:08,  4.11it/s, loss=2.1, v_num=1, train_loss_step=1.450, val_loss=3.160, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 25:  49%|████▍    | 33/67 [00:08<00:08,  4.07it/s, loss=2.1, v_num=1, train_loss_step=1.450, val_loss=3.160, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 25:  51%|████▌    | 34/67 [00:08<00:08,  4.03it/s, loss=2.1, v_num=1, train_loss_step=1.450, val_loss=3.160, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 25:  52%|████▋    | 35/67 [00:08<00:08,  3.99it/s, loss=2.1, v_num=1, train_loss_step=1.450, val_loss=3.160, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 25:  54%|████▊    | 36/67 [00:09<00:07,  3.96it/s, loss=2.1, v_num=1, train_loss_step=1.450, val_loss=3.160, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 25:  55%|████▉    | 37/67 [00:09<00:07,  3.93it/s, loss=2.1, v_num=1, train_loss_step=1.450, val_loss=3.160, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 25:  57%|█████    | 38/67 [00:09<00:07,  3.90it/s, loss=2.1, v_num=1, train_loss_step=1.450, val_loss=3.160, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 25:  58%|█████▏   | 39/67 [00:10<00:07,  3.87it/s, loss=2.1, v_num=1, train_loss_step=1.450, val_loss=3.160, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 25:  60%|█████▎   | 40/67 [00:10<00:07,  3.85it/s, loss=2.1, v_num=1, train_loss_step=1.450, val_loss=3.160, train_loss_epoch=1.580]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 25:  61%|█████▌   | 41/67 [00:10<00:06,  3.78it/s, loss=2.1, v_num=1, train_loss_step=1.450, val_loss=3.160, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 25:  63%|█████▋   | 42/67 [00:11<00:06,  3.76it/s, loss=2.1, v_num=1, train_loss_step=1.450, val_loss=3.160, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 25:  64%|█████▊   | 43/67 [00:11<00:06,  3.74it/s, loss=2.1, v_num=1, train_loss_step=1.450, val_loss=3.160, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 25:  66%|█████▉   | 44/67 [00:11<00:06,  3.72it/s, loss=2.1, v_num=1, train_loss_step=1.450, val_loss=3.160, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 25:  67%|██████   | 45/67 [00:12<00:05,  3.70it/s, loss=2.1, v_num=1, train_loss_step=1.450, val_loss=3.160, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 25:  69%|██████▏  | 46/67 [00:12<00:05,  3.68it/s, loss=2.1, v_num=1, train_loss_step=1.450, val_loss=3.160, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 25:  70%|██████▎  | 47/67 [00:12<00:05,  3.67it/s, loss=2.1, v_num=1, train_loss_step=1.450, val_loss=3.160, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 25:  72%|██████▍  | 48/67 [00:13<00:05,  3.65it/s, loss=2.1, v_num=1, train_loss_step=1.450, val_loss=3.160, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 25:  73%|██████▌  | 49/67 [00:13<00:04,  3.64it/s, loss=2.1, v_num=1, train_loss_step=1.450, val_loss=3.160, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 25:  75%|██████▋  | 50/67 [00:13<00:04,  3.62it/s, loss=2.1, v_num=1, train_loss_step=1.450, val_loss=3.160, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 25:  76%|██████▊  | 51/67 [00:14<00:04,  3.58it/s, loss=2.1, v_num=1, train_loss_step=1.450, val_loss=3.160, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 25:  78%|██████▉  | 52/67 [00:14<00:04,  3.57it/s, loss=2.1, v_num=1, train_loss_step=1.450, val_loss=3.160, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 25:  79%|███████  | 53/67 [00:14<00:03,  3.55it/s, loss=2.1, v_num=1, train_loss_step=1.450, val_loss=3.160, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 25:  81%|███████▎ | 54/67 [00:15<00:03,  3.54it/s, loss=2.1, v_num=1, train_loss_step=1.450, val_loss=3.160, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 25:  82%|███████▍ | 55/67 [00:15<00:03,  3.53it/s, loss=2.1, v_num=1, train_loss_step=1.450, val_loss=3.160, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 25:  84%|███████▌ | 56/67 [00:15<00:03,  3.52it/s, loss=2.1, v_num=1, train_loss_step=1.450, val_loss=3.160, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 25:  85%|███████▋ | 57/67 [00:16<00:02,  3.51it/s, loss=2.1, v_num=1, train_loss_step=1.450, val_loss=3.160, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 25:  87%|███████▊ | 58/67 [00:16<00:02,  3.51it/s, loss=2.1, v_num=1, train_loss_step=1.450, val_loss=3.160, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 25:  88%|███████▉ | 59/67 [00:16<00:02,  3.50it/s, loss=2.1, v_num=1, train_loss_step=1.450, val_loss=3.160, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 25:  90%|████████ | 60/67 [00:17<00:02,  3.49it/s, loss=2.1, v_num=1, train_loss_step=1.450, val_loss=3.160, train_loss_epoch=1.580]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 25:  91%|████████▏| 61/67 [00:17<00:01,  3.46it/s, loss=2.1, v_num=1, train_loss_step=1.450, val_loss=3.160, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 25:  93%|████████▎| 62/67 [00:17<00:01,  3.45it/s, loss=2.1, v_num=1, train_loss_step=1.450, val_loss=3.160, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 25:  94%|████████▍| 63/67 [00:18<00:01,  3.45it/s, loss=2.1, v_num=1, train_loss_step=1.450, val_loss=3.160, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 25:  96%|████████▌| 64/67 [00:18<00:00,  3.44it/s, loss=2.1, v_num=1, train_loss_step=1.450, val_loss=3.160, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 25:  97%|████████▋| 65/67 [00:18<00:00,  3.44it/s, loss=2.1, v_num=1, train_loss_step=1.450, val_loss=3.160, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 25:  99%|████████▊| 66/67 [00:19<00:00,  3.43it/s, loss=2.1, v_num=1, train_loss_step=1.450, val_loss=3.160, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 25: 100%|█████████| 67/67 [00:19<00:00,  3.36it/s, loss=2.1, v_num=1, train_loss_step=1.450, val_loss=3.160, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 26:  45%|███▌    | 30/67 [00:05<00:06,  5.39it/s, loss=2.47, v_num=1, train_loss_step=2.460, val_loss=3.160, train_loss_epoch=2.110]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 26:  46%|███▋    | 31/67 [00:07<00:08,  4.14it/s, loss=2.47, v_num=1, train_loss_step=2.460, val_loss=3.160, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 26:  48%|███▊    | 32/67 [00:07<00:08,  4.09it/s, loss=2.47, v_num=1, train_loss_step=2.460, val_loss=3.160, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 26:  49%|███▉    | 33/67 [00:08<00:08,  4.05it/s, loss=2.47, v_num=1, train_loss_step=2.460, val_loss=3.160, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 26:  51%|████    | 34/67 [00:08<00:08,  4.01it/s, loss=2.47, v_num=1, train_loss_step=2.460, val_loss=3.160, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 26:  52%|████▏   | 35/67 [00:08<00:08,  3.98it/s, loss=2.47, v_num=1, train_loss_step=2.460, val_loss=3.160, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 26:  54%|████▎   | 36/67 [00:09<00:07,  3.95it/s, loss=2.47, v_num=1, train_loss_step=2.460, val_loss=3.160, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 26:  55%|████▍   | 37/67 [00:09<00:07,  3.91it/s, loss=2.47, v_num=1, train_loss_step=2.460, val_loss=3.160, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 26:  57%|████▌   | 38/67 [00:09<00:07,  3.88it/s, loss=2.47, v_num=1, train_loss_step=2.460, val_loss=3.160, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 26:  58%|████▋   | 39/67 [00:10<00:07,  3.86it/s, loss=2.47, v_num=1, train_loss_step=2.460, val_loss=3.160, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 26:  60%|████▊   | 40/67 [00:10<00:07,  3.83it/s, loss=2.47, v_num=1, train_loss_step=2.460, val_loss=3.160, train_loss_epoch=2.110]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 26:  61%|████▉   | 41/67 [00:10<00:06,  3.77it/s, loss=2.47, v_num=1, train_loss_step=2.460, val_loss=3.160, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 26:  63%|█████   | 42/67 [00:11<00:06,  3.75it/s, loss=2.47, v_num=1, train_loss_step=2.460, val_loss=3.160, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 26:  64%|█████▏  | 43/67 [00:11<00:06,  3.73it/s, loss=2.47, v_num=1, train_loss_step=2.460, val_loss=3.160, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 26:  66%|█████▎  | 44/67 [00:11<00:06,  3.72it/s, loss=2.47, v_num=1, train_loss_step=2.460, val_loss=3.160, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 26:  67%|█████▎  | 45/67 [00:12<00:05,  3.70it/s, loss=2.47, v_num=1, train_loss_step=2.460, val_loss=3.160, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 26:  69%|█████▍  | 46/67 [00:12<00:05,  3.68it/s, loss=2.47, v_num=1, train_loss_step=2.460, val_loss=3.160, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 26:  70%|█████▌  | 47/67 [00:12<00:05,  3.67it/s, loss=2.47, v_num=1, train_loss_step=2.460, val_loss=3.160, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 26:  72%|█████▋  | 48/67 [00:13<00:05,  3.65it/s, loss=2.47, v_num=1, train_loss_step=2.460, val_loss=3.160, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 26:  73%|█████▊  | 49/67 [00:13<00:04,  3.64it/s, loss=2.47, v_num=1, train_loss_step=2.460, val_loss=3.160, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 26:  75%|█████▉  | 50/67 [00:13<00:04,  3.62it/s, loss=2.47, v_num=1, train_loss_step=2.460, val_loss=3.160, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 26:  76%|██████  | 51/67 [00:14<00:04,  3.58it/s, loss=2.47, v_num=1, train_loss_step=2.460, val_loss=3.160, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 26:  78%|██████▏ | 52/67 [00:14<00:04,  3.57it/s, loss=2.47, v_num=1, train_loss_step=2.460, val_loss=3.160, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 26:  79%|██████▎ | 53/67 [00:14<00:03,  3.56it/s, loss=2.47, v_num=1, train_loss_step=2.460, val_loss=3.160, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 26:  81%|██████▍ | 54/67 [00:15<00:03,  3.55it/s, loss=2.47, v_num=1, train_loss_step=2.460, val_loss=3.160, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 26:  82%|██████▌ | 55/67 [00:15<00:03,  3.53it/s, loss=2.47, v_num=1, train_loss_step=2.460, val_loss=3.160, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 26:  84%|██████▋ | 56/67 [00:15<00:03,  3.53it/s, loss=2.47, v_num=1, train_loss_step=2.460, val_loss=3.160, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 26:  85%|██████▊ | 57/67 [00:16<00:02,  3.52it/s, loss=2.47, v_num=1, train_loss_step=2.460, val_loss=3.160, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 26:  87%|██████▉ | 58/67 [00:16<00:02,  3.51it/s, loss=2.47, v_num=1, train_loss_step=2.460, val_loss=3.160, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 26:  88%|███████ | 59/67 [00:16<00:02,  3.49it/s, loss=2.47, v_num=1, train_loss_step=2.460, val_loss=3.160, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 26:  90%|███████▏| 60/67 [00:17<00:02,  3.48it/s, loss=2.47, v_num=1, train_loss_step=2.460, val_loss=3.160, train_loss_epoch=2.110]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 26:  91%|███████▎| 61/67 [00:17<00:01,  3.45it/s, loss=2.47, v_num=1, train_loss_step=2.460, val_loss=3.160, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 26:  93%|███████▍| 62/67 [00:17<00:01,  3.45it/s, loss=2.47, v_num=1, train_loss_step=2.460, val_loss=3.160, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 26:  94%|███████▌| 63/67 [00:18<00:01,  3.44it/s, loss=2.47, v_num=1, train_loss_step=2.460, val_loss=3.160, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 26:  96%|███████▋| 64/67 [00:18<00:00,  3.43it/s, loss=2.47, v_num=1, train_loss_step=2.460, val_loss=3.160, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 26:  97%|███████▊| 65/67 [00:18<00:00,  3.43it/s, loss=2.47, v_num=1, train_loss_step=2.460, val_loss=3.160, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 26:  99%|███████▉| 66/67 [00:19<00:00,  3.42it/s, loss=2.47, v_num=1, train_loss_step=2.460, val_loss=3.160, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 26: 100%|████████| 67/67 [00:19<00:00,  3.36it/s, loss=2.47, v_num=1, train_loss_step=2.460, val_loss=3.160, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 27:  45%|███▌    | 30/67 [00:05<00:06,  5.38it/s, loss=1.94, v_num=1, train_loss_step=2.250, val_loss=3.160, train_loss_epoch=2.280]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 27:  46%|███▋    | 31/67 [00:07<00:08,  4.19it/s, loss=1.94, v_num=1, train_loss_step=2.250, val_loss=3.160, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 27:  48%|███▊    | 32/67 [00:07<00:08,  4.14it/s, loss=1.94, v_num=1, train_loss_step=2.250, val_loss=3.160, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 27:  49%|███▉    | 33/67 [00:08<00:08,  4.10it/s, loss=1.94, v_num=1, train_loss_step=2.250, val_loss=3.160, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 27:  51%|████    | 34/67 [00:08<00:08,  4.05it/s, loss=1.94, v_num=1, train_loss_step=2.250, val_loss=3.160, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 27:  52%|████▏   | 35/67 [00:08<00:07,  4.01it/s, loss=1.94, v_num=1, train_loss_step=2.250, val_loss=3.160, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 27:  54%|████▎   | 36/67 [00:09<00:07,  3.97it/s, loss=1.94, v_num=1, train_loss_step=2.250, val_loss=3.160, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 27:  55%|████▍   | 37/67 [00:09<00:07,  3.94it/s, loss=1.94, v_num=1, train_loss_step=2.250, val_loss=3.160, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 27:  57%|████▌   | 38/67 [00:09<00:07,  3.91it/s, loss=1.94, v_num=1, train_loss_step=2.250, val_loss=3.160, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 27:  58%|████▋   | 39/67 [00:10<00:07,  3.88it/s, loss=1.94, v_num=1, train_loss_step=2.250, val_loss=3.160, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 27:  60%|████▊   | 40/67 [00:10<00:07,  3.85it/s, loss=1.94, v_num=1, train_loss_step=2.250, val_loss=3.160, train_loss_epoch=2.280]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 27:  61%|████▉   | 41/67 [00:10<00:06,  3.79it/s, loss=1.94, v_num=1, train_loss_step=2.250, val_loss=3.160, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 27:  63%|█████   | 42/67 [00:11<00:06,  3.77it/s, loss=1.94, v_num=1, train_loss_step=2.250, val_loss=3.160, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 27:  64%|█████▏  | 43/67 [00:11<00:06,  3.75it/s, loss=1.94, v_num=1, train_loss_step=2.250, val_loss=3.160, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 27:  66%|█████▎  | 44/67 [00:11<00:06,  3.73it/s, loss=1.94, v_num=1, train_loss_step=2.250, val_loss=3.160, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 27:  67%|█████▎  | 45/67 [00:12<00:05,  3.71it/s, loss=1.94, v_num=1, train_loss_step=2.250, val_loss=3.160, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 27:  69%|█████▍  | 46/67 [00:12<00:05,  3.69it/s, loss=1.94, v_num=1, train_loss_step=2.250, val_loss=3.160, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 27:  70%|█████▌  | 47/67 [00:12<00:05,  3.67it/s, loss=1.94, v_num=1, train_loss_step=2.250, val_loss=3.160, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 27:  72%|█████▋  | 48/67 [00:13<00:05,  3.66it/s, loss=1.94, v_num=1, train_loss_step=2.250, val_loss=3.160, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 27:  73%|█████▊  | 49/67 [00:13<00:04,  3.64it/s, loss=1.94, v_num=1, train_loss_step=2.250, val_loss=3.160, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 27:  75%|█████▉  | 50/67 [00:13<00:04,  3.63it/s, loss=1.94, v_num=1, train_loss_step=2.250, val_loss=3.160, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 27:  76%|██████  | 51/67 [00:14<00:04,  3.58it/s, loss=1.94, v_num=1, train_loss_step=2.250, val_loss=3.160, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 27:  78%|██████▏ | 52/67 [00:14<00:04,  3.57it/s, loss=1.94, v_num=1, train_loss_step=2.250, val_loss=3.160, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 27:  79%|██████▎ | 53/67 [00:14<00:03,  3.56it/s, loss=1.94, v_num=1, train_loss_step=2.250, val_loss=3.160, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 27:  81%|██████▍ | 54/67 [00:15<00:03,  3.55it/s, loss=1.94, v_num=1, train_loss_step=2.250, val_loss=3.160, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 27:  82%|██████▌ | 55/67 [00:15<00:03,  3.53it/s, loss=1.94, v_num=1, train_loss_step=2.250, val_loss=3.160, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 27:  84%|██████▋ | 56/67 [00:15<00:03,  3.52it/s, loss=1.94, v_num=1, train_loss_step=2.250, val_loss=3.160, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 27:  85%|██████▊ | 57/67 [00:16<00:02,  3.52it/s, loss=1.94, v_num=1, train_loss_step=2.250, val_loss=3.160, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 27:  87%|██████▉ | 58/67 [00:17<00:02,  3.41it/s, loss=1.94, v_num=1, train_loss_step=2.250, val_loss=3.160, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 27:  88%|███████ | 59/67 [00:17<00:02,  3.41it/s, loss=1.94, v_num=1, train_loss_step=2.250, val_loss=3.160, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 27:  90%|███████▏| 60/67 [00:17<00:02,  3.40it/s, loss=1.94, v_num=1, train_loss_step=2.250, val_loss=3.160, train_loss_epoch=2.280]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 27:  91%|███████▎| 61/67 [00:18<00:01,  3.37it/s, loss=1.94, v_num=1, train_loss_step=2.250, val_loss=3.160, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 27:  93%|███████▍| 62/67 [00:18<00:01,  3.36it/s, loss=1.94, v_num=1, train_loss_step=2.250, val_loss=3.160, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 27:  94%|███████▌| 63/67 [00:18<00:01,  3.36it/s, loss=1.94, v_num=1, train_loss_step=2.250, val_loss=3.160, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 27:  96%|███████▋| 64/67 [00:19<00:00,  3.35it/s, loss=1.94, v_num=1, train_loss_step=2.250, val_loss=3.160, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 27:  97%|███████▊| 65/67 [00:19<00:00,  3.35it/s, loss=1.94, v_num=1, train_loss_step=2.250, val_loss=3.160, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 27:  99%|███████▉| 66/67 [00:19<00:00,  3.34it/s, loss=1.94, v_num=1, train_loss_step=2.250, val_loss=3.160, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 27: 100%|████████| 67/67 [00:20<00:00,  3.28it/s, loss=1.94, v_num=1, train_loss_step=2.250, val_loss=3.160, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 28:  45%|███▌    | 30/67 [00:05<00:06,  5.54it/s, loss=2.68, v_num=1, train_loss_step=2.800, val_loss=3.160, train_loss_epoch=1.980]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 28:  46%|███▋    | 31/67 [00:07<00:08,  4.25it/s, loss=2.68, v_num=1, train_loss_step=2.800, val_loss=3.160, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 28:  48%|███▊    | 32/67 [00:07<00:08,  4.18it/s, loss=2.68, v_num=1, train_loss_step=2.800, val_loss=3.160, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 28:  49%|███▉    | 33/67 [00:07<00:08,  4.14it/s, loss=2.68, v_num=1, train_loss_step=2.800, val_loss=3.160, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 28:  51%|████    | 34/67 [00:08<00:08,  4.09it/s, loss=2.68, v_num=1, train_loss_step=2.800, val_loss=3.160, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 28:  52%|████▏   | 35/67 [00:08<00:07,  4.05it/s, loss=2.68, v_num=1, train_loss_step=2.800, val_loss=3.160, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 28:  54%|████▎   | 36/67 [00:08<00:07,  4.02it/s, loss=2.68, v_num=1, train_loss_step=2.800, val_loss=3.160, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 28:  55%|████▍   | 37/67 [00:09<00:07,  3.97it/s, loss=2.68, v_num=1, train_loss_step=2.800, val_loss=3.160, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 28:  57%|████▌   | 38/67 [00:09<00:07,  3.94it/s, loss=2.68, v_num=1, train_loss_step=2.800, val_loss=3.160, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 28:  58%|████▋   | 39/67 [00:09<00:07,  3.91it/s, loss=2.68, v_num=1, train_loss_step=2.800, val_loss=3.160, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 28:  60%|████▊   | 40/67 [00:10<00:06,  3.88it/s, loss=2.68, v_num=1, train_loss_step=2.800, val_loss=3.160, train_loss_epoch=1.980]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 28:  61%|████▉   | 41/67 [00:10<00:06,  3.82it/s, loss=2.68, v_num=1, train_loss_step=2.800, val_loss=3.160, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 28:  63%|█████   | 42/67 [00:11<00:06,  3.80it/s, loss=2.68, v_num=1, train_loss_step=2.800, val_loss=3.160, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 28:  64%|█████▏  | 43/67 [00:11<00:06,  3.77it/s, loss=2.68, v_num=1, train_loss_step=2.800, val_loss=3.160, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 28:  66%|█████▎  | 44/67 [00:11<00:06,  3.75it/s, loss=2.68, v_num=1, train_loss_step=2.800, val_loss=3.160, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 28:  67%|█████▎  | 45/67 [00:12<00:05,  3.73it/s, loss=2.68, v_num=1, train_loss_step=2.800, val_loss=3.160, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 28:  69%|█████▍  | 46/67 [00:12<00:05,  3.71it/s, loss=2.68, v_num=1, train_loss_step=2.800, val_loss=3.160, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 28:  70%|█████▌  | 47/67 [00:12<00:05,  3.69it/s, loss=2.68, v_num=1, train_loss_step=2.800, val_loss=3.160, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 28:  72%|█████▋  | 48/67 [00:13<00:05,  3.53it/s, loss=2.68, v_num=1, train_loss_step=2.800, val_loss=3.160, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 28:  73%|█████▊  | 49/67 [00:13<00:05,  3.52it/s, loss=2.68, v_num=1, train_loss_step=2.800, val_loss=3.160, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 28:  75%|█████▉  | 50/67 [00:14<00:04,  3.51it/s, loss=2.68, v_num=1, train_loss_step=2.800, val_loss=3.160, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 28:  76%|██████  | 51/67 [00:14<00:04,  3.47it/s, loss=2.68, v_num=1, train_loss_step=2.800, val_loss=3.160, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 28:  78%|██████▏ | 52/67 [00:15<00:04,  3.46it/s, loss=2.68, v_num=1, train_loss_step=2.800, val_loss=3.160, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 28:  79%|██████▎ | 53/67 [00:15<00:04,  3.45it/s, loss=2.68, v_num=1, train_loss_step=2.800, val_loss=3.160, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 28:  81%|██████▍ | 54/67 [00:15<00:03,  3.44it/s, loss=2.68, v_num=1, train_loss_step=2.800, val_loss=3.160, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 28:  82%|██████▌ | 55/67 [00:16<00:03,  3.43it/s, loss=2.68, v_num=1, train_loss_step=2.800, val_loss=3.160, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 28:  84%|██████▋ | 56/67 [00:16<00:03,  3.42it/s, loss=2.68, v_num=1, train_loss_step=2.800, val_loss=3.160, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 28:  85%|██████▊ | 57/67 [00:16<00:02,  3.42it/s, loss=2.68, v_num=1, train_loss_step=2.800, val_loss=3.160, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 28:  87%|██████▉ | 58/67 [00:17<00:02,  3.41it/s, loss=2.68, v_num=1, train_loss_step=2.800, val_loss=3.160, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 28:  88%|███████ | 59/67 [00:17<00:02,  3.40it/s, loss=2.68, v_num=1, train_loss_step=2.800, val_loss=3.160, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 28:  90%|███████▏| 60/67 [00:17<00:02,  3.40it/s, loss=2.68, v_num=1, train_loss_step=2.800, val_loss=3.160, train_loss_epoch=1.980]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 28:  91%|███████▎| 61/67 [00:18<00:01,  3.37it/s, loss=2.68, v_num=1, train_loss_step=2.800, val_loss=3.160, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 28:  93%|███████▍| 62/67 [00:18<00:01,  3.36it/s, loss=2.68, v_num=1, train_loss_step=2.800, val_loss=3.160, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 28:  94%|███████▌| 63/67 [00:18<00:01,  3.36it/s, loss=2.68, v_num=1, train_loss_step=2.800, val_loss=3.160, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 28:  96%|███████▋| 64/67 [00:19<00:00,  3.35it/s, loss=2.68, v_num=1, train_loss_step=2.800, val_loss=3.160, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 28:  97%|███████▊| 65/67 [00:19<00:00,  3.35it/s, loss=2.68, v_num=1, train_loss_step=2.800, val_loss=3.160, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 28:  99%|███████▉| 66/67 [00:19<00:00,  3.34it/s, loss=2.68, v_num=1, train_loss_step=2.800, val_loss=3.160, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 28: 100%|████████| 67/67 [00:20<00:00,  3.28it/s, loss=2.68, v_num=1, train_loss_step=2.800, val_loss=3.150, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 29:  45%|███▌    | 30/67 [00:05<00:06,  5.45it/s, loss=2.19, v_num=1, train_loss_step=1.800, val_loss=3.150, train_loss_epoch=2.400]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 29:  46%|███▋    | 31/67 [00:07<00:08,  4.19it/s, loss=2.19, v_num=1, train_loss_step=1.800, val_loss=3.150, train_loss_epoch=2.400]\u001b[A\n",
      "Epoch 29:  48%|███▊    | 32/67 [00:07<00:08,  4.14it/s, loss=2.19, v_num=1, train_loss_step=1.800, val_loss=3.150, train_loss_epoch=2.400]\u001b[A\n",
      "Epoch 29:  49%|███▉    | 33/67 [00:08<00:08,  4.10it/s, loss=2.19, v_num=1, train_loss_step=1.800, val_loss=3.150, train_loss_epoch=2.400]\u001b[A\n",
      "Epoch 29:  51%|████    | 34/67 [00:08<00:08,  4.04it/s, loss=2.19, v_num=1, train_loss_step=1.800, val_loss=3.150, train_loss_epoch=2.400]\u001b[A\n",
      "Epoch 29:  52%|████▏   | 35/67 [00:08<00:07,  4.01it/s, loss=2.19, v_num=1, train_loss_step=1.800, val_loss=3.150, train_loss_epoch=2.400]\u001b[A\n",
      "Epoch 29:  54%|████▎   | 36/67 [00:09<00:07,  3.97it/s, loss=2.19, v_num=1, train_loss_step=1.800, val_loss=3.150, train_loss_epoch=2.400]\u001b[A\n",
      "Epoch 29:  55%|████▍   | 37/67 [00:09<00:07,  3.94it/s, loss=2.19, v_num=1, train_loss_step=1.800, val_loss=3.150, train_loss_epoch=2.400]\u001b[A\n",
      "Epoch 29:  57%|████▌   | 38/67 [00:09<00:07,  3.91it/s, loss=2.19, v_num=1, train_loss_step=1.800, val_loss=3.150, train_loss_epoch=2.400]\u001b[A\n",
      "Epoch 29:  58%|████▋   | 39/67 [00:10<00:07,  3.88it/s, loss=2.19, v_num=1, train_loss_step=1.800, val_loss=3.150, train_loss_epoch=2.400]\u001b[A\n",
      "Epoch 29:  60%|████▊   | 40/67 [00:10<00:07,  3.71it/s, loss=2.19, v_num=1, train_loss_step=1.800, val_loss=3.150, train_loss_epoch=2.400]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 29:  61%|████▉   | 41/67 [00:11<00:07,  3.64it/s, loss=2.19, v_num=1, train_loss_step=1.800, val_loss=3.150, train_loss_epoch=2.400]\u001b[A\n",
      "Epoch 29:  63%|█████   | 42/67 [00:11<00:06,  3.62it/s, loss=2.19, v_num=1, train_loss_step=1.800, val_loss=3.150, train_loss_epoch=2.400]\u001b[A\n",
      "Epoch 29:  64%|█████▏  | 43/67 [00:11<00:06,  3.60it/s, loss=2.19, v_num=1, train_loss_step=1.800, val_loss=3.150, train_loss_epoch=2.400]\u001b[A\n",
      "Epoch 29:  66%|█████▎  | 44/67 [00:12<00:06,  3.59it/s, loss=2.19, v_num=1, train_loss_step=1.800, val_loss=3.150, train_loss_epoch=2.400]\u001b[A\n",
      "Epoch 29:  67%|█████▎  | 45/67 [00:12<00:06,  3.58it/s, loss=2.19, v_num=1, train_loss_step=1.800, val_loss=3.150, train_loss_epoch=2.400]\u001b[A\n",
      "Epoch 29:  69%|█████▍  | 46/67 [00:12<00:05,  3.56it/s, loss=2.19, v_num=1, train_loss_step=1.800, val_loss=3.150, train_loss_epoch=2.400]\u001b[A\n",
      "Epoch 29:  70%|█████▌  | 47/67 [00:13<00:05,  3.55it/s, loss=2.19, v_num=1, train_loss_step=1.800, val_loss=3.150, train_loss_epoch=2.400]\u001b[A\n",
      "Epoch 29:  72%|█████▋  | 48/67 [00:13<00:05,  3.54it/s, loss=2.19, v_num=1, train_loss_step=1.800, val_loss=3.150, train_loss_epoch=2.400]\u001b[A\n",
      "Epoch 29:  73%|█████▊  | 49/67 [00:13<00:05,  3.52it/s, loss=2.19, v_num=1, train_loss_step=1.800, val_loss=3.150, train_loss_epoch=2.400]\u001b[A\n",
      "Epoch 29:  75%|█████▉  | 50/67 [00:14<00:04,  3.51it/s, loss=2.19, v_num=1, train_loss_step=1.800, val_loss=3.150, train_loss_epoch=2.400]\u001b[A\n",
      "Epoch 29:  76%|██████  | 51/67 [00:14<00:04,  3.46it/s, loss=2.19, v_num=1, train_loss_step=1.800, val_loss=3.150, train_loss_epoch=2.400]\u001b[A\n",
      "Epoch 29:  78%|██████▏ | 52/67 [00:15<00:04,  3.45it/s, loss=2.19, v_num=1, train_loss_step=1.800, val_loss=3.150, train_loss_epoch=2.400]\u001b[A\n",
      "Epoch 29:  79%|██████▎ | 53/67 [00:15<00:04,  3.44it/s, loss=2.19, v_num=1, train_loss_step=1.800, val_loss=3.150, train_loss_epoch=2.400]\u001b[A\n",
      "Epoch 29:  81%|██████▍ | 54/67 [00:15<00:03,  3.43it/s, loss=2.19, v_num=1, train_loss_step=1.800, val_loss=3.150, train_loss_epoch=2.400]\u001b[A\n",
      "Epoch 29:  82%|██████▌ | 55/67 [00:16<00:03,  3.42it/s, loss=2.19, v_num=1, train_loss_step=1.800, val_loss=3.150, train_loss_epoch=2.400]\u001b[A\n",
      "Epoch 29:  84%|██████▋ | 56/67 [00:16<00:03,  3.42it/s, loss=2.19, v_num=1, train_loss_step=1.800, val_loss=3.150, train_loss_epoch=2.400]\u001b[A\n",
      "Epoch 29:  85%|██████▊ | 57/67 [00:16<00:02,  3.41it/s, loss=2.19, v_num=1, train_loss_step=1.800, val_loss=3.150, train_loss_epoch=2.400]\u001b[A\n",
      "Epoch 29:  87%|██████▉ | 58/67 [00:17<00:02,  3.40it/s, loss=2.19, v_num=1, train_loss_step=1.800, val_loss=3.150, train_loss_epoch=2.400]\u001b[A\n",
      "Epoch 29:  88%|███████ | 59/67 [00:17<00:02,  3.39it/s, loss=2.19, v_num=1, train_loss_step=1.800, val_loss=3.150, train_loss_epoch=2.400]\u001b[A\n",
      "Epoch 29:  90%|███████▏| 60/67 [00:17<00:02,  3.39it/s, loss=2.19, v_num=1, train_loss_step=1.800, val_loss=3.150, train_loss_epoch=2.400]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 29:  91%|███████▎| 61/67 [00:18<00:01,  3.36it/s, loss=2.19, v_num=1, train_loss_step=1.800, val_loss=3.150, train_loss_epoch=2.400]\u001b[A\n",
      "Epoch 29:  93%|███████▍| 62/67 [00:18<00:01,  3.35it/s, loss=2.19, v_num=1, train_loss_step=1.800, val_loss=3.150, train_loss_epoch=2.400]\u001b[A\n",
      "Epoch 29:  94%|███████▌| 63/67 [00:18<00:01,  3.35it/s, loss=2.19, v_num=1, train_loss_step=1.800, val_loss=3.150, train_loss_epoch=2.400]\u001b[A\n",
      "Epoch 29:  96%|███████▋| 64/67 [00:19<00:00,  3.33it/s, loss=2.19, v_num=1, train_loss_step=1.800, val_loss=3.150, train_loss_epoch=2.400]\u001b[A\n",
      "Epoch 29:  97%|███████▊| 65/67 [00:19<00:00,  3.33it/s, loss=2.19, v_num=1, train_loss_step=1.800, val_loss=3.150, train_loss_epoch=2.400]\u001b[A\n",
      "Epoch 29:  99%|███████▉| 66/67 [00:19<00:00,  3.33it/s, loss=2.19, v_num=1, train_loss_step=1.800, val_loss=3.150, train_loss_epoch=2.400]\u001b[A\n",
      "Epoch 29: 100%|████████| 67/67 [00:20<00:00,  3.27it/s, loss=2.19, v_num=1, train_loss_step=1.800, val_loss=3.150, train_loss_epoch=2.400]\u001b[A\n",
      "Epoch 30:  45%|███▌    | 30/67 [00:05<00:06,  5.51it/s, loss=2.46, v_num=1, train_loss_step=3.310, val_loss=3.150, train_loss_epoch=2.230]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 30:  46%|███▋    | 31/67 [00:08<00:10,  3.52it/s, loss=2.46, v_num=1, train_loss_step=3.310, val_loss=3.150, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 30:  48%|███▊    | 32/67 [00:09<00:09,  3.51it/s, loss=2.46, v_num=1, train_loss_step=3.310, val_loss=3.150, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 30:  49%|███▉    | 33/67 [00:09<00:09,  3.49it/s, loss=2.46, v_num=1, train_loss_step=3.310, val_loss=3.150, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 30:  51%|████    | 34/67 [00:09<00:09,  3.48it/s, loss=2.46, v_num=1, train_loss_step=3.310, val_loss=3.150, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 30:  52%|████▏   | 35/67 [00:10<00:09,  3.46it/s, loss=2.46, v_num=1, train_loss_step=3.310, val_loss=3.150, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 30:  54%|████▎   | 36/67 [00:10<00:08,  3.45it/s, loss=2.46, v_num=1, train_loss_step=3.310, val_loss=3.150, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 30:  55%|████▍   | 37/67 [00:10<00:08,  3.44it/s, loss=2.46, v_num=1, train_loss_step=3.310, val_loss=3.150, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 30:  57%|████▌   | 38/67 [00:11<00:08,  3.43it/s, loss=2.46, v_num=1, train_loss_step=3.310, val_loss=3.150, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 30:  58%|████▋   | 39/67 [00:11<00:08,  3.42it/s, loss=2.46, v_num=1, train_loss_step=3.310, val_loss=3.150, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 30:  60%|████▊   | 40/67 [00:11<00:07,  3.40it/s, loss=2.46, v_num=1, train_loss_step=3.310, val_loss=3.150, train_loss_epoch=2.230]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 30:  61%|████▉   | 41/67 [00:12<00:07,  3.36it/s, loss=2.46, v_num=1, train_loss_step=3.310, val_loss=3.150, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 30:  63%|█████   | 42/67 [00:12<00:07,  3.36it/s, loss=2.46, v_num=1, train_loss_step=3.310, val_loss=3.150, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 30:  64%|█████▏  | 43/67 [00:12<00:07,  3.35it/s, loss=2.46, v_num=1, train_loss_step=3.310, val_loss=3.150, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 30:  66%|█████▎  | 44/67 [00:13<00:06,  3.34it/s, loss=2.46, v_num=1, train_loss_step=3.310, val_loss=3.150, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 30:  67%|█████▎  | 45/67 [00:13<00:06,  3.34it/s, loss=2.46, v_num=1, train_loss_step=3.310, val_loss=3.150, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 30:  69%|█████▍  | 46/67 [00:13<00:06,  3.33it/s, loss=2.46, v_num=1, train_loss_step=3.310, val_loss=3.150, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 30:  70%|█████▌  | 47/67 [00:14<00:06,  3.32it/s, loss=2.46, v_num=1, train_loss_step=3.310, val_loss=3.150, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 30:  72%|█████▋  | 48/67 [00:14<00:05,  3.32it/s, loss=2.46, v_num=1, train_loss_step=3.310, val_loss=3.150, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 30:  73%|█████▊  | 49/67 [00:14<00:05,  3.31it/s, loss=2.46, v_num=1, train_loss_step=3.310, val_loss=3.150, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 30:  75%|█████▉  | 50/67 [00:15<00:05,  3.31it/s, loss=2.46, v_num=1, train_loss_step=3.310, val_loss=3.150, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 30:  76%|██████  | 51/67 [00:15<00:04,  3.28it/s, loss=2.46, v_num=1, train_loss_step=3.310, val_loss=3.150, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 30:  78%|██████▏ | 52/67 [00:15<00:04,  3.27it/s, loss=2.46, v_num=1, train_loss_step=3.310, val_loss=3.150, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 30:  79%|██████▎ | 53/67 [00:16<00:04,  3.27it/s, loss=2.46, v_num=1, train_loss_step=3.310, val_loss=3.150, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 30:  81%|██████▍ | 54/67 [00:16<00:03,  3.27it/s, loss=2.46, v_num=1, train_loss_step=3.310, val_loss=3.150, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 30:  82%|██████▌ | 55/67 [00:16<00:03,  3.26it/s, loss=2.46, v_num=1, train_loss_step=3.310, val_loss=3.150, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 30:  84%|██████▋ | 56/67 [00:17<00:03,  3.26it/s, loss=2.46, v_num=1, train_loss_step=3.310, val_loss=3.150, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 30:  85%|██████▊ | 57/67 [00:17<00:03,  3.25it/s, loss=2.46, v_num=1, train_loss_step=3.310, val_loss=3.150, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 30:  87%|██████▉ | 58/67 [00:17<00:02,  3.25it/s, loss=2.46, v_num=1, train_loss_step=3.310, val_loss=3.150, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 30:  88%|███████ | 59/67 [00:18<00:02,  3.25it/s, loss=2.46, v_num=1, train_loss_step=3.310, val_loss=3.150, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 30:  90%|███████▏| 60/67 [00:18<00:02,  3.24it/s, loss=2.46, v_num=1, train_loss_step=3.310, val_loss=3.150, train_loss_epoch=2.230]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 30:  91%|███████▎| 61/67 [00:18<00:01,  3.22it/s, loss=2.46, v_num=1, train_loss_step=3.310, val_loss=3.150, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 30:  93%|███████▍| 62/67 [00:19<00:01,  3.22it/s, loss=2.46, v_num=1, train_loss_step=3.310, val_loss=3.150, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 30:  94%|███████▌| 63/67 [00:19<00:01,  3.22it/s, loss=2.46, v_num=1, train_loss_step=3.310, val_loss=3.150, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 30:  96%|███████▋| 64/67 [00:19<00:00,  3.22it/s, loss=2.46, v_num=1, train_loss_step=3.310, val_loss=3.150, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 30:  97%|███████▊| 65/67 [00:20<00:00,  3.22it/s, loss=2.46, v_num=1, train_loss_step=3.310, val_loss=3.150, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 30:  99%|███████▉| 66/67 [00:20<00:00,  3.22it/s, loss=2.46, v_num=1, train_loss_step=3.310, val_loss=3.150, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 30: 100%|████████| 67/67 [00:21<00:00,  3.16it/s, loss=2.46, v_num=1, train_loss_step=3.310, val_loss=3.150, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 31:  45%|███▌    | 30/67 [00:05<00:06,  5.44it/s, loss=2.15, v_num=1, train_loss_step=1.780, val_loss=3.150, train_loss_epoch=2.310]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 31:  46%|███▋    | 31/67 [00:07<00:08,  4.19it/s, loss=2.15, v_num=1, train_loss_step=1.780, val_loss=3.150, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 31:  48%|███▊    | 32/67 [00:07<00:08,  4.14it/s, loss=2.15, v_num=1, train_loss_step=1.780, val_loss=3.150, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 31:  49%|███▉    | 33/67 [00:08<00:08,  4.10it/s, loss=2.15, v_num=1, train_loss_step=1.780, val_loss=3.150, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 31:  51%|████    | 34/67 [00:08<00:08,  4.05it/s, loss=2.15, v_num=1, train_loss_step=1.780, val_loss=3.150, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 31:  52%|████▏   | 35/67 [00:08<00:07,  4.01it/s, loss=2.15, v_num=1, train_loss_step=1.780, val_loss=3.150, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 31:  54%|████▎   | 36/67 [00:09<00:07,  3.98it/s, loss=2.15, v_num=1, train_loss_step=1.780, val_loss=3.150, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 31:  55%|████▍   | 37/67 [00:09<00:07,  3.94it/s, loss=2.15, v_num=1, train_loss_step=1.780, val_loss=3.150, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 31:  57%|████▌   | 38/67 [00:09<00:07,  3.92it/s, loss=2.15, v_num=1, train_loss_step=1.780, val_loss=3.150, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 31:  58%|████▋   | 39/67 [00:10<00:07,  3.89it/s, loss=2.15, v_num=1, train_loss_step=1.780, val_loss=3.150, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 31:  60%|████▊   | 40/67 [00:10<00:06,  3.86it/s, loss=2.15, v_num=1, train_loss_step=1.780, val_loss=3.150, train_loss_epoch=2.310]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 31:  61%|████▉   | 41/67 [00:10<00:06,  3.80it/s, loss=2.15, v_num=1, train_loss_step=1.780, val_loss=3.150, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 31:  63%|█████   | 42/67 [00:11<00:06,  3.78it/s, loss=2.15, v_num=1, train_loss_step=1.780, val_loss=3.150, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 31:  64%|█████▏  | 43/67 [00:11<00:06,  3.75it/s, loss=2.15, v_num=1, train_loss_step=1.780, val_loss=3.150, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 31:  66%|█████▎  | 44/67 [00:11<00:06,  3.74it/s, loss=2.15, v_num=1, train_loss_step=1.780, val_loss=3.150, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 31:  67%|█████▎  | 45/67 [00:12<00:05,  3.72it/s, loss=2.15, v_num=1, train_loss_step=1.780, val_loss=3.150, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 31:  69%|█████▍  | 46/67 [00:12<00:05,  3.70it/s, loss=2.15, v_num=1, train_loss_step=1.780, val_loss=3.150, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 31:  70%|█████▌  | 47/67 [00:12<00:05,  3.68it/s, loss=2.15, v_num=1, train_loss_step=1.780, val_loss=3.150, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 31:  72%|█████▋  | 48/67 [00:13<00:05,  3.67it/s, loss=2.15, v_num=1, train_loss_step=1.780, val_loss=3.150, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 31:  73%|█████▊  | 49/67 [00:13<00:04,  3.65it/s, loss=2.15, v_num=1, train_loss_step=1.780, val_loss=3.150, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 31:  75%|█████▉  | 50/67 [00:13<00:04,  3.64it/s, loss=2.15, v_num=1, train_loss_step=1.780, val_loss=3.150, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 31:  76%|██████  | 51/67 [00:14<00:04,  3.59it/s, loss=2.15, v_num=1, train_loss_step=1.780, val_loss=3.150, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 31:  78%|██████▏ | 52/67 [00:14<00:04,  3.58it/s, loss=2.15, v_num=1, train_loss_step=1.780, val_loss=3.150, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 31:  79%|██████▎ | 53/67 [00:14<00:03,  3.57it/s, loss=2.15, v_num=1, train_loss_step=1.780, val_loss=3.150, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 31:  81%|██████▍ | 54/67 [00:15<00:03,  3.56it/s, loss=2.15, v_num=1, train_loss_step=1.780, val_loss=3.150, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 31:  82%|██████▌ | 55/67 [00:15<00:03,  3.54it/s, loss=2.15, v_num=1, train_loss_step=1.780, val_loss=3.150, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 31:  84%|██████▋ | 56/67 [00:15<00:03,  3.53it/s, loss=2.15, v_num=1, train_loss_step=1.780, val_loss=3.150, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 31:  85%|██████▊ | 57/67 [00:16<00:02,  3.52it/s, loss=2.15, v_num=1, train_loss_step=1.780, val_loss=3.150, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 31:  87%|██████▉ | 58/67 [00:16<00:02,  3.51it/s, loss=2.15, v_num=1, train_loss_step=1.780, val_loss=3.150, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 31:  88%|███████ | 59/67 [00:16<00:02,  3.50it/s, loss=2.15, v_num=1, train_loss_step=1.780, val_loss=3.150, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 31:  90%|███████▏| 60/67 [00:17<00:02,  3.49it/s, loss=2.15, v_num=1, train_loss_step=1.780, val_loss=3.150, train_loss_epoch=2.310]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 31:  91%|███████▎| 61/67 [00:17<00:01,  3.46it/s, loss=2.15, v_num=1, train_loss_step=1.780, val_loss=3.150, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 31:  93%|███████▍| 62/67 [00:17<00:01,  3.46it/s, loss=2.15, v_num=1, train_loss_step=1.780, val_loss=3.150, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 31:  94%|███████▌| 63/67 [00:18<00:01,  3.45it/s, loss=2.15, v_num=1, train_loss_step=1.780, val_loss=3.150, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 31:  96%|███████▋| 64/67 [00:18<00:00,  3.45it/s, loss=2.15, v_num=1, train_loss_step=1.780, val_loss=3.150, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 31:  97%|███████▊| 65/67 [00:18<00:00,  3.44it/s, loss=2.15, v_num=1, train_loss_step=1.780, val_loss=3.150, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 31:  99%|███████▉| 66/67 [00:19<00:00,  3.43it/s, loss=2.15, v_num=1, train_loss_step=1.780, val_loss=3.150, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 31: 100%|████████| 67/67 [00:19<00:00,  3.37it/s, loss=2.15, v_num=1, train_loss_step=1.780, val_loss=3.150, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 32:  45%|███▌    | 30/67 [00:05<00:06,  5.51it/s, loss=1.78, v_num=1, train_loss_step=1.570, val_loss=3.150, train_loss_epoch=2.180]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 32:  46%|███▋    | 31/67 [00:07<00:08,  4.22it/s, loss=1.78, v_num=1, train_loss_step=1.570, val_loss=3.150, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 32:  48%|███▊    | 32/67 [00:07<00:08,  4.17it/s, loss=1.78, v_num=1, train_loss_step=1.570, val_loss=3.150, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 32:  49%|███▉    | 33/67 [00:08<00:08,  4.12it/s, loss=1.78, v_num=1, train_loss_step=1.570, val_loss=3.150, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 32:  51%|████    | 34/67 [00:08<00:08,  4.07it/s, loss=1.78, v_num=1, train_loss_step=1.570, val_loss=3.150, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 32:  52%|████▏   | 35/67 [00:08<00:07,  4.04it/s, loss=1.78, v_num=1, train_loss_step=1.570, val_loss=3.150, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 32:  54%|████▎   | 36/67 [00:08<00:07,  4.01it/s, loss=1.78, v_num=1, train_loss_step=1.570, val_loss=3.150, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 32:  55%|████▍   | 37/67 [00:09<00:07,  3.97it/s, loss=1.78, v_num=1, train_loss_step=1.570, val_loss=3.150, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 32:  57%|████▌   | 38/67 [00:09<00:07,  3.94it/s, loss=1.78, v_num=1, train_loss_step=1.570, val_loss=3.150, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 32:  58%|████▋   | 39/67 [00:09<00:07,  3.92it/s, loss=1.78, v_num=1, train_loss_step=1.570, val_loss=3.150, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 32:  60%|████▊   | 40/67 [00:10<00:06,  3.87it/s, loss=1.78, v_num=1, train_loss_step=1.570, val_loss=3.150, train_loss_epoch=2.180]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 32:  61%|████▉   | 41/67 [00:10<00:06,  3.81it/s, loss=1.78, v_num=1, train_loss_step=1.570, val_loss=3.150, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 32:  63%|█████   | 42/67 [00:11<00:06,  3.79it/s, loss=1.78, v_num=1, train_loss_step=1.570, val_loss=3.150, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 32:  64%|█████▏  | 43/67 [00:11<00:06,  3.77it/s, loss=1.78, v_num=1, train_loss_step=1.570, val_loss=3.150, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 32:  66%|█████▎  | 44/67 [00:11<00:06,  3.75it/s, loss=1.78, v_num=1, train_loss_step=1.570, val_loss=3.150, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 32:  67%|█████▎  | 45/67 [00:12<00:05,  3.73it/s, loss=1.78, v_num=1, train_loss_step=1.570, val_loss=3.150, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 32:  69%|█████▍  | 46/67 [00:12<00:05,  3.71it/s, loss=1.78, v_num=1, train_loss_step=1.570, val_loss=3.150, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 32:  70%|█████▌  | 47/67 [00:12<00:05,  3.69it/s, loss=1.78, v_num=1, train_loss_step=1.570, val_loss=3.150, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 32:  72%|█████▋  | 48/67 [00:13<00:05,  3.67it/s, loss=1.78, v_num=1, train_loss_step=1.570, val_loss=3.150, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 32:  73%|█████▊  | 49/67 [00:13<00:04,  3.66it/s, loss=1.78, v_num=1, train_loss_step=1.570, val_loss=3.150, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 32:  75%|█████▉  | 50/67 [00:13<00:04,  3.64it/s, loss=1.78, v_num=1, train_loss_step=1.570, val_loss=3.150, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 32:  76%|██████  | 51/67 [00:14<00:04,  3.60it/s, loss=1.78, v_num=1, train_loss_step=1.570, val_loss=3.150, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 32:  78%|██████▏ | 52/67 [00:14<00:04,  3.59it/s, loss=1.78, v_num=1, train_loss_step=1.570, val_loss=3.150, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 32:  79%|██████▎ | 53/67 [00:14<00:03,  3.57it/s, loss=1.78, v_num=1, train_loss_step=1.570, val_loss=3.150, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 32:  81%|██████▍ | 54/67 [00:15<00:03,  3.56it/s, loss=1.78, v_num=1, train_loss_step=1.570, val_loss=3.150, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 32:  82%|██████▌ | 55/67 [00:15<00:03,  3.55it/s, loss=1.78, v_num=1, train_loss_step=1.570, val_loss=3.150, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 32:  84%|██████▋ | 56/67 [00:15<00:03,  3.54it/s, loss=1.78, v_num=1, train_loss_step=1.570, val_loss=3.150, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 32:  85%|██████▊ | 57/67 [00:16<00:02,  3.53it/s, loss=1.78, v_num=1, train_loss_step=1.570, val_loss=3.150, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 32:  87%|██████▉ | 58/67 [00:16<00:02,  3.52it/s, loss=1.78, v_num=1, train_loss_step=1.570, val_loss=3.150, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 32:  88%|███████ | 59/67 [00:16<00:02,  3.51it/s, loss=1.78, v_num=1, train_loss_step=1.570, val_loss=3.150, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 32:  90%|███████▏| 60/67 [00:17<00:01,  3.50it/s, loss=1.78, v_num=1, train_loss_step=1.570, val_loss=3.150, train_loss_epoch=2.180]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 32:  91%|███████▎| 61/67 [00:17<00:01,  3.47it/s, loss=1.78, v_num=1, train_loss_step=1.570, val_loss=3.150, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 32:  93%|███████▍| 62/67 [00:17<00:01,  3.46it/s, loss=1.78, v_num=1, train_loss_step=1.570, val_loss=3.150, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 32:  94%|███████▌| 63/67 [00:18<00:01,  3.46it/s, loss=1.78, v_num=1, train_loss_step=1.570, val_loss=3.150, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 32:  96%|███████▋| 64/67 [00:18<00:00,  3.45it/s, loss=1.78, v_num=1, train_loss_step=1.570, val_loss=3.150, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 32:  97%|███████▊| 65/67 [00:18<00:00,  3.44it/s, loss=1.78, v_num=1, train_loss_step=1.570, val_loss=3.150, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 32:  99%|███████▉| 66/67 [00:19<00:00,  3.44it/s, loss=1.78, v_num=1, train_loss_step=1.570, val_loss=3.150, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 32: 100%|████████| 67/67 [00:19<00:00,  3.37it/s, loss=1.78, v_num=1, train_loss_step=1.570, val_loss=3.150, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 33:  45%|███▌    | 30/67 [00:05<00:06,  5.49it/s, loss=2.07, v_num=1, train_loss_step=4.030, val_loss=3.150, train_loss_epoch=1.840]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 33:  46%|███▋    | 31/67 [00:07<00:08,  4.22it/s, loss=2.07, v_num=1, train_loss_step=4.030, val_loss=3.150, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 33:  48%|███▊    | 32/67 [00:07<00:08,  4.17it/s, loss=2.07, v_num=1, train_loss_step=4.030, val_loss=3.150, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 33:  49%|███▉    | 33/67 [00:08<00:08,  4.12it/s, loss=2.07, v_num=1, train_loss_step=4.030, val_loss=3.150, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 33:  51%|████    | 34/67 [00:08<00:08,  4.07it/s, loss=2.07, v_num=1, train_loss_step=4.030, val_loss=3.150, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 33:  52%|████▏   | 35/67 [00:08<00:07,  4.03it/s, loss=2.07, v_num=1, train_loss_step=4.030, val_loss=3.150, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 33:  54%|████▎   | 36/67 [00:09<00:07,  4.00it/s, loss=2.07, v_num=1, train_loss_step=4.030, val_loss=3.150, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 33:  55%|████▍   | 37/67 [00:09<00:07,  3.96it/s, loss=2.07, v_num=1, train_loss_step=4.030, val_loss=3.150, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 33:  57%|████▌   | 38/67 [00:09<00:07,  3.93it/s, loss=2.07, v_num=1, train_loss_step=4.030, val_loss=3.150, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 33:  58%|████▋   | 39/67 [00:09<00:07,  3.90it/s, loss=2.07, v_num=1, train_loss_step=4.030, val_loss=3.150, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 33:  60%|████▊   | 40/67 [00:10<00:06,  3.87it/s, loss=2.07, v_num=1, train_loss_step=4.030, val_loss=3.150, train_loss_epoch=1.840]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 33:  61%|████▉   | 41/67 [00:10<00:06,  3.80it/s, loss=2.07, v_num=1, train_loss_step=4.030, val_loss=3.150, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 33:  63%|█████   | 42/67 [00:11<00:06,  3.78it/s, loss=2.07, v_num=1, train_loss_step=4.030, val_loss=3.150, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 33:  64%|█████▏  | 43/67 [00:11<00:06,  3.76it/s, loss=2.07, v_num=1, train_loss_step=4.030, val_loss=3.150, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 33:  66%|█████▎  | 44/67 [00:11<00:06,  3.74it/s, loss=2.07, v_num=1, train_loss_step=4.030, val_loss=3.150, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 33:  67%|█████▎  | 45/67 [00:12<00:05,  3.72it/s, loss=2.07, v_num=1, train_loss_step=4.030, val_loss=3.150, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 33:  69%|█████▍  | 46/67 [00:12<00:05,  3.70it/s, loss=2.07, v_num=1, train_loss_step=4.030, val_loss=3.150, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 33:  70%|█████▌  | 47/67 [00:12<00:05,  3.68it/s, loss=2.07, v_num=1, train_loss_step=4.030, val_loss=3.150, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 33:  72%|█████▋  | 48/67 [00:13<00:05,  3.67it/s, loss=2.07, v_num=1, train_loss_step=4.030, val_loss=3.150, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 33:  73%|█████▊  | 49/67 [00:13<00:04,  3.65it/s, loss=2.07, v_num=1, train_loss_step=4.030, val_loss=3.150, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 33:  75%|█████▉  | 50/67 [00:13<00:04,  3.63it/s, loss=2.07, v_num=1, train_loss_step=4.030, val_loss=3.150, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 33:  76%|██████  | 51/67 [00:14<00:04,  3.59it/s, loss=2.07, v_num=1, train_loss_step=4.030, val_loss=3.150, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 33:  78%|██████▏ | 52/67 [00:14<00:04,  3.58it/s, loss=2.07, v_num=1, train_loss_step=4.030, val_loss=3.150, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 33:  79%|██████▎ | 53/67 [00:14<00:03,  3.56it/s, loss=2.07, v_num=1, train_loss_step=4.030, val_loss=3.150, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 33:  81%|██████▍ | 54/67 [00:15<00:03,  3.55it/s, loss=2.07, v_num=1, train_loss_step=4.030, val_loss=3.150, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 33:  82%|██████▌ | 55/67 [00:15<00:03,  3.54it/s, loss=2.07, v_num=1, train_loss_step=4.030, val_loss=3.150, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 33:  84%|██████▋ | 56/67 [00:15<00:03,  3.53it/s, loss=2.07, v_num=1, train_loss_step=4.030, val_loss=3.150, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 33:  85%|██████▊ | 57/67 [00:16<00:02,  3.52it/s, loss=2.07, v_num=1, train_loss_step=4.030, val_loss=3.150, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 33:  87%|██████▉ | 58/67 [00:16<00:02,  3.51it/s, loss=2.07, v_num=1, train_loss_step=4.030, val_loss=3.150, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 33:  88%|███████ | 59/67 [00:16<00:02,  3.50it/s, loss=2.07, v_num=1, train_loss_step=4.030, val_loss=3.150, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 33:  90%|███████▏| 60/67 [00:17<00:02,  3.49it/s, loss=2.07, v_num=1, train_loss_step=4.030, val_loss=3.150, train_loss_epoch=1.840]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 33:  91%|███████▎| 61/67 [00:17<00:01,  3.46it/s, loss=2.07, v_num=1, train_loss_step=4.030, val_loss=3.150, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 33:  93%|███████▍| 62/67 [00:17<00:01,  3.45it/s, loss=2.07, v_num=1, train_loss_step=4.030, val_loss=3.150, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 33:  94%|███████▌| 63/67 [00:18<00:01,  3.45it/s, loss=2.07, v_num=1, train_loss_step=4.030, val_loss=3.150, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 33:  96%|███████▋| 64/67 [00:18<00:00,  3.43it/s, loss=2.07, v_num=1, train_loss_step=4.030, val_loss=3.150, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 33:  97%|███████▊| 65/67 [00:18<00:00,  3.43it/s, loss=2.07, v_num=1, train_loss_step=4.030, val_loss=3.150, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 33:  99%|███████▉| 66/67 [00:19<00:00,  3.42it/s, loss=2.07, v_num=1, train_loss_step=4.030, val_loss=3.150, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 33: 100%|████████| 67/67 [00:19<00:00,  3.35it/s, loss=2.07, v_num=1, train_loss_step=4.030, val_loss=3.150, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 34:  45%|███▌    | 30/67 [00:05<00:06,  5.42it/s, loss=2.98, v_num=1, train_loss_step=3.220, val_loss=3.150, train_loss_epoch=2.050]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 34:  46%|███▋    | 31/67 [00:07<00:08,  4.18it/s, loss=2.98, v_num=1, train_loss_step=3.220, val_loss=3.150, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 34:  48%|███▊    | 32/67 [00:07<00:08,  4.13it/s, loss=2.98, v_num=1, train_loss_step=3.220, val_loss=3.150, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 34:  49%|███▉    | 33/67 [00:08<00:08,  4.08it/s, loss=2.98, v_num=1, train_loss_step=3.220, val_loss=3.150, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 34:  51%|████    | 34/67 [00:08<00:08,  4.05it/s, loss=2.98, v_num=1, train_loss_step=3.220, val_loss=3.150, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 34:  52%|████▏   | 35/67 [00:08<00:07,  4.00it/s, loss=2.98, v_num=1, train_loss_step=3.220, val_loss=3.150, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 34:  54%|████▎   | 36/67 [00:09<00:07,  3.97it/s, loss=2.98, v_num=1, train_loss_step=3.220, val_loss=3.150, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 34:  55%|████▍   | 37/67 [00:09<00:07,  3.94it/s, loss=2.98, v_num=1, train_loss_step=3.220, val_loss=3.150, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 34:  57%|████▌   | 38/67 [00:09<00:07,  3.91it/s, loss=2.98, v_num=1, train_loss_step=3.220, val_loss=3.150, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 34:  58%|████▋   | 39/67 [00:10<00:07,  3.88it/s, loss=2.98, v_num=1, train_loss_step=3.220, val_loss=3.150, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 34:  60%|████▊   | 40/67 [00:10<00:07,  3.85it/s, loss=2.98, v_num=1, train_loss_step=3.220, val_loss=3.150, train_loss_epoch=2.050]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 34:  61%|████▉   | 41/67 [00:10<00:06,  3.79it/s, loss=2.98, v_num=1, train_loss_step=3.220, val_loss=3.150, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 34:  63%|█████   | 42/67 [00:11<00:06,  3.77it/s, loss=2.98, v_num=1, train_loss_step=3.220, val_loss=3.150, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 34:  64%|█████▏  | 43/67 [00:11<00:06,  3.74it/s, loss=2.98, v_num=1, train_loss_step=3.220, val_loss=3.150, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 34:  66%|█████▎  | 44/67 [00:11<00:06,  3.72it/s, loss=2.98, v_num=1, train_loss_step=3.220, val_loss=3.150, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 34:  67%|█████▎  | 45/67 [00:12<00:05,  3.71it/s, loss=2.98, v_num=1, train_loss_step=3.220, val_loss=3.150, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 34:  69%|█████▍  | 46/67 [00:12<00:05,  3.68it/s, loss=2.98, v_num=1, train_loss_step=3.220, val_loss=3.150, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 34:  70%|█████▌  | 47/67 [00:12<00:05,  3.67it/s, loss=2.98, v_num=1, train_loss_step=3.220, val_loss=3.150, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 34:  72%|█████▋  | 48/67 [00:13<00:05,  3.66it/s, loss=2.98, v_num=1, train_loss_step=3.220, val_loss=3.150, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 34:  73%|█████▊  | 49/67 [00:13<00:04,  3.64it/s, loss=2.98, v_num=1, train_loss_step=3.220, val_loss=3.150, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 34:  75%|█████▉  | 50/67 [00:13<00:04,  3.62it/s, loss=2.98, v_num=1, train_loss_step=3.220, val_loss=3.150, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 34:  76%|██████  | 51/67 [00:14<00:04,  3.58it/s, loss=2.98, v_num=1, train_loss_step=3.220, val_loss=3.150, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 34:  78%|██████▏ | 52/67 [00:14<00:04,  3.56it/s, loss=2.98, v_num=1, train_loss_step=3.220, val_loss=3.150, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 34:  79%|██████▎ | 53/67 [00:14<00:03,  3.55it/s, loss=2.98, v_num=1, train_loss_step=3.220, val_loss=3.150, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 34:  81%|██████▍ | 54/67 [00:15<00:03,  3.54it/s, loss=2.98, v_num=1, train_loss_step=3.220, val_loss=3.150, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 34:  82%|██████▌ | 55/67 [00:15<00:03,  3.52it/s, loss=2.98, v_num=1, train_loss_step=3.220, val_loss=3.150, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 34:  84%|██████▋ | 56/67 [00:15<00:03,  3.52it/s, loss=2.98, v_num=1, train_loss_step=3.220, val_loss=3.150, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 34:  85%|██████▊ | 57/67 [00:16<00:02,  3.51it/s, loss=2.98, v_num=1, train_loss_step=3.220, val_loss=3.150, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 34:  87%|██████▉ | 58/67 [00:16<00:02,  3.50it/s, loss=2.98, v_num=1, train_loss_step=3.220, val_loss=3.150, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 34:  88%|███████ | 59/67 [00:16<00:02,  3.49it/s, loss=2.98, v_num=1, train_loss_step=3.220, val_loss=3.150, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 34:  90%|███████▏| 60/67 [00:17<00:02,  3.48it/s, loss=2.98, v_num=1, train_loss_step=3.220, val_loss=3.150, train_loss_epoch=2.050]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 34:  91%|███████▎| 61/67 [00:17<00:01,  3.45it/s, loss=2.98, v_num=1, train_loss_step=3.220, val_loss=3.150, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 34:  93%|███████▍| 62/67 [00:18<00:01,  3.44it/s, loss=2.98, v_num=1, train_loss_step=3.220, val_loss=3.150, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 34:  94%|███████▌| 63/67 [00:18<00:01,  3.43it/s, loss=2.98, v_num=1, train_loss_step=3.220, val_loss=3.150, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 34:  96%|███████▋| 64/67 [00:18<00:00,  3.43it/s, loss=2.98, v_num=1, train_loss_step=3.220, val_loss=3.150, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 34:  97%|███████▊| 65/67 [00:19<00:00,  3.42it/s, loss=2.98, v_num=1, train_loss_step=3.220, val_loss=3.150, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 34:  99%|███████▉| 66/67 [00:19<00:00,  3.41it/s, loss=2.98, v_num=1, train_loss_step=3.220, val_loss=3.150, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 34: 100%|████████| 67/67 [00:20<00:00,  3.34it/s, loss=2.98, v_num=1, train_loss_step=3.220, val_loss=3.150, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 35:  45%|███▌    | 30/67 [00:05<00:06,  5.38it/s, loss=2.55, v_num=1, train_loss_step=2.880, val_loss=3.150, train_loss_epoch=2.610]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 35:  46%|███▋    | 31/67 [00:07<00:08,  4.07it/s, loss=2.55, v_num=1, train_loss_step=2.880, val_loss=3.150, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 35:  48%|███▊    | 32/67 [00:07<00:08,  4.03it/s, loss=2.55, v_num=1, train_loss_step=2.880, val_loss=3.150, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 35:  49%|███▉    | 33/67 [00:08<00:08,  3.99it/s, loss=2.55, v_num=1, train_loss_step=2.880, val_loss=3.150, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 35:  51%|████    | 34/67 [00:08<00:08,  3.95it/s, loss=2.55, v_num=1, train_loss_step=2.880, val_loss=3.150, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 35:  52%|████▏   | 35/67 [00:08<00:08,  3.92it/s, loss=2.55, v_num=1, train_loss_step=2.880, val_loss=3.150, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 35:  54%|████▎   | 36/67 [00:09<00:07,  3.89it/s, loss=2.55, v_num=1, train_loss_step=2.880, val_loss=3.150, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 35:  55%|████▍   | 37/67 [00:09<00:07,  3.85it/s, loss=2.55, v_num=1, train_loss_step=2.880, val_loss=3.150, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 35:  57%|████▌   | 38/67 [00:09<00:07,  3.82it/s, loss=2.55, v_num=1, train_loss_step=2.880, val_loss=3.150, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 35:  58%|████▋   | 39/67 [00:10<00:07,  3.80it/s, loss=2.55, v_num=1, train_loss_step=2.880, val_loss=3.150, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 35:  60%|████▊   | 40/67 [00:10<00:07,  3.77it/s, loss=2.55, v_num=1, train_loss_step=2.880, val_loss=3.150, train_loss_epoch=2.610]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 35:  61%|████▉   | 41/67 [00:11<00:06,  3.72it/s, loss=2.55, v_num=1, train_loss_step=2.880, val_loss=3.150, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 35:  63%|█████   | 42/67 [00:11<00:06,  3.70it/s, loss=2.55, v_num=1, train_loss_step=2.880, val_loss=3.150, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 35:  64%|█████▏  | 43/67 [00:11<00:06,  3.67it/s, loss=2.55, v_num=1, train_loss_step=2.880, val_loss=3.150, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 35:  66%|█████▎  | 44/67 [00:12<00:06,  3.65it/s, loss=2.55, v_num=1, train_loss_step=2.880, val_loss=3.150, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 35:  67%|█████▎  | 45/67 [00:12<00:06,  3.64it/s, loss=2.55, v_num=1, train_loss_step=2.880, val_loss=3.150, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 35:  69%|█████▍  | 46/67 [00:12<00:05,  3.62it/s, loss=2.55, v_num=1, train_loss_step=2.880, val_loss=3.150, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 35:  70%|█████▌  | 47/67 [00:13<00:05,  3.60it/s, loss=2.55, v_num=1, train_loss_step=2.880, val_loss=3.150, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 35:  72%|█████▋  | 48/67 [00:13<00:05,  3.59it/s, loss=2.55, v_num=1, train_loss_step=2.880, val_loss=3.150, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 35:  73%|█████▊  | 49/67 [00:13<00:05,  3.56it/s, loss=2.55, v_num=1, train_loss_step=2.880, val_loss=3.150, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 35:  75%|█████▉  | 50/67 [00:14<00:04,  3.55it/s, loss=2.55, v_num=1, train_loss_step=2.880, val_loss=3.150, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 35:  76%|██████  | 51/67 [00:14<00:04,  3.51it/s, loss=2.55, v_num=1, train_loss_step=2.880, val_loss=3.150, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 35:  78%|██████▏ | 52/67 [00:14<00:04,  3.50it/s, loss=2.55, v_num=1, train_loss_step=2.880, val_loss=3.150, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 35:  79%|██████▎ | 53/67 [00:15<00:04,  3.48it/s, loss=2.55, v_num=1, train_loss_step=2.880, val_loss=3.150, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 35:  81%|██████▍ | 54/67 [00:15<00:03,  3.47it/s, loss=2.55, v_num=1, train_loss_step=2.880, val_loss=3.150, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 35:  82%|██████▌ | 55/67 [00:15<00:03,  3.46it/s, loss=2.55, v_num=1, train_loss_step=2.880, val_loss=3.150, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 35:  84%|██████▋ | 56/67 [00:16<00:03,  3.45it/s, loss=2.55, v_num=1, train_loss_step=2.880, val_loss=3.150, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 35:  85%|██████▊ | 57/67 [00:16<00:02,  3.44it/s, loss=2.55, v_num=1, train_loss_step=2.880, val_loss=3.150, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 35:  87%|██████▉ | 58/67 [00:16<00:02,  3.43it/s, loss=2.55, v_num=1, train_loss_step=2.880, val_loss=3.150, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 35:  88%|███████ | 59/67 [00:17<00:02,  3.42it/s, loss=2.55, v_num=1, train_loss_step=2.880, val_loss=3.150, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 35:  90%|███████▏| 60/67 [00:17<00:02,  3.42it/s, loss=2.55, v_num=1, train_loss_step=2.880, val_loss=3.150, train_loss_epoch=2.610]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 35:  91%|███████▎| 61/67 [00:18<00:01,  3.38it/s, loss=2.55, v_num=1, train_loss_step=2.880, val_loss=3.150, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 35:  93%|███████▍| 62/67 [00:18<00:01,  3.38it/s, loss=2.55, v_num=1, train_loss_step=2.880, val_loss=3.150, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 35:  94%|███████▌| 63/67 [00:18<00:01,  3.37it/s, loss=2.55, v_num=1, train_loss_step=2.880, val_loss=3.150, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 35:  96%|███████▋| 64/67 [00:19<00:00,  3.36it/s, loss=2.55, v_num=1, train_loss_step=2.880, val_loss=3.150, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 35:  97%|███████▊| 65/67 [00:19<00:00,  3.36it/s, loss=2.55, v_num=1, train_loss_step=2.880, val_loss=3.150, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 35:  99%|███████▉| 66/67 [00:19<00:00,  3.36it/s, loss=2.55, v_num=1, train_loss_step=2.880, val_loss=3.150, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 35: 100%|████████| 67/67 [00:20<00:00,  3.30it/s, loss=2.55, v_num=1, train_loss_step=2.880, val_loss=3.140, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 36:  45%|███▌    | 30/67 [00:05<00:06,  5.46it/s, loss=2.35, v_num=1, train_loss_step=1.100, val_loss=3.140, train_loss_epoch=2.360]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 36:  46%|███▋    | 31/67 [00:07<00:08,  4.17it/s, loss=2.35, v_num=1, train_loss_step=1.100, val_loss=3.140, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 36:  48%|███▊    | 32/67 [00:07<00:08,  4.12it/s, loss=2.35, v_num=1, train_loss_step=1.100, val_loss=3.140, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 36:  49%|███▉    | 33/67 [00:08<00:08,  4.08it/s, loss=2.35, v_num=1, train_loss_step=1.100, val_loss=3.140, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 36:  51%|████    | 34/67 [00:08<00:08,  4.04it/s, loss=2.35, v_num=1, train_loss_step=1.100, val_loss=3.140, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 36:  52%|████▏   | 35/67 [00:08<00:07,  4.00it/s, loss=2.35, v_num=1, train_loss_step=1.100, val_loss=3.140, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 36:  54%|████▎   | 36/67 [00:09<00:07,  3.97it/s, loss=2.35, v_num=1, train_loss_step=1.100, val_loss=3.140, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 36:  55%|████▍   | 37/67 [00:09<00:07,  3.93it/s, loss=2.35, v_num=1, train_loss_step=1.100, val_loss=3.140, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 36:  57%|████▌   | 38/67 [00:09<00:07,  3.90it/s, loss=2.35, v_num=1, train_loss_step=1.100, val_loss=3.140, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 36:  58%|████▋   | 39/67 [00:10<00:07,  3.87it/s, loss=2.35, v_num=1, train_loss_step=1.100, val_loss=3.140, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 36:  60%|████▊   | 40/67 [00:10<00:07,  3.83it/s, loss=2.35, v_num=1, train_loss_step=1.100, val_loss=3.140, train_loss_epoch=2.360]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 36:  61%|████▉   | 41/67 [00:10<00:06,  3.77it/s, loss=2.35, v_num=1, train_loss_step=1.100, val_loss=3.140, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 36:  63%|█████   | 42/67 [00:11<00:06,  3.75it/s, loss=2.35, v_num=1, train_loss_step=1.100, val_loss=3.140, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 36:  64%|█████▏  | 43/67 [00:11<00:06,  3.72it/s, loss=2.35, v_num=1, train_loss_step=1.100, val_loss=3.140, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 36:  66%|█████▎  | 44/67 [00:11<00:06,  3.71it/s, loss=2.35, v_num=1, train_loss_step=1.100, val_loss=3.140, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 36:  67%|█████▎  | 45/67 [00:12<00:05,  3.69it/s, loss=2.35, v_num=1, train_loss_step=1.100, val_loss=3.140, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 36:  69%|█████▍  | 46/67 [00:12<00:05,  3.67it/s, loss=2.35, v_num=1, train_loss_step=1.100, val_loss=3.140, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 36:  70%|█████▌  | 47/67 [00:12<00:05,  3.65it/s, loss=2.35, v_num=1, train_loss_step=1.100, val_loss=3.140, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 36:  72%|█████▋  | 48/67 [00:13<00:05,  3.64it/s, loss=2.35, v_num=1, train_loss_step=1.100, val_loss=3.140, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 36:  73%|█████▊  | 49/67 [00:13<00:04,  3.62it/s, loss=2.35, v_num=1, train_loss_step=1.100, val_loss=3.140, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 36:  75%|█████▉  | 50/67 [00:13<00:04,  3.61it/s, loss=2.35, v_num=1, train_loss_step=1.100, val_loss=3.140, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 36:  76%|██████  | 51/67 [00:14<00:04,  3.56it/s, loss=2.35, v_num=1, train_loss_step=1.100, val_loss=3.140, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 36:  78%|██████▏ | 52/67 [00:14<00:04,  3.54it/s, loss=2.35, v_num=1, train_loss_step=1.100, val_loss=3.140, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 36:  79%|██████▎ | 53/67 [00:15<00:03,  3.53it/s, loss=2.35, v_num=1, train_loss_step=1.100, val_loss=3.140, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 36:  81%|██████▍ | 54/67 [00:15<00:03,  3.52it/s, loss=2.35, v_num=1, train_loss_step=1.100, val_loss=3.140, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 36:  82%|██████▌ | 55/67 [00:15<00:03,  3.51it/s, loss=2.35, v_num=1, train_loss_step=1.100, val_loss=3.140, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 36:  84%|██████▋ | 56/67 [00:16<00:03,  3.50it/s, loss=2.35, v_num=1, train_loss_step=1.100, val_loss=3.140, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 36:  85%|██████▊ | 57/67 [00:16<00:02,  3.49it/s, loss=2.35, v_num=1, train_loss_step=1.100, val_loss=3.140, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 36:  87%|██████▉ | 58/67 [00:16<00:02,  3.47it/s, loss=2.35, v_num=1, train_loss_step=1.100, val_loss=3.140, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 36:  88%|███████ | 59/67 [00:17<00:02,  3.46it/s, loss=2.35, v_num=1, train_loss_step=1.100, val_loss=3.140, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 36:  90%|███████▏| 60/67 [00:18<00:02,  3.32it/s, loss=2.35, v_num=1, train_loss_step=1.100, val_loss=3.140, train_loss_epoch=2.360]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 36:  91%|███████▎| 61/67 [00:18<00:01,  3.29it/s, loss=2.35, v_num=1, train_loss_step=1.100, val_loss=3.140, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 36:  93%|███████▍| 62/67 [00:18<00:01,  3.29it/s, loss=2.35, v_num=1, train_loss_step=1.100, val_loss=3.140, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 36:  94%|███████▌| 63/67 [00:19<00:01,  3.28it/s, loss=2.35, v_num=1, train_loss_step=1.100, val_loss=3.140, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 36:  96%|███████▋| 64/67 [00:19<00:00,  3.28it/s, loss=2.35, v_num=1, train_loss_step=1.100, val_loss=3.140, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 36:  97%|███████▊| 65/67 [00:19<00:00,  3.28it/s, loss=2.35, v_num=1, train_loss_step=1.100, val_loss=3.140, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 36:  99%|███████▉| 66/67 [00:20<00:00,  3.27it/s, loss=2.35, v_num=1, train_loss_step=1.100, val_loss=3.140, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 36: 100%|████████| 67/67 [00:20<00:00,  3.21it/s, loss=2.35, v_num=1, train_loss_step=1.100, val_loss=3.140, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 37:  45%|███▌    | 30/67 [00:05<00:06,  5.40it/s, loss=2.02, v_num=1, train_loss_step=1.880, val_loss=3.140, train_loss_epoch=2.380]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 37:  46%|███▋    | 31/67 [00:07<00:08,  4.15it/s, loss=2.02, v_num=1, train_loss_step=1.880, val_loss=3.140, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 37:  48%|███▊    | 32/67 [00:07<00:08,  4.10it/s, loss=2.02, v_num=1, train_loss_step=1.880, val_loss=3.140, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 37:  49%|███▉    | 33/67 [00:08<00:08,  4.06it/s, loss=2.02, v_num=1, train_loss_step=1.880, val_loss=3.140, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 37:  51%|████    | 34/67 [00:08<00:08,  4.02it/s, loss=2.02, v_num=1, train_loss_step=1.880, val_loss=3.140, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 37:  52%|████▏   | 35/67 [00:08<00:08,  3.98it/s, loss=2.02, v_num=1, train_loss_step=1.880, val_loss=3.140, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 37:  54%|████▎   | 36/67 [00:09<00:07,  3.94it/s, loss=2.02, v_num=1, train_loss_step=1.880, val_loss=3.140, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 37:  55%|████▍   | 37/67 [00:09<00:07,  3.91it/s, loss=2.02, v_num=1, train_loss_step=1.880, val_loss=3.140, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 37:  57%|████▌   | 38/67 [00:09<00:07,  3.87it/s, loss=2.02, v_num=1, train_loss_step=1.880, val_loss=3.140, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 37:  58%|████▋   | 39/67 [00:10<00:07,  3.85it/s, loss=2.02, v_num=1, train_loss_step=1.880, val_loss=3.140, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 37:  60%|████▊   | 40/67 [00:10<00:07,  3.82it/s, loss=2.02, v_num=1, train_loss_step=1.880, val_loss=3.140, train_loss_epoch=2.380]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 37:  61%|████▉   | 41/67 [00:10<00:06,  3.76it/s, loss=2.02, v_num=1, train_loss_step=1.880, val_loss=3.140, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 37:  63%|█████   | 42/67 [00:11<00:06,  3.74it/s, loss=2.02, v_num=1, train_loss_step=1.880, val_loss=3.140, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 37:  64%|█████▏  | 43/67 [00:11<00:06,  3.72it/s, loss=2.02, v_num=1, train_loss_step=1.880, val_loss=3.140, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 37:  66%|█████▎  | 44/67 [00:11<00:06,  3.70it/s, loss=2.02, v_num=1, train_loss_step=1.880, val_loss=3.140, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 37:  67%|█████▎  | 45/67 [00:12<00:05,  3.68it/s, loss=2.02, v_num=1, train_loss_step=1.880, val_loss=3.140, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 37:  69%|█████▍  | 46/67 [00:12<00:05,  3.66it/s, loss=2.02, v_num=1, train_loss_step=1.880, val_loss=3.140, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 37:  70%|█████▌  | 47/67 [00:12<00:05,  3.64it/s, loss=2.02, v_num=1, train_loss_step=1.880, val_loss=3.140, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 37:  72%|█████▋  | 48/67 [00:13<00:05,  3.63it/s, loss=2.02, v_num=1, train_loss_step=1.880, val_loss=3.140, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 37:  73%|█████▊  | 49/67 [00:13<00:04,  3.61it/s, loss=2.02, v_num=1, train_loss_step=1.880, val_loss=3.140, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 37:  75%|█████▉  | 50/67 [00:13<00:04,  3.60it/s, loss=2.02, v_num=1, train_loss_step=1.880, val_loss=3.140, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 37:  76%|██████  | 51/67 [00:15<00:04,  3.38it/s, loss=2.02, v_num=1, train_loss_step=1.880, val_loss=3.140, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 37:  78%|██████▏ | 52/67 [00:15<00:04,  3.38it/s, loss=2.02, v_num=1, train_loss_step=1.880, val_loss=3.140, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 37:  79%|██████▎ | 53/67 [00:15<00:04,  3.37it/s, loss=2.02, v_num=1, train_loss_step=1.880, val_loss=3.140, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 37:  81%|██████▍ | 54/67 [00:16<00:03,  3.36it/s, loss=2.02, v_num=1, train_loss_step=1.880, val_loss=3.140, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 37:  82%|██████▌ | 55/67 [00:16<00:03,  3.35it/s, loss=2.02, v_num=1, train_loss_step=1.880, val_loss=3.140, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 37:  84%|██████▋ | 56/67 [00:16<00:03,  3.35it/s, loss=2.02, v_num=1, train_loss_step=1.880, val_loss=3.140, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 37:  85%|██████▊ | 57/67 [00:17<00:02,  3.34it/s, loss=2.02, v_num=1, train_loss_step=1.880, val_loss=3.140, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 37:  87%|██████▉ | 58/67 [00:17<00:02,  3.33it/s, loss=2.02, v_num=1, train_loss_step=1.880, val_loss=3.140, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 37:  88%|███████ | 59/67 [00:17<00:02,  3.33it/s, loss=2.02, v_num=1, train_loss_step=1.880, val_loss=3.140, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 37:  90%|███████▏| 60/67 [00:18<00:02,  3.32it/s, loss=2.02, v_num=1, train_loss_step=1.880, val_loss=3.140, train_loss_epoch=2.380]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 37:  91%|███████▎| 61/67 [00:18<00:01,  3.30it/s, loss=2.02, v_num=1, train_loss_step=1.880, val_loss=3.140, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 37:  93%|███████▍| 62/67 [00:18<00:01,  3.29it/s, loss=2.02, v_num=1, train_loss_step=1.880, val_loss=3.140, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 37:  94%|███████▌| 63/67 [00:19<00:01,  3.29it/s, loss=2.02, v_num=1, train_loss_step=1.880, val_loss=3.140, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 37:  96%|███████▋| 64/67 [00:19<00:00,  3.28it/s, loss=2.02, v_num=1, train_loss_step=1.880, val_loss=3.140, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 37:  97%|███████▊| 65/67 [00:19<00:00,  3.28it/s, loss=2.02, v_num=1, train_loss_step=1.880, val_loss=3.140, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 37:  99%|███████▉| 66/67 [00:20<00:00,  3.28it/s, loss=2.02, v_num=1, train_loss_step=1.880, val_loss=3.140, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 37: 100%|████████| 67/67 [00:20<00:00,  3.22it/s, loss=2.02, v_num=1, train_loss_step=1.880, val_loss=3.140, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 38:  45%|███▌    | 30/67 [00:05<00:07,  5.26it/s, loss=2.51, v_num=1, train_loss_step=1.420, val_loss=3.140, train_loss_epoch=2.000]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 38:  46%|███▋    | 31/67 [00:07<00:08,  4.04it/s, loss=2.51, v_num=1, train_loss_step=1.420, val_loss=3.140, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 38:  48%|███▊    | 32/67 [00:07<00:08,  4.00it/s, loss=2.51, v_num=1, train_loss_step=1.420, val_loss=3.140, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 38:  49%|███▉    | 33/67 [00:08<00:08,  3.97it/s, loss=2.51, v_num=1, train_loss_step=1.420, val_loss=3.140, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 38:  51%|████    | 34/67 [00:08<00:08,  3.93it/s, loss=2.51, v_num=1, train_loss_step=1.420, val_loss=3.140, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 38:  52%|████▏   | 35/67 [00:08<00:08,  3.90it/s, loss=2.51, v_num=1, train_loss_step=1.420, val_loss=3.140, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 38:  54%|████▎   | 36/67 [00:09<00:08,  3.86it/s, loss=2.51, v_num=1, train_loss_step=1.420, val_loss=3.140, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 38:  55%|████▍   | 37/67 [00:09<00:07,  3.82it/s, loss=2.51, v_num=1, train_loss_step=1.420, val_loss=3.140, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 38:  57%|████▌   | 38/67 [00:10<00:07,  3.80it/s, loss=2.51, v_num=1, train_loss_step=1.420, val_loss=3.140, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 38:  58%|████▋   | 39/67 [00:10<00:07,  3.77it/s, loss=2.51, v_num=1, train_loss_step=1.420, val_loss=3.140, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 38:  60%|████▊   | 40/67 [00:10<00:07,  3.74it/s, loss=2.51, v_num=1, train_loss_step=1.420, val_loss=3.140, train_loss_epoch=2.000]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 38:  61%|████▉   | 41/67 [00:11<00:07,  3.67it/s, loss=2.51, v_num=1, train_loss_step=1.420, val_loss=3.140, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 38:  63%|█████   | 42/67 [00:11<00:06,  3.65it/s, loss=2.51, v_num=1, train_loss_step=1.420, val_loss=3.140, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 38:  64%|█████▏  | 43/67 [00:12<00:06,  3.45it/s, loss=2.51, v_num=1, train_loss_step=1.420, val_loss=3.140, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 38:  66%|█████▎  | 44/67 [00:12<00:06,  3.44it/s, loss=2.51, v_num=1, train_loss_step=1.420, val_loss=3.140, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 38:  67%|█████▎  | 45/67 [00:13<00:06,  3.43it/s, loss=2.51, v_num=1, train_loss_step=1.420, val_loss=3.140, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 38:  69%|█████▍  | 46/67 [00:13<00:06,  3.41it/s, loss=2.51, v_num=1, train_loss_step=1.420, val_loss=3.140, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 38:  70%|█████▌  | 47/67 [00:13<00:05,  3.41it/s, loss=2.51, v_num=1, train_loss_step=1.420, val_loss=3.140, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 38:  72%|█████▋  | 48/67 [00:14<00:05,  3.40it/s, loss=2.51, v_num=1, train_loss_step=1.420, val_loss=3.140, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 38:  73%|█████▊  | 49/67 [00:14<00:05,  3.39it/s, loss=2.51, v_num=1, train_loss_step=1.420, val_loss=3.140, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 38:  75%|█████▉  | 50/67 [00:14<00:05,  3.38it/s, loss=2.51, v_num=1, train_loss_step=1.420, val_loss=3.140, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 38:  76%|██████  | 51/67 [00:15<00:04,  3.35it/s, loss=2.51, v_num=1, train_loss_step=1.420, val_loss=3.140, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 38:  78%|██████▏ | 52/67 [00:15<00:04,  3.34it/s, loss=2.51, v_num=1, train_loss_step=1.420, val_loss=3.140, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 38:  79%|██████▎ | 53/67 [00:15<00:04,  3.34it/s, loss=2.51, v_num=1, train_loss_step=1.420, val_loss=3.140, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 38:  81%|██████▍ | 54/67 [00:16<00:03,  3.33it/s, loss=2.51, v_num=1, train_loss_step=1.420, val_loss=3.140, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 38:  82%|██████▌ | 55/67 [00:16<00:03,  3.32it/s, loss=2.51, v_num=1, train_loss_step=1.420, val_loss=3.140, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 38:  84%|██████▋ | 56/67 [00:16<00:03,  3.32it/s, loss=2.51, v_num=1, train_loss_step=1.420, val_loss=3.140, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 38:  85%|██████▊ | 57/67 [00:17<00:03,  3.32it/s, loss=2.51, v_num=1, train_loss_step=1.420, val_loss=3.140, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 38:  87%|██████▉ | 58/67 [00:17<00:02,  3.31it/s, loss=2.51, v_num=1, train_loss_step=1.420, val_loss=3.140, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 38:  88%|███████ | 59/67 [00:17<00:02,  3.30it/s, loss=2.51, v_num=1, train_loss_step=1.420, val_loss=3.140, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 38:  90%|███████▏| 60/67 [00:18<00:02,  3.30it/s, loss=2.51, v_num=1, train_loss_step=1.420, val_loss=3.140, train_loss_epoch=2.000]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 38:  91%|███████▎| 61/67 [00:18<00:01,  3.28it/s, loss=2.51, v_num=1, train_loss_step=1.420, val_loss=3.140, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 38:  93%|███████▍| 62/67 [00:18<00:01,  3.27it/s, loss=2.51, v_num=1, train_loss_step=1.420, val_loss=3.140, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 38:  94%|███████▌| 63/67 [00:19<00:01,  3.27it/s, loss=2.51, v_num=1, train_loss_step=1.420, val_loss=3.140, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 38:  96%|███████▋| 64/67 [00:19<00:00,  3.26it/s, loss=2.51, v_num=1, train_loss_step=1.420, val_loss=3.140, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 38:  97%|███████▊| 65/67 [00:19<00:00,  3.26it/s, loss=2.51, v_num=1, train_loss_step=1.420, val_loss=3.140, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 38:  99%|███████▉| 66/67 [00:20<00:00,  3.26it/s, loss=2.51, v_num=1, train_loss_step=1.420, val_loss=3.140, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 38: 100%|████████| 67/67 [00:20<00:00,  3.20it/s, loss=2.51, v_num=1, train_loss_step=1.420, val_loss=3.140, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 39:  45%|███▌    | 30/67 [00:05<00:06,  5.44it/s, loss=2.02, v_num=1, train_loss_step=3.990, val_loss=3.140, train_loss_epoch=2.250]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 39:  46%|███▋    | 31/67 [00:07<00:08,  4.14it/s, loss=2.02, v_num=1, train_loss_step=3.990, val_loss=3.140, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 39:  48%|███▊    | 32/67 [00:08<00:09,  3.87it/s, loss=2.02, v_num=1, train_loss_step=3.990, val_loss=3.140, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 39:  49%|███▉    | 33/67 [00:08<00:08,  3.84it/s, loss=2.02, v_num=1, train_loss_step=3.990, val_loss=3.140, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 39:  51%|████    | 34/67 [00:09<00:08,  3.72it/s, loss=2.02, v_num=1, train_loss_step=3.990, val_loss=3.140, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 39:  52%|████▏   | 35/67 [00:09<00:08,  3.70it/s, loss=2.02, v_num=1, train_loss_step=3.990, val_loss=3.140, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 39:  54%|████▎   | 36/67 [00:09<00:08,  3.67it/s, loss=2.02, v_num=1, train_loss_step=3.990, val_loss=3.140, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 39:  55%|████▍   | 37/67 [00:10<00:08,  3.65it/s, loss=2.02, v_num=1, train_loss_step=3.990, val_loss=3.140, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 39:  57%|████▌   | 38/67 [00:10<00:07,  3.63it/s, loss=2.02, v_num=1, train_loss_step=3.990, val_loss=3.140, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 39:  58%|████▋   | 39/67 [00:10<00:07,  3.61it/s, loss=2.02, v_num=1, train_loss_step=3.990, val_loss=3.140, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 39:  60%|████▊   | 40/67 [00:11<00:07,  3.60it/s, loss=2.02, v_num=1, train_loss_step=3.990, val_loss=3.140, train_loss_epoch=2.250]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 39:  61%|████▉   | 41/67 [00:11<00:07,  3.54it/s, loss=2.02, v_num=1, train_loss_step=3.990, val_loss=3.140, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 39:  63%|█████   | 42/67 [00:11<00:07,  3.53it/s, loss=2.02, v_num=1, train_loss_step=3.990, val_loss=3.140, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 39:  64%|█████▏  | 43/67 [00:12<00:06,  3.51it/s, loss=2.02, v_num=1, train_loss_step=3.990, val_loss=3.140, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 39:  66%|█████▎  | 44/67 [00:12<00:06,  3.50it/s, loss=2.02, v_num=1, train_loss_step=3.990, val_loss=3.140, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 39:  67%|█████▎  | 45/67 [00:12<00:06,  3.48it/s, loss=2.02, v_num=1, train_loss_step=3.990, val_loss=3.140, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 39:  69%|█████▍  | 46/67 [00:13<00:06,  3.47it/s, loss=2.02, v_num=1, train_loss_step=3.990, val_loss=3.140, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 39:  70%|█████▌  | 47/67 [00:13<00:05,  3.46it/s, loss=2.02, v_num=1, train_loss_step=3.990, val_loss=3.140, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 39:  72%|█████▋  | 48/67 [00:13<00:05,  3.45it/s, loss=2.02, v_num=1, train_loss_step=3.990, val_loss=3.140, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 39:  73%|█████▊  | 49/67 [00:14<00:05,  3.44it/s, loss=2.02, v_num=1, train_loss_step=3.990, val_loss=3.140, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 39:  75%|█████▉  | 50/67 [00:14<00:04,  3.43it/s, loss=2.02, v_num=1, train_loss_step=3.990, val_loss=3.140, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 39:  76%|██████  | 51/67 [00:15<00:04,  3.39it/s, loss=2.02, v_num=1, train_loss_step=3.990, val_loss=3.140, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 39:  78%|██████▏ | 52/67 [00:15<00:04,  3.38it/s, loss=2.02, v_num=1, train_loss_step=3.990, val_loss=3.140, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 39:  79%|██████▎ | 53/67 [00:15<00:04,  3.37it/s, loss=2.02, v_num=1, train_loss_step=3.990, val_loss=3.140, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 39:  81%|██████▍ | 54/67 [00:16<00:03,  3.37it/s, loss=2.02, v_num=1, train_loss_step=3.990, val_loss=3.140, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 39:  82%|██████▌ | 55/67 [00:16<00:03,  3.36it/s, loss=2.02, v_num=1, train_loss_step=3.990, val_loss=3.140, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 39:  84%|██████▋ | 56/67 [00:16<00:03,  3.35it/s, loss=2.02, v_num=1, train_loss_step=3.990, val_loss=3.140, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 39:  85%|██████▊ | 57/67 [00:17<00:02,  3.35it/s, loss=2.02, v_num=1, train_loss_step=3.990, val_loss=3.140, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 39:  87%|██████▉ | 58/67 [00:17<00:02,  3.34it/s, loss=2.02, v_num=1, train_loss_step=3.990, val_loss=3.140, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 39:  88%|███████ | 59/67 [00:17<00:02,  3.33it/s, loss=2.02, v_num=1, train_loss_step=3.990, val_loss=3.140, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 39:  90%|███████▏| 60/67 [00:18<00:02,  3.33it/s, loss=2.02, v_num=1, train_loss_step=3.990, val_loss=3.140, train_loss_epoch=2.250]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 39:  91%|███████▎| 61/67 [00:19<00:01,  3.14it/s, loss=2.02, v_num=1, train_loss_step=3.990, val_loss=3.140, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 39:  93%|███████▍| 62/67 [00:19<00:01,  3.14it/s, loss=2.02, v_num=1, train_loss_step=3.990, val_loss=3.140, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 39:  94%|███████▌| 63/67 [00:20<00:01,  3.14it/s, loss=2.02, v_num=1, train_loss_step=3.990, val_loss=3.140, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 39:  96%|███████▋| 64/67 [00:20<00:00,  3.13it/s, loss=2.02, v_num=1, train_loss_step=3.990, val_loss=3.140, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 39:  97%|███████▊| 65/67 [00:20<00:00,  3.13it/s, loss=2.02, v_num=1, train_loss_step=3.990, val_loss=3.140, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 39:  99%|███████▉| 66/67 [00:21<00:00,  3.13it/s, loss=2.02, v_num=1, train_loss_step=3.990, val_loss=3.140, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 39: 100%|████████| 67/67 [00:21<00:00,  3.08it/s, loss=2.02, v_num=1, train_loss_step=3.990, val_loss=3.140, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 40:  45%|███▌    | 30/67 [00:05<00:06,  5.33it/s, loss=2.73, v_num=1, train_loss_step=1.600, val_loss=3.140, train_loss_epoch=2.060]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 40:  46%|███▋    | 31/67 [00:07<00:08,  4.11it/s, loss=2.73, v_num=1, train_loss_step=1.600, val_loss=3.140, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 40:  48%|███▊    | 32/67 [00:07<00:08,  4.06it/s, loss=2.73, v_num=1, train_loss_step=1.600, val_loss=3.140, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 40:  49%|███▉    | 33/67 [00:08<00:08,  4.02it/s, loss=2.73, v_num=1, train_loss_step=1.600, val_loss=3.140, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 40:  51%|████    | 34/67 [00:08<00:08,  3.98it/s, loss=2.73, v_num=1, train_loss_step=1.600, val_loss=3.140, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 40:  52%|████▏   | 35/67 [00:08<00:08,  3.94it/s, loss=2.73, v_num=1, train_loss_step=1.600, val_loss=3.140, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 40:  54%|████▎   | 36/67 [00:09<00:07,  3.91it/s, loss=2.73, v_num=1, train_loss_step=1.600, val_loss=3.140, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 40:  55%|████▍   | 37/67 [00:09<00:07,  3.88it/s, loss=2.73, v_num=1, train_loss_step=1.600, val_loss=3.140, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 40:  57%|████▌   | 38/67 [00:09<00:07,  3.86it/s, loss=2.73, v_num=1, train_loss_step=1.600, val_loss=3.140, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 40:  58%|████▋   | 39/67 [00:10<00:07,  3.83it/s, loss=2.73, v_num=1, train_loss_step=1.600, val_loss=3.140, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 40:  60%|████▊   | 40/67 [00:10<00:07,  3.80it/s, loss=2.73, v_num=1, train_loss_step=1.600, val_loss=3.140, train_loss_epoch=2.060]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 40:  61%|████▉   | 41/67 [00:10<00:06,  3.74it/s, loss=2.73, v_num=1, train_loss_step=1.600, val_loss=3.140, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 40:  63%|█████   | 42/67 [00:11<00:06,  3.72it/s, loss=2.73, v_num=1, train_loss_step=1.600, val_loss=3.140, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 40:  64%|█████▏  | 43/67 [00:11<00:06,  3.70it/s, loss=2.73, v_num=1, train_loss_step=1.600, val_loss=3.140, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 40:  66%|█████▎  | 44/67 [00:11<00:06,  3.68it/s, loss=2.73, v_num=1, train_loss_step=1.600, val_loss=3.140, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 40:  67%|█████▎  | 45/67 [00:12<00:06,  3.66it/s, loss=2.73, v_num=1, train_loss_step=1.600, val_loss=3.140, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 40:  69%|█████▍  | 46/67 [00:12<00:05,  3.65it/s, loss=2.73, v_num=1, train_loss_step=1.600, val_loss=3.140, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 40:  70%|█████▌  | 47/67 [00:12<00:05,  3.63it/s, loss=2.73, v_num=1, train_loss_step=1.600, val_loss=3.140, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 40:  72%|█████▋  | 48/67 [00:13<00:05,  3.62it/s, loss=2.73, v_num=1, train_loss_step=1.600, val_loss=3.140, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 40:  73%|█████▊  | 49/67 [00:13<00:05,  3.60it/s, loss=2.73, v_num=1, train_loss_step=1.600, val_loss=3.140, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 40:  75%|█████▉  | 50/67 [00:13<00:04,  3.58it/s, loss=2.73, v_num=1, train_loss_step=1.600, val_loss=3.140, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 40:  76%|██████  | 51/67 [00:14<00:04,  3.55it/s, loss=2.73, v_num=1, train_loss_step=1.600, val_loss=3.140, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 40:  78%|██████▏ | 52/67 [00:14<00:04,  3.53it/s, loss=2.73, v_num=1, train_loss_step=1.600, val_loss=3.140, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 40:  79%|██████▎ | 53/67 [00:15<00:03,  3.52it/s, loss=2.73, v_num=1, train_loss_step=1.600, val_loss=3.140, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 40:  81%|██████▍ | 54/67 [00:15<00:03,  3.51it/s, loss=2.73, v_num=1, train_loss_step=1.600, val_loss=3.140, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 40:  82%|██████▌ | 55/67 [00:15<00:03,  3.50it/s, loss=2.73, v_num=1, train_loss_step=1.600, val_loss=3.140, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 40:  84%|██████▋ | 56/67 [00:16<00:03,  3.49it/s, loss=2.73, v_num=1, train_loss_step=1.600, val_loss=3.140, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 40:  85%|██████▊ | 57/67 [00:16<00:02,  3.48it/s, loss=2.73, v_num=1, train_loss_step=1.600, val_loss=3.140, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 40:  87%|██████▉ | 58/67 [00:16<00:02,  3.46it/s, loss=2.73, v_num=1, train_loss_step=1.600, val_loss=3.140, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 40:  88%|███████ | 59/67 [00:17<00:02,  3.45it/s, loss=2.73, v_num=1, train_loss_step=1.600, val_loss=3.140, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 40:  90%|███████▏| 60/67 [00:17<00:02,  3.44it/s, loss=2.73, v_num=1, train_loss_step=1.600, val_loss=3.140, train_loss_epoch=2.060]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 40:  91%|███████▎| 61/67 [00:17<00:01,  3.42it/s, loss=2.73, v_num=1, train_loss_step=1.600, val_loss=3.140, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 40:  93%|███████▍| 62/67 [00:18<00:01,  3.41it/s, loss=2.73, v_num=1, train_loss_step=1.600, val_loss=3.140, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 40:  94%|███████▌| 63/67 [00:18<00:01,  3.40it/s, loss=2.73, v_num=1, train_loss_step=1.600, val_loss=3.140, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 40:  96%|███████▋| 64/67 [00:18<00:00,  3.39it/s, loss=2.73, v_num=1, train_loss_step=1.600, val_loss=3.140, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 40:  97%|███████▊| 65/67 [00:19<00:00,  3.39it/s, loss=2.73, v_num=1, train_loss_step=1.600, val_loss=3.140, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 40:  99%|███████▉| 66/67 [00:19<00:00,  3.39it/s, loss=2.73, v_num=1, train_loss_step=1.600, val_loss=3.140, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 40: 100%|████████| 67/67 [00:20<00:00,  3.32it/s, loss=2.73, v_num=1, train_loss_step=1.600, val_loss=3.140, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 41:  45%|████     | 30/67 [00:05<00:07,  5.21it/s, loss=2.3, v_num=1, train_loss_step=0.788, val_loss=3.140, train_loss_epoch=2.430]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 41:  46%|████▏    | 31/67 [00:07<00:08,  4.03it/s, loss=2.3, v_num=1, train_loss_step=0.788, val_loss=3.140, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 41:  48%|████▎    | 32/67 [00:08<00:08,  3.99it/s, loss=2.3, v_num=1, train_loss_step=0.788, val_loss=3.140, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 41:  49%|████▍    | 33/67 [00:08<00:08,  3.94it/s, loss=2.3, v_num=1, train_loss_step=0.788, val_loss=3.140, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 41:  51%|████▌    | 34/67 [00:08<00:08,  3.90it/s, loss=2.3, v_num=1, train_loss_step=0.788, val_loss=3.140, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 41:  52%|████▋    | 35/67 [00:09<00:08,  3.86it/s, loss=2.3, v_num=1, train_loss_step=0.788, val_loss=3.140, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 41:  54%|████▊    | 36/67 [00:09<00:08,  3.83it/s, loss=2.3, v_num=1, train_loss_step=0.788, val_loss=3.140, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 41:  55%|████▉    | 37/67 [00:09<00:07,  3.80it/s, loss=2.3, v_num=1, train_loss_step=0.788, val_loss=3.140, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 41:  57%|█████    | 38/67 [00:10<00:07,  3.77it/s, loss=2.3, v_num=1, train_loss_step=0.788, val_loss=3.140, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 41:  58%|█████▏   | 39/67 [00:10<00:07,  3.75it/s, loss=2.3, v_num=1, train_loss_step=0.788, val_loss=3.140, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 41:  60%|█████▎   | 40/67 [00:10<00:07,  3.73it/s, loss=2.3, v_num=1, train_loss_step=0.788, val_loss=3.140, train_loss_epoch=2.430]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 41:  61%|█████▌   | 41/67 [00:11<00:07,  3.67it/s, loss=2.3, v_num=1, train_loss_step=0.788, val_loss=3.140, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 41:  63%|█████▋   | 42/67 [00:11<00:06,  3.66it/s, loss=2.3, v_num=1, train_loss_step=0.788, val_loss=3.140, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 41:  64%|█████▊   | 43/67 [00:11<00:06,  3.64it/s, loss=2.3, v_num=1, train_loss_step=0.788, val_loss=3.140, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 41:  66%|█████▉   | 44/67 [00:12<00:06,  3.62it/s, loss=2.3, v_num=1, train_loss_step=0.788, val_loss=3.140, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 41:  67%|██████   | 45/67 [00:12<00:06,  3.61it/s, loss=2.3, v_num=1, train_loss_step=0.788, val_loss=3.140, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 41:  69%|██████▏  | 46/67 [00:12<00:05,  3.59it/s, loss=2.3, v_num=1, train_loss_step=0.788, val_loss=3.140, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 41:  70%|██████▎  | 47/67 [00:13<00:05,  3.58it/s, loss=2.3, v_num=1, train_loss_step=0.788, val_loss=3.140, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 41:  72%|██████▍  | 48/67 [00:13<00:05,  3.56it/s, loss=2.3, v_num=1, train_loss_step=0.788, val_loss=3.140, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 41:  73%|██████▌  | 49/67 [00:13<00:05,  3.55it/s, loss=2.3, v_num=1, train_loss_step=0.788, val_loss=3.140, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 41:  75%|██████▋  | 50/67 [00:14<00:04,  3.53it/s, loss=2.3, v_num=1, train_loss_step=0.788, val_loss=3.140, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 41:  76%|██████▊  | 51/67 [00:14<00:04,  3.50it/s, loss=2.3, v_num=1, train_loss_step=0.788, val_loss=3.140, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 41:  78%|██████▉  | 52/67 [00:14<00:04,  3.49it/s, loss=2.3, v_num=1, train_loss_step=0.788, val_loss=3.140, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 41:  79%|███████  | 53/67 [00:15<00:04,  3.47it/s, loss=2.3, v_num=1, train_loss_step=0.788, val_loss=3.140, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 41:  81%|███████▎ | 54/67 [00:15<00:03,  3.47it/s, loss=2.3, v_num=1, train_loss_step=0.788, val_loss=3.140, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 41:  82%|███████▍ | 55/67 [00:15<00:03,  3.45it/s, loss=2.3, v_num=1, train_loss_step=0.788, val_loss=3.140, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 41:  84%|███████▌ | 56/67 [00:16<00:03,  3.44it/s, loss=2.3, v_num=1, train_loss_step=0.788, val_loss=3.140, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 41:  85%|███████▋ | 57/67 [00:16<00:02,  3.43it/s, loss=2.3, v_num=1, train_loss_step=0.788, val_loss=3.140, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 41:  87%|███████▊ | 58/67 [00:16<00:02,  3.43it/s, loss=2.3, v_num=1, train_loss_step=0.788, val_loss=3.140, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 41:  88%|███████▉ | 59/67 [00:17<00:02,  3.42it/s, loss=2.3, v_num=1, train_loss_step=0.788, val_loss=3.140, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 41:  90%|████████ | 60/67 [00:17<00:02,  3.41it/s, loss=2.3, v_num=1, train_loss_step=0.788, val_loss=3.140, train_loss_epoch=2.430]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 41:  91%|████████▏| 61/67 [00:18<00:01,  3.38it/s, loss=2.3, v_num=1, train_loss_step=0.788, val_loss=3.140, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 41:  93%|████████▎| 62/67 [00:18<00:01,  3.37it/s, loss=2.3, v_num=1, train_loss_step=0.788, val_loss=3.140, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 41:  94%|████████▍| 63/67 [00:18<00:01,  3.36it/s, loss=2.3, v_num=1, train_loss_step=0.788, val_loss=3.140, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 41:  96%|████████▌| 64/67 [00:19<00:00,  3.36it/s, loss=2.3, v_num=1, train_loss_step=0.788, val_loss=3.140, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 41:  97%|████████▋| 65/67 [00:19<00:00,  3.35it/s, loss=2.3, v_num=1, train_loss_step=0.788, val_loss=3.140, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 41:  99%|████████▊| 66/67 [00:19<00:00,  3.35it/s, loss=2.3, v_num=1, train_loss_step=0.788, val_loss=3.140, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 41: 100%|█████████| 67/67 [00:20<00:00,  3.28it/s, loss=2.3, v_num=1, train_loss_step=0.788, val_loss=3.140, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 42:  45%|███▌    | 30/67 [00:05<00:06,  5.36it/s, loss=2.36, v_num=1, train_loss_step=1.930, val_loss=3.140, train_loss_epoch=2.540]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 42:  46%|███▋    | 31/67 [00:07<00:08,  4.09it/s, loss=2.36, v_num=1, train_loss_step=1.930, val_loss=3.140, train_loss_epoch=2.540]\u001b[A\n",
      "Epoch 42:  48%|███▊    | 32/67 [00:07<00:08,  4.04it/s, loss=2.36, v_num=1, train_loss_step=1.930, val_loss=3.140, train_loss_epoch=2.540]\u001b[A\n",
      "Epoch 42:  49%|███▉    | 33/67 [00:08<00:08,  4.00it/s, loss=2.36, v_num=1, train_loss_step=1.930, val_loss=3.140, train_loss_epoch=2.540]\u001b[A\n",
      "Epoch 42:  51%|████    | 34/67 [00:08<00:08,  3.96it/s, loss=2.36, v_num=1, train_loss_step=1.930, val_loss=3.140, train_loss_epoch=2.540]\u001b[A\n",
      "Epoch 42:  52%|████▏   | 35/67 [00:08<00:08,  3.92it/s, loss=2.36, v_num=1, train_loss_step=1.930, val_loss=3.140, train_loss_epoch=2.540]\u001b[A\n",
      "Epoch 42:  54%|████▎   | 36/67 [00:09<00:07,  3.88it/s, loss=2.36, v_num=1, train_loss_step=1.930, val_loss=3.140, train_loss_epoch=2.540]\u001b[A\n",
      "Epoch 42:  55%|████▍   | 37/67 [00:09<00:07,  3.85it/s, loss=2.36, v_num=1, train_loss_step=1.930, val_loss=3.140, train_loss_epoch=2.540]\u001b[A\n",
      "Epoch 42:  57%|████▌   | 38/67 [00:09<00:07,  3.81it/s, loss=2.36, v_num=1, train_loss_step=1.930, val_loss=3.140, train_loss_epoch=2.540]\u001b[A\n",
      "Epoch 42:  58%|████▋   | 39/67 [00:10<00:07,  3.78it/s, loss=2.36, v_num=1, train_loss_step=1.930, val_loss=3.140, train_loss_epoch=2.540]\u001b[A\n",
      "Epoch 42:  60%|████▊   | 40/67 [00:10<00:07,  3.76it/s, loss=2.36, v_num=1, train_loss_step=1.930, val_loss=3.140, train_loss_epoch=2.540]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 42:  61%|████▉   | 41/67 [00:11<00:07,  3.69it/s, loss=2.36, v_num=1, train_loss_step=1.930, val_loss=3.140, train_loss_epoch=2.540]\u001b[A\n",
      "Epoch 42:  63%|█████   | 42/67 [00:11<00:06,  3.67it/s, loss=2.36, v_num=1, train_loss_step=1.930, val_loss=3.140, train_loss_epoch=2.540]\u001b[A\n",
      "Epoch 42:  64%|█████▏  | 43/67 [00:11<00:06,  3.65it/s, loss=2.36, v_num=1, train_loss_step=1.930, val_loss=3.140, train_loss_epoch=2.540]\u001b[A\n",
      "Epoch 42:  66%|█████▎  | 44/67 [00:12<00:06,  3.62it/s, loss=2.36, v_num=1, train_loss_step=1.930, val_loss=3.140, train_loss_epoch=2.540]\u001b[A\n",
      "Epoch 42:  67%|█████▎  | 45/67 [00:12<00:06,  3.61it/s, loss=2.36, v_num=1, train_loss_step=1.930, val_loss=3.140, train_loss_epoch=2.540]\u001b[A\n",
      "Epoch 42:  69%|█████▍  | 46/67 [00:12<00:05,  3.59it/s, loss=2.36, v_num=1, train_loss_step=1.930, val_loss=3.140, train_loss_epoch=2.540]\u001b[A\n",
      "Epoch 42:  70%|█████▌  | 47/67 [00:13<00:05,  3.57it/s, loss=2.36, v_num=1, train_loss_step=1.930, val_loss=3.140, train_loss_epoch=2.540]\u001b[A\n",
      "Epoch 42:  72%|█████▋  | 48/67 [00:13<00:05,  3.55it/s, loss=2.36, v_num=1, train_loss_step=1.930, val_loss=3.140, train_loss_epoch=2.540]\u001b[A\n",
      "Epoch 42:  73%|█████▊  | 49/67 [00:13<00:05,  3.53it/s, loss=2.36, v_num=1, train_loss_step=1.930, val_loss=3.140, train_loss_epoch=2.540]\u001b[A\n",
      "Epoch 42:  75%|█████▉  | 50/67 [00:14<00:04,  3.52it/s, loss=2.36, v_num=1, train_loss_step=1.930, val_loss=3.140, train_loss_epoch=2.540]\u001b[A\n",
      "Epoch 42:  76%|██████  | 51/67 [00:14<00:04,  3.48it/s, loss=2.36, v_num=1, train_loss_step=1.930, val_loss=3.140, train_loss_epoch=2.540]\u001b[A\n",
      "Epoch 42:  78%|██████▏ | 52/67 [00:15<00:04,  3.46it/s, loss=2.36, v_num=1, train_loss_step=1.930, val_loss=3.140, train_loss_epoch=2.540]\u001b[A\n",
      "Epoch 42:  79%|██████▎ | 53/67 [00:15<00:04,  3.45it/s, loss=2.36, v_num=1, train_loss_step=1.930, val_loss=3.140, train_loss_epoch=2.540]\u001b[A\n",
      "Epoch 42:  81%|██████▍ | 54/67 [00:15<00:03,  3.44it/s, loss=2.36, v_num=1, train_loss_step=1.930, val_loss=3.140, train_loss_epoch=2.540]\u001b[A\n",
      "Epoch 42:  82%|██████▌ | 55/67 [00:16<00:03,  3.43it/s, loss=2.36, v_num=1, train_loss_step=1.930, val_loss=3.140, train_loss_epoch=2.540]\u001b[A\n",
      "Epoch 42:  84%|██████▋ | 56/67 [00:16<00:03,  3.42it/s, loss=2.36, v_num=1, train_loss_step=1.930, val_loss=3.140, train_loss_epoch=2.540]\u001b[A\n",
      "Epoch 42:  85%|██████▊ | 57/67 [00:16<00:02,  3.41it/s, loss=2.36, v_num=1, train_loss_step=1.930, val_loss=3.140, train_loss_epoch=2.540]\u001b[A\n",
      "Epoch 42:  87%|██████▉ | 58/67 [00:17<00:02,  3.41it/s, loss=2.36, v_num=1, train_loss_step=1.930, val_loss=3.140, train_loss_epoch=2.540]\u001b[A\n",
      "Epoch 42:  88%|███████ | 59/67 [00:17<00:02,  3.40it/s, loss=2.36, v_num=1, train_loss_step=1.930, val_loss=3.140, train_loss_epoch=2.540]\u001b[A\n",
      "Epoch 42:  90%|███████▏| 60/67 [00:17<00:02,  3.39it/s, loss=2.36, v_num=1, train_loss_step=1.930, val_loss=3.140, train_loss_epoch=2.540]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 42:  91%|███████▎| 61/67 [00:18<00:01,  3.37it/s, loss=2.36, v_num=1, train_loss_step=1.930, val_loss=3.140, train_loss_epoch=2.540]\u001b[A\n",
      "Epoch 42:  93%|███████▍| 62/67 [00:18<00:01,  3.36it/s, loss=2.36, v_num=1, train_loss_step=1.930, val_loss=3.140, train_loss_epoch=2.540]\u001b[A\n",
      "Epoch 42:  94%|███████▌| 63/67 [00:18<00:01,  3.36it/s, loss=2.36, v_num=1, train_loss_step=1.930, val_loss=3.140, train_loss_epoch=2.540]\u001b[A\n",
      "Epoch 42:  96%|███████▋| 64/67 [00:19<00:00,  3.35it/s, loss=2.36, v_num=1, train_loss_step=1.930, val_loss=3.140, train_loss_epoch=2.540]\u001b[A\n",
      "Epoch 42:  97%|███████▊| 65/67 [00:19<00:00,  3.35it/s, loss=2.36, v_num=1, train_loss_step=1.930, val_loss=3.140, train_loss_epoch=2.540]\u001b[A\n",
      "Epoch 42:  99%|███████▉| 66/67 [00:19<00:00,  3.34it/s, loss=2.36, v_num=1, train_loss_step=1.930, val_loss=3.140, train_loss_epoch=2.540]\u001b[A\n",
      "Epoch 42: 100%|████████| 67/67 [00:20<00:00,  3.28it/s, loss=2.36, v_num=1, train_loss_step=1.930, val_loss=3.130, train_loss_epoch=2.540]\u001b[A\n",
      "Epoch 43:  45%|███▌    | 30/67 [00:05<00:06,  5.36it/s, loss=2.11, v_num=1, train_loss_step=3.750, val_loss=3.130, train_loss_epoch=2.260]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 43:  46%|███▋    | 31/67 [00:07<00:08,  4.07it/s, loss=2.11, v_num=1, train_loss_step=3.750, val_loss=3.130, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 43:  48%|███▊    | 32/67 [00:07<00:08,  4.00it/s, loss=2.11, v_num=1, train_loss_step=3.750, val_loss=3.130, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 43:  49%|███▉    | 33/67 [00:08<00:08,  3.96it/s, loss=2.11, v_num=1, train_loss_step=3.750, val_loss=3.130, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 43:  51%|████    | 34/67 [00:08<00:08,  3.93it/s, loss=2.11, v_num=1, train_loss_step=3.750, val_loss=3.130, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 43:  52%|████▏   | 35/67 [00:09<00:08,  3.89it/s, loss=2.11, v_num=1, train_loss_step=3.750, val_loss=3.130, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 43:  54%|████▎   | 36/67 [00:09<00:08,  3.85it/s, loss=2.11, v_num=1, train_loss_step=3.750, val_loss=3.130, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 43:  55%|████▍   | 37/67 [00:09<00:07,  3.82it/s, loss=2.11, v_num=1, train_loss_step=3.750, val_loss=3.130, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 43:  57%|████▌   | 38/67 [00:10<00:07,  3.79it/s, loss=2.11, v_num=1, train_loss_step=3.750, val_loss=3.130, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 43:  58%|████▋   | 39/67 [00:10<00:07,  3.76it/s, loss=2.11, v_num=1, train_loss_step=3.750, val_loss=3.130, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 43:  60%|████▊   | 40/67 [00:10<00:07,  3.74it/s, loss=2.11, v_num=1, train_loss_step=3.750, val_loss=3.130, train_loss_epoch=2.260]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 43:  61%|████▉   | 41/67 [00:11<00:07,  3.68it/s, loss=2.11, v_num=1, train_loss_step=3.750, val_loss=3.130, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 43:  63%|█████   | 42/67 [00:11<00:06,  3.67it/s, loss=2.11, v_num=1, train_loss_step=3.750, val_loss=3.130, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 43:  64%|█████▏  | 43/67 [00:11<00:06,  3.64it/s, loss=2.11, v_num=1, train_loss_step=3.750, val_loss=3.130, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 43:  66%|█████▎  | 44/67 [00:12<00:06,  3.62it/s, loss=2.11, v_num=1, train_loss_step=3.750, val_loss=3.130, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 43:  67%|█████▎  | 45/67 [00:12<00:06,  3.60it/s, loss=2.11, v_num=1, train_loss_step=3.750, val_loss=3.130, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 43:  69%|█████▍  | 46/67 [00:12<00:05,  3.58it/s, loss=2.11, v_num=1, train_loss_step=3.750, val_loss=3.130, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 43:  70%|█████▌  | 47/67 [00:13<00:05,  3.56it/s, loss=2.11, v_num=1, train_loss_step=3.750, val_loss=3.130, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 43:  72%|█████▋  | 48/67 [00:13<00:05,  3.54it/s, loss=2.11, v_num=1, train_loss_step=3.750, val_loss=3.130, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 43:  73%|█████▊  | 49/67 [00:13<00:05,  3.53it/s, loss=2.11, v_num=1, train_loss_step=3.750, val_loss=3.130, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 43:  75%|█████▉  | 50/67 [00:14<00:04,  3.52it/s, loss=2.11, v_num=1, train_loss_step=3.750, val_loss=3.130, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 43:  76%|██████  | 51/67 [00:14<00:04,  3.48it/s, loss=2.11, v_num=1, train_loss_step=3.750, val_loss=3.130, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 43:  78%|██████▏ | 52/67 [00:15<00:04,  3.47it/s, loss=2.11, v_num=1, train_loss_step=3.750, val_loss=3.130, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 43:  79%|██████▎ | 53/67 [00:15<00:04,  3.46it/s, loss=2.11, v_num=1, train_loss_step=3.750, val_loss=3.130, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 43:  81%|██████▍ | 54/67 [00:15<00:03,  3.45it/s, loss=2.11, v_num=1, train_loss_step=3.750, val_loss=3.130, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 43:  82%|██████▌ | 55/67 [00:15<00:03,  3.44it/s, loss=2.11, v_num=1, train_loss_step=3.750, val_loss=3.130, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 43:  84%|██████▋ | 56/67 [00:16<00:03,  3.43it/s, loss=2.11, v_num=1, train_loss_step=3.750, val_loss=3.130, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 43:  85%|██████▊ | 57/67 [00:16<00:02,  3.43it/s, loss=2.11, v_num=1, train_loss_step=3.750, val_loss=3.130, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 43:  87%|██████▉ | 58/67 [00:16<00:02,  3.42it/s, loss=2.11, v_num=1, train_loss_step=3.750, val_loss=3.130, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 43:  88%|███████ | 59/67 [00:17<00:02,  3.41it/s, loss=2.11, v_num=1, train_loss_step=3.750, val_loss=3.130, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 43:  90%|███████▏| 60/67 [00:17<00:02,  3.41it/s, loss=2.11, v_num=1, train_loss_step=3.750, val_loss=3.130, train_loss_epoch=2.260]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 43:  91%|███████▎| 61/67 [00:18<00:01,  3.38it/s, loss=2.11, v_num=1, train_loss_step=3.750, val_loss=3.130, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 43:  93%|███████▍| 62/67 [00:18<00:01,  3.37it/s, loss=2.11, v_num=1, train_loss_step=3.750, val_loss=3.130, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 43:  94%|███████▌| 63/67 [00:18<00:01,  3.37it/s, loss=2.11, v_num=1, train_loss_step=3.750, val_loss=3.130, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 43:  96%|███████▋| 64/67 [00:19<00:00,  3.36it/s, loss=2.11, v_num=1, train_loss_step=3.750, val_loss=3.130, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 43:  97%|███████▊| 65/67 [00:19<00:00,  3.36it/s, loss=2.11, v_num=1, train_loss_step=3.750, val_loss=3.130, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 43:  99%|███████▉| 66/67 [00:19<00:00,  3.36it/s, loss=2.11, v_num=1, train_loss_step=3.750, val_loss=3.130, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 43: 100%|████████| 67/67 [00:20<00:00,  3.29it/s, loss=2.11, v_num=1, train_loss_step=3.750, val_loss=3.130, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 44:  45%|████▉      | 30/67 [00:05<00:07,  5.07it/s, loss=2, v_num=1, train_loss_step=3.710, val_loss=3.130, train_loss_epoch=2.440]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 44:  46%|█████      | 31/67 [00:07<00:09,  3.94it/s, loss=2, v_num=1, train_loss_step=3.710, val_loss=3.130, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 44:  48%|█████▎     | 32/67 [00:08<00:08,  3.90it/s, loss=2, v_num=1, train_loss_step=3.710, val_loss=3.130, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 44:  49%|█████▍     | 33/67 [00:08<00:08,  3.87it/s, loss=2, v_num=1, train_loss_step=3.710, val_loss=3.130, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 44:  51%|█████▌     | 34/67 [00:08<00:08,  3.84it/s, loss=2, v_num=1, train_loss_step=3.710, val_loss=3.130, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 44:  52%|█████▋     | 35/67 [00:09<00:08,  3.81it/s, loss=2, v_num=1, train_loss_step=3.710, val_loss=3.130, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 44:  54%|█████▉     | 36/67 [00:09<00:08,  3.79it/s, loss=2, v_num=1, train_loss_step=3.710, val_loss=3.130, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 44:  55%|██████     | 37/67 [00:09<00:07,  3.76it/s, loss=2, v_num=1, train_loss_step=3.710, val_loss=3.130, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 44:  57%|██████▏    | 38/67 [00:10<00:07,  3.74it/s, loss=2, v_num=1, train_loss_step=3.710, val_loss=3.130, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 44:  58%|██████▍    | 39/67 [00:10<00:07,  3.72it/s, loss=2, v_num=1, train_loss_step=3.710, val_loss=3.130, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 44:  60%|██████▌    | 40/67 [00:10<00:07,  3.69it/s, loss=2, v_num=1, train_loss_step=3.710, val_loss=3.130, train_loss_epoch=2.440]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 44:  61%|██████▋    | 41/67 [00:11<00:07,  3.64it/s, loss=2, v_num=1, train_loss_step=3.710, val_loss=3.130, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 44:  63%|██████▉    | 42/67 [00:11<00:06,  3.62it/s, loss=2, v_num=1, train_loss_step=3.710, val_loss=3.130, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 44:  64%|███████    | 43/67 [00:11<00:06,  3.60it/s, loss=2, v_num=1, train_loss_step=3.710, val_loss=3.130, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 44:  66%|███████▏   | 44/67 [00:12<00:06,  3.58it/s, loss=2, v_num=1, train_loss_step=3.710, val_loss=3.130, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 44:  67%|███████▍   | 45/67 [00:12<00:06,  3.57it/s, loss=2, v_num=1, train_loss_step=3.710, val_loss=3.130, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 44:  69%|███████▌   | 46/67 [00:12<00:05,  3.55it/s, loss=2, v_num=1, train_loss_step=3.710, val_loss=3.130, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 44:  70%|███████▋   | 47/67 [00:13<00:05,  3.54it/s, loss=2, v_num=1, train_loss_step=3.710, val_loss=3.130, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 44:  72%|███████▉   | 48/67 [00:13<00:05,  3.53it/s, loss=2, v_num=1, train_loss_step=3.710, val_loss=3.130, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 44:  73%|████████   | 49/67 [00:13<00:05,  3.52it/s, loss=2, v_num=1, train_loss_step=3.710, val_loss=3.130, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 44:  75%|████████▏  | 50/67 [00:14<00:04,  3.51it/s, loss=2, v_num=1, train_loss_step=3.710, val_loss=3.130, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 44:  76%|████████▎  | 51/67 [00:14<00:04,  3.46it/s, loss=2, v_num=1, train_loss_step=3.710, val_loss=3.130, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 44:  78%|████████▌  | 52/67 [00:15<00:04,  3.44it/s, loss=2, v_num=1, train_loss_step=3.710, val_loss=3.130, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 44:  79%|████████▋  | 53/67 [00:15<00:04,  3.43it/s, loss=2, v_num=1, train_loss_step=3.710, val_loss=3.130, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 44:  81%|████████▊  | 54/67 [00:15<00:03,  3.42it/s, loss=2, v_num=1, train_loss_step=3.710, val_loss=3.130, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 44:  82%|█████████  | 55/67 [00:16<00:03,  3.40it/s, loss=2, v_num=1, train_loss_step=3.710, val_loss=3.130, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 44:  84%|█████████▏ | 56/67 [00:16<00:03,  3.39it/s, loss=2, v_num=1, train_loss_step=3.710, val_loss=3.130, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 44:  85%|█████████▎ | 57/67 [00:16<00:02,  3.37it/s, loss=2, v_num=1, train_loss_step=3.710, val_loss=3.130, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 44:  87%|█████████▌ | 58/67 [00:17<00:02,  3.35it/s, loss=2, v_num=1, train_loss_step=3.710, val_loss=3.130, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 44:  88%|█████████▋ | 59/67 [00:17<00:02,  3.34it/s, loss=2, v_num=1, train_loss_step=3.710, val_loss=3.130, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 44:  90%|█████████▊ | 60/67 [00:18<00:02,  3.33it/s, loss=2, v_num=1, train_loss_step=3.710, val_loss=3.130, train_loss_epoch=2.440]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 44:  91%|██████████ | 61/67 [00:18<00:01,  3.29it/s, loss=2, v_num=1, train_loss_step=3.710, val_loss=3.130, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 44:  93%|██████████▏| 62/67 [00:18<00:01,  3.28it/s, loss=2, v_num=1, train_loss_step=3.710, val_loss=3.130, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 44:  94%|██████████▎| 63/67 [00:19<00:01,  3.28it/s, loss=2, v_num=1, train_loss_step=3.710, val_loss=3.130, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 44:  96%|██████████▌| 64/67 [00:19<00:00,  3.28it/s, loss=2, v_num=1, train_loss_step=3.710, val_loss=3.130, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 44:  97%|██████████▋| 65/67 [00:19<00:00,  3.27it/s, loss=2, v_num=1, train_loss_step=3.710, val_loss=3.130, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 44:  99%|██████████▊| 66/67 [00:20<00:00,  3.27it/s, loss=2, v_num=1, train_loss_step=3.710, val_loss=3.130, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 44: 100%|███████████| 67/67 [00:20<00:00,  3.21it/s, loss=2, v_num=1, train_loss_step=3.710, val_loss=3.130, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 45:  45%|████     | 30/67 [00:05<00:07,  5.19it/s, loss=2.1, v_num=1, train_loss_step=1.480, val_loss=3.130, train_loss_epoch=1.950]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 45:  46%|████▏    | 31/67 [00:07<00:09,  3.97it/s, loss=2.1, v_num=1, train_loss_step=1.480, val_loss=3.130, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 45:  48%|████▎    | 32/67 [00:08<00:08,  3.93it/s, loss=2.1, v_num=1, train_loss_step=1.480, val_loss=3.130, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 45:  49%|████▍    | 33/67 [00:08<00:08,  3.89it/s, loss=2.1, v_num=1, train_loss_step=1.480, val_loss=3.130, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 45:  51%|████▌    | 34/67 [00:08<00:08,  3.84it/s, loss=2.1, v_num=1, train_loss_step=1.480, val_loss=3.130, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 45:  52%|████▋    | 35/67 [00:09<00:08,  3.80it/s, loss=2.1, v_num=1, train_loss_step=1.480, val_loss=3.130, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 45:  54%|████▊    | 36/67 [00:09<00:08,  3.77it/s, loss=2.1, v_num=1, train_loss_step=1.480, val_loss=3.130, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 45:  55%|████▉    | 37/67 [00:09<00:08,  3.72it/s, loss=2.1, v_num=1, train_loss_step=1.480, val_loss=3.130, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 45:  57%|█████    | 38/67 [00:10<00:07,  3.69it/s, loss=2.1, v_num=1, train_loss_step=1.480, val_loss=3.130, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 45:  58%|█████▏   | 39/67 [00:10<00:07,  3.66it/s, loss=2.1, v_num=1, train_loss_step=1.480, val_loss=3.130, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 45:  60%|█████▎   | 40/67 [00:11<00:07,  3.63it/s, loss=2.1, v_num=1, train_loss_step=1.480, val_loss=3.130, train_loss_epoch=1.950]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 45:  61%|█████▌   | 41/67 [00:11<00:07,  3.55it/s, loss=2.1, v_num=1, train_loss_step=1.480, val_loss=3.130, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 45:  63%|█████▋   | 42/67 [00:11<00:07,  3.53it/s, loss=2.1, v_num=1, train_loss_step=1.480, val_loss=3.130, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 45:  64%|█████▊   | 43/67 [00:12<00:06,  3.50it/s, loss=2.1, v_num=1, train_loss_step=1.480, val_loss=3.130, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 45:  66%|█████▉   | 44/67 [00:12<00:06,  3.48it/s, loss=2.1, v_num=1, train_loss_step=1.480, val_loss=3.130, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 45:  67%|██████   | 45/67 [00:12<00:06,  3.47it/s, loss=2.1, v_num=1, train_loss_step=1.480, val_loss=3.130, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 45:  69%|██████▏  | 46/67 [00:13<00:06,  3.45it/s, loss=2.1, v_num=1, train_loss_step=1.480, val_loss=3.130, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 45:  70%|██████▎  | 47/67 [00:13<00:05,  3.43it/s, loss=2.1, v_num=1, train_loss_step=1.480, val_loss=3.130, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 45:  72%|██████▍  | 48/67 [00:14<00:05,  3.42it/s, loss=2.1, v_num=1, train_loss_step=1.480, val_loss=3.130, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 45:  73%|██████▌  | 49/67 [00:14<00:05,  3.40it/s, loss=2.1, v_num=1, train_loss_step=1.480, val_loss=3.130, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 45:  75%|██████▋  | 50/67 [00:14<00:05,  3.39it/s, loss=2.1, v_num=1, train_loss_step=1.480, val_loss=3.130, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 45:  76%|██████▊  | 51/67 [00:15<00:04,  3.36it/s, loss=2.1, v_num=1, train_loss_step=1.480, val_loss=3.130, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 45:  78%|██████▉  | 52/67 [00:15<00:04,  3.34it/s, loss=2.1, v_num=1, train_loss_step=1.480, val_loss=3.130, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 45:  79%|███████  | 53/67 [00:15<00:04,  3.33it/s, loss=2.1, v_num=1, train_loss_step=1.480, val_loss=3.130, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 45:  81%|███████▎ | 54/67 [00:16<00:03,  3.33it/s, loss=2.1, v_num=1, train_loss_step=1.480, val_loss=3.130, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 45:  82%|███████▍ | 55/67 [00:16<00:03,  3.32it/s, loss=2.1, v_num=1, train_loss_step=1.480, val_loss=3.130, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 45:  84%|███████▌ | 56/67 [00:16<00:03,  3.31it/s, loss=2.1, v_num=1, train_loss_step=1.480, val_loss=3.130, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 45:  85%|███████▋ | 57/67 [00:17<00:03,  3.31it/s, loss=2.1, v_num=1, train_loss_step=1.480, val_loss=3.130, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 45:  87%|███████▊ | 58/67 [00:17<00:02,  3.30it/s, loss=2.1, v_num=1, train_loss_step=1.480, val_loss=3.130, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 45:  88%|███████▉ | 59/67 [00:17<00:02,  3.29it/s, loss=2.1, v_num=1, train_loss_step=1.480, val_loss=3.130, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 45:  90%|████████ | 60/67 [00:18<00:02,  3.29it/s, loss=2.1, v_num=1, train_loss_step=1.480, val_loss=3.130, train_loss_epoch=1.950]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 45:  91%|████████▏| 61/67 [00:18<00:01,  3.26it/s, loss=2.1, v_num=1, train_loss_step=1.480, val_loss=3.130, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 45:  93%|████████▎| 62/67 [00:19<00:01,  3.26it/s, loss=2.1, v_num=1, train_loss_step=1.480, val_loss=3.130, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 45:  94%|████████▍| 63/67 [00:19<00:01,  3.25it/s, loss=2.1, v_num=1, train_loss_step=1.480, val_loss=3.130, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 45:  96%|████████▌| 64/67 [00:19<00:00,  3.25it/s, loss=2.1, v_num=1, train_loss_step=1.480, val_loss=3.130, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 45:  97%|████████▋| 65/67 [00:20<00:00,  3.24it/s, loss=2.1, v_num=1, train_loss_step=1.480, val_loss=3.130, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 45:  99%|████████▊| 66/67 [00:20<00:00,  3.24it/s, loss=2.1, v_num=1, train_loss_step=1.480, val_loss=3.130, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 45: 100%|█████████| 67/67 [00:21<00:00,  3.16it/s, loss=2.1, v_num=1, train_loss_step=1.480, val_loss=3.130, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 46:  45%|███▌    | 30/67 [00:06<00:07,  4.72it/s, loss=1.87, v_num=1, train_loss_step=1.840, val_loss=3.130, train_loss_epoch=2.030]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 46:  46%|███▋    | 31/67 [00:08<00:09,  3.72it/s, loss=1.87, v_num=1, train_loss_step=1.840, val_loss=3.130, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 46:  48%|███▊    | 32/67 [00:08<00:09,  3.68it/s, loss=1.87, v_num=1, train_loss_step=1.840, val_loss=3.130, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 46:  49%|███▉    | 33/67 [00:09<00:09,  3.65it/s, loss=1.87, v_num=1, train_loss_step=1.840, val_loss=3.130, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 46:  51%|████    | 34/67 [00:09<00:09,  3.63it/s, loss=1.87, v_num=1, train_loss_step=1.840, val_loss=3.130, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 46:  52%|████▏   | 35/67 [00:09<00:08,  3.60it/s, loss=1.87, v_num=1, train_loss_step=1.840, val_loss=3.130, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 46:  54%|████▎   | 36/67 [00:10<00:08,  3.57it/s, loss=1.87, v_num=1, train_loss_step=1.840, val_loss=3.130, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 46:  55%|████▍   | 37/67 [00:10<00:08,  3.53it/s, loss=1.87, v_num=1, train_loss_step=1.840, val_loss=3.130, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 46:  57%|████▌   | 38/67 [00:10<00:08,  3.51it/s, loss=1.87, v_num=1, train_loss_step=1.840, val_loss=3.130, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 46:  58%|████▋   | 39/67 [00:11<00:08,  3.50it/s, loss=1.87, v_num=1, train_loss_step=1.840, val_loss=3.130, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 46:  60%|████▊   | 40/67 [00:11<00:07,  3.48it/s, loss=1.87, v_num=1, train_loss_step=1.840, val_loss=3.130, train_loss_epoch=2.030]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 46:  61%|████▉   | 41/67 [00:11<00:07,  3.43it/s, loss=1.87, v_num=1, train_loss_step=1.840, val_loss=3.130, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 46:  63%|█████   | 42/67 [00:12<00:07,  3.42it/s, loss=1.87, v_num=1, train_loss_step=1.840, val_loss=3.130, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 46:  64%|█████▏  | 43/67 [00:12<00:07,  3.40it/s, loss=1.87, v_num=1, train_loss_step=1.840, val_loss=3.130, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 46:  66%|█████▎  | 44/67 [00:12<00:06,  3.39it/s, loss=1.87, v_num=1, train_loss_step=1.840, val_loss=3.130, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 46:  67%|█████▎  | 45/67 [00:13<00:06,  3.38it/s, loss=1.87, v_num=1, train_loss_step=1.840, val_loss=3.130, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 46:  69%|█████▍  | 46/67 [00:13<00:06,  3.37it/s, loss=1.87, v_num=1, train_loss_step=1.840, val_loss=3.130, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 46:  70%|█████▌  | 47/67 [00:13<00:05,  3.36it/s, loss=1.87, v_num=1, train_loss_step=1.840, val_loss=3.130, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 46:  72%|█████▋  | 48/67 [00:14<00:05,  3.35it/s, loss=1.87, v_num=1, train_loss_step=1.840, val_loss=3.130, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 46:  73%|█████▊  | 49/67 [00:14<00:05,  3.34it/s, loss=1.87, v_num=1, train_loss_step=1.840, val_loss=3.130, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 46:  75%|█████▉  | 50/67 [00:15<00:05,  3.33it/s, loss=1.87, v_num=1, train_loss_step=1.840, val_loss=3.130, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 46:  76%|██████  | 51/67 [00:15<00:04,  3.29it/s, loss=1.87, v_num=1, train_loss_step=1.840, val_loss=3.130, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 46:  78%|██████▏ | 52/67 [00:15<00:04,  3.28it/s, loss=1.87, v_num=1, train_loss_step=1.840, val_loss=3.130, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 46:  79%|██████▎ | 53/67 [00:16<00:04,  3.27it/s, loss=1.87, v_num=1, train_loss_step=1.840, val_loss=3.130, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 46:  81%|██████▍ | 54/67 [00:16<00:03,  3.26it/s, loss=1.87, v_num=1, train_loss_step=1.840, val_loss=3.130, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 46:  82%|██████▌ | 55/67 [00:16<00:03,  3.25it/s, loss=1.87, v_num=1, train_loss_step=1.840, val_loss=3.130, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 46:  84%|██████▋ | 56/67 [00:17<00:03,  3.24it/s, loss=1.87, v_num=1, train_loss_step=1.840, val_loss=3.130, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 46:  85%|██████▊ | 57/67 [00:17<00:03,  3.24it/s, loss=1.87, v_num=1, train_loss_step=1.840, val_loss=3.130, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 46:  87%|██████▉ | 58/67 [00:17<00:02,  3.23it/s, loss=1.87, v_num=1, train_loss_step=1.840, val_loss=3.130, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 46:  88%|███████ | 59/67 [00:18<00:02,  3.22it/s, loss=1.87, v_num=1, train_loss_step=1.840, val_loss=3.130, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 46:  90%|███████▏| 60/67 [00:18<00:02,  3.22it/s, loss=1.87, v_num=1, train_loss_step=1.840, val_loss=3.130, train_loss_epoch=2.030]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 46:  91%|███████▎| 61/67 [00:19<00:01,  3.19it/s, loss=1.87, v_num=1, train_loss_step=1.840, val_loss=3.130, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 46:  93%|███████▍| 62/67 [00:19<00:01,  3.18it/s, loss=1.87, v_num=1, train_loss_step=1.840, val_loss=3.130, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 46:  94%|███████▌| 63/67 [00:19<00:01,  3.18it/s, loss=1.87, v_num=1, train_loss_step=1.840, val_loss=3.130, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 46:  96%|███████▋| 64/67 [00:20<00:00,  3.18it/s, loss=1.87, v_num=1, train_loss_step=1.840, val_loss=3.130, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 46:  97%|███████▊| 65/67 [00:20<00:00,  3.17it/s, loss=1.87, v_num=1, train_loss_step=1.840, val_loss=3.130, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 46:  99%|███████▉| 66/67 [00:20<00:00,  3.17it/s, loss=1.87, v_num=1, train_loss_step=1.840, val_loss=3.130, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 46: 100%|████████| 67/67 [00:21<00:00,  3.12it/s, loss=1.87, v_num=1, train_loss_step=1.840, val_loss=3.130, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 47:  45%|███▌    | 30/67 [00:06<00:07,  4.97it/s, loss=2.11, v_num=1, train_loss_step=3.760, val_loss=3.130, train_loss_epoch=2.010]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 47:  46%|███▋    | 31/67 [00:07<00:09,  3.88it/s, loss=2.11, v_num=1, train_loss_step=3.760, val_loss=3.130, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 47:  48%|███▊    | 32/67 [00:08<00:09,  3.84it/s, loss=2.11, v_num=1, train_loss_step=3.760, val_loss=3.130, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 47:  49%|███▉    | 33/67 [00:08<00:08,  3.80it/s, loss=2.11, v_num=1, train_loss_step=3.760, val_loss=3.130, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 47:  51%|████    | 34/67 [00:09<00:08,  3.76it/s, loss=2.11, v_num=1, train_loss_step=3.760, val_loss=3.130, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 47:  52%|████▏   | 35/67 [00:09<00:08,  3.73it/s, loss=2.11, v_num=1, train_loss_step=3.760, val_loss=3.130, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 47:  54%|████▎   | 36/67 [00:09<00:08,  3.70it/s, loss=2.11, v_num=1, train_loss_step=3.760, val_loss=3.130, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 47:  55%|████▍   | 37/67 [00:10<00:08,  3.66it/s, loss=2.11, v_num=1, train_loss_step=3.760, val_loss=3.130, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 47:  57%|████▌   | 38/67 [00:10<00:07,  3.64it/s, loss=2.11, v_num=1, train_loss_step=3.760, val_loss=3.130, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 47:  58%|████▋   | 39/67 [00:10<00:07,  3.62it/s, loss=2.11, v_num=1, train_loss_step=3.760, val_loss=3.130, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 47:  60%|████▊   | 40/67 [00:11<00:07,  3.59it/s, loss=2.11, v_num=1, train_loss_step=3.760, val_loss=3.130, train_loss_epoch=2.010]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 47:  61%|████▉   | 41/67 [00:11<00:07,  3.54it/s, loss=2.11, v_num=1, train_loss_step=3.760, val_loss=3.130, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 47:  63%|█████   | 42/67 [00:11<00:07,  3.52it/s, loss=2.11, v_num=1, train_loss_step=3.760, val_loss=3.130, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 47:  64%|█████▏  | 43/67 [00:12<00:06,  3.50it/s, loss=2.11, v_num=1, train_loss_step=3.760, val_loss=3.130, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 47:  66%|█████▎  | 44/67 [00:12<00:06,  3.49it/s, loss=2.11, v_num=1, train_loss_step=3.760, val_loss=3.130, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 47:  67%|█████▎  | 45/67 [00:12<00:06,  3.48it/s, loss=2.11, v_num=1, train_loss_step=3.760, val_loss=3.130, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 47:  69%|█████▍  | 46/67 [00:13<00:06,  3.46it/s, loss=2.11, v_num=1, train_loss_step=3.760, val_loss=3.130, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 47:  70%|█████▌  | 47/67 [00:13<00:05,  3.45it/s, loss=2.11, v_num=1, train_loss_step=3.760, val_loss=3.130, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 47:  72%|█████▋  | 48/67 [00:13<00:05,  3.44it/s, loss=2.11, v_num=1, train_loss_step=3.760, val_loss=3.130, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 47:  73%|█████▊  | 49/67 [00:14<00:05,  3.42it/s, loss=2.11, v_num=1, train_loss_step=3.760, val_loss=3.130, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 47:  75%|█████▉  | 50/67 [00:14<00:04,  3.41it/s, loss=2.11, v_num=1, train_loss_step=3.760, val_loss=3.130, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 47:  76%|██████  | 51/67 [00:15<00:04,  3.38it/s, loss=2.11, v_num=1, train_loss_step=3.760, val_loss=3.130, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 47:  78%|██████▏ | 52/67 [00:15<00:04,  3.36it/s, loss=2.11, v_num=1, train_loss_step=3.760, val_loss=3.130, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 47:  79%|██████▎ | 53/67 [00:15<00:04,  3.36it/s, loss=2.11, v_num=1, train_loss_step=3.760, val_loss=3.130, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 47:  81%|██████▍ | 54/67 [00:16<00:03,  3.35it/s, loss=2.11, v_num=1, train_loss_step=3.760, val_loss=3.130, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 47:  82%|██████▌ | 55/67 [00:16<00:03,  3.34it/s, loss=2.11, v_num=1, train_loss_step=3.760, val_loss=3.130, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 47:  84%|██████▋ | 56/67 [00:16<00:03,  3.34it/s, loss=2.11, v_num=1, train_loss_step=3.760, val_loss=3.130, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 47:  85%|██████▊ | 57/67 [00:17<00:03,  3.33it/s, loss=2.11, v_num=1, train_loss_step=3.760, val_loss=3.130, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 47:  87%|██████▉ | 58/67 [00:17<00:02,  3.32it/s, loss=2.11, v_num=1, train_loss_step=3.760, val_loss=3.130, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 47:  88%|███████ | 59/67 [00:17<00:02,  3.31it/s, loss=2.11, v_num=1, train_loss_step=3.760, val_loss=3.130, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 47:  90%|███████▏| 60/67 [00:18<00:02,  3.31it/s, loss=2.11, v_num=1, train_loss_step=3.760, val_loss=3.130, train_loss_epoch=2.010]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 47:  91%|███████▎| 61/67 [00:18<00:01,  3.27it/s, loss=2.11, v_num=1, train_loss_step=3.760, val_loss=3.130, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 47:  93%|███████▍| 62/67 [00:18<00:01,  3.27it/s, loss=2.11, v_num=1, train_loss_step=3.760, val_loss=3.130, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 47:  94%|███████▌| 63/67 [00:19<00:01,  3.26it/s, loss=2.11, v_num=1, train_loss_step=3.760, val_loss=3.130, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 47:  96%|███████▋| 64/67 [00:21<00:01,  2.92it/s, loss=2.11, v_num=1, train_loss_step=3.760, val_loss=3.130, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 47:  97%|███████▊| 65/67 [00:22<00:00,  2.92it/s, loss=2.11, v_num=1, train_loss_step=3.760, val_loss=3.130, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 47:  99%|███████▉| 66/67 [00:22<00:00,  2.92it/s, loss=2.11, v_num=1, train_loss_step=3.760, val_loss=3.130, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 47: 100%|████████| 67/67 [00:23<00:00,  2.86it/s, loss=2.11, v_num=1, train_loss_step=3.760, val_loss=3.130, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 48:  45%|███▌    | 30/67 [00:05<00:07,  5.13it/s, loss=2.36, v_num=1, train_loss_step=0.977, val_loss=3.130, train_loss_epoch=2.150]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 48:  46%|███▋    | 31/67 [00:08<00:09,  3.87it/s, loss=2.36, v_num=1, train_loss_step=0.977, val_loss=3.130, train_loss_epoch=2.150]\u001b[A\n",
      "Epoch 48:  48%|███▊    | 32/67 [00:08<00:09,  3.84it/s, loss=2.36, v_num=1, train_loss_step=0.977, val_loss=3.130, train_loss_epoch=2.150]\u001b[A\n",
      "Epoch 48:  49%|███▉    | 33/67 [00:08<00:08,  3.80it/s, loss=2.36, v_num=1, train_loss_step=0.977, val_loss=3.130, train_loss_epoch=2.150]\u001b[A\n",
      "Epoch 48:  51%|████    | 34/67 [00:09<00:08,  3.76it/s, loss=2.36, v_num=1, train_loss_step=0.977, val_loss=3.130, train_loss_epoch=2.150]\u001b[A\n",
      "Epoch 48:  52%|████▏   | 35/67 [00:09<00:08,  3.73it/s, loss=2.36, v_num=1, train_loss_step=0.977, val_loss=3.130, train_loss_epoch=2.150]\u001b[A\n",
      "Epoch 48:  54%|████▎   | 36/67 [00:09<00:08,  3.71it/s, loss=2.36, v_num=1, train_loss_step=0.977, val_loss=3.130, train_loss_epoch=2.150]\u001b[A\n",
      "Epoch 48:  55%|████▍   | 37/67 [00:10<00:08,  3.68it/s, loss=2.36, v_num=1, train_loss_step=0.977, val_loss=3.130, train_loss_epoch=2.150]\u001b[A\n",
      "Epoch 48:  57%|████▌   | 38/67 [00:10<00:07,  3.66it/s, loss=2.36, v_num=1, train_loss_step=0.977, val_loss=3.130, train_loss_epoch=2.150]\u001b[A\n",
      "Epoch 48:  58%|████▋   | 39/67 [00:10<00:07,  3.63it/s, loss=2.36, v_num=1, train_loss_step=0.977, val_loss=3.130, train_loss_epoch=2.150]\u001b[A\n",
      "Epoch 48:  60%|████▊   | 40/67 [00:11<00:07,  3.61it/s, loss=2.36, v_num=1, train_loss_step=0.977, val_loss=3.130, train_loss_epoch=2.150]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 48:  61%|████▉   | 41/67 [00:11<00:07,  3.54it/s, loss=2.36, v_num=1, train_loss_step=0.977, val_loss=3.130, train_loss_epoch=2.150]\u001b[A\n",
      "Epoch 48:  63%|█████   | 42/67 [00:11<00:07,  3.52it/s, loss=2.36, v_num=1, train_loss_step=0.977, val_loss=3.130, train_loss_epoch=2.150]\u001b[A\n",
      "Epoch 48:  64%|█████▏  | 43/67 [00:12<00:06,  3.47it/s, loss=2.36, v_num=1, train_loss_step=0.977, val_loss=3.130, train_loss_epoch=2.150]\u001b[A\n",
      "Epoch 48:  66%|█████▎  | 44/67 [00:12<00:06,  3.46it/s, loss=2.36, v_num=1, train_loss_step=0.977, val_loss=3.130, train_loss_epoch=2.150]\u001b[A\n",
      "Epoch 48:  67%|█████▎  | 45/67 [00:13<00:06,  3.44it/s, loss=2.36, v_num=1, train_loss_step=0.977, val_loss=3.130, train_loss_epoch=2.150]\u001b[A\n",
      "Epoch 48:  69%|█████▍  | 46/67 [00:13<00:06,  3.40it/s, loss=2.36, v_num=1, train_loss_step=0.977, val_loss=3.130, train_loss_epoch=2.150]\u001b[A\n",
      "Epoch 48:  70%|█████▌  | 47/67 [00:13<00:05,  3.39it/s, loss=2.36, v_num=1, train_loss_step=0.977, val_loss=3.130, train_loss_epoch=2.150]\u001b[A\n",
      "Epoch 48:  72%|█████▋  | 48/67 [00:14<00:05,  3.37it/s, loss=2.36, v_num=1, train_loss_step=0.977, val_loss=3.130, train_loss_epoch=2.150]\u001b[A\n",
      "Epoch 48:  73%|█████▊  | 49/67 [00:14<00:05,  3.32it/s, loss=2.36, v_num=1, train_loss_step=0.977, val_loss=3.130, train_loss_epoch=2.150]\u001b[A\n",
      "Epoch 48:  75%|█████▉  | 50/67 [00:15<00:05,  3.31it/s, loss=2.36, v_num=1, train_loss_step=0.977, val_loss=3.130, train_loss_epoch=2.150]\u001b[A\n",
      "Epoch 48:  76%|██████  | 51/67 [00:15<00:04,  3.28it/s, loss=2.36, v_num=1, train_loss_step=0.977, val_loss=3.130, train_loss_epoch=2.150]\u001b[A\n",
      "Epoch 48:  78%|██████▏ | 52/67 [00:15<00:04,  3.27it/s, loss=2.36, v_num=1, train_loss_step=0.977, val_loss=3.130, train_loss_epoch=2.150]\u001b[A\n",
      "Epoch 48:  79%|██████▎ | 53/67 [00:16<00:04,  3.27it/s, loss=2.36, v_num=1, train_loss_step=0.977, val_loss=3.130, train_loss_epoch=2.150]\u001b[A\n",
      "Epoch 48:  81%|██████▍ | 54/67 [00:16<00:03,  3.26it/s, loss=2.36, v_num=1, train_loss_step=0.977, val_loss=3.130, train_loss_epoch=2.150]\u001b[A\n",
      "Epoch 48:  82%|██████▌ | 55/67 [00:17<00:03,  3.07it/s, loss=2.36, v_num=1, train_loss_step=0.977, val_loss=3.130, train_loss_epoch=2.150]\u001b[A\n",
      "Epoch 48:  84%|██████▋ | 56/67 [00:18<00:03,  3.06it/s, loss=2.36, v_num=1, train_loss_step=0.977, val_loss=3.130, train_loss_epoch=2.150]\u001b[A\n",
      "Epoch 48:  85%|██████▊ | 57/67 [00:18<00:03,  3.06it/s, loss=2.36, v_num=1, train_loss_step=0.977, val_loss=3.130, train_loss_epoch=2.150]\u001b[A\n",
      "Epoch 48:  87%|██████▉ | 58/67 [00:19<00:02,  3.04it/s, loss=2.36, v_num=1, train_loss_step=0.977, val_loss=3.130, train_loss_epoch=2.150]\u001b[A\n",
      "Epoch 48:  88%|███████ | 59/67 [00:19<00:02,  3.04it/s, loss=2.36, v_num=1, train_loss_step=0.977, val_loss=3.130, train_loss_epoch=2.150]\u001b[A\n",
      "Epoch 48:  90%|███████▏| 60/67 [00:19<00:02,  3.03it/s, loss=2.36, v_num=1, train_loss_step=0.977, val_loss=3.130, train_loss_epoch=2.150]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 48:  91%|███████▎| 61/67 [00:20<00:01,  3.01it/s, loss=2.36, v_num=1, train_loss_step=0.977, val_loss=3.130, train_loss_epoch=2.150]\u001b[A\n",
      "Epoch 48:  93%|███████▍| 62/67 [00:20<00:01,  3.00it/s, loss=2.36, v_num=1, train_loss_step=0.977, val_loss=3.130, train_loss_epoch=2.150]\u001b[A\n",
      "Epoch 48:  94%|███████▌| 63/67 [00:20<00:01,  3.00it/s, loss=2.36, v_num=1, train_loss_step=0.977, val_loss=3.130, train_loss_epoch=2.150]\u001b[A\n",
      "Epoch 48:  96%|███████▋| 64/67 [00:21<00:00,  3.00it/s, loss=2.36, v_num=1, train_loss_step=0.977, val_loss=3.130, train_loss_epoch=2.150]\u001b[A\n",
      "Epoch 48:  97%|███████▊| 65/67 [00:21<00:00,  3.00it/s, loss=2.36, v_num=1, train_loss_step=0.977, val_loss=3.130, train_loss_epoch=2.150]\u001b[A\n",
      "Epoch 48:  99%|███████▉| 66/67 [00:21<00:00,  3.00it/s, loss=2.36, v_num=1, train_loss_step=0.977, val_loss=3.130, train_loss_epoch=2.150]\u001b[A\n",
      "Epoch 48: 100%|████████| 67/67 [00:22<00:00,  2.95it/s, loss=2.36, v_num=1, train_loss_step=0.977, val_loss=3.130, train_loss_epoch=2.150]\u001b[A\n",
      "Epoch 49:  45%|████▉      | 30/67 [00:06<00:07,  4.89it/s, loss=2, v_num=1, train_loss_step=0.940, val_loss=3.130, train_loss_epoch=2.240]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 49:  46%|█████      | 31/67 [00:08<00:09,  3.83it/s, loss=2, v_num=1, train_loss_step=0.940, val_loss=3.130, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 49:  48%|█████▎     | 32/67 [00:08<00:09,  3.80it/s, loss=2, v_num=1, train_loss_step=0.940, val_loss=3.130, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 49:  49%|█████▍     | 33/67 [00:08<00:09,  3.77it/s, loss=2, v_num=1, train_loss_step=0.940, val_loss=3.130, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 49:  51%|█████▌     | 34/67 [00:09<00:08,  3.74it/s, loss=2, v_num=1, train_loss_step=0.940, val_loss=3.130, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 49:  52%|█████▋     | 35/67 [00:09<00:08,  3.72it/s, loss=2, v_num=1, train_loss_step=0.940, val_loss=3.130, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 49:  54%|█████▉     | 36/67 [00:09<00:08,  3.69it/s, loss=2, v_num=1, train_loss_step=0.940, val_loss=3.130, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 49:  55%|██████     | 37/67 [00:10<00:08,  3.66it/s, loss=2, v_num=1, train_loss_step=0.940, val_loss=3.130, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 49:  57%|██████▏    | 38/67 [00:10<00:07,  3.64it/s, loss=2, v_num=1, train_loss_step=0.940, val_loss=3.130, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 49:  58%|██████▍    | 39/67 [00:10<00:07,  3.62it/s, loss=2, v_num=1, train_loss_step=0.940, val_loss=3.130, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 49:  60%|██████▌    | 40/67 [00:11<00:07,  3.60it/s, loss=2, v_num=1, train_loss_step=0.940, val_loss=3.130, train_loss_epoch=2.240]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 49:  61%|██████▋    | 41/67 [00:11<00:07,  3.54it/s, loss=2, v_num=1, train_loss_step=0.940, val_loss=3.130, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 49:  63%|██████▉    | 42/67 [00:11<00:07,  3.52it/s, loss=2, v_num=1, train_loss_step=0.940, val_loss=3.130, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 49:  64%|███████    | 43/67 [00:13<00:07,  3.28it/s, loss=2, v_num=1, train_loss_step=0.940, val_loss=3.130, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 49:  66%|███████▏   | 44/67 [00:13<00:07,  3.27it/s, loss=2, v_num=1, train_loss_step=0.940, val_loss=3.130, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 49:  67%|███████▍   | 45/67 [00:13<00:06,  3.27it/s, loss=2, v_num=1, train_loss_step=0.940, val_loss=3.130, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 49:  69%|███████▌   | 46/67 [00:14<00:06,  3.26it/s, loss=2, v_num=1, train_loss_step=0.940, val_loss=3.130, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 49:  70%|███████▋   | 47/67 [00:14<00:06,  3.25it/s, loss=2, v_num=1, train_loss_step=0.940, val_loss=3.130, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 49:  72%|███████▉   | 48/67 [00:14<00:05,  3.25it/s, loss=2, v_num=1, train_loss_step=0.940, val_loss=3.130, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 49:  73%|████████   | 49/67 [00:15<00:05,  3.25it/s, loss=2, v_num=1, train_loss_step=0.940, val_loss=3.130, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 49:  75%|████████▏  | 50/67 [00:15<00:05,  3.24it/s, loss=2, v_num=1, train_loss_step=0.940, val_loss=3.130, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 49:  76%|████████▎  | 51/67 [00:15<00:04,  3.21it/s, loss=2, v_num=1, train_loss_step=0.940, val_loss=3.130, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 49:  78%|████████▌  | 52/67 [00:16<00:04,  3.21it/s, loss=2, v_num=1, train_loss_step=0.940, val_loss=3.130, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 49:  79%|████████▋  | 53/67 [00:16<00:04,  3.20it/s, loss=2, v_num=1, train_loss_step=0.940, val_loss=3.130, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 49:  81%|████████▊  | 54/67 [00:16<00:04,  3.20it/s, loss=2, v_num=1, train_loss_step=0.940, val_loss=3.130, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 49:  82%|█████████  | 55/67 [00:17<00:03,  3.19it/s, loss=2, v_num=1, train_loss_step=0.940, val_loss=3.130, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 49:  84%|█████████▏ | 56/67 [00:17<00:03,  3.18it/s, loss=2, v_num=1, train_loss_step=0.940, val_loss=3.130, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 49:  85%|█████████▎ | 57/67 [00:17<00:03,  3.18it/s, loss=2, v_num=1, train_loss_step=0.940, val_loss=3.130, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 49:  87%|█████████▌ | 58/67 [00:18<00:02,  3.18it/s, loss=2, v_num=1, train_loss_step=0.940, val_loss=3.130, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 49:  88%|█████████▋ | 59/67 [00:18<00:02,  3.18it/s, loss=2, v_num=1, train_loss_step=0.940, val_loss=3.130, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 49:  90%|█████████▊ | 60/67 [00:18<00:02,  3.17it/s, loss=2, v_num=1, train_loss_step=0.940, val_loss=3.130, train_loss_epoch=2.240]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 49:  91%|██████████ | 61/67 [00:19<00:01,  3.14it/s, loss=2, v_num=1, train_loss_step=0.940, val_loss=3.130, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 49:  93%|██████████▏| 62/67 [00:19<00:01,  3.13it/s, loss=2, v_num=1, train_loss_step=0.940, val_loss=3.130, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 49:  94%|██████████▎| 63/67 [00:20<00:01,  3.13it/s, loss=2, v_num=1, train_loss_step=0.940, val_loss=3.130, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 49:  96%|██████████▌| 64/67 [00:20<00:00,  3.13it/s, loss=2, v_num=1, train_loss_step=0.940, val_loss=3.130, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 49:  97%|██████████▋| 65/67 [00:20<00:00,  3.13it/s, loss=2, v_num=1, train_loss_step=0.940, val_loss=3.130, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 49:  99%|██████████▊| 66/67 [00:21<00:00,  3.13it/s, loss=2, v_num=1, train_loss_step=0.940, val_loss=3.130, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 49: 100%|███████████| 67/67 [00:21<00:00,  3.07it/s, loss=2, v_num=1, train_loss_step=0.940, val_loss=3.130, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 50:  45%|███▌    | 30/67 [00:05<00:06,  5.36it/s, loss=1.86, v_num=1, train_loss_step=1.460, val_loss=3.130, train_loss_epoch=1.930]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 50:  46%|███▋    | 31/67 [00:07<00:08,  4.14it/s, loss=1.86, v_num=1, train_loss_step=1.460, val_loss=3.130, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 50:  48%|███▊    | 32/67 [00:07<00:08,  4.09it/s, loss=1.86, v_num=1, train_loss_step=1.460, val_loss=3.130, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 50:  49%|███▉    | 33/67 [00:08<00:08,  4.04it/s, loss=1.86, v_num=1, train_loss_step=1.460, val_loss=3.130, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 50:  51%|████    | 34/67 [00:08<00:08,  4.00it/s, loss=1.86, v_num=1, train_loss_step=1.460, val_loss=3.130, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 50:  52%|████▏   | 35/67 [00:09<00:08,  3.62it/s, loss=1.86, v_num=1, train_loss_step=1.460, val_loss=3.130, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 50:  54%|████▎   | 36/67 [00:09<00:08,  3.60it/s, loss=1.86, v_num=1, train_loss_step=1.460, val_loss=3.130, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 50:  55%|████▍   | 37/67 [00:10<00:08,  3.58it/s, loss=1.86, v_num=1, train_loss_step=1.460, val_loss=3.130, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 50:  57%|████▌   | 38/67 [00:10<00:08,  3.57it/s, loss=1.86, v_num=1, train_loss_step=1.460, val_loss=3.130, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 50:  58%|████▋   | 39/67 [00:10<00:07,  3.55it/s, loss=1.86, v_num=1, train_loss_step=1.460, val_loss=3.130, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 50:  60%|████▊   | 40/67 [00:11<00:07,  3.53it/s, loss=1.86, v_num=1, train_loss_step=1.460, val_loss=3.130, train_loss_epoch=1.930]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 50:  61%|████▉   | 41/67 [00:11<00:07,  3.49it/s, loss=1.86, v_num=1, train_loss_step=1.460, val_loss=3.130, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 50:  63%|█████   | 42/67 [00:12<00:07,  3.47it/s, loss=1.86, v_num=1, train_loss_step=1.460, val_loss=3.130, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 50:  64%|█████▏  | 43/67 [00:12<00:06,  3.46it/s, loss=1.86, v_num=1, train_loss_step=1.460, val_loss=3.130, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 50:  66%|█████▎  | 44/67 [00:12<00:06,  3.45it/s, loss=1.86, v_num=1, train_loss_step=1.460, val_loss=3.130, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 50:  67%|█████▎  | 45/67 [00:13<00:06,  3.44it/s, loss=1.86, v_num=1, train_loss_step=1.460, val_loss=3.130, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 50:  69%|█████▍  | 46/67 [00:13<00:06,  3.43it/s, loss=1.86, v_num=1, train_loss_step=1.460, val_loss=3.130, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 50:  70%|█████▌  | 47/67 [00:13<00:05,  3.42it/s, loss=1.86, v_num=1, train_loss_step=1.460, val_loss=3.130, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 50:  72%|█████▋  | 48/67 [00:14<00:05,  3.41it/s, loss=1.86, v_num=1, train_loss_step=1.460, val_loss=3.130, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 50:  73%|█████▊  | 49/67 [00:14<00:05,  3.40it/s, loss=1.86, v_num=1, train_loss_step=1.460, val_loss=3.130, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 50:  75%|█████▉  | 50/67 [00:14<00:05,  3.39it/s, loss=1.86, v_num=1, train_loss_step=1.460, val_loss=3.130, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 50:  76%|██████  | 51/67 [00:15<00:04,  3.36it/s, loss=1.86, v_num=1, train_loss_step=1.460, val_loss=3.130, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 50:  78%|██████▏ | 52/67 [00:15<00:04,  3.35it/s, loss=1.86, v_num=1, train_loss_step=1.460, val_loss=3.130, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 50:  79%|██████▎ | 53/67 [00:15<00:04,  3.34it/s, loss=1.86, v_num=1, train_loss_step=1.460, val_loss=3.130, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 50:  81%|██████▍ | 54/67 [00:16<00:03,  3.33it/s, loss=1.86, v_num=1, train_loss_step=1.460, val_loss=3.130, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 50:  82%|██████▌ | 55/67 [00:16<00:03,  3.32it/s, loss=1.86, v_num=1, train_loss_step=1.460, val_loss=3.130, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 50:  84%|██████▋ | 56/67 [00:16<00:03,  3.32it/s, loss=1.86, v_num=1, train_loss_step=1.460, val_loss=3.130, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 50:  85%|██████▊ | 57/67 [00:17<00:03,  3.31it/s, loss=1.86, v_num=1, train_loss_step=1.460, val_loss=3.130, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 50:  87%|██████▉ | 58/67 [00:17<00:02,  3.31it/s, loss=1.86, v_num=1, train_loss_step=1.460, val_loss=3.130, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 50:  88%|███████ | 59/67 [00:17<00:02,  3.30it/s, loss=1.86, v_num=1, train_loss_step=1.460, val_loss=3.130, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 50:  90%|███████▏| 60/67 [00:18<00:02,  3.29it/s, loss=1.86, v_num=1, train_loss_step=1.460, val_loss=3.130, train_loss_epoch=1.930]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 50:  91%|███████▎| 61/67 [00:18<00:01,  3.27it/s, loss=1.86, v_num=1, train_loss_step=1.460, val_loss=3.130, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 50:  93%|███████▍| 62/67 [00:19<00:01,  3.26it/s, loss=1.86, v_num=1, train_loss_step=1.460, val_loss=3.130, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 50:  94%|███████▌| 63/67 [00:19<00:01,  3.26it/s, loss=1.86, v_num=1, train_loss_step=1.460, val_loss=3.130, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 50:  96%|███████▋| 64/67 [00:19<00:00,  3.26it/s, loss=1.86, v_num=1, train_loss_step=1.460, val_loss=3.130, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 50:  97%|███████▊| 65/67 [00:19<00:00,  3.25it/s, loss=1.86, v_num=1, train_loss_step=1.460, val_loss=3.130, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 50:  99%|███████▉| 66/67 [00:20<00:00,  3.25it/s, loss=1.86, v_num=1, train_loss_step=1.460, val_loss=3.130, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 50: 100%|████████| 67/67 [00:21<00:00,  3.06it/s, loss=1.86, v_num=1, train_loss_step=1.460, val_loss=3.120, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 51:  45%|███▌    | 30/67 [00:05<00:07,  5.22it/s, loss=2.43, v_num=1, train_loss_step=1.150, val_loss=3.120, train_loss_epoch=1.900]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 51:  46%|███▋    | 31/67 [00:07<00:08,  4.03it/s, loss=2.43, v_num=1, train_loss_step=1.150, val_loss=3.120, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 51:  48%|███▊    | 32/67 [00:08<00:08,  3.99it/s, loss=2.43, v_num=1, train_loss_step=1.150, val_loss=3.120, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 51:  49%|███▉    | 33/67 [00:08<00:08,  3.95it/s, loss=2.43, v_num=1, train_loss_step=1.150, val_loss=3.120, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 51:  51%|████    | 34/67 [00:08<00:08,  3.92it/s, loss=2.43, v_num=1, train_loss_step=1.150, val_loss=3.120, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 51:  52%|████▏   | 35/67 [00:09<00:08,  3.88it/s, loss=2.43, v_num=1, train_loss_step=1.150, val_loss=3.120, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 51:  54%|████▎   | 36/67 [00:09<00:08,  3.85it/s, loss=2.43, v_num=1, train_loss_step=1.150, val_loss=3.120, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 51:  55%|████▍   | 37/67 [00:09<00:07,  3.83it/s, loss=2.43, v_num=1, train_loss_step=1.150, val_loss=3.120, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 51:  57%|████▌   | 38/67 [00:10<00:07,  3.80it/s, loss=2.43, v_num=1, train_loss_step=1.150, val_loss=3.120, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 51:  58%|████▋   | 39/67 [00:10<00:07,  3.77it/s, loss=2.43, v_num=1, train_loss_step=1.150, val_loss=3.120, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 51:  60%|████▊   | 40/67 [00:10<00:07,  3.75it/s, loss=2.43, v_num=1, train_loss_step=1.150, val_loss=3.120, train_loss_epoch=1.900]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 51:  61%|████▉   | 41/67 [00:11<00:07,  3.67it/s, loss=2.43, v_num=1, train_loss_step=1.150, val_loss=3.120, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 51:  63%|█████   | 42/67 [00:11<00:06,  3.65it/s, loss=2.43, v_num=1, train_loss_step=1.150, val_loss=3.120, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 51:  64%|█████▏  | 43/67 [00:11<00:06,  3.63it/s, loss=2.43, v_num=1, train_loss_step=1.150, val_loss=3.120, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 51:  66%|█████▎  | 44/67 [00:12<00:06,  3.61it/s, loss=2.43, v_num=1, train_loss_step=1.150, val_loss=3.120, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 51:  67%|█████▎  | 45/67 [00:12<00:06,  3.60it/s, loss=2.43, v_num=1, train_loss_step=1.150, val_loss=3.120, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 51:  69%|█████▍  | 46/67 [00:12<00:05,  3.58it/s, loss=2.43, v_num=1, train_loss_step=1.150, val_loss=3.120, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 51:  70%|█████▌  | 47/67 [00:13<00:05,  3.57it/s, loss=2.43, v_num=1, train_loss_step=1.150, val_loss=3.120, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 51:  72%|█████▋  | 48/67 [00:13<00:05,  3.55it/s, loss=2.43, v_num=1, train_loss_step=1.150, val_loss=3.120, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 51:  73%|█████▊  | 49/67 [00:13<00:05,  3.54it/s, loss=2.43, v_num=1, train_loss_step=1.150, val_loss=3.120, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 51:  75%|█████▉  | 50/67 [00:14<00:04,  3.53it/s, loss=2.43, v_num=1, train_loss_step=1.150, val_loss=3.120, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 51:  76%|██████  | 51/67 [00:14<00:04,  3.49it/s, loss=2.43, v_num=1, train_loss_step=1.150, val_loss=3.120, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 51:  78%|██████▏ | 52/67 [00:14<00:04,  3.48it/s, loss=2.43, v_num=1, train_loss_step=1.150, val_loss=3.120, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 51:  79%|██████▎ | 53/67 [00:15<00:04,  3.47it/s, loss=2.43, v_num=1, train_loss_step=1.150, val_loss=3.120, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 51:  81%|██████▍ | 54/67 [00:15<00:03,  3.45it/s, loss=2.43, v_num=1, train_loss_step=1.150, val_loss=3.120, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 51:  82%|██████▌ | 55/67 [00:15<00:03,  3.44it/s, loss=2.43, v_num=1, train_loss_step=1.150, val_loss=3.120, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 51:  84%|██████▋ | 56/67 [00:16<00:03,  3.44it/s, loss=2.43, v_num=1, train_loss_step=1.150, val_loss=3.120, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 51:  85%|██████▊ | 57/67 [00:16<00:02,  3.43it/s, loss=2.43, v_num=1, train_loss_step=1.150, val_loss=3.120, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 51:  87%|██████▉ | 58/67 [00:16<00:02,  3.42it/s, loss=2.43, v_num=1, train_loss_step=1.150, val_loss=3.120, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 51:  88%|███████ | 59/67 [00:17<00:02,  3.41it/s, loss=2.43, v_num=1, train_loss_step=1.150, val_loss=3.120, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 51:  90%|███████▏| 60/67 [00:17<00:02,  3.40it/s, loss=2.43, v_num=1, train_loss_step=1.150, val_loss=3.120, train_loss_epoch=1.900]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 51:  91%|███████▎| 61/67 [00:18<00:01,  3.37it/s, loss=2.43, v_num=1, train_loss_step=1.150, val_loss=3.120, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 51:  93%|███████▍| 62/67 [00:18<00:01,  3.37it/s, loss=2.43, v_num=1, train_loss_step=1.150, val_loss=3.120, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 51:  94%|███████▌| 63/67 [00:18<00:01,  3.36it/s, loss=2.43, v_num=1, train_loss_step=1.150, val_loss=3.120, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 51:  96%|███████▋| 64/67 [00:19<00:00,  3.36it/s, loss=2.43, v_num=1, train_loss_step=1.150, val_loss=3.120, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 51:  97%|███████▊| 65/67 [00:19<00:00,  3.35it/s, loss=2.43, v_num=1, train_loss_step=1.150, val_loss=3.120, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 51:  99%|███████▉| 66/67 [00:19<00:00,  3.35it/s, loss=2.43, v_num=1, train_loss_step=1.150, val_loss=3.120, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 51: 100%|████████| 67/67 [00:20<00:00,  3.28it/s, loss=2.43, v_num=1, train_loss_step=1.150, val_loss=3.130, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 52:  45%|███▌    | 30/67 [00:05<00:07,  5.19it/s, loss=1.55, v_num=1, train_loss_step=0.591, val_loss=3.130, train_loss_epoch=2.360]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 52:  46%|███▋    | 31/67 [00:07<00:08,  4.02it/s, loss=1.55, v_num=1, train_loss_step=0.591, val_loss=3.130, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 52:  48%|███▊    | 32/67 [00:08<00:08,  3.97it/s, loss=1.55, v_num=1, train_loss_step=0.591, val_loss=3.130, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 52:  49%|███▉    | 33/67 [00:08<00:08,  3.93it/s, loss=1.55, v_num=1, train_loss_step=0.591, val_loss=3.130, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 52:  51%|████    | 34/67 [00:08<00:08,  3.89it/s, loss=1.55, v_num=1, train_loss_step=0.591, val_loss=3.130, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 52:  52%|████▏   | 35/67 [00:09<00:08,  3.86it/s, loss=1.55, v_num=1, train_loss_step=0.591, val_loss=3.130, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 52:  54%|████▎   | 36/67 [00:09<00:08,  3.83it/s, loss=1.55, v_num=1, train_loss_step=0.591, val_loss=3.130, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 52:  55%|████▍   | 37/67 [00:09<00:07,  3.79it/s, loss=1.55, v_num=1, train_loss_step=0.591, val_loss=3.130, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 52:  57%|████▌   | 38/67 [00:10<00:07,  3.77it/s, loss=1.55, v_num=1, train_loss_step=0.591, val_loss=3.130, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 52:  58%|████▋   | 39/67 [00:10<00:07,  3.75it/s, loss=1.55, v_num=1, train_loss_step=0.591, val_loss=3.130, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 52:  60%|████▊   | 40/67 [00:10<00:07,  3.72it/s, loss=1.55, v_num=1, train_loss_step=0.591, val_loss=3.130, train_loss_epoch=2.360]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 52:  61%|████▉   | 41/67 [00:11<00:07,  3.67it/s, loss=1.55, v_num=1, train_loss_step=0.591, val_loss=3.130, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 52:  63%|█████   | 42/67 [00:11<00:06,  3.65it/s, loss=1.55, v_num=1, train_loss_step=0.591, val_loss=3.130, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 52:  64%|█████▏  | 43/67 [00:11<00:06,  3.63it/s, loss=1.55, v_num=1, train_loss_step=0.591, val_loss=3.130, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 52:  66%|█████▎  | 44/67 [00:12<00:06,  3.61it/s, loss=1.55, v_num=1, train_loss_step=0.591, val_loss=3.130, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 52:  67%|█████▎  | 45/67 [00:12<00:06,  3.59it/s, loss=1.55, v_num=1, train_loss_step=0.591, val_loss=3.130, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 52:  69%|█████▍  | 46/67 [00:12<00:05,  3.58it/s, loss=1.55, v_num=1, train_loss_step=0.591, val_loss=3.130, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 52:  70%|█████▌  | 47/67 [00:13<00:05,  3.56it/s, loss=1.55, v_num=1, train_loss_step=0.591, val_loss=3.130, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 52:  72%|█████▋  | 48/67 [00:13<00:05,  3.55it/s, loss=1.55, v_num=1, train_loss_step=0.591, val_loss=3.130, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 52:  73%|█████▊  | 49/67 [00:13<00:05,  3.54it/s, loss=1.55, v_num=1, train_loss_step=0.591, val_loss=3.130, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 52:  75%|█████▉  | 50/67 [00:14<00:04,  3.53it/s, loss=1.55, v_num=1, train_loss_step=0.591, val_loss=3.130, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 52:  76%|██████  | 51/67 [00:14<00:04,  3.49it/s, loss=1.55, v_num=1, train_loss_step=0.591, val_loss=3.130, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 52:  78%|██████▏ | 52/67 [00:14<00:04,  3.48it/s, loss=1.55, v_num=1, train_loss_step=0.591, val_loss=3.130, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 52:  79%|██████▎ | 53/67 [00:15<00:04,  3.47it/s, loss=1.55, v_num=1, train_loss_step=0.591, val_loss=3.130, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 52:  81%|██████▍ | 54/67 [00:15<00:03,  3.46it/s, loss=1.55, v_num=1, train_loss_step=0.591, val_loss=3.130, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 52:  82%|██████▌ | 55/67 [00:15<00:03,  3.45it/s, loss=1.55, v_num=1, train_loss_step=0.591, val_loss=3.130, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 52:  84%|██████▋ | 56/67 [00:16<00:03,  3.44it/s, loss=1.55, v_num=1, train_loss_step=0.591, val_loss=3.130, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 52:  85%|██████▊ | 57/67 [00:16<00:02,  3.44it/s, loss=1.55, v_num=1, train_loss_step=0.591, val_loss=3.130, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 52:  87%|██████▉ | 58/67 [00:16<00:02,  3.43it/s, loss=1.55, v_num=1, train_loss_step=0.591, val_loss=3.130, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 52:  88%|███████ | 59/67 [00:17<00:02,  3.42it/s, loss=1.55, v_num=1, train_loss_step=0.591, val_loss=3.130, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 52:  90%|███████▏| 60/67 [00:17<00:02,  3.42it/s, loss=1.55, v_num=1, train_loss_step=0.591, val_loss=3.130, train_loss_epoch=2.360]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 52:  91%|███████▎| 61/67 [00:18<00:01,  3.39it/s, loss=1.55, v_num=1, train_loss_step=0.591, val_loss=3.130, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 52:  93%|███████▍| 62/67 [00:18<00:01,  3.38it/s, loss=1.55, v_num=1, train_loss_step=0.591, val_loss=3.130, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 52:  94%|███████▌| 63/67 [00:18<00:01,  3.37it/s, loss=1.55, v_num=1, train_loss_step=0.591, val_loss=3.130, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 52:  96%|███████▋| 64/67 [00:19<00:00,  3.36it/s, loss=1.55, v_num=1, train_loss_step=0.591, val_loss=3.130, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 52:  97%|███████▊| 65/67 [00:19<00:00,  3.35it/s, loss=1.55, v_num=1, train_loss_step=0.591, val_loss=3.130, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 52:  99%|███████▉| 66/67 [00:19<00:00,  3.35it/s, loss=1.55, v_num=1, train_loss_step=0.591, val_loss=3.130, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 52: 100%|████████| 67/67 [00:20<00:00,  3.29it/s, loss=1.55, v_num=1, train_loss_step=0.591, val_loss=3.120, train_loss_epoch=2.360]\u001b[A\n",
      "Epoch 53:  45%|███▌    | 30/67 [00:05<00:06,  5.50it/s, loss=2.02, v_num=1, train_loss_step=3.180, val_loss=3.120, train_loss_epoch=1.640]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 53:  46%|███▋    | 31/67 [00:07<00:08,  4.13it/s, loss=2.02, v_num=1, train_loss_step=3.180, val_loss=3.120, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 53:  48%|███▊    | 32/67 [00:07<00:08,  4.08it/s, loss=2.02, v_num=1, train_loss_step=3.180, val_loss=3.120, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 53:  49%|███▉    | 33/67 [00:08<00:08,  4.03it/s, loss=2.02, v_num=1, train_loss_step=3.180, val_loss=3.120, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 53:  51%|████    | 34/67 [00:08<00:08,  3.99it/s, loss=2.02, v_num=1, train_loss_step=3.180, val_loss=3.120, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 53:  52%|████▏   | 35/67 [00:08<00:08,  3.95it/s, loss=2.02, v_num=1, train_loss_step=3.180, val_loss=3.120, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 53:  54%|████▎   | 36/67 [00:09<00:07,  3.91it/s, loss=2.02, v_num=1, train_loss_step=3.180, val_loss=3.120, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 53:  55%|████▍   | 37/67 [00:09<00:07,  3.88it/s, loss=2.02, v_num=1, train_loss_step=3.180, val_loss=3.120, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 53:  57%|████▌   | 38/67 [00:09<00:07,  3.86it/s, loss=2.02, v_num=1, train_loss_step=3.180, val_loss=3.120, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 53:  58%|████▋   | 39/67 [00:10<00:07,  3.83it/s, loss=2.02, v_num=1, train_loss_step=3.180, val_loss=3.120, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 53:  60%|████▊   | 40/67 [00:10<00:07,  3.80it/s, loss=2.02, v_num=1, train_loss_step=3.180, val_loss=3.120, train_loss_epoch=1.640]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 53:  61%|████▉   | 41/67 [00:10<00:06,  3.74it/s, loss=2.02, v_num=1, train_loss_step=3.180, val_loss=3.120, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 53:  63%|█████   | 42/67 [00:11<00:06,  3.72it/s, loss=2.02, v_num=1, train_loss_step=3.180, val_loss=3.120, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 53:  64%|█████▏  | 43/67 [00:11<00:06,  3.70it/s, loss=2.02, v_num=1, train_loss_step=3.180, val_loss=3.120, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 53:  66%|█████▎  | 44/67 [00:11<00:06,  3.68it/s, loss=2.02, v_num=1, train_loss_step=3.180, val_loss=3.120, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 53:  67%|█████▎  | 45/67 [00:12<00:06,  3.66it/s, loss=2.02, v_num=1, train_loss_step=3.180, val_loss=3.120, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 53:  69%|█████▍  | 46/67 [00:12<00:05,  3.64it/s, loss=2.02, v_num=1, train_loss_step=3.180, val_loss=3.120, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 53:  70%|█████▌  | 47/67 [00:12<00:05,  3.62it/s, loss=2.02, v_num=1, train_loss_step=3.180, val_loss=3.120, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 53:  72%|█████▋  | 48/67 [00:13<00:05,  3.61it/s, loss=2.02, v_num=1, train_loss_step=3.180, val_loss=3.120, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 53:  73%|█████▊  | 49/67 [00:13<00:05,  3.59it/s, loss=2.02, v_num=1, train_loss_step=3.180, val_loss=3.120, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 53:  75%|█████▉  | 50/67 [00:13<00:04,  3.57it/s, loss=2.02, v_num=1, train_loss_step=3.180, val_loss=3.120, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 53:  76%|██████  | 51/67 [00:14<00:04,  3.53it/s, loss=2.02, v_num=1, train_loss_step=3.180, val_loss=3.120, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 53:  78%|██████▏ | 52/67 [00:14<00:04,  3.52it/s, loss=2.02, v_num=1, train_loss_step=3.180, val_loss=3.120, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 53:  79%|██████▎ | 53/67 [00:15<00:03,  3.51it/s, loss=2.02, v_num=1, train_loss_step=3.180, val_loss=3.120, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 53:  81%|██████▍ | 54/67 [00:15<00:03,  3.50it/s, loss=2.02, v_num=1, train_loss_step=3.180, val_loss=3.120, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 53:  82%|██████▌ | 55/67 [00:15<00:03,  3.49it/s, loss=2.02, v_num=1, train_loss_step=3.180, val_loss=3.120, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 53:  84%|██████▋ | 56/67 [00:16<00:03,  3.48it/s, loss=2.02, v_num=1, train_loss_step=3.180, val_loss=3.120, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 53:  85%|██████▊ | 57/67 [00:16<00:02,  3.47it/s, loss=2.02, v_num=1, train_loss_step=3.180, val_loss=3.120, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 53:  87%|██████▉ | 58/67 [00:16<00:02,  3.46it/s, loss=2.02, v_num=1, train_loss_step=3.180, val_loss=3.120, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 53:  88%|███████ | 59/67 [00:17<00:02,  3.45it/s, loss=2.02, v_num=1, train_loss_step=3.180, val_loss=3.120, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 53:  90%|███████▏| 60/67 [00:17<00:02,  3.45it/s, loss=2.02, v_num=1, train_loss_step=3.180, val_loss=3.120, train_loss_epoch=1.640]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 53:  91%|███████▎| 61/67 [00:17<00:01,  3.41it/s, loss=2.02, v_num=1, train_loss_step=3.180, val_loss=3.120, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 53:  93%|███████▍| 62/67 [00:18<00:01,  3.40it/s, loss=2.02, v_num=1, train_loss_step=3.180, val_loss=3.120, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 53:  94%|███████▌| 63/67 [00:18<00:01,  3.40it/s, loss=2.02, v_num=1, train_loss_step=3.180, val_loss=3.120, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 53:  96%|███████▋| 64/67 [00:18<00:00,  3.39it/s, loss=2.02, v_num=1, train_loss_step=3.180, val_loss=3.120, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 53:  97%|███████▊| 65/67 [00:19<00:00,  3.39it/s, loss=2.02, v_num=1, train_loss_step=3.180, val_loss=3.120, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 53:  99%|███████▉| 66/67 [00:19<00:00,  3.38it/s, loss=2.02, v_num=1, train_loss_step=3.180, val_loss=3.120, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 53: 100%|████████| 67/67 [00:20<00:00,  3.31it/s, loss=2.02, v_num=1, train_loss_step=3.180, val_loss=3.120, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 54:  45%|███▌    | 30/67 [00:05<00:07,  5.18it/s, loss=2.31, v_num=1, train_loss_step=5.080, val_loss=3.120, train_loss_epoch=1.940]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 54:  46%|███▋    | 31/67 [00:07<00:08,  4.07it/s, loss=2.31, v_num=1, train_loss_step=5.080, val_loss=3.120, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 54:  48%|███▊    | 32/67 [00:07<00:08,  4.03it/s, loss=2.31, v_num=1, train_loss_step=5.080, val_loss=3.120, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 54:  49%|███▉    | 33/67 [00:08<00:08,  3.99it/s, loss=2.31, v_num=1, train_loss_step=5.080, val_loss=3.120, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 54:  51%|████    | 34/67 [00:08<00:08,  3.95it/s, loss=2.31, v_num=1, train_loss_step=5.080, val_loss=3.120, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 54:  52%|████▏   | 35/67 [00:08<00:08,  3.92it/s, loss=2.31, v_num=1, train_loss_step=5.080, val_loss=3.120, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 54:  54%|████▎   | 36/67 [00:09<00:07,  3.88it/s, loss=2.31, v_num=1, train_loss_step=5.080, val_loss=3.120, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 54:  55%|████▍   | 37/67 [00:09<00:07,  3.85it/s, loss=2.31, v_num=1, train_loss_step=5.080, val_loss=3.120, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 54:  57%|████▌   | 38/67 [00:09<00:07,  3.82it/s, loss=2.31, v_num=1, train_loss_step=5.080, val_loss=3.120, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 54:  58%|████▋   | 39/67 [00:10<00:07,  3.79it/s, loss=2.31, v_num=1, train_loss_step=5.080, val_loss=3.120, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 54:  60%|████▊   | 40/67 [00:10<00:07,  3.77it/s, loss=2.31, v_num=1, train_loss_step=5.080, val_loss=3.120, train_loss_epoch=1.940]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 54:  61%|████▉   | 41/67 [00:11<00:07,  3.70it/s, loss=2.31, v_num=1, train_loss_step=5.080, val_loss=3.120, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 54:  63%|█████   | 42/67 [00:11<00:06,  3.69it/s, loss=2.31, v_num=1, train_loss_step=5.080, val_loss=3.120, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 54:  64%|█████▏  | 43/67 [00:11<00:06,  3.67it/s, loss=2.31, v_num=1, train_loss_step=5.080, val_loss=3.120, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 54:  66%|█████▎  | 44/67 [00:12<00:06,  3.64it/s, loss=2.31, v_num=1, train_loss_step=5.080, val_loss=3.120, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 54:  67%|█████▎  | 45/67 [00:12<00:06,  3.63it/s, loss=2.31, v_num=1, train_loss_step=5.080, val_loss=3.120, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 54:  69%|█████▍  | 46/67 [00:12<00:05,  3.61it/s, loss=2.31, v_num=1, train_loss_step=5.080, val_loss=3.120, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 54:  70%|█████▌  | 47/67 [00:13<00:05,  3.59it/s, loss=2.31, v_num=1, train_loss_step=5.080, val_loss=3.120, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 54:  72%|█████▋  | 48/67 [00:13<00:05,  3.58it/s, loss=2.31, v_num=1, train_loss_step=5.080, val_loss=3.120, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 54:  73%|█████▊  | 49/67 [00:13<00:05,  3.57it/s, loss=2.31, v_num=1, train_loss_step=5.080, val_loss=3.120, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 54:  75%|█████▉  | 50/67 [00:14<00:04,  3.55it/s, loss=2.31, v_num=1, train_loss_step=5.080, val_loss=3.120, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 54:  76%|██████  | 51/67 [00:14<00:04,  3.51it/s, loss=2.31, v_num=1, train_loss_step=5.080, val_loss=3.120, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 54:  78%|██████▏ | 52/67 [00:14<00:04,  3.49it/s, loss=2.31, v_num=1, train_loss_step=5.080, val_loss=3.120, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 54:  79%|██████▎ | 53/67 [00:15<00:04,  3.47it/s, loss=2.31, v_num=1, train_loss_step=5.080, val_loss=3.120, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 54:  81%|██████▍ | 54/67 [00:15<00:03,  3.47it/s, loss=2.31, v_num=1, train_loss_step=5.080, val_loss=3.120, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 54:  82%|██████▌ | 55/67 [00:15<00:03,  3.46it/s, loss=2.31, v_num=1, train_loss_step=5.080, val_loss=3.120, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 54:  84%|██████▋ | 56/67 [00:16<00:03,  3.44it/s, loss=2.31, v_num=1, train_loss_step=5.080, val_loss=3.120, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 54:  85%|██████▊ | 57/67 [00:16<00:02,  3.43it/s, loss=2.31, v_num=1, train_loss_step=5.080, val_loss=3.120, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 54:  87%|██████▉ | 58/67 [00:16<00:02,  3.42it/s, loss=2.31, v_num=1, train_loss_step=5.080, val_loss=3.120, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 54:  88%|███████ | 59/67 [00:17<00:02,  3.41it/s, loss=2.31, v_num=1, train_loss_step=5.080, val_loss=3.120, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 54:  90%|███████▏| 60/67 [00:17<00:02,  3.41it/s, loss=2.31, v_num=1, train_loss_step=5.080, val_loss=3.120, train_loss_epoch=1.940]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 54:  91%|███████▎| 61/67 [00:18<00:01,  3.37it/s, loss=2.31, v_num=1, train_loss_step=5.080, val_loss=3.120, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 54:  93%|███████▍| 62/67 [00:18<00:01,  3.36it/s, loss=2.31, v_num=1, train_loss_step=5.080, val_loss=3.120, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 54:  94%|███████▌| 63/67 [00:18<00:01,  3.36it/s, loss=2.31, v_num=1, train_loss_step=5.080, val_loss=3.120, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 54:  96%|███████▋| 64/67 [00:19<00:00,  3.35it/s, loss=2.31, v_num=1, train_loss_step=5.080, val_loss=3.120, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 54:  97%|███████▊| 65/67 [00:19<00:00,  3.35it/s, loss=2.31, v_num=1, train_loss_step=5.080, val_loss=3.120, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 54:  99%|███████▉| 66/67 [00:19<00:00,  3.34it/s, loss=2.31, v_num=1, train_loss_step=5.080, val_loss=3.120, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 54: 100%|████████| 67/67 [00:20<00:00,  3.27it/s, loss=2.31, v_num=1, train_loss_step=5.080, val_loss=3.120, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 55:  45%|███▌    | 30/67 [00:06<00:08,  4.58it/s, loss=1.57, v_num=1, train_loss_step=0.793, val_loss=3.120, train_loss_epoch=2.170]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 55:  46%|███▋    | 31/67 [00:08<00:09,  3.66it/s, loss=1.57, v_num=1, train_loss_step=0.793, val_loss=3.120, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 55:  48%|███▊    | 32/67 [00:08<00:09,  3.64it/s, loss=1.57, v_num=1, train_loss_step=0.793, val_loss=3.120, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 55:  49%|███▉    | 33/67 [00:09<00:09,  3.61it/s, loss=1.57, v_num=1, train_loss_step=0.793, val_loss=3.120, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 55:  51%|████    | 34/67 [00:09<00:09,  3.59it/s, loss=1.57, v_num=1, train_loss_step=0.793, val_loss=3.120, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 55:  52%|████▏   | 35/67 [00:09<00:08,  3.57it/s, loss=1.57, v_num=1, train_loss_step=0.793, val_loss=3.120, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 55:  54%|████▎   | 36/67 [00:10<00:08,  3.55it/s, loss=1.57, v_num=1, train_loss_step=0.793, val_loss=3.120, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 55:  55%|████▍   | 37/67 [00:10<00:08,  3.53it/s, loss=1.57, v_num=1, train_loss_step=0.793, val_loss=3.120, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 55:  57%|████▌   | 38/67 [00:10<00:08,  3.52it/s, loss=1.57, v_num=1, train_loss_step=0.793, val_loss=3.120, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 55:  58%|████▋   | 39/67 [00:11<00:07,  3.50it/s, loss=1.57, v_num=1, train_loss_step=0.793, val_loss=3.120, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 55:  60%|████▊   | 40/67 [00:11<00:07,  3.48it/s, loss=1.57, v_num=1, train_loss_step=0.793, val_loss=3.120, train_loss_epoch=2.170]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 55:  61%|████▉   | 41/67 [00:11<00:07,  3.43it/s, loss=1.57, v_num=1, train_loss_step=0.793, val_loss=3.120, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 55:  63%|█████   | 42/67 [00:12<00:07,  3.42it/s, loss=1.57, v_num=1, train_loss_step=0.793, val_loss=3.120, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 55:  64%|█████▏  | 43/67 [00:12<00:07,  3.41it/s, loss=1.57, v_num=1, train_loss_step=0.793, val_loss=3.120, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 55:  66%|█████▎  | 44/67 [00:12<00:06,  3.40it/s, loss=1.57, v_num=1, train_loss_step=0.793, val_loss=3.120, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 55:  67%|█████▎  | 45/67 [00:13<00:06,  3.40it/s, loss=1.57, v_num=1, train_loss_step=0.793, val_loss=3.120, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 55:  69%|█████▍  | 46/67 [00:13<00:06,  3.38it/s, loss=1.57, v_num=1, train_loss_step=0.793, val_loss=3.120, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 55:  70%|█████▌  | 47/67 [00:13<00:05,  3.38it/s, loss=1.57, v_num=1, train_loss_step=0.793, val_loss=3.120, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 55:  72%|█████▋  | 48/67 [00:14<00:05,  3.37it/s, loss=1.57, v_num=1, train_loss_step=0.793, val_loss=3.120, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 55:  73%|█████▊  | 49/67 [00:14<00:05,  3.36it/s, loss=1.57, v_num=1, train_loss_step=0.793, val_loss=3.120, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 55:  75%|█████▉  | 50/67 [00:14<00:05,  3.36it/s, loss=1.57, v_num=1, train_loss_step=0.793, val_loss=3.120, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 55:  76%|██████  | 51/67 [00:15<00:04,  3.32it/s, loss=1.57, v_num=1, train_loss_step=0.793, val_loss=3.120, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 55:  78%|██████▏ | 52/67 [00:15<00:04,  3.32it/s, loss=1.57, v_num=1, train_loss_step=0.793, val_loss=3.120, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 55:  79%|██████▎ | 53/67 [00:16<00:04,  3.31it/s, loss=1.57, v_num=1, train_loss_step=0.793, val_loss=3.120, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 55:  81%|██████▍ | 54/67 [00:16<00:03,  3.31it/s, loss=1.57, v_num=1, train_loss_step=0.793, val_loss=3.120, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 55:  82%|██████▌ | 55/67 [00:16<00:03,  3.30it/s, loss=1.57, v_num=1, train_loss_step=0.793, val_loss=3.120, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 55:  84%|██████▋ | 56/67 [00:16<00:03,  3.30it/s, loss=1.57, v_num=1, train_loss_step=0.793, val_loss=3.120, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 55:  85%|██████▊ | 57/67 [00:17<00:03,  3.29it/s, loss=1.57, v_num=1, train_loss_step=0.793, val_loss=3.120, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 55:  87%|██████▉ | 58/67 [00:17<00:02,  3.29it/s, loss=1.57, v_num=1, train_loss_step=0.793, val_loss=3.120, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 55:  88%|███████ | 59/67 [00:17<00:02,  3.28it/s, loss=1.57, v_num=1, train_loss_step=0.793, val_loss=3.120, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 55:  90%|███████▏| 60/67 [00:18<00:02,  3.28it/s, loss=1.57, v_num=1, train_loss_step=0.793, val_loss=3.120, train_loss_epoch=2.170]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 55:  91%|███████▎| 61/67 [00:18<00:01,  3.25it/s, loss=1.57, v_num=1, train_loss_step=0.793, val_loss=3.120, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 55:  93%|███████▍| 62/67 [00:19<00:01,  3.25it/s, loss=1.57, v_num=1, train_loss_step=0.793, val_loss=3.120, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 55:  94%|███████▌| 63/67 [00:19<00:01,  3.25it/s, loss=1.57, v_num=1, train_loss_step=0.793, val_loss=3.120, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 55:  96%|███████▋| 64/67 [00:19<00:00,  3.25it/s, loss=1.57, v_num=1, train_loss_step=0.793, val_loss=3.120, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 55:  97%|███████▊| 65/67 [00:20<00:00,  3.24it/s, loss=1.57, v_num=1, train_loss_step=0.793, val_loss=3.120, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 55:  99%|███████▉| 66/67 [00:20<00:00,  3.24it/s, loss=1.57, v_num=1, train_loss_step=0.793, val_loss=3.120, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 55: 100%|████████| 67/67 [00:21<00:00,  3.18it/s, loss=1.57, v_num=1, train_loss_step=0.793, val_loss=3.110, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 56:  45%|████     | 30/67 [00:05<00:07,  5.25it/s, loss=1.7, v_num=1, train_loss_step=2.770, val_loss=3.110, train_loss_epoch=2.160]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 56:  46%|████▏    | 31/67 [00:07<00:08,  4.04it/s, loss=1.7, v_num=1, train_loss_step=2.770, val_loss=3.110, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 56:  48%|████▎    | 32/67 [00:08<00:08,  4.00it/s, loss=1.7, v_num=1, train_loss_step=2.770, val_loss=3.110, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 56:  49%|████▍    | 33/67 [00:08<00:08,  3.96it/s, loss=1.7, v_num=1, train_loss_step=2.770, val_loss=3.110, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 56:  51%|████▌    | 34/67 [00:08<00:08,  3.92it/s, loss=1.7, v_num=1, train_loss_step=2.770, val_loss=3.110, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 56:  52%|████▋    | 35/67 [00:09<00:08,  3.89it/s, loss=1.7, v_num=1, train_loss_step=2.770, val_loss=3.110, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 56:  54%|████▊    | 36/67 [00:09<00:08,  3.86it/s, loss=1.7, v_num=1, train_loss_step=2.770, val_loss=3.110, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 56:  55%|████▉    | 37/67 [00:09<00:07,  3.83it/s, loss=1.7, v_num=1, train_loss_step=2.770, val_loss=3.110, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 56:  57%|█████    | 38/67 [00:09<00:07,  3.80it/s, loss=1.7, v_num=1, train_loss_step=2.770, val_loss=3.110, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 56:  58%|█████▏   | 39/67 [00:10<00:07,  3.78it/s, loss=1.7, v_num=1, train_loss_step=2.770, val_loss=3.110, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 56:  60%|█████▎   | 40/67 [00:10<00:07,  3.75it/s, loss=1.7, v_num=1, train_loss_step=2.770, val_loss=3.110, train_loss_epoch=2.160]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 56:  61%|█████▌   | 41/67 [00:11<00:07,  3.69it/s, loss=1.7, v_num=1, train_loss_step=2.770, val_loss=3.110, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 56:  63%|█████▋   | 42/67 [00:11<00:06,  3.67it/s, loss=1.7, v_num=1, train_loss_step=2.770, val_loss=3.110, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 56:  64%|█████▊   | 43/67 [00:11<00:06,  3.65it/s, loss=1.7, v_num=1, train_loss_step=2.770, val_loss=3.110, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 56:  66%|█████▉   | 44/67 [00:12<00:06,  3.64it/s, loss=1.7, v_num=1, train_loss_step=2.770, val_loss=3.110, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 56:  67%|██████   | 45/67 [00:12<00:06,  3.62it/s, loss=1.7, v_num=1, train_loss_step=2.770, val_loss=3.110, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 56:  69%|██████▏  | 46/67 [00:12<00:05,  3.60it/s, loss=1.7, v_num=1, train_loss_step=2.770, val_loss=3.110, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 56:  70%|██████▎  | 47/67 [00:13<00:05,  3.59it/s, loss=1.7, v_num=1, train_loss_step=2.770, val_loss=3.110, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 56:  72%|██████▍  | 48/67 [00:13<00:05,  3.58it/s, loss=1.7, v_num=1, train_loss_step=2.770, val_loss=3.110, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 56:  73%|██████▌  | 49/67 [00:13<00:05,  3.57it/s, loss=1.7, v_num=1, train_loss_step=2.770, val_loss=3.110, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 56:  75%|██████▋  | 50/67 [00:14<00:04,  3.55it/s, loss=1.7, v_num=1, train_loss_step=2.770, val_loss=3.110, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 56:  76%|██████▊  | 51/67 [00:14<00:04,  3.51it/s, loss=1.7, v_num=1, train_loss_step=2.770, val_loss=3.110, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 56:  78%|██████▉  | 52/67 [00:14<00:04,  3.50it/s, loss=1.7, v_num=1, train_loss_step=2.770, val_loss=3.110, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 56:  79%|███████  | 53/67 [00:15<00:04,  3.49it/s, loss=1.7, v_num=1, train_loss_step=2.770, val_loss=3.110, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 56:  81%|███████▎ | 54/67 [00:15<00:03,  3.48it/s, loss=1.7, v_num=1, train_loss_step=2.770, val_loss=3.110, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 56:  82%|███████▍ | 55/67 [00:15<00:03,  3.47it/s, loss=1.7, v_num=1, train_loss_step=2.770, val_loss=3.110, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 56:  84%|███████▌ | 56/67 [00:16<00:03,  3.46it/s, loss=1.7, v_num=1, train_loss_step=2.770, val_loss=3.110, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 56:  85%|███████▋ | 57/67 [00:16<00:02,  3.45it/s, loss=1.7, v_num=1, train_loss_step=2.770, val_loss=3.110, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 56:  87%|███████▊ | 58/67 [00:16<00:02,  3.44it/s, loss=1.7, v_num=1, train_loss_step=2.770, val_loss=3.110, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 56:  88%|███████▉ | 59/67 [00:17<00:02,  3.44it/s, loss=1.7, v_num=1, train_loss_step=2.770, val_loss=3.110, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 56:  90%|████████ | 60/67 [00:17<00:02,  3.43it/s, loss=1.7, v_num=1, train_loss_step=2.770, val_loss=3.110, train_loss_epoch=2.160]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 56:  91%|████████▏| 61/67 [00:17<00:01,  3.40it/s, loss=1.7, v_num=1, train_loss_step=2.770, val_loss=3.110, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 56:  93%|████████▎| 62/67 [00:18<00:01,  3.40it/s, loss=1.7, v_num=1, train_loss_step=2.770, val_loss=3.110, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 56:  94%|████████▍| 63/67 [00:18<00:01,  3.39it/s, loss=1.7, v_num=1, train_loss_step=2.770, val_loss=3.110, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 56:  96%|████████▌| 64/67 [00:18<00:00,  3.39it/s, loss=1.7, v_num=1, train_loss_step=2.770, val_loss=3.110, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 56:  97%|████████▋| 65/67 [00:19<00:00,  3.38it/s, loss=1.7, v_num=1, train_loss_step=2.770, val_loss=3.110, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 56:  99%|████████▊| 66/67 [00:19<00:00,  3.38it/s, loss=1.7, v_num=1, train_loss_step=2.770, val_loss=3.110, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 56: 100%|█████████| 67/67 [00:20<00:00,  3.31it/s, loss=1.7, v_num=1, train_loss_step=2.770, val_loss=3.110, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 57:  45%|███▌    | 30/67 [00:05<00:06,  5.30it/s, loss=2.75, v_num=1, train_loss_step=11.60, val_loss=3.110, train_loss_epoch=1.840]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 57:  46%|███▋    | 31/67 [00:07<00:08,  4.05it/s, loss=2.75, v_num=1, train_loss_step=11.60, val_loss=3.110, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 57:  48%|███▊    | 32/67 [00:07<00:08,  4.01it/s, loss=2.75, v_num=1, train_loss_step=11.60, val_loss=3.110, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 57:  49%|███▉    | 33/67 [00:08<00:08,  3.97it/s, loss=2.75, v_num=1, train_loss_step=11.60, val_loss=3.110, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 57:  51%|████    | 34/67 [00:08<00:08,  3.94it/s, loss=2.75, v_num=1, train_loss_step=11.60, val_loss=3.110, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 57:  52%|████▏   | 35/67 [00:08<00:08,  3.90it/s, loss=2.75, v_num=1, train_loss_step=11.60, val_loss=3.110, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 57:  54%|████▎   | 36/67 [00:09<00:08,  3.87it/s, loss=2.75, v_num=1, train_loss_step=11.60, val_loss=3.110, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 57:  55%|████▍   | 37/67 [00:09<00:07,  3.84it/s, loss=2.75, v_num=1, train_loss_step=11.60, val_loss=3.110, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 57:  57%|████▌   | 38/67 [00:09<00:07,  3.81it/s, loss=2.75, v_num=1, train_loss_step=11.60, val_loss=3.110, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 57:  58%|████▋   | 39/67 [00:10<00:07,  3.79it/s, loss=2.75, v_num=1, train_loss_step=11.60, val_loss=3.110, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 57:  60%|████▊   | 40/67 [00:10<00:07,  3.76it/s, loss=2.75, v_num=1, train_loss_step=11.60, val_loss=3.110, train_loss_epoch=1.840]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 57:  61%|████▉   | 41/67 [00:11<00:07,  3.70it/s, loss=2.75, v_num=1, train_loss_step=11.60, val_loss=3.110, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 57:  63%|█████   | 42/67 [00:11<00:06,  3.68it/s, loss=2.75, v_num=1, train_loss_step=11.60, val_loss=3.110, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 57:  64%|█████▏  | 43/67 [00:11<00:06,  3.66it/s, loss=2.75, v_num=1, train_loss_step=11.60, val_loss=3.110, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 57:  66%|█████▎  | 44/67 [00:12<00:06,  3.64it/s, loss=2.75, v_num=1, train_loss_step=11.60, val_loss=3.110, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 57:  67%|█████▎  | 45/67 [00:12<00:06,  3.63it/s, loss=2.75, v_num=1, train_loss_step=11.60, val_loss=3.110, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 57:  69%|█████▍  | 46/67 [00:12<00:05,  3.61it/s, loss=2.75, v_num=1, train_loss_step=11.60, val_loss=3.110, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 57:  70%|█████▌  | 47/67 [00:13<00:05,  3.60it/s, loss=2.75, v_num=1, train_loss_step=11.60, val_loss=3.110, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 57:  72%|█████▋  | 48/67 [00:13<00:05,  3.59it/s, loss=2.75, v_num=1, train_loss_step=11.60, val_loss=3.110, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 57:  73%|█████▊  | 49/67 [00:13<00:05,  3.57it/s, loss=2.75, v_num=1, train_loss_step=11.60, val_loss=3.110, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 57:  75%|█████▉  | 50/67 [00:14<00:04,  3.56it/s, loss=2.75, v_num=1, train_loss_step=11.60, val_loss=3.110, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 57:  76%|██████  | 51/67 [00:14<00:04,  3.52it/s, loss=2.75, v_num=1, train_loss_step=11.60, val_loss=3.110, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 57:  78%|██████▏ | 52/67 [00:14<00:04,  3.51it/s, loss=2.75, v_num=1, train_loss_step=11.60, val_loss=3.110, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 57:  79%|██████▎ | 53/67 [00:15<00:04,  3.50it/s, loss=2.75, v_num=1, train_loss_step=11.60, val_loss=3.110, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 57:  81%|██████▍ | 54/67 [00:15<00:03,  3.49it/s, loss=2.75, v_num=1, train_loss_step=11.60, val_loss=3.110, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 57:  82%|██████▌ | 55/67 [00:15<00:03,  3.48it/s, loss=2.75, v_num=1, train_loss_step=11.60, val_loss=3.110, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 57:  84%|██████▋ | 56/67 [00:16<00:03,  3.47it/s, loss=2.75, v_num=1, train_loss_step=11.60, val_loss=3.110, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 57:  85%|██████▊ | 57/67 [00:16<00:02,  3.46it/s, loss=2.75, v_num=1, train_loss_step=11.60, val_loss=3.110, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 57:  87%|██████▉ | 58/67 [00:16<00:02,  3.45it/s, loss=2.75, v_num=1, train_loss_step=11.60, val_loss=3.110, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 57:  88%|███████ | 59/67 [00:17<00:02,  3.44it/s, loss=2.75, v_num=1, train_loss_step=11.60, val_loss=3.110, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 57:  90%|███████▏| 60/67 [00:17<00:02,  3.44it/s, loss=2.75, v_num=1, train_loss_step=11.60, val_loss=3.110, train_loss_epoch=1.840]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 57:  91%|███████▎| 61/67 [00:17<00:01,  3.41it/s, loss=2.75, v_num=1, train_loss_step=11.60, val_loss=3.110, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 57:  93%|███████▍| 62/67 [00:18<00:01,  3.40it/s, loss=2.75, v_num=1, train_loss_step=11.60, val_loss=3.110, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 57:  94%|███████▌| 63/67 [00:18<00:01,  3.40it/s, loss=2.75, v_num=1, train_loss_step=11.60, val_loss=3.110, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 57:  96%|███████▋| 64/67 [00:18<00:00,  3.39it/s, loss=2.75, v_num=1, train_loss_step=11.60, val_loss=3.110, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 57:  97%|███████▊| 65/67 [00:19<00:00,  3.38it/s, loss=2.75, v_num=1, train_loss_step=11.60, val_loss=3.110, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 57:  99%|███████▉| 66/67 [00:19<00:00,  3.38it/s, loss=2.75, v_num=1, train_loss_step=11.60, val_loss=3.110, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 57: 100%|████████| 67/67 [00:20<00:00,  3.31it/s, loss=2.75, v_num=1, train_loss_step=11.60, val_loss=3.110, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 58:  45%|███▌    | 30/67 [00:05<00:07,  5.28it/s, loss=2.13, v_num=1, train_loss_step=7.470, val_loss=3.110, train_loss_epoch=2.480]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 58:  46%|███▋    | 31/67 [00:07<00:08,  4.05it/s, loss=2.13, v_num=1, train_loss_step=7.470, val_loss=3.110, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 58:  48%|███▊    | 32/67 [00:07<00:08,  4.01it/s, loss=2.13, v_num=1, train_loss_step=7.470, val_loss=3.110, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 58:  49%|███▉    | 33/67 [00:08<00:08,  3.97it/s, loss=2.13, v_num=1, train_loss_step=7.470, val_loss=3.110, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 58:  51%|████    | 34/67 [00:08<00:08,  3.93it/s, loss=2.13, v_num=1, train_loss_step=7.470, val_loss=3.110, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 58:  52%|████▏   | 35/67 [00:08<00:08,  3.89it/s, loss=2.13, v_num=1, train_loss_step=7.470, val_loss=3.110, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 58:  54%|████▎   | 36/67 [00:09<00:08,  3.87it/s, loss=2.13, v_num=1, train_loss_step=7.470, val_loss=3.110, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 58:  55%|████▍   | 37/67 [00:09<00:07,  3.83it/s, loss=2.13, v_num=1, train_loss_step=7.470, val_loss=3.110, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 58:  57%|████▌   | 38/67 [00:09<00:07,  3.81it/s, loss=2.13, v_num=1, train_loss_step=7.470, val_loss=3.110, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 58:  58%|████▋   | 39/67 [00:10<00:07,  3.79it/s, loss=2.13, v_num=1, train_loss_step=7.470, val_loss=3.110, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 58:  60%|████▊   | 40/67 [00:10<00:07,  3.76it/s, loss=2.13, v_num=1, train_loss_step=7.470, val_loss=3.110, train_loss_epoch=2.480]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 58:  61%|████▉   | 41/67 [00:11<00:07,  3.71it/s, loss=2.13, v_num=1, train_loss_step=7.470, val_loss=3.110, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 58:  63%|█████   | 42/67 [00:11<00:06,  3.69it/s, loss=2.13, v_num=1, train_loss_step=7.470, val_loss=3.110, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 58:  64%|█████▏  | 43/67 [00:11<00:06,  3.67it/s, loss=2.13, v_num=1, train_loss_step=7.470, val_loss=3.110, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 58:  66%|█████▎  | 44/67 [00:12<00:06,  3.65it/s, loss=2.13, v_num=1, train_loss_step=7.470, val_loss=3.110, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 58:  67%|█████▎  | 45/67 [00:12<00:06,  3.64it/s, loss=2.13, v_num=1, train_loss_step=7.470, val_loss=3.110, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 58:  69%|█████▍  | 46/67 [00:12<00:05,  3.62it/s, loss=2.13, v_num=1, train_loss_step=7.470, val_loss=3.110, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 58:  70%|█████▌  | 47/67 [00:13<00:05,  3.60it/s, loss=2.13, v_num=1, train_loss_step=7.470, val_loss=3.110, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 58:  72%|█████▋  | 48/67 [00:13<00:05,  3.59it/s, loss=2.13, v_num=1, train_loss_step=7.470, val_loss=3.110, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 58:  73%|█████▊  | 49/67 [00:13<00:05,  3.58it/s, loss=2.13, v_num=1, train_loss_step=7.470, val_loss=3.110, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 58:  75%|█████▉  | 50/67 [00:14<00:04,  3.56it/s, loss=2.13, v_num=1, train_loss_step=7.470, val_loss=3.110, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 58:  76%|██████  | 51/67 [00:14<00:04,  3.52it/s, loss=2.13, v_num=1, train_loss_step=7.470, val_loss=3.110, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 58:  78%|██████▏ | 52/67 [00:14<00:04,  3.51it/s, loss=2.13, v_num=1, train_loss_step=7.470, val_loss=3.110, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 58:  79%|██████▎ | 53/67 [00:15<00:04,  3.50it/s, loss=2.13, v_num=1, train_loss_step=7.470, val_loss=3.110, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 58:  81%|██████▍ | 54/67 [00:15<00:03,  3.49it/s, loss=2.13, v_num=1, train_loss_step=7.470, val_loss=3.110, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 58:  82%|██████▌ | 55/67 [00:15<00:03,  3.48it/s, loss=2.13, v_num=1, train_loss_step=7.470, val_loss=3.110, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 58:  84%|██████▋ | 56/67 [00:16<00:03,  3.47it/s, loss=2.13, v_num=1, train_loss_step=7.470, val_loss=3.110, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 58:  85%|██████▊ | 57/67 [00:16<00:02,  3.46it/s, loss=2.13, v_num=1, train_loss_step=7.470, val_loss=3.110, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 58:  87%|██████▉ | 58/67 [00:16<00:02,  3.45it/s, loss=2.13, v_num=1, train_loss_step=7.470, val_loss=3.110, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 58:  88%|███████ | 59/67 [00:17<00:02,  3.44it/s, loss=2.13, v_num=1, train_loss_step=7.470, val_loss=3.110, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 58:  90%|███████▏| 60/67 [00:17<00:02,  3.44it/s, loss=2.13, v_num=1, train_loss_step=7.470, val_loss=3.110, train_loss_epoch=2.480]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 58:  91%|███████▎| 61/67 [00:17<00:01,  3.40it/s, loss=2.13, v_num=1, train_loss_step=7.470, val_loss=3.110, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 58:  93%|███████▍| 62/67 [00:18<00:01,  3.40it/s, loss=2.13, v_num=1, train_loss_step=7.470, val_loss=3.110, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 58:  94%|███████▌| 63/67 [00:18<00:01,  3.39it/s, loss=2.13, v_num=1, train_loss_step=7.470, val_loss=3.110, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 58:  96%|███████▋| 64/67 [00:18<00:00,  3.39it/s, loss=2.13, v_num=1, train_loss_step=7.470, val_loss=3.110, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 58:  97%|███████▊| 65/67 [00:19<00:00,  3.38it/s, loss=2.13, v_num=1, train_loss_step=7.470, val_loss=3.110, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 58:  99%|███████▉| 66/67 [00:19<00:00,  3.38it/s, loss=2.13, v_num=1, train_loss_step=7.470, val_loss=3.110, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 58: 100%|████████| 67/67 [00:20<00:00,  3.31it/s, loss=2.13, v_num=1, train_loss_step=7.470, val_loss=3.110, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 59:  45%|███▌    | 30/67 [00:05<00:07,  5.24it/s, loss=2.66, v_num=1, train_loss_step=1.150, val_loss=3.110, train_loss_epoch=2.080]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 59:  46%|███▋    | 31/67 [00:07<00:08,  4.07it/s, loss=2.66, v_num=1, train_loss_step=1.150, val_loss=3.110, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 59:  48%|███▊    | 32/67 [00:07<00:08,  4.02it/s, loss=2.66, v_num=1, train_loss_step=1.150, val_loss=3.110, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 59:  49%|███▉    | 33/67 [00:08<00:08,  3.98it/s, loss=2.66, v_num=1, train_loss_step=1.150, val_loss=3.110, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 59:  51%|████    | 34/67 [00:08<00:08,  3.94it/s, loss=2.66, v_num=1, train_loss_step=1.150, val_loss=3.110, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 59:  52%|████▏   | 35/67 [00:08<00:08,  3.91it/s, loss=2.66, v_num=1, train_loss_step=1.150, val_loss=3.110, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 59:  54%|████▎   | 36/67 [00:09<00:07,  3.88it/s, loss=2.66, v_num=1, train_loss_step=1.150, val_loss=3.110, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 59:  55%|████▍   | 37/67 [00:09<00:07,  3.85it/s, loss=2.66, v_num=1, train_loss_step=1.150, val_loss=3.110, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 59:  57%|████▌   | 38/67 [00:09<00:07,  3.82it/s, loss=2.66, v_num=1, train_loss_step=1.150, val_loss=3.110, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 59:  58%|████▋   | 39/67 [00:10<00:07,  3.80it/s, loss=2.66, v_num=1, train_loss_step=1.150, val_loss=3.110, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 59:  60%|████▊   | 40/67 [00:10<00:07,  3.77it/s, loss=2.66, v_num=1, train_loss_step=1.150, val_loss=3.110, train_loss_epoch=2.080]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 59:  61%|████▉   | 41/67 [00:11<00:07,  3.71it/s, loss=2.66, v_num=1, train_loss_step=1.150, val_loss=3.110, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 59:  63%|█████   | 42/67 [00:11<00:06,  3.70it/s, loss=2.66, v_num=1, train_loss_step=1.150, val_loss=3.110, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 59:  64%|█████▏  | 43/67 [00:11<00:06,  3.67it/s, loss=2.66, v_num=1, train_loss_step=1.150, val_loss=3.110, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 59:  66%|█████▎  | 44/67 [00:12<00:06,  3.66it/s, loss=2.66, v_num=1, train_loss_step=1.150, val_loss=3.110, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 59:  67%|█████▎  | 45/67 [00:12<00:06,  3.64it/s, loss=2.66, v_num=1, train_loss_step=1.150, val_loss=3.110, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 59:  69%|█████▍  | 46/67 [00:12<00:05,  3.63it/s, loss=2.66, v_num=1, train_loss_step=1.150, val_loss=3.110, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 59:  70%|█████▌  | 47/67 [00:13<00:05,  3.61it/s, loss=2.66, v_num=1, train_loss_step=1.150, val_loss=3.110, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 59:  72%|█████▋  | 48/67 [00:13<00:05,  3.60it/s, loss=2.66, v_num=1, train_loss_step=1.150, val_loss=3.110, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 59:  73%|█████▊  | 49/67 [00:13<00:05,  3.58it/s, loss=2.66, v_num=1, train_loss_step=1.150, val_loss=3.110, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 59:  75%|█████▉  | 50/67 [00:13<00:04,  3.57it/s, loss=2.66, v_num=1, train_loss_step=1.150, val_loss=3.110, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 59:  76%|██████  | 51/67 [00:14<00:04,  3.53it/s, loss=2.66, v_num=1, train_loss_step=1.150, val_loss=3.110, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 59:  78%|██████▏ | 52/67 [00:14<00:04,  3.52it/s, loss=2.66, v_num=1, train_loss_step=1.150, val_loss=3.110, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 59:  79%|██████▎ | 53/67 [00:15<00:03,  3.51it/s, loss=2.66, v_num=1, train_loss_step=1.150, val_loss=3.110, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 59:  81%|██████▍ | 54/67 [00:15<00:03,  3.50it/s, loss=2.66, v_num=1, train_loss_step=1.150, val_loss=3.110, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 59:  82%|██████▌ | 55/67 [00:15<00:03,  3.49it/s, loss=2.66, v_num=1, train_loss_step=1.150, val_loss=3.110, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 59:  84%|██████▋ | 56/67 [00:16<00:03,  3.48it/s, loss=2.66, v_num=1, train_loss_step=1.150, val_loss=3.110, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 59:  85%|██████▊ | 57/67 [00:16<00:02,  3.48it/s, loss=2.66, v_num=1, train_loss_step=1.150, val_loss=3.110, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 59:  87%|██████▉ | 58/67 [00:16<00:02,  3.47it/s, loss=2.66, v_num=1, train_loss_step=1.150, val_loss=3.110, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 59:  88%|███████ | 59/67 [00:17<00:02,  3.46it/s, loss=2.66, v_num=1, train_loss_step=1.150, val_loss=3.110, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 59:  90%|███████▏| 60/67 [00:17<00:02,  3.45it/s, loss=2.66, v_num=1, train_loss_step=1.150, val_loss=3.110, train_loss_epoch=2.080]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 59:  91%|███████▎| 61/67 [00:17<00:01,  3.42it/s, loss=2.66, v_num=1, train_loss_step=1.150, val_loss=3.110, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 59:  93%|███████▍| 62/67 [00:18<00:01,  3.42it/s, loss=2.66, v_num=1, train_loss_step=1.150, val_loss=3.110, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 59:  94%|███████▌| 63/67 [00:18<00:01,  3.41it/s, loss=2.66, v_num=1, train_loss_step=1.150, val_loss=3.110, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 59:  96%|███████▋| 64/67 [00:18<00:00,  3.40it/s, loss=2.66, v_num=1, train_loss_step=1.150, val_loss=3.110, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 59:  97%|███████▊| 65/67 [00:19<00:00,  3.40it/s, loss=2.66, v_num=1, train_loss_step=1.150, val_loss=3.110, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 59:  99%|███████▉| 66/67 [00:19<00:00,  3.39it/s, loss=2.66, v_num=1, train_loss_step=1.150, val_loss=3.110, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 59: 100%|████████| 67/67 [00:20<00:00,  3.32it/s, loss=2.66, v_num=1, train_loss_step=1.150, val_loss=3.110, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 60:  45%|███▌    | 30/67 [00:05<00:06,  5.34it/s, loss=1.77, v_num=1, train_loss_step=1.660, val_loss=3.110, train_loss_epoch=2.390]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 60:  46%|███▋    | 31/67 [00:07<00:08,  4.12it/s, loss=1.77, v_num=1, train_loss_step=1.660, val_loss=3.110, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 60:  48%|███▊    | 32/67 [00:07<00:08,  4.08it/s, loss=1.77, v_num=1, train_loss_step=1.660, val_loss=3.110, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 60:  49%|███▉    | 33/67 [00:08<00:08,  4.04it/s, loss=1.77, v_num=1, train_loss_step=1.660, val_loss=3.110, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 60:  51%|████    | 34/67 [00:08<00:08,  3.99it/s, loss=1.77, v_num=1, train_loss_step=1.660, val_loss=3.110, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 60:  52%|████▏   | 35/67 [00:08<00:08,  3.96it/s, loss=1.77, v_num=1, train_loss_step=1.660, val_loss=3.110, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 60:  54%|████▎   | 36/67 [00:09<00:07,  3.92it/s, loss=1.77, v_num=1, train_loss_step=1.660, val_loss=3.110, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 60:  55%|████▍   | 37/67 [00:09<00:07,  3.89it/s, loss=1.77, v_num=1, train_loss_step=1.660, val_loss=3.110, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 60:  57%|████▌   | 38/67 [00:09<00:07,  3.86it/s, loss=1.77, v_num=1, train_loss_step=1.660, val_loss=3.110, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 60:  58%|████▋   | 39/67 [00:10<00:07,  3.83it/s, loss=1.77, v_num=1, train_loss_step=1.660, val_loss=3.110, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 60:  60%|████▊   | 40/67 [00:10<00:07,  3.80it/s, loss=1.77, v_num=1, train_loss_step=1.660, val_loss=3.110, train_loss_epoch=2.390]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 60:  61%|████▉   | 41/67 [00:10<00:06,  3.74it/s, loss=1.77, v_num=1, train_loss_step=1.660, val_loss=3.110, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 60:  63%|█████   | 42/67 [00:11<00:06,  3.72it/s, loss=1.77, v_num=1, train_loss_step=1.660, val_loss=3.110, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 60:  64%|█████▏  | 43/67 [00:11<00:06,  3.70it/s, loss=1.77, v_num=1, train_loss_step=1.660, val_loss=3.110, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 60:  66%|█████▎  | 44/67 [00:11<00:06,  3.68it/s, loss=1.77, v_num=1, train_loss_step=1.660, val_loss=3.110, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 60:  67%|█████▎  | 45/67 [00:12<00:06,  3.66it/s, loss=1.77, v_num=1, train_loss_step=1.660, val_loss=3.110, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 60:  69%|█████▍  | 46/67 [00:12<00:05,  3.64it/s, loss=1.77, v_num=1, train_loss_step=1.660, val_loss=3.110, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 60:  70%|█████▌  | 47/67 [00:12<00:05,  3.63it/s, loss=1.77, v_num=1, train_loss_step=1.660, val_loss=3.110, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 60:  72%|█████▋  | 48/67 [00:13<00:05,  3.61it/s, loss=1.77, v_num=1, train_loss_step=1.660, val_loss=3.110, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 60:  73%|█████▊  | 49/67 [00:13<00:05,  3.59it/s, loss=1.77, v_num=1, train_loss_step=1.660, val_loss=3.110, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 60:  75%|█████▉  | 50/67 [00:13<00:04,  3.58it/s, loss=1.77, v_num=1, train_loss_step=1.660, val_loss=3.110, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 60:  76%|██████  | 51/67 [00:14<00:04,  3.54it/s, loss=1.77, v_num=1, train_loss_step=1.660, val_loss=3.110, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 60:  78%|██████▏ | 52/67 [00:14<00:04,  3.52it/s, loss=1.77, v_num=1, train_loss_step=1.660, val_loss=3.110, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 60:  79%|██████▎ | 53/67 [00:15<00:03,  3.51it/s, loss=1.77, v_num=1, train_loss_step=1.660, val_loss=3.110, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 60:  81%|██████▍ | 54/67 [00:15<00:03,  3.50it/s, loss=1.77, v_num=1, train_loss_step=1.660, val_loss=3.110, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 60:  82%|██████▌ | 55/67 [00:15<00:03,  3.49it/s, loss=1.77, v_num=1, train_loss_step=1.660, val_loss=3.110, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 60:  84%|██████▋ | 56/67 [00:16<00:03,  3.48it/s, loss=1.77, v_num=1, train_loss_step=1.660, val_loss=3.110, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 60:  85%|██████▊ | 57/67 [00:16<00:02,  3.47it/s, loss=1.77, v_num=1, train_loss_step=1.660, val_loss=3.110, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 60:  87%|██████▉ | 58/67 [00:16<00:02,  3.46it/s, loss=1.77, v_num=1, train_loss_step=1.660, val_loss=3.110, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 60:  88%|███████ | 59/67 [00:17<00:02,  3.45it/s, loss=1.77, v_num=1, train_loss_step=1.660, val_loss=3.110, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 60:  90%|███████▏| 60/67 [00:17<00:02,  3.44it/s, loss=1.77, v_num=1, train_loss_step=1.660, val_loss=3.110, train_loss_epoch=2.390]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 60:  91%|███████▎| 61/67 [00:17<00:01,  3.41it/s, loss=1.77, v_num=1, train_loss_step=1.660, val_loss=3.110, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 60:  93%|███████▍| 62/67 [00:18<00:01,  3.40it/s, loss=1.77, v_num=1, train_loss_step=1.660, val_loss=3.110, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 60:  94%|███████▌| 63/67 [00:18<00:01,  3.39it/s, loss=1.77, v_num=1, train_loss_step=1.660, val_loss=3.110, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 60:  96%|███████▋| 64/67 [00:18<00:00,  3.39it/s, loss=1.77, v_num=1, train_loss_step=1.660, val_loss=3.110, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 60:  97%|███████▊| 65/67 [00:19<00:00,  3.38it/s, loss=1.77, v_num=1, train_loss_step=1.660, val_loss=3.110, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 60:  99%|███████▉| 66/67 [00:19<00:00,  3.38it/s, loss=1.77, v_num=1, train_loss_step=1.660, val_loss=3.110, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 60: 100%|████████| 67/67 [00:20<00:00,  3.31it/s, loss=1.77, v_num=1, train_loss_step=1.660, val_loss=3.110, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 61:  45%|███▌    | 30/67 [00:05<00:07,  5.23it/s, loss=2.72, v_num=1, train_loss_step=3.600, val_loss=3.110, train_loss_epoch=2.240]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 61:  46%|███▋    | 31/67 [00:07<00:08,  4.04it/s, loss=2.72, v_num=1, train_loss_step=3.600, val_loss=3.110, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 61:  48%|███▊    | 32/67 [00:08<00:08,  4.00it/s, loss=2.72, v_num=1, train_loss_step=3.600, val_loss=3.110, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 61:  49%|███▉    | 33/67 [00:08<00:08,  3.95it/s, loss=2.72, v_num=1, train_loss_step=3.600, val_loss=3.110, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 61:  51%|████    | 34/67 [00:08<00:08,  3.91it/s, loss=2.72, v_num=1, train_loss_step=3.600, val_loss=3.110, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 61:  52%|████▏   | 35/67 [00:09<00:08,  3.88it/s, loss=2.72, v_num=1, train_loss_step=3.600, val_loss=3.110, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 61:  54%|████▎   | 36/67 [00:09<00:08,  3.85it/s, loss=2.72, v_num=1, train_loss_step=3.600, val_loss=3.110, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 61:  55%|████▍   | 37/67 [00:09<00:07,  3.81it/s, loss=2.72, v_num=1, train_loss_step=3.600, val_loss=3.110, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 61:  57%|████▌   | 38/67 [00:10<00:07,  3.78it/s, loss=2.72, v_num=1, train_loss_step=3.600, val_loss=3.110, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 61:  58%|████▋   | 39/67 [00:10<00:07,  3.75it/s, loss=2.72, v_num=1, train_loss_step=3.600, val_loss=3.110, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 61:  60%|████▊   | 40/67 [00:10<00:07,  3.73it/s, loss=2.72, v_num=1, train_loss_step=3.600, val_loss=3.110, train_loss_epoch=2.240]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 61:  61%|████▉   | 41/67 [00:11<00:07,  3.67it/s, loss=2.72, v_num=1, train_loss_step=3.600, val_loss=3.110, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 61:  63%|█████   | 42/67 [00:11<00:06,  3.65it/s, loss=2.72, v_num=1, train_loss_step=3.600, val_loss=3.110, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 61:  64%|█████▏  | 43/67 [00:11<00:06,  3.63it/s, loss=2.72, v_num=1, train_loss_step=3.600, val_loss=3.110, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 61:  66%|█████▎  | 44/67 [00:12<00:06,  3.60it/s, loss=2.72, v_num=1, train_loss_step=3.600, val_loss=3.110, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 61:  67%|█████▎  | 45/67 [00:12<00:06,  3.59it/s, loss=2.72, v_num=1, train_loss_step=3.600, val_loss=3.110, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 61:  69%|█████▍  | 46/67 [00:12<00:05,  3.57it/s, loss=2.72, v_num=1, train_loss_step=3.600, val_loss=3.110, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 61:  70%|█████▌  | 47/67 [00:13<00:05,  3.56it/s, loss=2.72, v_num=1, train_loss_step=3.600, val_loss=3.110, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 61:  72%|█████▋  | 48/67 [00:13<00:05,  3.54it/s, loss=2.72, v_num=1, train_loss_step=3.600, val_loss=3.110, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 61:  73%|█████▊  | 49/67 [00:13<00:05,  3.53it/s, loss=2.72, v_num=1, train_loss_step=3.600, val_loss=3.110, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 61:  75%|█████▉  | 50/67 [00:14<00:04,  3.52it/s, loss=2.72, v_num=1, train_loss_step=3.600, val_loss=3.110, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 61:  76%|██████  | 51/67 [00:14<00:04,  3.48it/s, loss=2.72, v_num=1, train_loss_step=3.600, val_loss=3.110, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 61:  78%|██████▏ | 52/67 [00:15<00:04,  3.46it/s, loss=2.72, v_num=1, train_loss_step=3.600, val_loss=3.110, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 61:  79%|██████▎ | 53/67 [00:15<00:04,  3.45it/s, loss=2.72, v_num=1, train_loss_step=3.600, val_loss=3.110, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 61:  81%|██████▍ | 54/67 [00:15<00:03,  3.44it/s, loss=2.72, v_num=1, train_loss_step=3.600, val_loss=3.110, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 61:  82%|██████▌ | 55/67 [00:16<00:03,  3.43it/s, loss=2.72, v_num=1, train_loss_step=3.600, val_loss=3.110, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 61:  84%|██████▋ | 56/67 [00:16<00:03,  3.42it/s, loss=2.72, v_num=1, train_loss_step=3.600, val_loss=3.110, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 61:  85%|██████▊ | 57/67 [00:16<00:02,  3.41it/s, loss=2.72, v_num=1, train_loss_step=3.600, val_loss=3.110, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 61:  87%|██████▉ | 58/67 [00:18<00:02,  3.18it/s, loss=2.72, v_num=1, train_loss_step=3.600, val_loss=3.110, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 61:  88%|███████ | 59/67 [00:18<00:02,  3.18it/s, loss=2.72, v_num=1, train_loss_step=3.600, val_loss=3.110, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 61:  90%|███████▏| 60/67 [00:18<00:02,  3.18it/s, loss=2.72, v_num=1, train_loss_step=3.600, val_loss=3.110, train_loss_epoch=2.240]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 61:  91%|███████▎| 61/67 [00:19<00:01,  3.15it/s, loss=2.72, v_num=1, train_loss_step=3.600, val_loss=3.110, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 61:  93%|███████▍| 62/67 [00:19<00:01,  3.15it/s, loss=2.72, v_num=1, train_loss_step=3.600, val_loss=3.110, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 61:  94%|███████▌| 63/67 [00:20<00:01,  3.15it/s, loss=2.72, v_num=1, train_loss_step=3.600, val_loss=3.110, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 61:  96%|███████▋| 64/67 [00:20<00:00,  3.15it/s, loss=2.72, v_num=1, train_loss_step=3.600, val_loss=3.110, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 61:  97%|███████▊| 65/67 [00:20<00:00,  3.14it/s, loss=2.72, v_num=1, train_loss_step=3.600, val_loss=3.110, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 61:  99%|███████▉| 66/67 [00:20<00:00,  3.14it/s, loss=2.72, v_num=1, train_loss_step=3.600, val_loss=3.110, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 61: 100%|████████| 67/67 [00:21<00:00,  3.09it/s, loss=2.72, v_num=1, train_loss_step=3.600, val_loss=3.110, train_loss_epoch=2.240]\u001b[A\n",
      "Epoch 62:  45%|███▌    | 30/67 [00:05<00:07,  5.26it/s, loss=2.05, v_num=1, train_loss_step=1.140, val_loss=3.110, train_loss_epoch=2.410]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 62:  46%|███▋    | 31/67 [00:07<00:08,  4.03it/s, loss=2.05, v_num=1, train_loss_step=1.140, val_loss=3.110, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 62:  48%|███▊    | 32/67 [00:08<00:08,  3.99it/s, loss=2.05, v_num=1, train_loss_step=1.140, val_loss=3.110, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 62:  49%|███▉    | 33/67 [00:08<00:08,  3.96it/s, loss=2.05, v_num=1, train_loss_step=1.140, val_loss=3.110, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 62:  51%|████    | 34/67 [00:08<00:08,  3.92it/s, loss=2.05, v_num=1, train_loss_step=1.140, val_loss=3.110, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 62:  52%|████▏   | 35/67 [00:09<00:08,  3.88it/s, loss=2.05, v_num=1, train_loss_step=1.140, val_loss=3.110, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 62:  54%|████▎   | 36/67 [00:09<00:08,  3.85it/s, loss=2.05, v_num=1, train_loss_step=1.140, val_loss=3.110, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 62:  55%|████▍   | 37/67 [00:09<00:07,  3.82it/s, loss=2.05, v_num=1, train_loss_step=1.140, val_loss=3.110, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 62:  57%|████▌   | 38/67 [00:09<00:07,  3.80it/s, loss=2.05, v_num=1, train_loss_step=1.140, val_loss=3.110, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 62:  58%|████▋   | 39/67 [00:10<00:07,  3.78it/s, loss=2.05, v_num=1, train_loss_step=1.140, val_loss=3.110, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 62:  60%|████▊   | 40/67 [00:10<00:07,  3.75it/s, loss=2.05, v_num=1, train_loss_step=1.140, val_loss=3.110, train_loss_epoch=2.410]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 62:  61%|████▉   | 41/67 [00:11<00:07,  3.70it/s, loss=2.05, v_num=1, train_loss_step=1.140, val_loss=3.110, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 62:  63%|█████   | 42/67 [00:11<00:06,  3.68it/s, loss=2.05, v_num=1, train_loss_step=1.140, val_loss=3.110, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 62:  64%|█████▏  | 43/67 [00:11<00:06,  3.66it/s, loss=2.05, v_num=1, train_loss_step=1.140, val_loss=3.110, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 62:  66%|█████▎  | 44/67 [00:12<00:06,  3.64it/s, loss=2.05, v_num=1, train_loss_step=1.140, val_loss=3.110, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 62:  67%|█████▎  | 45/67 [00:12<00:06,  3.62it/s, loss=2.05, v_num=1, train_loss_step=1.140, val_loss=3.110, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 62:  69%|█████▍  | 46/67 [00:12<00:05,  3.60it/s, loss=2.05, v_num=1, train_loss_step=1.140, val_loss=3.110, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 62:  70%|█████▌  | 47/67 [00:13<00:05,  3.59it/s, loss=2.05, v_num=1, train_loss_step=1.140, val_loss=3.110, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 62:  72%|█████▋  | 48/67 [00:14<00:05,  3.29it/s, loss=2.05, v_num=1, train_loss_step=1.140, val_loss=3.110, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 62:  73%|█████▊  | 49/67 [00:14<00:05,  3.28it/s, loss=2.05, v_num=1, train_loss_step=1.140, val_loss=3.110, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 62:  75%|█████▉  | 50/67 [00:15<00:05,  3.28it/s, loss=2.05, v_num=1, train_loss_step=1.140, val_loss=3.110, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 62:  76%|██████  | 51/67 [00:15<00:04,  3.25it/s, loss=2.05, v_num=1, train_loss_step=1.140, val_loss=3.110, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 62:  78%|██████▏ | 52/67 [00:16<00:04,  3.24it/s, loss=2.05, v_num=1, train_loss_step=1.140, val_loss=3.110, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 62:  79%|██████▎ | 53/67 [00:16<00:04,  3.23it/s, loss=2.05, v_num=1, train_loss_step=1.140, val_loss=3.110, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 62:  81%|██████▍ | 54/67 [00:16<00:04,  3.23it/s, loss=2.05, v_num=1, train_loss_step=1.140, val_loss=3.110, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 62:  82%|██████▌ | 55/67 [00:17<00:03,  3.22it/s, loss=2.05, v_num=1, train_loss_step=1.140, val_loss=3.110, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 62:  84%|██████▋ | 56/67 [00:17<00:03,  3.22it/s, loss=2.05, v_num=1, train_loss_step=1.140, val_loss=3.110, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 62:  85%|██████▊ | 57/67 [00:17<00:03,  3.21it/s, loss=2.05, v_num=1, train_loss_step=1.140, val_loss=3.110, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 62:  87%|██████▉ | 58/67 [00:18<00:02,  3.21it/s, loss=2.05, v_num=1, train_loss_step=1.140, val_loss=3.110, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 62:  88%|███████ | 59/67 [00:18<00:02,  3.20it/s, loss=2.05, v_num=1, train_loss_step=1.140, val_loss=3.110, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 62:  90%|███████▏| 60/67 [00:18<00:02,  3.20it/s, loss=2.05, v_num=1, train_loss_step=1.140, val_loss=3.110, train_loss_epoch=2.410]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 62:  91%|███████▎| 61/67 [00:19<00:01,  3.18it/s, loss=2.05, v_num=1, train_loss_step=1.140, val_loss=3.110, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 62:  93%|███████▍| 62/67 [00:19<00:01,  3.17it/s, loss=2.05, v_num=1, train_loss_step=1.140, val_loss=3.110, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 62:  94%|███████▌| 63/67 [00:19<00:01,  3.17it/s, loss=2.05, v_num=1, train_loss_step=1.140, val_loss=3.110, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 62:  96%|███████▋| 64/67 [00:20<00:00,  3.17it/s, loss=2.05, v_num=1, train_loss_step=1.140, val_loss=3.110, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 62:  97%|███████▊| 65/67 [00:20<00:00,  3.17it/s, loss=2.05, v_num=1, train_loss_step=1.140, val_loss=3.110, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 62:  99%|███████▉| 66/67 [00:20<00:00,  3.17it/s, loss=2.05, v_num=1, train_loss_step=1.140, val_loss=3.110, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 62: 100%|████████| 67/67 [00:21<00:00,  3.12it/s, loss=2.05, v_num=1, train_loss_step=1.140, val_loss=3.100, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 63:  45%|███▌    | 30/67 [00:05<00:07,  5.26it/s, loss=2.56, v_num=1, train_loss_step=0.818, val_loss=3.100, train_loss_epoch=2.550]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 63:  46%|███▋    | 31/67 [00:07<00:08,  4.04it/s, loss=2.56, v_num=1, train_loss_step=0.818, val_loss=3.100, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 63:  48%|███▊    | 32/67 [00:08<00:08,  3.99it/s, loss=2.56, v_num=1, train_loss_step=0.818, val_loss=3.100, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 63:  49%|███▉    | 33/67 [00:08<00:08,  3.95it/s, loss=2.56, v_num=1, train_loss_step=0.818, val_loss=3.100, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 63:  51%|████    | 34/67 [00:08<00:08,  3.91it/s, loss=2.56, v_num=1, train_loss_step=0.818, val_loss=3.100, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 63:  52%|████▏   | 35/67 [00:09<00:08,  3.87it/s, loss=2.56, v_num=1, train_loss_step=0.818, val_loss=3.100, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 63:  54%|████▎   | 36/67 [00:09<00:08,  3.84it/s, loss=2.56, v_num=1, train_loss_step=0.818, val_loss=3.100, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 63:  55%|████▍   | 37/67 [00:09<00:07,  3.81it/s, loss=2.56, v_num=1, train_loss_step=0.818, val_loss=3.100, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 63:  57%|████▌   | 38/67 [00:10<00:07,  3.78it/s, loss=2.56, v_num=1, train_loss_step=0.818, val_loss=3.100, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 63:  58%|████▋   | 39/67 [00:10<00:07,  3.76it/s, loss=2.56, v_num=1, train_loss_step=0.818, val_loss=3.100, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 63:  60%|████▊   | 40/67 [00:11<00:07,  3.39it/s, loss=2.56, v_num=1, train_loss_step=0.818, val_loss=3.100, train_loss_epoch=2.550]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 63:  61%|████▉   | 41/67 [00:12<00:07,  3.36it/s, loss=2.56, v_num=1, train_loss_step=0.818, val_loss=3.100, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 63:  63%|█████   | 42/67 [00:12<00:07,  3.35it/s, loss=2.56, v_num=1, train_loss_step=0.818, val_loss=3.100, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 63:  64%|█████▏  | 43/67 [00:12<00:07,  3.33it/s, loss=2.56, v_num=1, train_loss_step=0.818, val_loss=3.100, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 63:  66%|█████▎  | 44/67 [00:13<00:06,  3.33it/s, loss=2.56, v_num=1, train_loss_step=0.818, val_loss=3.100, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 63:  67%|█████▎  | 45/67 [00:13<00:06,  3.32it/s, loss=2.56, v_num=1, train_loss_step=0.818, val_loss=3.100, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 63:  69%|█████▍  | 46/67 [00:13<00:06,  3.31it/s, loss=2.56, v_num=1, train_loss_step=0.818, val_loss=3.100, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 63:  70%|█████▌  | 47/67 [00:14<00:06,  3.30it/s, loss=2.56, v_num=1, train_loss_step=0.818, val_loss=3.100, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 63:  72%|█████▋  | 48/67 [00:14<00:05,  3.30it/s, loss=2.56, v_num=1, train_loss_step=0.818, val_loss=3.100, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 63:  73%|█████▊  | 49/67 [00:14<00:05,  3.29it/s, loss=2.56, v_num=1, train_loss_step=0.818, val_loss=3.100, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 63:  75%|█████▉  | 50/67 [00:15<00:05,  3.28it/s, loss=2.56, v_num=1, train_loss_step=0.818, val_loss=3.100, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 63:  76%|██████  | 51/67 [00:15<00:04,  3.25it/s, loss=2.56, v_num=1, train_loss_step=0.818, val_loss=3.100, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 63:  78%|██████▏ | 52/67 [00:16<00:04,  3.24it/s, loss=2.56, v_num=1, train_loss_step=0.818, val_loss=3.100, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 63:  79%|██████▎ | 53/67 [00:16<00:04,  3.24it/s, loss=2.56, v_num=1, train_loss_step=0.818, val_loss=3.100, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 63:  81%|██████▍ | 54/67 [00:16<00:04,  3.23it/s, loss=2.56, v_num=1, train_loss_step=0.818, val_loss=3.100, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 63:  82%|██████▌ | 55/67 [00:17<00:03,  3.22it/s, loss=2.56, v_num=1, train_loss_step=0.818, val_loss=3.100, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 63:  84%|██████▋ | 56/67 [00:17<00:03,  3.22it/s, loss=2.56, v_num=1, train_loss_step=0.818, val_loss=3.100, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 63:  85%|██████▊ | 57/67 [00:17<00:03,  3.22it/s, loss=2.56, v_num=1, train_loss_step=0.818, val_loss=3.100, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 63:  87%|██████▉ | 58/67 [00:18<00:02,  3.21it/s, loss=2.56, v_num=1, train_loss_step=0.818, val_loss=3.100, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 63:  88%|███████ | 59/67 [00:18<00:02,  3.21it/s, loss=2.56, v_num=1, train_loss_step=0.818, val_loss=3.100, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 63:  90%|███████▏| 60/67 [00:18<00:02,  3.21it/s, loss=2.56, v_num=1, train_loss_step=0.818, val_loss=3.100, train_loss_epoch=2.550]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 63:  91%|███████▎| 61/67 [00:19<00:01,  3.18it/s, loss=2.56, v_num=1, train_loss_step=0.818, val_loss=3.100, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 63:  93%|███████▍| 62/67 [00:19<00:01,  3.18it/s, loss=2.56, v_num=1, train_loss_step=0.818, val_loss=3.100, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 63:  94%|███████▌| 63/67 [00:19<00:01,  3.18it/s, loss=2.56, v_num=1, train_loss_step=0.818, val_loss=3.100, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 63:  96%|███████▋| 64/67 [00:20<00:00,  3.17it/s, loss=2.56, v_num=1, train_loss_step=0.818, val_loss=3.100, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 63:  97%|███████▊| 65/67 [00:20<00:00,  3.17it/s, loss=2.56, v_num=1, train_loss_step=0.818, val_loss=3.100, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 63:  99%|███████▉| 66/67 [00:20<00:00,  3.17it/s, loss=2.56, v_num=1, train_loss_step=0.818, val_loss=3.100, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 63: 100%|████████| 67/67 [00:21<00:00,  3.11it/s, loss=2.56, v_num=1, train_loss_step=0.818, val_loss=3.100, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 64:  45%|███▌    | 30/67 [00:05<00:07,  5.22it/s, loss=2.04, v_num=1, train_loss_step=6.160, val_loss=3.100, train_loss_epoch=2.750]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 64:  46%|███▋    | 31/67 [00:10<00:11,  3.06it/s, loss=2.04, v_num=1, train_loss_step=6.160, val_loss=3.100, train_loss_epoch=2.750]\u001b[A\n",
      "Epoch 64:  48%|███▊    | 32/67 [00:10<00:11,  3.06it/s, loss=2.04, v_num=1, train_loss_step=6.160, val_loss=3.100, train_loss_epoch=2.750]\u001b[A\n",
      "Epoch 64:  49%|███▉    | 33/67 [00:10<00:11,  3.07it/s, loss=2.04, v_num=1, train_loss_step=6.160, val_loss=3.100, train_loss_epoch=2.750]\u001b[A\n",
      "Epoch 64:  51%|████    | 34/67 [00:11<00:10,  3.07it/s, loss=2.04, v_num=1, train_loss_step=6.160, val_loss=3.100, train_loss_epoch=2.750]\u001b[A\n",
      "Epoch 64:  52%|████▏   | 35/67 [00:11<00:10,  3.07it/s, loss=2.04, v_num=1, train_loss_step=6.160, val_loss=3.100, train_loss_epoch=2.750]\u001b[A\n",
      "Epoch 64:  54%|████▎   | 36/67 [00:11<00:10,  3.07it/s, loss=2.04, v_num=1, train_loss_step=6.160, val_loss=3.100, train_loss_epoch=2.750]\u001b[A\n",
      "Epoch 64:  55%|████▍   | 37/67 [00:12<00:09,  3.07it/s, loss=2.04, v_num=1, train_loss_step=6.160, val_loss=3.100, train_loss_epoch=2.750]\u001b[A\n",
      "Epoch 64:  57%|████▌   | 38/67 [00:12<00:09,  3.07it/s, loss=2.04, v_num=1, train_loss_step=6.160, val_loss=3.100, train_loss_epoch=2.750]\u001b[A\n",
      "Epoch 64:  58%|████▋   | 39/67 [00:12<00:09,  3.07it/s, loss=2.04, v_num=1, train_loss_step=6.160, val_loss=3.100, train_loss_epoch=2.750]\u001b[A\n",
      "Epoch 64:  60%|████▊   | 40/67 [00:13<00:08,  3.07it/s, loss=2.04, v_num=1, train_loss_step=6.160, val_loss=3.100, train_loss_epoch=2.750]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 64:  61%|████▉   | 41/67 [00:13<00:08,  3.04it/s, loss=2.04, v_num=1, train_loss_step=6.160, val_loss=3.100, train_loss_epoch=2.750]\u001b[A\n",
      "Epoch 64:  63%|█████   | 42/67 [00:13<00:08,  3.04it/s, loss=2.04, v_num=1, train_loss_step=6.160, val_loss=3.100, train_loss_epoch=2.750]\u001b[A\n",
      "Epoch 64:  64%|█████▏  | 43/67 [00:14<00:07,  3.04it/s, loss=2.04, v_num=1, train_loss_step=6.160, val_loss=3.100, train_loss_epoch=2.750]\u001b[A\n",
      "Epoch 64:  66%|█████▎  | 44/67 [00:14<00:07,  3.04it/s, loss=2.04, v_num=1, train_loss_step=6.160, val_loss=3.100, train_loss_epoch=2.750]\u001b[A\n",
      "Epoch 64:  67%|█████▎  | 45/67 [00:14<00:07,  3.04it/s, loss=2.04, v_num=1, train_loss_step=6.160, val_loss=3.100, train_loss_epoch=2.750]\u001b[A\n",
      "Epoch 64:  69%|█████▍  | 46/67 [00:15<00:06,  3.04it/s, loss=2.04, v_num=1, train_loss_step=6.160, val_loss=3.100, train_loss_epoch=2.750]\u001b[A\n",
      "Epoch 64:  70%|█████▌  | 47/67 [00:15<00:06,  3.04it/s, loss=2.04, v_num=1, train_loss_step=6.160, val_loss=3.100, train_loss_epoch=2.750]\u001b[A\n",
      "Epoch 64:  72%|█████▋  | 48/67 [00:15<00:06,  3.04it/s, loss=2.04, v_num=1, train_loss_step=6.160, val_loss=3.100, train_loss_epoch=2.750]\u001b[A\n",
      "Epoch 64:  73%|█████▊  | 49/67 [00:16<00:05,  3.04it/s, loss=2.04, v_num=1, train_loss_step=6.160, val_loss=3.100, train_loss_epoch=2.750]\u001b[A\n",
      "Epoch 64:  75%|█████▉  | 50/67 [00:16<00:05,  3.04it/s, loss=2.04, v_num=1, train_loss_step=6.160, val_loss=3.100, train_loss_epoch=2.750]\u001b[A\n",
      "Epoch 64:  76%|██████  | 51/67 [00:16<00:05,  3.02it/s, loss=2.04, v_num=1, train_loss_step=6.160, val_loss=3.100, train_loss_epoch=2.750]\u001b[A\n",
      "Epoch 64:  78%|██████▏ | 52/67 [00:17<00:04,  3.02it/s, loss=2.04, v_num=1, train_loss_step=6.160, val_loss=3.100, train_loss_epoch=2.750]\u001b[A\n",
      "Epoch 64:  79%|██████▎ | 53/67 [00:17<00:04,  3.02it/s, loss=2.04, v_num=1, train_loss_step=6.160, val_loss=3.100, train_loss_epoch=2.750]\u001b[A\n",
      "Epoch 64:  81%|██████▍ | 54/67 [00:17<00:04,  3.03it/s, loss=2.04, v_num=1, train_loss_step=6.160, val_loss=3.100, train_loss_epoch=2.750]\u001b[A\n",
      "Epoch 64:  82%|██████▌ | 55/67 [00:18<00:03,  3.03it/s, loss=2.04, v_num=1, train_loss_step=6.160, val_loss=3.100, train_loss_epoch=2.750]\u001b[A\n",
      "Epoch 64:  84%|██████▋ | 56/67 [00:18<00:03,  3.03it/s, loss=2.04, v_num=1, train_loss_step=6.160, val_loss=3.100, train_loss_epoch=2.750]\u001b[A\n",
      "Epoch 64:  85%|██████▊ | 57/67 [00:18<00:03,  3.02it/s, loss=2.04, v_num=1, train_loss_step=6.160, val_loss=3.100, train_loss_epoch=2.750]\u001b[A\n",
      "Epoch 64:  87%|██████▉ | 58/67 [00:19<00:02,  3.02it/s, loss=2.04, v_num=1, train_loss_step=6.160, val_loss=3.100, train_loss_epoch=2.750]\u001b[A\n",
      "Epoch 64:  88%|███████ | 59/67 [00:19<00:02,  3.02it/s, loss=2.04, v_num=1, train_loss_step=6.160, val_loss=3.100, train_loss_epoch=2.750]\u001b[A\n",
      "Epoch 64:  90%|███████▏| 60/67 [00:19<00:02,  3.02it/s, loss=2.04, v_num=1, train_loss_step=6.160, val_loss=3.100, train_loss_epoch=2.750]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 64:  91%|███████▎| 61/67 [00:20<00:01,  3.01it/s, loss=2.04, v_num=1, train_loss_step=6.160, val_loss=3.100, train_loss_epoch=2.750]\u001b[A\n",
      "Epoch 64:  93%|███████▍| 62/67 [00:20<00:01,  3.01it/s, loss=2.04, v_num=1, train_loss_step=6.160, val_loss=3.100, train_loss_epoch=2.750]\u001b[A\n",
      "Epoch 64:  94%|███████▌| 63/67 [00:20<00:01,  3.01it/s, loss=2.04, v_num=1, train_loss_step=6.160, val_loss=3.100, train_loss_epoch=2.750]\u001b[A\n",
      "Epoch 64:  96%|███████▋| 64/67 [00:21<00:00,  3.01it/s, loss=2.04, v_num=1, train_loss_step=6.160, val_loss=3.100, train_loss_epoch=2.750]\u001b[A\n",
      "Epoch 64:  97%|███████▊| 65/67 [00:21<00:00,  3.01it/s, loss=2.04, v_num=1, train_loss_step=6.160, val_loss=3.100, train_loss_epoch=2.750]\u001b[A\n",
      "Epoch 64:  99%|███████▉| 66/67 [00:21<00:00,  3.01it/s, loss=2.04, v_num=1, train_loss_step=6.160, val_loss=3.100, train_loss_epoch=2.750]\u001b[A\n",
      "Epoch 64: 100%|████████| 67/67 [00:22<00:00,  2.96it/s, loss=2.04, v_num=1, train_loss_step=6.160, val_loss=3.100, train_loss_epoch=2.750]\u001b[A\n",
      "Epoch 65:  45%|███▌    | 30/67 [00:05<00:07,  5.16it/s, loss=1.76, v_num=1, train_loss_step=2.240, val_loss=3.100, train_loss_epoch=2.270]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 65:  46%|███▋    | 31/67 [00:07<00:09,  3.98it/s, loss=1.76, v_num=1, train_loss_step=2.240, val_loss=3.100, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 65:  48%|███▊    | 32/67 [00:08<00:08,  3.95it/s, loss=1.76, v_num=1, train_loss_step=2.240, val_loss=3.100, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 65:  49%|███▉    | 33/67 [00:08<00:08,  3.91it/s, loss=1.76, v_num=1, train_loss_step=2.240, val_loss=3.100, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 65:  51%|████    | 34/67 [00:08<00:08,  3.88it/s, loss=1.76, v_num=1, train_loss_step=2.240, val_loss=3.100, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 65:  52%|████▏   | 35/67 [00:09<00:08,  3.84it/s, loss=1.76, v_num=1, train_loss_step=2.240, val_loss=3.100, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 65:  54%|████▎   | 36/67 [00:09<00:08,  3.82it/s, loss=1.76, v_num=1, train_loss_step=2.240, val_loss=3.100, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 65:  55%|████▍   | 37/67 [00:09<00:07,  3.78it/s, loss=1.76, v_num=1, train_loss_step=2.240, val_loss=3.100, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 65:  57%|████▌   | 38/67 [00:10<00:07,  3.76it/s, loss=1.76, v_num=1, train_loss_step=2.240, val_loss=3.100, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 65:  58%|████▋   | 39/67 [00:10<00:07,  3.74it/s, loss=1.76, v_num=1, train_loss_step=2.240, val_loss=3.100, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 65:  60%|████▊   | 40/67 [00:10<00:07,  3.71it/s, loss=1.76, v_num=1, train_loss_step=2.240, val_loss=3.100, train_loss_epoch=2.270]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 65:  61%|████▉   | 41/67 [00:11<00:07,  3.66it/s, loss=1.76, v_num=1, train_loss_step=2.240, val_loss=3.100, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 65:  63%|█████   | 42/67 [00:11<00:06,  3.64it/s, loss=1.76, v_num=1, train_loss_step=2.240, val_loss=3.100, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 65:  64%|█████▏  | 43/67 [00:11<00:06,  3.62it/s, loss=1.76, v_num=1, train_loss_step=2.240, val_loss=3.100, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 65:  66%|█████▎  | 44/67 [00:12<00:06,  3.60it/s, loss=1.76, v_num=1, train_loss_step=2.240, val_loss=3.100, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 65:  67%|█████▎  | 45/67 [00:12<00:06,  3.59it/s, loss=1.76, v_num=1, train_loss_step=2.240, val_loss=3.100, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 65:  69%|█████▍  | 46/67 [00:12<00:05,  3.58it/s, loss=1.76, v_num=1, train_loss_step=2.240, val_loss=3.100, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 65:  70%|█████▌  | 47/67 [00:13<00:05,  3.56it/s, loss=1.76, v_num=1, train_loss_step=2.240, val_loss=3.100, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 65:  72%|█████▋  | 48/67 [00:13<00:05,  3.55it/s, loss=1.76, v_num=1, train_loss_step=2.240, val_loss=3.100, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 65:  73%|█████▊  | 49/67 [00:13<00:05,  3.54it/s, loss=1.76, v_num=1, train_loss_step=2.240, val_loss=3.100, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 65:  75%|█████▉  | 50/67 [00:14<00:04,  3.53it/s, loss=1.76, v_num=1, train_loss_step=2.240, val_loss=3.100, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 65:  76%|██████  | 51/67 [00:14<00:04,  3.49it/s, loss=1.76, v_num=1, train_loss_step=2.240, val_loss=3.100, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 65:  78%|██████▏ | 52/67 [00:14<00:04,  3.47it/s, loss=1.76, v_num=1, train_loss_step=2.240, val_loss=3.100, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 65:  79%|██████▎ | 53/67 [00:15<00:04,  3.46it/s, loss=1.76, v_num=1, train_loss_step=2.240, val_loss=3.100, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 65:  81%|██████▍ | 54/67 [00:15<00:03,  3.45it/s, loss=1.76, v_num=1, train_loss_step=2.240, val_loss=3.100, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 65:  82%|██████▌ | 55/67 [00:15<00:03,  3.44it/s, loss=1.76, v_num=1, train_loss_step=2.240, val_loss=3.100, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 65:  84%|██████▋ | 56/67 [00:16<00:03,  3.44it/s, loss=1.76, v_num=1, train_loss_step=2.240, val_loss=3.100, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 65:  85%|██████▊ | 57/67 [00:16<00:02,  3.43it/s, loss=1.76, v_num=1, train_loss_step=2.240, val_loss=3.100, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 65:  87%|██████▉ | 58/67 [00:16<00:02,  3.42it/s, loss=1.76, v_num=1, train_loss_step=2.240, val_loss=3.100, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 65:  88%|███████ | 59/67 [00:17<00:02,  3.42it/s, loss=1.76, v_num=1, train_loss_step=2.240, val_loss=3.100, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 65:  90%|███████▏| 60/67 [00:17<00:02,  3.41it/s, loss=1.76, v_num=1, train_loss_step=2.240, val_loss=3.100, train_loss_epoch=2.270]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 65:  91%|███████▎| 61/67 [00:18<00:01,  3.38it/s, loss=1.76, v_num=1, train_loss_step=2.240, val_loss=3.100, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 65:  93%|███████▍| 62/67 [00:18<00:01,  3.38it/s, loss=1.76, v_num=1, train_loss_step=2.240, val_loss=3.100, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 65:  94%|███████▌| 63/67 [00:18<00:01,  3.37it/s, loss=1.76, v_num=1, train_loss_step=2.240, val_loss=3.100, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 65:  96%|███████▋| 64/67 [00:19<00:00,  3.37it/s, loss=1.76, v_num=1, train_loss_step=2.240, val_loss=3.100, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 65:  97%|███████▊| 65/67 [00:19<00:00,  3.36it/s, loss=1.76, v_num=1, train_loss_step=2.240, val_loss=3.100, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 65:  99%|███████▉| 66/67 [00:19<00:00,  3.36it/s, loss=1.76, v_num=1, train_loss_step=2.240, val_loss=3.100, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 65: 100%|████████| 67/67 [00:20<00:00,  3.30it/s, loss=1.76, v_num=1, train_loss_step=2.240, val_loss=3.100, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 66:  45%|███▌    | 30/67 [00:05<00:07,  5.26it/s, loss=1.88, v_num=1, train_loss_step=2.330, val_loss=3.100, train_loss_epoch=1.740]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 66:  46%|███▋    | 31/67 [00:07<00:08,  4.03it/s, loss=1.88, v_num=1, train_loss_step=2.330, val_loss=3.100, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 66:  48%|███▊    | 32/67 [00:08<00:08,  3.99it/s, loss=1.88, v_num=1, train_loss_step=2.330, val_loss=3.100, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 66:  49%|███▉    | 33/67 [00:08<00:08,  3.95it/s, loss=1.88, v_num=1, train_loss_step=2.330, val_loss=3.100, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 66:  51%|████    | 34/67 [00:08<00:08,  3.91it/s, loss=1.88, v_num=1, train_loss_step=2.330, val_loss=3.100, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 66:  52%|████▏   | 35/67 [00:09<00:08,  3.88it/s, loss=1.88, v_num=1, train_loss_step=2.330, val_loss=3.100, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 66:  54%|████▎   | 36/67 [00:09<00:08,  3.86it/s, loss=1.88, v_num=1, train_loss_step=2.330, val_loss=3.100, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 66:  55%|████▍   | 37/67 [00:09<00:07,  3.82it/s, loss=1.88, v_num=1, train_loss_step=2.330, val_loss=3.100, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 66:  57%|████▌   | 38/67 [00:09<00:07,  3.80it/s, loss=1.88, v_num=1, train_loss_step=2.330, val_loss=3.100, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 66:  58%|████▋   | 39/67 [00:10<00:07,  3.78it/s, loss=1.88, v_num=1, train_loss_step=2.330, val_loss=3.100, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 66:  60%|████▊   | 40/67 [00:10<00:07,  3.76it/s, loss=1.88, v_num=1, train_loss_step=2.330, val_loss=3.100, train_loss_epoch=1.740]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 66:  61%|████▉   | 41/67 [00:11<00:07,  3.70it/s, loss=1.88, v_num=1, train_loss_step=2.330, val_loss=3.100, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 66:  63%|█████   | 42/67 [00:11<00:06,  3.68it/s, loss=1.88, v_num=1, train_loss_step=2.330, val_loss=3.100, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 66:  64%|█████▏  | 43/67 [00:11<00:06,  3.67it/s, loss=1.88, v_num=1, train_loss_step=2.330, val_loss=3.100, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 66:  66%|█████▎  | 44/67 [00:12<00:06,  3.64it/s, loss=1.88, v_num=1, train_loss_step=2.330, val_loss=3.100, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 66:  67%|█████▎  | 45/67 [00:12<00:06,  3.63it/s, loss=1.88, v_num=1, train_loss_step=2.330, val_loss=3.100, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 66:  69%|█████▍  | 46/67 [00:12<00:05,  3.61it/s, loss=1.88, v_num=1, train_loss_step=2.330, val_loss=3.100, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 66:  70%|█████▌  | 47/67 [00:13<00:05,  3.60it/s, loss=1.88, v_num=1, train_loss_step=2.330, val_loss=3.100, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 66:  72%|█████▋  | 48/67 [00:13<00:05,  3.59it/s, loss=1.88, v_num=1, train_loss_step=2.330, val_loss=3.100, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 66:  73%|█████▊  | 49/67 [00:13<00:05,  3.58it/s, loss=1.88, v_num=1, train_loss_step=2.330, val_loss=3.100, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 66:  75%|█████▉  | 50/67 [00:14<00:04,  3.56it/s, loss=1.88, v_num=1, train_loss_step=2.330, val_loss=3.100, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 66:  76%|██████  | 51/67 [00:14<00:04,  3.52it/s, loss=1.88, v_num=1, train_loss_step=2.330, val_loss=3.100, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 66:  78%|██████▏ | 52/67 [00:14<00:04,  3.51it/s, loss=1.88, v_num=1, train_loss_step=2.330, val_loss=3.100, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 66:  79%|██████▎ | 53/67 [00:15<00:03,  3.50it/s, loss=1.88, v_num=1, train_loss_step=2.330, val_loss=3.100, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 66:  81%|██████▍ | 54/67 [00:15<00:03,  3.50it/s, loss=1.88, v_num=1, train_loss_step=2.330, val_loss=3.100, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 66:  82%|██████▌ | 55/67 [00:15<00:03,  3.49it/s, loss=1.88, v_num=1, train_loss_step=2.330, val_loss=3.100, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 66:  84%|██████▋ | 56/67 [00:16<00:03,  3.48it/s, loss=1.88, v_num=1, train_loss_step=2.330, val_loss=3.100, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 66:  85%|██████▊ | 57/67 [00:16<00:02,  3.47it/s, loss=1.88, v_num=1, train_loss_step=2.330, val_loss=3.100, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 66:  87%|██████▉ | 58/67 [00:16<00:02,  3.46it/s, loss=1.88, v_num=1, train_loss_step=2.330, val_loss=3.100, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 66:  88%|███████ | 59/67 [00:17<00:02,  3.45it/s, loss=1.88, v_num=1, train_loss_step=2.330, val_loss=3.100, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 66:  90%|███████▏| 60/67 [00:17<00:02,  3.45it/s, loss=1.88, v_num=1, train_loss_step=2.330, val_loss=3.100, train_loss_epoch=1.740]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 66:  91%|███████▎| 61/67 [00:17<00:01,  3.42it/s, loss=1.88, v_num=1, train_loss_step=2.330, val_loss=3.100, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 66:  93%|███████▍| 62/67 [00:18<00:01,  3.41it/s, loss=1.88, v_num=1, train_loss_step=2.330, val_loss=3.100, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 66:  94%|███████▌| 63/67 [00:18<00:01,  3.41it/s, loss=1.88, v_num=1, train_loss_step=2.330, val_loss=3.100, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 66:  96%|███████▋| 64/67 [00:18<00:00,  3.40it/s, loss=1.88, v_num=1, train_loss_step=2.330, val_loss=3.100, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 66:  97%|███████▊| 65/67 [00:19<00:00,  3.40it/s, loss=1.88, v_num=1, train_loss_step=2.330, val_loss=3.100, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 66:  99%|███████▉| 66/67 [00:19<00:00,  3.39it/s, loss=1.88, v_num=1, train_loss_step=2.330, val_loss=3.100, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 66: 100%|████████| 67/67 [00:20<00:00,  3.33it/s, loss=1.88, v_num=1, train_loss_step=2.330, val_loss=3.100, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 67:  45%|███▌    | 30/67 [00:05<00:06,  5.34it/s, loss=2.17, v_num=1, train_loss_step=6.610, val_loss=3.100, train_loss_epoch=1.850]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 67:  46%|███▋    | 31/67 [00:07<00:08,  4.12it/s, loss=2.17, v_num=1, train_loss_step=6.610, val_loss=3.100, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 67:  48%|███▊    | 32/67 [00:07<00:08,  4.08it/s, loss=2.17, v_num=1, train_loss_step=6.610, val_loss=3.100, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 67:  49%|███▉    | 33/67 [00:08<00:08,  4.04it/s, loss=2.17, v_num=1, train_loss_step=6.610, val_loss=3.100, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 67:  51%|████    | 34/67 [00:08<00:08,  4.00it/s, loss=2.17, v_num=1, train_loss_step=6.610, val_loss=3.100, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 67:  52%|████▏   | 35/67 [00:08<00:08,  3.97it/s, loss=2.17, v_num=1, train_loss_step=6.610, val_loss=3.100, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 67:  54%|████▎   | 36/67 [00:09<00:07,  3.93it/s, loss=2.17, v_num=1, train_loss_step=6.610, val_loss=3.100, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 67:  55%|████▍   | 37/67 [00:09<00:07,  3.90it/s, loss=2.17, v_num=1, train_loss_step=6.610, val_loss=3.100, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 67:  57%|████▌   | 38/67 [00:09<00:07,  3.87it/s, loss=2.17, v_num=1, train_loss_step=6.610, val_loss=3.100, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 67:  58%|████▋   | 39/67 [00:10<00:07,  3.85it/s, loss=2.17, v_num=1, train_loss_step=6.610, val_loss=3.100, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 67:  60%|████▊   | 40/67 [00:10<00:07,  3.82it/s, loss=2.17, v_num=1, train_loss_step=6.610, val_loss=3.100, train_loss_epoch=1.850]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 67:  61%|████▉   | 41/67 [00:10<00:06,  3.76it/s, loss=2.17, v_num=1, train_loss_step=6.610, val_loss=3.100, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 67:  63%|█████   | 42/67 [00:11<00:06,  3.74it/s, loss=2.17, v_num=1, train_loss_step=6.610, val_loss=3.100, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 67:  64%|█████▏  | 43/67 [00:11<00:06,  3.72it/s, loss=2.17, v_num=1, train_loss_step=6.610, val_loss=3.100, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 67:  66%|█████▎  | 44/67 [00:11<00:06,  3.70it/s, loss=2.17, v_num=1, train_loss_step=6.610, val_loss=3.100, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 67:  67%|█████▎  | 45/67 [00:12<00:05,  3.69it/s, loss=2.17, v_num=1, train_loss_step=6.610, val_loss=3.100, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 67:  69%|█████▍  | 46/67 [00:12<00:05,  3.67it/s, loss=2.17, v_num=1, train_loss_step=6.610, val_loss=3.100, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 67:  70%|█████▌  | 47/67 [00:12<00:05,  3.66it/s, loss=2.17, v_num=1, train_loss_step=6.610, val_loss=3.100, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 67:  72%|█████▋  | 48/67 [00:13<00:05,  3.64it/s, loss=2.17, v_num=1, train_loss_step=6.610, val_loss=3.100, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 67:  73%|█████▊  | 49/67 [00:13<00:04,  3.63it/s, loss=2.17, v_num=1, train_loss_step=6.610, val_loss=3.100, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 67:  75%|█████▉  | 50/67 [00:13<00:04,  3.62it/s, loss=2.17, v_num=1, train_loss_step=6.610, val_loss=3.100, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 67:  76%|██████  | 51/67 [00:14<00:04,  3.58it/s, loss=2.17, v_num=1, train_loss_step=6.610, val_loss=3.100, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 67:  78%|██████▏ | 52/67 [00:14<00:04,  3.56it/s, loss=2.17, v_num=1, train_loss_step=6.610, val_loss=3.100, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 67:  79%|██████▎ | 53/67 [00:14<00:03,  3.55it/s, loss=2.17, v_num=1, train_loss_step=6.610, val_loss=3.100, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 67:  81%|██████▍ | 54/67 [00:15<00:03,  3.54it/s, loss=2.17, v_num=1, train_loss_step=6.610, val_loss=3.100, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 67:  82%|██████▌ | 55/67 [00:15<00:03,  3.53it/s, loss=2.17, v_num=1, train_loss_step=6.610, val_loss=3.100, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 67:  84%|██████▋ | 56/67 [00:15<00:03,  3.52it/s, loss=2.17, v_num=1, train_loss_step=6.610, val_loss=3.100, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 67:  85%|██████▊ | 57/67 [00:16<00:02,  3.51it/s, loss=2.17, v_num=1, train_loss_step=6.610, val_loss=3.100, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 67:  87%|██████▉ | 58/67 [00:16<00:02,  3.50it/s, loss=2.17, v_num=1, train_loss_step=6.610, val_loss=3.100, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 67:  88%|███████ | 59/67 [00:16<00:02,  3.49it/s, loss=2.17, v_num=1, train_loss_step=6.610, val_loss=3.100, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 67:  90%|███████▏| 60/67 [00:17<00:02,  3.49it/s, loss=2.17, v_num=1, train_loss_step=6.610, val_loss=3.100, train_loss_epoch=1.850]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 67:  91%|███████▎| 61/67 [00:17<00:01,  3.46it/s, loss=2.17, v_num=1, train_loss_step=6.610, val_loss=3.100, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 67:  93%|███████▍| 62/67 [00:17<00:01,  3.45it/s, loss=2.17, v_num=1, train_loss_step=6.610, val_loss=3.100, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 67:  94%|███████▌| 63/67 [00:18<00:01,  3.44it/s, loss=2.17, v_num=1, train_loss_step=6.610, val_loss=3.100, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 67:  96%|███████▋| 64/67 [00:18<00:00,  3.44it/s, loss=2.17, v_num=1, train_loss_step=6.610, val_loss=3.100, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 67:  97%|███████▊| 65/67 [00:18<00:00,  3.43it/s, loss=2.17, v_num=1, train_loss_step=6.610, val_loss=3.100, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 67:  99%|███████▉| 66/67 [00:19<00:00,  3.43it/s, loss=2.17, v_num=1, train_loss_step=6.610, val_loss=3.100, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 67: 100%|████████| 67/67 [00:19<00:00,  3.36it/s, loss=2.17, v_num=1, train_loss_step=6.610, val_loss=3.100, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 68:  45%|███▌    | 30/67 [00:05<00:07,  5.13it/s, loss=1.96, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=1.990]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 68:  46%|███▋    | 31/67 [00:07<00:09,  3.97it/s, loss=1.96, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 68:  48%|███▊    | 32/67 [00:08<00:08,  3.93it/s, loss=1.96, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 68:  49%|███▉    | 33/67 [00:08<00:08,  3.90it/s, loss=1.96, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 68:  51%|████    | 34/67 [00:08<00:08,  3.87it/s, loss=1.96, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 68:  52%|████▏   | 35/67 [00:09<00:08,  3.84it/s, loss=1.96, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 68:  54%|████▎   | 36/67 [00:09<00:08,  3.82it/s, loss=1.96, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 68:  55%|████▍   | 37/67 [00:09<00:07,  3.79it/s, loss=1.96, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 68:  57%|████▌   | 38/67 [00:10<00:07,  3.76it/s, loss=1.96, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 68:  58%|████▋   | 39/67 [00:10<00:07,  3.75it/s, loss=1.96, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 68:  60%|████▊   | 40/67 [00:10<00:07,  3.72it/s, loss=1.96, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=1.990]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 68:  61%|████▉   | 41/67 [00:11<00:07,  3.66it/s, loss=1.96, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 68:  63%|█████   | 42/67 [00:11<00:06,  3.64it/s, loss=1.96, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 68:  64%|█████▏  | 43/67 [00:11<00:06,  3.62it/s, loss=1.96, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 68:  66%|█████▎  | 44/67 [00:12<00:06,  3.61it/s, loss=1.96, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 68:  67%|█████▎  | 45/67 [00:12<00:06,  3.60it/s, loss=1.96, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 68:  69%|█████▍  | 46/67 [00:12<00:05,  3.58it/s, loss=1.96, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 68:  70%|█████▌  | 47/67 [00:13<00:05,  3.57it/s, loss=1.96, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 68:  72%|█████▋  | 48/67 [00:13<00:05,  3.56it/s, loss=1.96, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 68:  73%|█████▊  | 49/67 [00:13<00:05,  3.54it/s, loss=1.96, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 68:  75%|█████▉  | 50/67 [00:14<00:04,  3.53it/s, loss=1.96, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 68:  76%|██████  | 51/67 [00:14<00:04,  3.49it/s, loss=1.96, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 68:  78%|██████▏ | 52/67 [00:14<00:04,  3.48it/s, loss=1.96, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 68:  79%|██████▎ | 53/67 [00:15<00:04,  3.47it/s, loss=1.96, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 68:  81%|██████▍ | 54/67 [00:15<00:03,  3.46it/s, loss=1.96, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 68:  82%|██████▌ | 55/67 [00:15<00:03,  3.46it/s, loss=1.96, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 68:  84%|██████▋ | 56/67 [00:16<00:03,  3.45it/s, loss=1.96, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 68:  85%|██████▊ | 57/67 [00:16<00:02,  3.44it/s, loss=1.96, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 68:  87%|██████▉ | 58/67 [00:16<00:02,  3.43it/s, loss=1.96, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 68:  88%|███████ | 59/67 [00:17<00:02,  3.43it/s, loss=1.96, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 68:  90%|███████▏| 60/67 [00:17<00:02,  3.42it/s, loss=1.96, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=1.990]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 68:  91%|███████▎| 61/67 [00:17<00:01,  3.39it/s, loss=1.96, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 68:  93%|███████▍| 62/67 [00:18<00:01,  3.39it/s, loss=1.96, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 68:  94%|███████▌| 63/67 [00:18<00:01,  3.38it/s, loss=1.96, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 68:  96%|███████▋| 64/67 [00:18<00:00,  3.37it/s, loss=1.96, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 68:  97%|███████▊| 65/67 [00:19<00:00,  3.37it/s, loss=1.96, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 68:  99%|███████▉| 66/67 [00:19<00:00,  3.36it/s, loss=1.96, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 68: 100%|████████| 67/67 [00:20<00:00,  3.30it/s, loss=1.96, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 69:  45%|████     | 30/67 [00:05<00:07,  5.06it/s, loss=1.9, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=2.030]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 69:  46%|████▏    | 31/67 [00:07<00:09,  3.92it/s, loss=1.9, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 69:  48%|████▎    | 32/67 [00:08<00:09,  3.88it/s, loss=1.9, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 69:  49%|████▍    | 33/67 [00:08<00:08,  3.85it/s, loss=1.9, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 69:  51%|████▌    | 34/67 [00:08<00:08,  3.82it/s, loss=1.9, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 69:  52%|████▋    | 35/67 [00:09<00:08,  3.79it/s, loss=1.9, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 69:  54%|████▊    | 36/67 [00:09<00:08,  3.76it/s, loss=1.9, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 69:  55%|████▉    | 37/67 [00:09<00:08,  3.73it/s, loss=1.9, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 69:  57%|█████    | 38/67 [00:10<00:07,  3.70it/s, loss=1.9, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 69:  58%|█████▏   | 39/67 [00:10<00:07,  3.68it/s, loss=1.9, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 69:  60%|█████▎   | 40/67 [00:10<00:07,  3.66it/s, loss=1.9, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=2.030]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 69:  61%|█████▌   | 41/67 [00:11<00:07,  3.60it/s, loss=1.9, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 69:  63%|█████▋   | 42/67 [00:11<00:06,  3.58it/s, loss=1.9, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 69:  64%|█████▊   | 43/67 [00:12<00:06,  3.57it/s, loss=1.9, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 69:  66%|█████▉   | 44/67 [00:12<00:06,  3.55it/s, loss=1.9, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 69:  67%|██████   | 45/67 [00:12<00:06,  3.53it/s, loss=1.9, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 69:  69%|██████▏  | 46/67 [00:13<00:05,  3.52it/s, loss=1.9, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 69:  70%|██████▎  | 47/67 [00:13<00:05,  3.51it/s, loss=1.9, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 69:  72%|██████▍  | 48/67 [00:13<00:05,  3.50it/s, loss=1.9, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 69:  73%|██████▌  | 49/67 [00:14<00:05,  3.49it/s, loss=1.9, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 69:  75%|██████▋  | 50/67 [00:14<00:04,  3.47it/s, loss=1.9, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 69:  76%|██████▊  | 51/67 [00:14<00:04,  3.44it/s, loss=1.9, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 69:  78%|██████▉  | 52/67 [00:15<00:04,  3.42it/s, loss=1.9, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 69:  79%|███████  | 53/67 [00:15<00:04,  3.41it/s, loss=1.9, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 69:  81%|███████▎ | 54/67 [00:15<00:03,  3.41it/s, loss=1.9, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 69:  82%|███████▍ | 55/67 [00:16<00:03,  3.40it/s, loss=1.9, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 69:  84%|███████▌ | 56/67 [00:16<00:03,  3.39it/s, loss=1.9, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 69:  85%|███████▋ | 57/67 [00:16<00:02,  3.38it/s, loss=1.9, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 69:  87%|███████▊ | 58/67 [00:17<00:02,  3.37it/s, loss=1.9, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 69:  88%|███████▉ | 59/67 [00:17<00:02,  3.37it/s, loss=1.9, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 69:  90%|████████ | 60/67 [00:17<00:02,  3.36it/s, loss=1.9, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=2.030]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 69:  91%|████████▏| 61/67 [00:18<00:01,  3.33it/s, loss=1.9, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 69:  93%|████████▎| 62/67 [00:18<00:01,  3.32it/s, loss=1.9, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 69:  94%|████████▍| 63/67 [00:18<00:01,  3.32it/s, loss=1.9, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 69:  96%|████████▌| 64/67 [00:19<00:00,  3.31it/s, loss=1.9, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 69:  97%|████████▋| 65/67 [00:19<00:00,  3.31it/s, loss=1.9, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 69:  99%|████████▊| 66/67 [00:19<00:00,  3.31it/s, loss=1.9, v_num=1, train_loss_step=1.810, val_loss=3.100, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 69: 100%|█████████| 67/67 [00:20<00:00,  3.25it/s, loss=1.9, v_num=1, train_loss_step=1.810, val_loss=3.090, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 70:  45%|███▌    | 30/67 [00:05<00:06,  5.30it/s, loss=2.04, v_num=1, train_loss_step=2.230, val_loss=3.090, train_loss_epoch=1.930]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 70:  46%|███▋    | 31/67 [00:07<00:08,  4.06it/s, loss=2.04, v_num=1, train_loss_step=2.230, val_loss=3.090, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 70:  48%|███▊    | 32/67 [00:07<00:08,  4.02it/s, loss=2.04, v_num=1, train_loss_step=2.230, val_loss=3.090, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 70:  49%|███▉    | 33/67 [00:08<00:08,  3.98it/s, loss=2.04, v_num=1, train_loss_step=2.230, val_loss=3.090, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 70:  51%|████    | 34/67 [00:08<00:08,  3.94it/s, loss=2.04, v_num=1, train_loss_step=2.230, val_loss=3.090, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 70:  52%|████▏   | 35/67 [00:08<00:08,  3.91it/s, loss=2.04, v_num=1, train_loss_step=2.230, val_loss=3.090, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 70:  54%|████▎   | 36/67 [00:09<00:08,  3.87it/s, loss=2.04, v_num=1, train_loss_step=2.230, val_loss=3.090, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 70:  55%|████▍   | 37/67 [00:09<00:07,  3.84it/s, loss=2.04, v_num=1, train_loss_step=2.230, val_loss=3.090, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 70:  57%|████▌   | 38/67 [00:09<00:07,  3.82it/s, loss=2.04, v_num=1, train_loss_step=2.230, val_loss=3.090, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 70:  58%|████▋   | 39/67 [00:10<00:07,  3.79it/s, loss=2.04, v_num=1, train_loss_step=2.230, val_loss=3.090, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 70:  60%|████▊   | 40/67 [00:10<00:07,  3.77it/s, loss=2.04, v_num=1, train_loss_step=2.230, val_loss=3.090, train_loss_epoch=1.930]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 70:  61%|████▉   | 41/67 [00:11<00:07,  3.71it/s, loss=2.04, v_num=1, train_loss_step=2.230, val_loss=3.090, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 70:  63%|█████   | 42/67 [00:11<00:06,  3.69it/s, loss=2.04, v_num=1, train_loss_step=2.230, val_loss=3.090, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 70:  64%|█████▏  | 43/67 [00:11<00:06,  3.67it/s, loss=2.04, v_num=1, train_loss_step=2.230, val_loss=3.090, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 70:  66%|█████▎  | 44/67 [00:12<00:06,  3.65it/s, loss=2.04, v_num=1, train_loss_step=2.230, val_loss=3.090, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 70:  67%|█████▎  | 45/67 [00:12<00:06,  3.63it/s, loss=2.04, v_num=1, train_loss_step=2.230, val_loss=3.090, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 70:  69%|█████▍  | 46/67 [00:12<00:05,  3.61it/s, loss=2.04, v_num=1, train_loss_step=2.230, val_loss=3.090, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 70:  70%|█████▌  | 47/67 [00:13<00:05,  3.60it/s, loss=2.04, v_num=1, train_loss_step=2.230, val_loss=3.090, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 70:  72%|█████▋  | 48/67 [00:13<00:05,  3.59it/s, loss=2.04, v_num=1, train_loss_step=2.230, val_loss=3.090, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 70:  73%|█████▊  | 49/67 [00:13<00:05,  3.57it/s, loss=2.04, v_num=1, train_loss_step=2.230, val_loss=3.090, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 70:  75%|█████▉  | 50/67 [00:14<00:04,  3.56it/s, loss=2.04, v_num=1, train_loss_step=2.230, val_loss=3.090, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 70:  76%|██████  | 51/67 [00:14<00:04,  3.52it/s, loss=2.04, v_num=1, train_loss_step=2.230, val_loss=3.090, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 70:  78%|██████▏ | 52/67 [00:14<00:04,  3.51it/s, loss=2.04, v_num=1, train_loss_step=2.230, val_loss=3.090, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 70:  79%|██████▎ | 53/67 [00:15<00:04,  3.50it/s, loss=2.04, v_num=1, train_loss_step=2.230, val_loss=3.090, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 70:  81%|██████▍ | 54/67 [00:15<00:03,  3.49it/s, loss=2.04, v_num=1, train_loss_step=2.230, val_loss=3.090, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 70:  82%|██████▌ | 55/67 [00:15<00:03,  3.48it/s, loss=2.04, v_num=1, train_loss_step=2.230, val_loss=3.090, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 70:  84%|██████▋ | 56/67 [00:16<00:03,  3.46it/s, loss=2.04, v_num=1, train_loss_step=2.230, val_loss=3.090, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 70:  85%|██████▊ | 57/67 [00:16<00:02,  3.45it/s, loss=2.04, v_num=1, train_loss_step=2.230, val_loss=3.090, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 70:  87%|██████▉ | 58/67 [00:16<00:02,  3.44it/s, loss=2.04, v_num=1, train_loss_step=2.230, val_loss=3.090, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 70:  88%|███████ | 59/67 [00:17<00:02,  3.44it/s, loss=2.04, v_num=1, train_loss_step=2.230, val_loss=3.090, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 70:  90%|███████▏| 60/67 [00:17<00:02,  3.43it/s, loss=2.04, v_num=1, train_loss_step=2.230, val_loss=3.090, train_loss_epoch=1.930]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 70:  91%|███████▎| 61/67 [00:17<00:01,  3.40it/s, loss=2.04, v_num=1, train_loss_step=2.230, val_loss=3.090, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 70:  93%|███████▍| 62/67 [00:18<00:01,  3.39it/s, loss=2.04, v_num=1, train_loss_step=2.230, val_loss=3.090, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 70:  94%|███████▌| 63/67 [00:18<00:01,  3.39it/s, loss=2.04, v_num=1, train_loss_step=2.230, val_loss=3.090, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 70:  96%|███████▋| 64/67 [00:18<00:00,  3.38it/s, loss=2.04, v_num=1, train_loss_step=2.230, val_loss=3.090, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 70:  97%|███████▊| 65/67 [00:19<00:00,  3.38it/s, loss=2.04, v_num=1, train_loss_step=2.230, val_loss=3.090, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 70:  99%|███████▉| 66/67 [00:19<00:00,  3.38it/s, loss=2.04, v_num=1, train_loss_step=2.230, val_loss=3.090, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 70: 100%|████████| 67/67 [00:20<00:00,  3.31it/s, loss=2.04, v_num=1, train_loss_step=2.230, val_loss=3.090, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 71:  45%|███▌    | 30/67 [00:05<00:07,  5.18it/s, loss=1.85, v_num=1, train_loss_step=7.100, val_loss=3.090, train_loss_epoch=1.970]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 71:  46%|███▋    | 31/67 [00:07<00:09,  4.00it/s, loss=1.85, v_num=1, train_loss_step=7.100, val_loss=3.090, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 71:  48%|███▊    | 32/67 [00:08<00:08,  3.96it/s, loss=1.85, v_num=1, train_loss_step=7.100, val_loss=3.090, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 71:  49%|███▉    | 33/67 [00:08<00:08,  3.93it/s, loss=1.85, v_num=1, train_loss_step=7.100, val_loss=3.090, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 71:  51%|████    | 34/67 [00:08<00:08,  3.89it/s, loss=1.85, v_num=1, train_loss_step=7.100, val_loss=3.090, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 71:  52%|████▏   | 35/67 [00:09<00:08,  3.86it/s, loss=1.85, v_num=1, train_loss_step=7.100, val_loss=3.090, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 71:  54%|████▎   | 36/67 [00:09<00:08,  3.84it/s, loss=1.85, v_num=1, train_loss_step=7.100, val_loss=3.090, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 71:  55%|████▍   | 37/67 [00:09<00:07,  3.80it/s, loss=1.85, v_num=1, train_loss_step=7.100, val_loss=3.090, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 71:  57%|████▌   | 38/67 [00:10<00:07,  3.78it/s, loss=1.85, v_num=1, train_loss_step=7.100, val_loss=3.090, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 71:  58%|████▋   | 39/67 [00:10<00:07,  3.76it/s, loss=1.85, v_num=1, train_loss_step=7.100, val_loss=3.090, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 71:  60%|████▊   | 40/67 [00:10<00:07,  3.73it/s, loss=1.85, v_num=1, train_loss_step=7.100, val_loss=3.090, train_loss_epoch=1.970]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 71:  61%|████▉   | 41/67 [00:11<00:07,  3.67it/s, loss=1.85, v_num=1, train_loss_step=7.100, val_loss=3.090, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 71:  63%|█████   | 42/67 [00:11<00:06,  3.66it/s, loss=1.85, v_num=1, train_loss_step=7.100, val_loss=3.090, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 71:  64%|█████▏  | 43/67 [00:11<00:06,  3.64it/s, loss=1.85, v_num=1, train_loss_step=7.100, val_loss=3.090, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 71:  66%|█████▎  | 44/67 [00:12<00:06,  3.62it/s, loss=1.85, v_num=1, train_loss_step=7.100, val_loss=3.090, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 71:  67%|█████▎  | 45/67 [00:12<00:06,  3.61it/s, loss=1.85, v_num=1, train_loss_step=7.100, val_loss=3.090, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 71:  69%|█████▍  | 46/67 [00:12<00:05,  3.60it/s, loss=1.85, v_num=1, train_loss_step=7.100, val_loss=3.090, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 71:  70%|█████▌  | 47/67 [00:13<00:05,  3.59it/s, loss=1.85, v_num=1, train_loss_step=7.100, val_loss=3.090, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 71:  72%|█████▋  | 48/67 [00:13<00:05,  3.57it/s, loss=1.85, v_num=1, train_loss_step=7.100, val_loss=3.090, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 71:  73%|█████▊  | 49/67 [00:13<00:05,  3.56it/s, loss=1.85, v_num=1, train_loss_step=7.100, val_loss=3.090, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 71:  75%|█████▉  | 50/67 [00:14<00:04,  3.55it/s, loss=1.85, v_num=1, train_loss_step=7.100, val_loss=3.090, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 71:  76%|██████  | 51/67 [00:14<00:04,  3.51it/s, loss=1.85, v_num=1, train_loss_step=7.100, val_loss=3.090, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 71:  78%|██████▏ | 52/67 [00:14<00:04,  3.50it/s, loss=1.85, v_num=1, train_loss_step=7.100, val_loss=3.090, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 71:  79%|██████▎ | 53/67 [00:15<00:04,  3.49it/s, loss=1.85, v_num=1, train_loss_step=7.100, val_loss=3.090, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 71:  81%|██████▍ | 54/67 [00:15<00:03,  3.48it/s, loss=1.85, v_num=1, train_loss_step=7.100, val_loss=3.090, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 71:  82%|██████▌ | 55/67 [00:15<00:03,  3.47it/s, loss=1.85, v_num=1, train_loss_step=7.100, val_loss=3.090, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 71:  84%|██████▋ | 56/67 [00:16<00:03,  3.46it/s, loss=1.85, v_num=1, train_loss_step=7.100, val_loss=3.090, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 71:  85%|██████▊ | 57/67 [00:16<00:02,  3.46it/s, loss=1.85, v_num=1, train_loss_step=7.100, val_loss=3.090, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 71:  87%|██████▉ | 58/67 [00:16<00:02,  3.45it/s, loss=1.85, v_num=1, train_loss_step=7.100, val_loss=3.090, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 71:  88%|███████ | 59/67 [00:17<00:02,  3.44it/s, loss=1.85, v_num=1, train_loss_step=7.100, val_loss=3.090, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 71:  90%|███████▏| 60/67 [00:17<00:02,  3.43it/s, loss=1.85, v_num=1, train_loss_step=7.100, val_loss=3.090, train_loss_epoch=1.970]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 71:  91%|███████▎| 61/67 [00:17<00:01,  3.40it/s, loss=1.85, v_num=1, train_loss_step=7.100, val_loss=3.090, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 71:  93%|███████▍| 62/67 [00:18<00:01,  3.39it/s, loss=1.85, v_num=1, train_loss_step=7.100, val_loss=3.090, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 71:  94%|███████▌| 63/67 [00:18<00:01,  3.39it/s, loss=1.85, v_num=1, train_loss_step=7.100, val_loss=3.090, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 71:  96%|███████▋| 64/67 [00:18<00:00,  3.38it/s, loss=1.85, v_num=1, train_loss_step=7.100, val_loss=3.090, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 71:  97%|███████▊| 65/67 [00:19<00:00,  3.38it/s, loss=1.85, v_num=1, train_loss_step=7.100, val_loss=3.090, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 71:  99%|███████▉| 66/67 [00:19<00:00,  3.38it/s, loss=1.85, v_num=1, train_loss_step=7.100, val_loss=3.090, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 71: 100%|████████| 67/67 [00:20<00:00,  3.31it/s, loss=1.85, v_num=1, train_loss_step=7.100, val_loss=3.100, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 72:  45%|███▌    | 30/67 [00:05<00:07,  5.26it/s, loss=2.04, v_num=1, train_loss_step=1.710, val_loss=3.100, train_loss_epoch=2.010]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 72:  46%|███▋    | 31/67 [00:07<00:08,  4.04it/s, loss=2.04, v_num=1, train_loss_step=1.710, val_loss=3.100, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 72:  48%|███▊    | 32/67 [00:07<00:08,  4.00it/s, loss=2.04, v_num=1, train_loss_step=1.710, val_loss=3.100, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 72:  49%|███▉    | 33/67 [00:08<00:08,  3.96it/s, loss=2.04, v_num=1, train_loss_step=1.710, val_loss=3.100, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 72:  51%|████    | 34/67 [00:08<00:08,  3.92it/s, loss=2.04, v_num=1, train_loss_step=1.710, val_loss=3.100, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 72:  52%|████▏   | 35/67 [00:08<00:08,  3.89it/s, loss=2.04, v_num=1, train_loss_step=1.710, val_loss=3.100, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 72:  54%|████▎   | 36/67 [00:09<00:08,  3.87it/s, loss=2.04, v_num=1, train_loss_step=1.710, val_loss=3.100, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 72:  55%|████▍   | 37/67 [00:09<00:07,  3.84it/s, loss=2.04, v_num=1, train_loss_step=1.710, val_loss=3.100, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 72:  57%|████▌   | 38/67 [00:09<00:07,  3.81it/s, loss=2.04, v_num=1, train_loss_step=1.710, val_loss=3.100, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 72:  58%|████▋   | 39/67 [00:10<00:07,  3.79it/s, loss=2.04, v_num=1, train_loss_step=1.710, val_loss=3.100, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 72:  60%|████▊   | 40/67 [00:10<00:07,  3.77it/s, loss=2.04, v_num=1, train_loss_step=1.710, val_loss=3.100, train_loss_epoch=2.010]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 72:  61%|████▉   | 41/67 [00:11<00:07,  3.70it/s, loss=2.04, v_num=1, train_loss_step=1.710, val_loss=3.100, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 72:  63%|█████   | 42/67 [00:11<00:06,  3.69it/s, loss=2.04, v_num=1, train_loss_step=1.710, val_loss=3.100, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 72:  64%|█████▏  | 43/67 [00:11<00:06,  3.67it/s, loss=2.04, v_num=1, train_loss_step=1.710, val_loss=3.100, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 72:  66%|█████▎  | 44/67 [00:12<00:06,  3.65it/s, loss=2.04, v_num=1, train_loss_step=1.710, val_loss=3.100, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 72:  67%|█████▎  | 45/67 [00:12<00:06,  3.64it/s, loss=2.04, v_num=1, train_loss_step=1.710, val_loss=3.100, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 72:  69%|█████▍  | 46/67 [00:12<00:05,  3.63it/s, loss=2.04, v_num=1, train_loss_step=1.710, val_loss=3.100, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 72:  70%|█████▌  | 47/67 [00:13<00:05,  3.61it/s, loss=2.04, v_num=1, train_loss_step=1.710, val_loss=3.100, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 72:  72%|█████▋  | 48/67 [00:13<00:05,  3.60it/s, loss=2.04, v_num=1, train_loss_step=1.710, val_loss=3.100, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 72:  73%|█████▊  | 49/67 [00:13<00:05,  3.59it/s, loss=2.04, v_num=1, train_loss_step=1.710, val_loss=3.100, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 72:  75%|█████▉  | 50/67 [00:13<00:04,  3.57it/s, loss=2.04, v_num=1, train_loss_step=1.710, val_loss=3.100, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 72:  76%|██████  | 51/67 [00:14<00:04,  3.53it/s, loss=2.04, v_num=1, train_loss_step=1.710, val_loss=3.100, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 72:  78%|██████▏ | 52/67 [00:14<00:04,  3.52it/s, loss=2.04, v_num=1, train_loss_step=1.710, val_loss=3.100, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 72:  79%|██████▎ | 53/67 [00:15<00:03,  3.51it/s, loss=2.04, v_num=1, train_loss_step=1.710, val_loss=3.100, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 72:  81%|██████▍ | 54/67 [00:15<00:03,  3.50it/s, loss=2.04, v_num=1, train_loss_step=1.710, val_loss=3.100, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 72:  82%|██████▌ | 55/67 [00:15<00:03,  3.49it/s, loss=2.04, v_num=1, train_loss_step=1.710, val_loss=3.100, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 72:  84%|██████▋ | 56/67 [00:16<00:03,  3.48it/s, loss=2.04, v_num=1, train_loss_step=1.710, val_loss=3.100, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 72:  85%|██████▊ | 57/67 [00:16<00:02,  3.48it/s, loss=2.04, v_num=1, train_loss_step=1.710, val_loss=3.100, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 72:  87%|██████▉ | 58/67 [00:16<00:02,  3.47it/s, loss=2.04, v_num=1, train_loss_step=1.710, val_loss=3.100, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 72:  88%|███████ | 59/67 [00:17<00:02,  3.46it/s, loss=2.04, v_num=1, train_loss_step=1.710, val_loss=3.100, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 72:  90%|███████▏| 60/67 [00:17<00:02,  3.45it/s, loss=2.04, v_num=1, train_loss_step=1.710, val_loss=3.100, train_loss_epoch=2.010]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 72:  91%|███████▎| 61/67 [00:17<00:01,  3.42it/s, loss=2.04, v_num=1, train_loss_step=1.710, val_loss=3.100, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 72:  93%|███████▍| 62/67 [00:18<00:01,  3.42it/s, loss=2.04, v_num=1, train_loss_step=1.710, val_loss=3.100, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 72:  94%|███████▌| 63/67 [00:18<00:01,  3.41it/s, loss=2.04, v_num=1, train_loss_step=1.710, val_loss=3.100, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 72:  96%|███████▋| 64/67 [00:18<00:00,  3.41it/s, loss=2.04, v_num=1, train_loss_step=1.710, val_loss=3.100, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 72:  97%|███████▊| 65/67 [00:19<00:00,  3.40it/s, loss=2.04, v_num=1, train_loss_step=1.710, val_loss=3.100, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 72:  99%|███████▉| 66/67 [00:19<00:00,  3.40it/s, loss=2.04, v_num=1, train_loss_step=1.710, val_loss=3.100, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 72: 100%|████████| 67/67 [00:20<00:00,  3.33it/s, loss=2.04, v_num=1, train_loss_step=1.710, val_loss=3.090, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 73:  45%|███▌    | 30/67 [00:05<00:07,  5.21it/s, loss=1.94, v_num=1, train_loss_step=3.210, val_loss=3.090, train_loss_epoch=2.120]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 73:  46%|███▋    | 31/67 [00:07<00:08,  4.04it/s, loss=1.94, v_num=1, train_loss_step=3.210, val_loss=3.090, train_loss_epoch=2.120]\u001b[A\n",
      "Epoch 73:  48%|███▊    | 32/67 [00:08<00:08,  3.99it/s, loss=1.94, v_num=1, train_loss_step=3.210, val_loss=3.090, train_loss_epoch=2.120]\u001b[A\n",
      "Epoch 73:  49%|███▉    | 33/67 [00:08<00:08,  3.95it/s, loss=1.94, v_num=1, train_loss_step=3.210, val_loss=3.090, train_loss_epoch=2.120]\u001b[A\n",
      "Epoch 73:  51%|████    | 34/67 [00:08<00:08,  3.91it/s, loss=1.94, v_num=1, train_loss_step=3.210, val_loss=3.090, train_loss_epoch=2.120]\u001b[A\n",
      "Epoch 73:  52%|████▏   | 35/67 [00:09<00:08,  3.88it/s, loss=1.94, v_num=1, train_loss_step=3.210, val_loss=3.090, train_loss_epoch=2.120]\u001b[A\n",
      "Epoch 73:  54%|████▎   | 36/67 [00:09<00:08,  3.85it/s, loss=1.94, v_num=1, train_loss_step=3.210, val_loss=3.090, train_loss_epoch=2.120]\u001b[A\n",
      "Epoch 73:  55%|████▍   | 37/67 [00:09<00:07,  3.82it/s, loss=1.94, v_num=1, train_loss_step=3.210, val_loss=3.090, train_loss_epoch=2.120]\u001b[A\n",
      "Epoch 73:  57%|████▌   | 38/67 [00:10<00:07,  3.80it/s, loss=1.94, v_num=1, train_loss_step=3.210, val_loss=3.090, train_loss_epoch=2.120]\u001b[A\n",
      "Epoch 73:  58%|████▋   | 39/67 [00:10<00:07,  3.78it/s, loss=1.94, v_num=1, train_loss_step=3.210, val_loss=3.090, train_loss_epoch=2.120]\u001b[A\n",
      "Epoch 73:  60%|████▊   | 40/67 [00:10<00:07,  3.75it/s, loss=1.94, v_num=1, train_loss_step=3.210, val_loss=3.090, train_loss_epoch=2.120]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 73:  61%|████▉   | 41/67 [00:11<00:07,  3.69it/s, loss=1.94, v_num=1, train_loss_step=3.210, val_loss=3.090, train_loss_epoch=2.120]\u001b[A\n",
      "Epoch 73:  63%|█████   | 42/67 [00:11<00:06,  3.67it/s, loss=1.94, v_num=1, train_loss_step=3.210, val_loss=3.090, train_loss_epoch=2.120]\u001b[A\n",
      "Epoch 73:  64%|█████▏  | 43/67 [00:11<00:06,  3.66it/s, loss=1.94, v_num=1, train_loss_step=3.210, val_loss=3.090, train_loss_epoch=2.120]\u001b[A\n",
      "Epoch 73:  66%|█████▎  | 44/67 [00:12<00:06,  3.64it/s, loss=1.94, v_num=1, train_loss_step=3.210, val_loss=3.090, train_loss_epoch=2.120]\u001b[A\n",
      "Epoch 73:  67%|█████▎  | 45/67 [00:12<00:06,  3.62it/s, loss=1.94, v_num=1, train_loss_step=3.210, val_loss=3.090, train_loss_epoch=2.120]\u001b[A\n",
      "Epoch 73:  69%|█████▍  | 46/67 [00:12<00:05,  3.61it/s, loss=1.94, v_num=1, train_loss_step=3.210, val_loss=3.090, train_loss_epoch=2.120]\u001b[A\n",
      "Epoch 73:  70%|█████▌  | 47/67 [00:13<00:05,  3.59it/s, loss=1.94, v_num=1, train_loss_step=3.210, val_loss=3.090, train_loss_epoch=2.120]\u001b[A\n",
      "Epoch 73:  72%|█████▋  | 48/67 [00:13<00:05,  3.58it/s, loss=1.94, v_num=1, train_loss_step=3.210, val_loss=3.090, train_loss_epoch=2.120]\u001b[A\n",
      "Epoch 73:  73%|█████▊  | 49/67 [00:13<00:05,  3.56it/s, loss=1.94, v_num=1, train_loss_step=3.210, val_loss=3.090, train_loss_epoch=2.120]\u001b[A\n",
      "Epoch 73:  75%|█████▉  | 50/67 [00:14<00:04,  3.55it/s, loss=1.94, v_num=1, train_loss_step=3.210, val_loss=3.090, train_loss_epoch=2.120]\u001b[A\n",
      "Epoch 73:  76%|██████  | 51/67 [00:14<00:04,  3.51it/s, loss=1.94, v_num=1, train_loss_step=3.210, val_loss=3.090, train_loss_epoch=2.120]\u001b[A\n",
      "Epoch 73:  78%|██████▏ | 52/67 [00:14<00:04,  3.50it/s, loss=1.94, v_num=1, train_loss_step=3.210, val_loss=3.090, train_loss_epoch=2.120]\u001b[A\n",
      "Epoch 73:  79%|██████▎ | 53/67 [00:15<00:04,  3.49it/s, loss=1.94, v_num=1, train_loss_step=3.210, val_loss=3.090, train_loss_epoch=2.120]\u001b[A\n",
      "Epoch 73:  81%|██████▍ | 54/67 [00:15<00:03,  3.48it/s, loss=1.94, v_num=1, train_loss_step=3.210, val_loss=3.090, train_loss_epoch=2.120]\u001b[A\n",
      "Epoch 73:  82%|██████▌ | 55/67 [00:15<00:03,  3.47it/s, loss=1.94, v_num=1, train_loss_step=3.210, val_loss=3.090, train_loss_epoch=2.120]\u001b[A\n",
      "Epoch 73:  84%|██████▋ | 56/67 [00:16<00:03,  3.46it/s, loss=1.94, v_num=1, train_loss_step=3.210, val_loss=3.090, train_loss_epoch=2.120]\u001b[A\n",
      "Epoch 73:  85%|██████▊ | 57/67 [00:16<00:02,  3.45it/s, loss=1.94, v_num=1, train_loss_step=3.210, val_loss=3.090, train_loss_epoch=2.120]\u001b[A\n",
      "Epoch 73:  87%|██████▉ | 58/67 [00:16<00:02,  3.45it/s, loss=1.94, v_num=1, train_loss_step=3.210, val_loss=3.090, train_loss_epoch=2.120]\u001b[A\n",
      "Epoch 73:  88%|███████ | 59/67 [00:17<00:02,  3.44it/s, loss=1.94, v_num=1, train_loss_step=3.210, val_loss=3.090, train_loss_epoch=2.120]\u001b[A\n",
      "Epoch 73:  90%|███████▏| 60/67 [00:17<00:02,  3.43it/s, loss=1.94, v_num=1, train_loss_step=3.210, val_loss=3.090, train_loss_epoch=2.120]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 73:  91%|███████▎| 61/67 [00:17<00:01,  3.40it/s, loss=1.94, v_num=1, train_loss_step=3.210, val_loss=3.090, train_loss_epoch=2.120]\u001b[A\n",
      "Epoch 73:  93%|███████▍| 62/67 [00:18<00:01,  3.40it/s, loss=1.94, v_num=1, train_loss_step=3.210, val_loss=3.090, train_loss_epoch=2.120]\u001b[A\n",
      "Epoch 73:  94%|███████▌| 63/67 [00:18<00:01,  3.39it/s, loss=1.94, v_num=1, train_loss_step=3.210, val_loss=3.090, train_loss_epoch=2.120]\u001b[A\n",
      "Epoch 73:  96%|███████▋| 64/67 [00:18<00:00,  3.39it/s, loss=1.94, v_num=1, train_loss_step=3.210, val_loss=3.090, train_loss_epoch=2.120]\u001b[A\n",
      "Epoch 73:  97%|███████▊| 65/67 [00:19<00:00,  3.38it/s, loss=1.94, v_num=1, train_loss_step=3.210, val_loss=3.090, train_loss_epoch=2.120]\u001b[A\n",
      "Epoch 73:  99%|███████▉| 66/67 [00:19<00:00,  3.38it/s, loss=1.94, v_num=1, train_loss_step=3.210, val_loss=3.090, train_loss_epoch=2.120]\u001b[A\n",
      "Epoch 73: 100%|████████| 67/67 [00:20<00:00,  3.31it/s, loss=1.94, v_num=1, train_loss_step=3.210, val_loss=3.090, train_loss_epoch=2.120]\u001b[A\n",
      "Epoch 74:  45%|███▌    | 30/67 [00:06<00:07,  4.99it/s, loss=1.68, v_num=1, train_loss_step=2.240, val_loss=3.090, train_loss_epoch=2.130]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 74:  46%|███▋    | 31/67 [00:07<00:09,  3.88it/s, loss=1.68, v_num=1, train_loss_step=2.240, val_loss=3.090, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 74:  48%|███▊    | 32/67 [00:08<00:09,  3.85it/s, loss=1.68, v_num=1, train_loss_step=2.240, val_loss=3.090, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 74:  49%|███▉    | 33/67 [00:08<00:08,  3.81it/s, loss=1.68, v_num=1, train_loss_step=2.240, val_loss=3.090, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 74:  51%|████    | 34/67 [00:08<00:08,  3.78it/s, loss=1.68, v_num=1, train_loss_step=2.240, val_loss=3.090, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 74:  52%|████▏   | 35/67 [00:09<00:08,  3.75it/s, loss=1.68, v_num=1, train_loss_step=2.240, val_loss=3.090, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 74:  54%|████▎   | 36/67 [00:09<00:08,  3.72it/s, loss=1.68, v_num=1, train_loss_step=2.240, val_loss=3.090, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 74:  55%|████▍   | 37/67 [00:10<00:08,  3.70it/s, loss=1.68, v_num=1, train_loss_step=2.240, val_loss=3.090, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 74:  57%|████▌   | 38/67 [00:10<00:07,  3.67it/s, loss=1.68, v_num=1, train_loss_step=2.240, val_loss=3.090, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 74:  58%|████▋   | 39/67 [00:10<00:07,  3.65it/s, loss=1.68, v_num=1, train_loss_step=2.240, val_loss=3.090, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 74:  60%|████▊   | 40/67 [00:11<00:07,  3.63it/s, loss=1.68, v_num=1, train_loss_step=2.240, val_loss=3.090, train_loss_epoch=2.130]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 74:  61%|████▉   | 41/67 [00:11<00:07,  3.57it/s, loss=1.68, v_num=1, train_loss_step=2.240, val_loss=3.090, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 74:  63%|█████   | 42/67 [00:11<00:07,  3.54it/s, loss=1.68, v_num=1, train_loss_step=2.240, val_loss=3.090, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 74:  64%|█████▏  | 43/67 [00:12<00:06,  3.53it/s, loss=1.68, v_num=1, train_loss_step=2.240, val_loss=3.090, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 74:  66%|█████▎  | 44/67 [00:12<00:06,  3.51it/s, loss=1.68, v_num=1, train_loss_step=2.240, val_loss=3.090, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 74:  67%|█████▎  | 45/67 [00:12<00:06,  3.50it/s, loss=1.68, v_num=1, train_loss_step=2.240, val_loss=3.090, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 74:  69%|█████▍  | 46/67 [00:13<00:06,  3.48it/s, loss=1.68, v_num=1, train_loss_step=2.240, val_loss=3.090, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 74:  70%|█████▌  | 47/67 [00:13<00:05,  3.47it/s, loss=1.68, v_num=1, train_loss_step=2.240, val_loss=3.090, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 74:  72%|█████▋  | 48/67 [00:13<00:05,  3.46it/s, loss=1.68, v_num=1, train_loss_step=2.240, val_loss=3.090, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 74:  73%|█████▊  | 49/67 [00:14<00:05,  3.45it/s, loss=1.68, v_num=1, train_loss_step=2.240, val_loss=3.090, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 74:  75%|█████▉  | 50/67 [00:14<00:04,  3.43it/s, loss=1.68, v_num=1, train_loss_step=2.240, val_loss=3.090, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 74:  76%|██████  | 51/67 [00:15<00:04,  3.40it/s, loss=1.68, v_num=1, train_loss_step=2.240, val_loss=3.090, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 74:  78%|██████▏ | 52/67 [00:15<00:04,  3.39it/s, loss=1.68, v_num=1, train_loss_step=2.240, val_loss=3.090, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 74:  79%|██████▎ | 53/67 [00:15<00:04,  3.37it/s, loss=1.68, v_num=1, train_loss_step=2.240, val_loss=3.090, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 74:  81%|██████▍ | 54/67 [00:16<00:03,  3.36it/s, loss=1.68, v_num=1, train_loss_step=2.240, val_loss=3.090, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 74:  82%|██████▌ | 55/67 [00:16<00:03,  3.35it/s, loss=1.68, v_num=1, train_loss_step=2.240, val_loss=3.090, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 74:  84%|██████▋ | 56/67 [00:16<00:03,  3.34it/s, loss=1.68, v_num=1, train_loss_step=2.240, val_loss=3.090, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 74:  85%|██████▊ | 57/67 [00:17<00:03,  3.33it/s, loss=1.68, v_num=1, train_loss_step=2.240, val_loss=3.090, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 74:  87%|██████▉ | 58/67 [00:17<00:02,  3.33it/s, loss=1.68, v_num=1, train_loss_step=2.240, val_loss=3.090, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 74:  88%|███████ | 59/67 [00:17<00:02,  3.32it/s, loss=1.68, v_num=1, train_loss_step=2.240, val_loss=3.090, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 74:  90%|███████▏| 60/67 [00:18<00:02,  3.31it/s, loss=1.68, v_num=1, train_loss_step=2.240, val_loss=3.090, train_loss_epoch=2.130]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 74:  91%|███████▎| 61/67 [00:18<00:01,  3.29it/s, loss=1.68, v_num=1, train_loss_step=2.240, val_loss=3.090, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 74:  93%|███████▍| 62/67 [00:18<00:01,  3.28it/s, loss=1.68, v_num=1, train_loss_step=2.240, val_loss=3.090, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 74:  94%|███████▌| 63/67 [00:19<00:01,  3.28it/s, loss=1.68, v_num=1, train_loss_step=2.240, val_loss=3.090, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 74:  96%|███████▋| 64/67 [00:19<00:00,  3.27it/s, loss=1.68, v_num=1, train_loss_step=2.240, val_loss=3.090, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 74:  97%|███████▊| 65/67 [00:19<00:00,  3.27it/s, loss=1.68, v_num=1, train_loss_step=2.240, val_loss=3.090, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 74:  99%|███████▉| 66/67 [00:20<00:00,  3.27it/s, loss=1.68, v_num=1, train_loss_step=2.240, val_loss=3.090, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 74: 100%|████████| 67/67 [00:20<00:00,  3.21it/s, loss=1.68, v_num=1, train_loss_step=2.240, val_loss=3.090, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 75:  45%|███▌    | 30/67 [00:06<00:07,  4.85it/s, loss=2.33, v_num=1, train_loss_step=1.380, val_loss=3.090, train_loss_epoch=1.740]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 75:  46%|███▋    | 31/67 [00:08<00:09,  3.73it/s, loss=2.33, v_num=1, train_loss_step=1.380, val_loss=3.090, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 75:  48%|███▊    | 32/67 [00:08<00:09,  3.69it/s, loss=2.33, v_num=1, train_loss_step=1.380, val_loss=3.090, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 75:  49%|███▉    | 33/67 [00:09<00:09,  3.66it/s, loss=2.33, v_num=1, train_loss_step=1.380, val_loss=3.090, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 75:  51%|████    | 34/67 [00:09<00:09,  3.62it/s, loss=2.33, v_num=1, train_loss_step=1.380, val_loss=3.090, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 75:  52%|████▏   | 35/67 [00:09<00:08,  3.59it/s, loss=2.33, v_num=1, train_loss_step=1.380, val_loss=3.090, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 75:  54%|████▎   | 36/67 [00:10<00:08,  3.56it/s, loss=2.33, v_num=1, train_loss_step=1.380, val_loss=3.090, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 75:  55%|████▍   | 37/67 [00:10<00:08,  3.53it/s, loss=2.33, v_num=1, train_loss_step=1.380, val_loss=3.090, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 75:  57%|████▌   | 38/67 [00:10<00:08,  3.51it/s, loss=2.33, v_num=1, train_loss_step=1.380, val_loss=3.090, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 75:  58%|████▋   | 39/67 [00:11<00:08,  3.49it/s, loss=2.33, v_num=1, train_loss_step=1.380, val_loss=3.090, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 75:  60%|████▊   | 40/67 [00:11<00:07,  3.47it/s, loss=2.33, v_num=1, train_loss_step=1.380, val_loss=3.090, train_loss_epoch=1.740]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 75:  61%|████▉   | 41/67 [00:12<00:07,  3.41it/s, loss=2.33, v_num=1, train_loss_step=1.380, val_loss=3.090, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 75:  63%|█████   | 42/67 [00:12<00:07,  3.39it/s, loss=2.33, v_num=1, train_loss_step=1.380, val_loss=3.090, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 75:  64%|█████▏  | 43/67 [00:12<00:07,  3.37it/s, loss=2.33, v_num=1, train_loss_step=1.380, val_loss=3.090, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 75:  66%|█████▎  | 44/67 [00:13<00:06,  3.36it/s, loss=2.33, v_num=1, train_loss_step=1.380, val_loss=3.090, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 75:  67%|█████▎  | 45/67 [00:13<00:06,  3.35it/s, loss=2.33, v_num=1, train_loss_step=1.380, val_loss=3.090, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 75:  69%|█████▍  | 46/67 [00:13<00:06,  3.34it/s, loss=2.33, v_num=1, train_loss_step=1.380, val_loss=3.090, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 75:  70%|█████▌  | 47/67 [00:14<00:05,  3.34it/s, loss=2.33, v_num=1, train_loss_step=1.380, val_loss=3.090, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 75:  72%|█████▋  | 48/67 [00:14<00:05,  3.33it/s, loss=2.33, v_num=1, train_loss_step=1.380, val_loss=3.090, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 75:  73%|█████▊  | 49/67 [00:14<00:05,  3.32it/s, loss=2.33, v_num=1, train_loss_step=1.380, val_loss=3.090, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 75:  75%|█████▉  | 50/67 [00:15<00:05,  3.31it/s, loss=2.33, v_num=1, train_loss_step=1.380, val_loss=3.090, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 75:  76%|██████  | 51/67 [00:15<00:04,  3.28it/s, loss=2.33, v_num=1, train_loss_step=1.380, val_loss=3.090, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 75:  78%|██████▏ | 52/67 [00:15<00:04,  3.27it/s, loss=2.33, v_num=1, train_loss_step=1.380, val_loss=3.090, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 75:  79%|██████▎ | 53/67 [00:16<00:04,  3.26it/s, loss=2.33, v_num=1, train_loss_step=1.380, val_loss=3.090, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 75:  81%|██████▍ | 54/67 [00:16<00:03,  3.26it/s, loss=2.33, v_num=1, train_loss_step=1.380, val_loss=3.090, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 75:  82%|██████▌ | 55/67 [00:16<00:03,  3.25it/s, loss=2.33, v_num=1, train_loss_step=1.380, val_loss=3.090, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 75:  84%|██████▋ | 56/67 [00:17<00:03,  3.25it/s, loss=2.33, v_num=1, train_loss_step=1.380, val_loss=3.090, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 75:  85%|██████▊ | 57/67 [00:17<00:03,  3.24it/s, loss=2.33, v_num=1, train_loss_step=1.380, val_loss=3.090, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 75:  87%|██████▉ | 58/67 [00:17<00:02,  3.24it/s, loss=2.33, v_num=1, train_loss_step=1.380, val_loss=3.090, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 75:  88%|███████ | 59/67 [00:18<00:02,  3.23it/s, loss=2.33, v_num=1, train_loss_step=1.380, val_loss=3.090, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 75:  90%|███████▏| 60/67 [00:18<00:02,  3.23it/s, loss=2.33, v_num=1, train_loss_step=1.380, val_loss=3.090, train_loss_epoch=1.740]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 75:  91%|███████▎| 61/67 [00:19<00:01,  3.21it/s, loss=2.33, v_num=1, train_loss_step=1.380, val_loss=3.090, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 75:  93%|███████▍| 62/67 [00:19<00:01,  3.20it/s, loss=2.33, v_num=1, train_loss_step=1.380, val_loss=3.090, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 75:  94%|███████▌| 63/67 [00:19<00:01,  3.20it/s, loss=2.33, v_num=1, train_loss_step=1.380, val_loss=3.090, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 75:  96%|███████▋| 64/67 [00:20<00:00,  3.19it/s, loss=2.33, v_num=1, train_loss_step=1.380, val_loss=3.090, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 75:  97%|███████▊| 65/67 [00:20<00:00,  3.19it/s, loss=2.33, v_num=1, train_loss_step=1.380, val_loss=3.090, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 75:  99%|███████▉| 66/67 [00:20<00:00,  3.19it/s, loss=2.33, v_num=1, train_loss_step=1.380, val_loss=3.090, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 75: 100%|████████| 67/67 [00:21<00:00,  3.12it/s, loss=2.33, v_num=1, train_loss_step=1.380, val_loss=3.090, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 76:  45%|███▌    | 30/67 [00:06<00:07,  4.90it/s, loss=2.02, v_num=1, train_loss_step=2.320, val_loss=3.090, train_loss_epoch=2.180]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 76:  46%|███▋    | 31/67 [00:08<00:09,  3.78it/s, loss=2.02, v_num=1, train_loss_step=2.320, val_loss=3.090, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 76:  48%|███▊    | 32/67 [00:08<00:09,  3.75it/s, loss=2.02, v_num=1, train_loss_step=2.320, val_loss=3.090, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 76:  49%|███▉    | 33/67 [00:08<00:09,  3.72it/s, loss=2.02, v_num=1, train_loss_step=2.320, val_loss=3.090, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 76:  51%|████    | 34/67 [00:09<00:08,  3.69it/s, loss=2.02, v_num=1, train_loss_step=2.320, val_loss=3.090, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 76:  52%|████▏   | 35/67 [00:09<00:08,  3.67it/s, loss=2.02, v_num=1, train_loss_step=2.320, val_loss=3.090, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 76:  54%|████▎   | 36/67 [00:09<00:08,  3.64it/s, loss=2.02, v_num=1, train_loss_step=2.320, val_loss=3.090, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 76:  55%|████▍   | 37/67 [00:10<00:08,  3.62it/s, loss=2.02, v_num=1, train_loss_step=2.320, val_loss=3.090, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 76:  57%|████▌   | 38/67 [00:10<00:08,  3.60it/s, loss=2.02, v_num=1, train_loss_step=2.320, val_loss=3.090, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 76:  58%|████▋   | 39/67 [00:10<00:07,  3.58it/s, loss=2.02, v_num=1, train_loss_step=2.320, val_loss=3.090, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 76:  60%|████▊   | 40/67 [00:11<00:07,  3.56it/s, loss=2.02, v_num=1, train_loss_step=2.320, val_loss=3.090, train_loss_epoch=2.180]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 76:  61%|████▉   | 41/67 [00:11<00:07,  3.50it/s, loss=2.02, v_num=1, train_loss_step=2.320, val_loss=3.090, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 76:  63%|█████   | 42/67 [00:12<00:07,  3.48it/s, loss=2.02, v_num=1, train_loss_step=2.320, val_loss=3.090, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 76:  64%|█████▏  | 43/67 [00:12<00:06,  3.46it/s, loss=2.02, v_num=1, train_loss_step=2.320, val_loss=3.090, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 76:  66%|█████▎  | 44/67 [00:12<00:06,  3.45it/s, loss=2.02, v_num=1, train_loss_step=2.320, val_loss=3.090, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 76:  67%|█████▎  | 45/67 [00:13<00:06,  3.43it/s, loss=2.02, v_num=1, train_loss_step=2.320, val_loss=3.090, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 76:  69%|█████▍  | 46/67 [00:13<00:06,  3.42it/s, loss=2.02, v_num=1, train_loss_step=2.320, val_loss=3.090, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 76:  70%|█████▌  | 47/67 [00:13<00:05,  3.41it/s, loss=2.02, v_num=1, train_loss_step=2.320, val_loss=3.090, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 76:  72%|█████▋  | 48/67 [00:14<00:05,  3.40it/s, loss=2.02, v_num=1, train_loss_step=2.320, val_loss=3.090, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 76:  73%|█████▊  | 49/67 [00:14<00:05,  3.39it/s, loss=2.02, v_num=1, train_loss_step=2.320, val_loss=3.090, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 76:  75%|█████▉  | 50/67 [00:14<00:05,  3.38it/s, loss=2.02, v_num=1, train_loss_step=2.320, val_loss=3.090, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 76:  76%|██████  | 51/67 [00:15<00:04,  3.34it/s, loss=2.02, v_num=1, train_loss_step=2.320, val_loss=3.090, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 76:  78%|██████▏ | 52/67 [00:15<00:04,  3.33it/s, loss=2.02, v_num=1, train_loss_step=2.320, val_loss=3.090, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 76:  79%|██████▎ | 53/67 [00:15<00:04,  3.32it/s, loss=2.02, v_num=1, train_loss_step=2.320, val_loss=3.090, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 76:  81%|██████▍ | 54/67 [00:16<00:03,  3.32it/s, loss=2.02, v_num=1, train_loss_step=2.320, val_loss=3.090, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 76:  82%|██████▌ | 55/67 [00:16<00:03,  3.31it/s, loss=2.02, v_num=1, train_loss_step=2.320, val_loss=3.090, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 76:  84%|██████▋ | 56/67 [00:16<00:03,  3.31it/s, loss=2.02, v_num=1, train_loss_step=2.320, val_loss=3.090, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 76:  85%|██████▊ | 57/67 [00:17<00:03,  3.30it/s, loss=2.02, v_num=1, train_loss_step=2.320, val_loss=3.090, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 76:  87%|██████▉ | 58/67 [00:17<00:02,  3.29it/s, loss=2.02, v_num=1, train_loss_step=2.320, val_loss=3.090, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 76:  88%|███████ | 59/67 [00:17<00:02,  3.29it/s, loss=2.02, v_num=1, train_loss_step=2.320, val_loss=3.090, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 76:  90%|███████▏| 60/67 [00:18<00:02,  3.28it/s, loss=2.02, v_num=1, train_loss_step=2.320, val_loss=3.090, train_loss_epoch=2.180]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 76:  91%|███████▎| 61/67 [00:18<00:01,  3.25it/s, loss=2.02, v_num=1, train_loss_step=2.320, val_loss=3.090, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 76:  93%|███████▍| 62/67 [00:19<00:01,  3.25it/s, loss=2.02, v_num=1, train_loss_step=2.320, val_loss=3.090, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 76:  94%|███████▌| 63/67 [00:19<00:01,  3.24it/s, loss=2.02, v_num=1, train_loss_step=2.320, val_loss=3.090, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 76:  96%|███████▋| 64/67 [00:19<00:00,  3.24it/s, loss=2.02, v_num=1, train_loss_step=2.320, val_loss=3.090, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 76:  97%|███████▊| 65/67 [00:20<00:00,  3.24it/s, loss=2.02, v_num=1, train_loss_step=2.320, val_loss=3.090, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 76:  99%|███████▉| 66/67 [00:20<00:00,  3.23it/s, loss=2.02, v_num=1, train_loss_step=2.320, val_loss=3.090, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 76: 100%|████████| 67/67 [00:21<00:00,  3.06it/s, loss=2.02, v_num=1, train_loss_step=2.320, val_loss=3.080, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 77:  45%|███▌    | 30/67 [00:05<00:07,  5.13it/s, loss=5.09, v_num=1, train_loss_step=1.760, val_loss=3.080, train_loss_epoch=2.010]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 77:  46%|███▋    | 31/67 [00:07<00:08,  4.02it/s, loss=5.09, v_num=1, train_loss_step=1.760, val_loss=3.080, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 77:  48%|███▊    | 32/67 [00:08<00:08,  3.98it/s, loss=5.09, v_num=1, train_loss_step=1.760, val_loss=3.080, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 77:  49%|███▉    | 33/67 [00:08<00:08,  3.94it/s, loss=5.09, v_num=1, train_loss_step=1.760, val_loss=3.080, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 77:  51%|████    | 34/67 [00:08<00:08,  3.90it/s, loss=5.09, v_num=1, train_loss_step=1.760, val_loss=3.080, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 77:  52%|████▏   | 35/67 [00:09<00:08,  3.87it/s, loss=5.09, v_num=1, train_loss_step=1.760, val_loss=3.080, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 77:  54%|████▎   | 36/67 [00:09<00:08,  3.84it/s, loss=5.09, v_num=1, train_loss_step=1.760, val_loss=3.080, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 77:  55%|████▍   | 37/67 [00:09<00:07,  3.82it/s, loss=5.09, v_num=1, train_loss_step=1.760, val_loss=3.080, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 77:  57%|████▌   | 38/67 [00:10<00:07,  3.79it/s, loss=5.09, v_num=1, train_loss_step=1.760, val_loss=3.080, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 77:  58%|████▋   | 39/67 [00:10<00:07,  3.77it/s, loss=5.09, v_num=1, train_loss_step=1.760, val_loss=3.080, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 77:  60%|████▊   | 40/67 [00:10<00:07,  3.74it/s, loss=5.09, v_num=1, train_loss_step=1.760, val_loss=3.080, train_loss_epoch=2.010]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 77:  61%|████▉   | 41/67 [00:11<00:07,  3.68it/s, loss=5.09, v_num=1, train_loss_step=1.760, val_loss=3.080, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 77:  63%|█████   | 42/67 [00:11<00:06,  3.66it/s, loss=5.09, v_num=1, train_loss_step=1.760, val_loss=3.080, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 77:  64%|█████▏  | 43/67 [00:11<00:06,  3.64it/s, loss=5.09, v_num=1, train_loss_step=1.760, val_loss=3.080, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 77:  66%|█████▎  | 44/67 [00:12<00:06,  3.63it/s, loss=5.09, v_num=1, train_loss_step=1.760, val_loss=3.080, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 77:  67%|█████▎  | 45/67 [00:12<00:06,  3.61it/s, loss=5.09, v_num=1, train_loss_step=1.760, val_loss=3.080, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 77:  69%|█████▍  | 46/67 [00:12<00:05,  3.60it/s, loss=5.09, v_num=1, train_loss_step=1.760, val_loss=3.080, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 77:  70%|█████▌  | 47/67 [00:13<00:05,  3.58it/s, loss=5.09, v_num=1, train_loss_step=1.760, val_loss=3.080, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 77:  72%|█████▋  | 48/67 [00:13<00:05,  3.57it/s, loss=5.09, v_num=1, train_loss_step=1.760, val_loss=3.080, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 77:  73%|█████▊  | 49/67 [00:13<00:05,  3.55it/s, loss=5.09, v_num=1, train_loss_step=1.760, val_loss=3.080, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 77:  75%|█████▉  | 50/67 [00:14<00:04,  3.54it/s, loss=5.09, v_num=1, train_loss_step=1.760, val_loss=3.080, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 77:  76%|██████  | 51/67 [00:14<00:04,  3.50it/s, loss=5.09, v_num=1, train_loss_step=1.760, val_loss=3.080, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 77:  78%|██████▏ | 52/67 [00:14<00:04,  3.49it/s, loss=5.09, v_num=1, train_loss_step=1.760, val_loss=3.080, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 77:  79%|██████▎ | 53/67 [00:15<00:04,  3.48it/s, loss=5.09, v_num=1, train_loss_step=1.760, val_loss=3.080, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 77:  81%|██████▍ | 54/67 [00:15<00:03,  3.47it/s, loss=5.09, v_num=1, train_loss_step=1.760, val_loss=3.080, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 77:  82%|██████▌ | 55/67 [00:15<00:03,  3.46it/s, loss=5.09, v_num=1, train_loss_step=1.760, val_loss=3.080, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 77:  84%|██████▋ | 56/67 [00:16<00:03,  3.45it/s, loss=5.09, v_num=1, train_loss_step=1.760, val_loss=3.080, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 77:  85%|██████▊ | 57/67 [00:16<00:02,  3.44it/s, loss=5.09, v_num=1, train_loss_step=1.760, val_loss=3.080, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 77:  87%|██████▉ | 58/67 [00:18<00:02,  3.14it/s, loss=5.09, v_num=1, train_loss_step=1.760, val_loss=3.080, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 77:  88%|███████ | 59/67 [00:18<00:02,  3.14it/s, loss=5.09, v_num=1, train_loss_step=1.760, val_loss=3.080, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 77:  90%|███████▏| 60/67 [00:19<00:02,  3.13it/s, loss=5.09, v_num=1, train_loss_step=1.760, val_loss=3.080, train_loss_epoch=2.010]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 77:  91%|███████▎| 61/67 [00:19<00:01,  3.11it/s, loss=5.09, v_num=1, train_loss_step=1.760, val_loss=3.080, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 77:  93%|███████▍| 62/67 [00:19<00:01,  3.11it/s, loss=5.09, v_num=1, train_loss_step=1.760, val_loss=3.080, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 77:  94%|███████▌| 63/67 [00:20<00:01,  3.11it/s, loss=5.09, v_num=1, train_loss_step=1.760, val_loss=3.080, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 77:  96%|███████▋| 64/67 [00:20<00:00,  3.11it/s, loss=5.09, v_num=1, train_loss_step=1.760, val_loss=3.080, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 77:  97%|███████▊| 65/67 [00:20<00:00,  3.11it/s, loss=5.09, v_num=1, train_loss_step=1.760, val_loss=3.080, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 77:  99%|███████▉| 66/67 [00:21<00:00,  3.11it/s, loss=5.09, v_num=1, train_loss_step=1.760, val_loss=3.080, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 77: 100%|████████| 67/67 [00:21<00:00,  3.06it/s, loss=5.09, v_num=1, train_loss_step=1.760, val_loss=3.080, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 78:  45%|████     | 30/67 [00:05<00:07,  5.11it/s, loss=1.8, v_num=1, train_loss_step=3.030, val_loss=3.080, train_loss_epoch=4.510]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 78:  46%|████▏    | 31/67 [00:07<00:09,  3.97it/s, loss=1.8, v_num=1, train_loss_step=3.030, val_loss=3.080, train_loss_epoch=4.510]\u001b[A\n",
      "Epoch 78:  48%|████▎    | 32/67 [00:08<00:08,  3.92it/s, loss=1.8, v_num=1, train_loss_step=3.030, val_loss=3.080, train_loss_epoch=4.510]\u001b[A\n",
      "Epoch 78:  49%|████▍    | 33/67 [00:08<00:08,  3.89it/s, loss=1.8, v_num=1, train_loss_step=3.030, val_loss=3.080, train_loss_epoch=4.510]\u001b[A\n",
      "Epoch 78:  51%|████▌    | 34/67 [00:08<00:08,  3.85it/s, loss=1.8, v_num=1, train_loss_step=3.030, val_loss=3.080, train_loss_epoch=4.510]\u001b[A\n",
      "Epoch 78:  52%|████▋    | 35/67 [00:09<00:08,  3.82it/s, loss=1.8, v_num=1, train_loss_step=3.030, val_loss=3.080, train_loss_epoch=4.510]\u001b[A\n",
      "Epoch 78:  54%|████▊    | 36/67 [00:09<00:08,  3.79it/s, loss=1.8, v_num=1, train_loss_step=3.030, val_loss=3.080, train_loss_epoch=4.510]\u001b[A\n",
      "Epoch 78:  55%|████▉    | 37/67 [00:09<00:07,  3.77it/s, loss=1.8, v_num=1, train_loss_step=3.030, val_loss=3.080, train_loss_epoch=4.510]\u001b[A\n",
      "Epoch 78:  57%|█████    | 38/67 [00:10<00:07,  3.74it/s, loss=1.8, v_num=1, train_loss_step=3.030, val_loss=3.080, train_loss_epoch=4.510]\u001b[A\n",
      "Epoch 78:  58%|█████▏   | 39/67 [00:10<00:07,  3.72it/s, loss=1.8, v_num=1, train_loss_step=3.030, val_loss=3.080, train_loss_epoch=4.510]\u001b[A\n",
      "Epoch 78:  60%|█████▎   | 40/67 [00:10<00:07,  3.70it/s, loss=1.8, v_num=1, train_loss_step=3.030, val_loss=3.080, train_loss_epoch=4.510]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 78:  61%|█████▌   | 41/67 [00:11<00:07,  3.64it/s, loss=1.8, v_num=1, train_loss_step=3.030, val_loss=3.080, train_loss_epoch=4.510]\u001b[A\n",
      "Epoch 78:  63%|█████▋   | 42/67 [00:11<00:06,  3.62it/s, loss=1.8, v_num=1, train_loss_step=3.030, val_loss=3.080, train_loss_epoch=4.510]\u001b[A\n",
      "Epoch 78:  64%|█████▊   | 43/67 [00:11<00:06,  3.61it/s, loss=1.8, v_num=1, train_loss_step=3.030, val_loss=3.080, train_loss_epoch=4.510]\u001b[A\n",
      "Epoch 78:  66%|█████▉   | 44/67 [00:12<00:06,  3.59it/s, loss=1.8, v_num=1, train_loss_step=3.030, val_loss=3.080, train_loss_epoch=4.510]\u001b[A\n",
      "Epoch 78:  67%|██████   | 45/67 [00:12<00:06,  3.58it/s, loss=1.8, v_num=1, train_loss_step=3.030, val_loss=3.080, train_loss_epoch=4.510]\u001b[A\n",
      "Epoch 78:  69%|██████▏  | 46/67 [00:12<00:05,  3.56it/s, loss=1.8, v_num=1, train_loss_step=3.030, val_loss=3.080, train_loss_epoch=4.510]\u001b[A\n",
      "Epoch 78:  70%|██████▎  | 47/67 [00:13<00:05,  3.54it/s, loss=1.8, v_num=1, train_loss_step=3.030, val_loss=3.080, train_loss_epoch=4.510]\u001b[A\n",
      "Epoch 78:  72%|██████▍  | 48/67 [00:13<00:05,  3.53it/s, loss=1.8, v_num=1, train_loss_step=3.030, val_loss=3.080, train_loss_epoch=4.510]\u001b[A\n",
      "Epoch 78:  73%|██████▌  | 49/67 [00:15<00:05,  3.20it/s, loss=1.8, v_num=1, train_loss_step=3.030, val_loss=3.080, train_loss_epoch=4.510]\u001b[A\n",
      "Epoch 78:  75%|██████▋  | 50/67 [00:15<00:05,  3.20it/s, loss=1.8, v_num=1, train_loss_step=3.030, val_loss=3.080, train_loss_epoch=4.510]\u001b[A\n",
      "Epoch 78:  76%|██████▊  | 51/67 [00:16<00:05,  3.17it/s, loss=1.8, v_num=1, train_loss_step=3.030, val_loss=3.080, train_loss_epoch=4.510]\u001b[A\n",
      "Epoch 78:  78%|██████▉  | 52/67 [00:16<00:04,  3.17it/s, loss=1.8, v_num=1, train_loss_step=3.030, val_loss=3.080, train_loss_epoch=4.510]\u001b[A\n",
      "Epoch 78:  79%|███████  | 53/67 [00:16<00:04,  3.17it/s, loss=1.8, v_num=1, train_loss_step=3.030, val_loss=3.080, train_loss_epoch=4.510]\u001b[A\n",
      "Epoch 78:  81%|███████▎ | 54/67 [00:17<00:04,  3.16it/s, loss=1.8, v_num=1, train_loss_step=3.030, val_loss=3.080, train_loss_epoch=4.510]\u001b[A\n",
      "Epoch 78:  82%|███████▍ | 55/67 [00:17<00:03,  3.16it/s, loss=1.8, v_num=1, train_loss_step=3.030, val_loss=3.080, train_loss_epoch=4.510]\u001b[A\n",
      "Epoch 78:  84%|███████▌ | 56/67 [00:17<00:03,  3.16it/s, loss=1.8, v_num=1, train_loss_step=3.030, val_loss=3.080, train_loss_epoch=4.510]\u001b[A\n",
      "Epoch 78:  85%|███████▋ | 57/67 [00:18<00:03,  3.16it/s, loss=1.8, v_num=1, train_loss_step=3.030, val_loss=3.080, train_loss_epoch=4.510]\u001b[A\n",
      "Epoch 78:  87%|███████▊ | 58/67 [00:18<00:02,  3.15it/s, loss=1.8, v_num=1, train_loss_step=3.030, val_loss=3.080, train_loss_epoch=4.510]\u001b[A\n",
      "Epoch 78:  88%|███████▉ | 59/67 [00:18<00:02,  3.14it/s, loss=1.8, v_num=1, train_loss_step=3.030, val_loss=3.080, train_loss_epoch=4.510]\u001b[A\n",
      "Epoch 78:  90%|████████ | 60/67 [00:19<00:02,  3.14it/s, loss=1.8, v_num=1, train_loss_step=3.030, val_loss=3.080, train_loss_epoch=4.510]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 78:  91%|████████▏| 61/67 [00:19<00:01,  3.10it/s, loss=1.8, v_num=1, train_loss_step=3.030, val_loss=3.080, train_loss_epoch=4.510]\u001b[A\n",
      "Epoch 78:  93%|████████▎| 62/67 [00:19<00:01,  3.10it/s, loss=1.8, v_num=1, train_loss_step=3.030, val_loss=3.080, train_loss_epoch=4.510]\u001b[A\n",
      "Epoch 78:  94%|████████▍| 63/67 [00:20<00:01,  3.10it/s, loss=1.8, v_num=1, train_loss_step=3.030, val_loss=3.080, train_loss_epoch=4.510]\u001b[A\n",
      "Epoch 78:  96%|████████▌| 64/67 [00:20<00:00,  3.10it/s, loss=1.8, v_num=1, train_loss_step=3.030, val_loss=3.080, train_loss_epoch=4.510]\u001b[A\n",
      "Epoch 78:  97%|████████▋| 65/67 [00:20<00:00,  3.10it/s, loss=1.8, v_num=1, train_loss_step=3.030, val_loss=3.080, train_loss_epoch=4.510]\u001b[A\n",
      "Epoch 78:  99%|████████▊| 66/67 [00:21<00:00,  3.10it/s, loss=1.8, v_num=1, train_loss_step=3.030, val_loss=3.080, train_loss_epoch=4.510]\u001b[A\n",
      "Epoch 78: 100%|█████████| 67/67 [00:21<00:00,  3.05it/s, loss=1.8, v_num=1, train_loss_step=3.030, val_loss=3.080, train_loss_epoch=4.510]\u001b[A\n",
      "Epoch 79:  45%|███▌    | 30/67 [00:05<00:07,  5.20it/s, loss=2.69, v_num=1, train_loss_step=1.520, val_loss=3.080, train_loss_epoch=1.940]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 79:  46%|███▋    | 31/67 [00:07<00:08,  4.01it/s, loss=2.69, v_num=1, train_loss_step=1.520, val_loss=3.080, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 79:  48%|███▊    | 32/67 [00:08<00:08,  3.97it/s, loss=2.69, v_num=1, train_loss_step=1.520, val_loss=3.080, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 79:  49%|███▉    | 33/67 [00:08<00:08,  3.93it/s, loss=2.69, v_num=1, train_loss_step=1.520, val_loss=3.080, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 79:  51%|████    | 34/67 [00:08<00:08,  3.90it/s, loss=2.69, v_num=1, train_loss_step=1.520, val_loss=3.080, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 79:  52%|████▏   | 35/67 [00:09<00:08,  3.87it/s, loss=2.69, v_num=1, train_loss_step=1.520, val_loss=3.080, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 79:  54%|████▎   | 36/67 [00:09<00:08,  3.83it/s, loss=2.69, v_num=1, train_loss_step=1.520, val_loss=3.080, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 79:  55%|████▍   | 37/67 [00:11<00:08,  3.35it/s, loss=2.69, v_num=1, train_loss_step=1.520, val_loss=3.080, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 79:  57%|████▌   | 38/67 [00:11<00:08,  3.35it/s, loss=2.69, v_num=1, train_loss_step=1.520, val_loss=3.080, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 79:  58%|████▋   | 39/67 [00:11<00:08,  3.34it/s, loss=2.69, v_num=1, train_loss_step=1.520, val_loss=3.080, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 79:  60%|████▊   | 40/67 [00:12<00:08,  3.33it/s, loss=2.69, v_num=1, train_loss_step=1.520, val_loss=3.080, train_loss_epoch=1.940]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 79:  61%|████▉   | 41/67 [00:12<00:07,  3.29it/s, loss=2.69, v_num=1, train_loss_step=1.520, val_loss=3.080, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 79:  63%|█████   | 42/67 [00:12<00:07,  3.29it/s, loss=2.69, v_num=1, train_loss_step=1.520, val_loss=3.080, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 79:  64%|█████▏  | 43/67 [00:13<00:07,  3.28it/s, loss=2.69, v_num=1, train_loss_step=1.520, val_loss=3.080, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 79:  66%|█████▎  | 44/67 [00:13<00:07,  3.28it/s, loss=2.69, v_num=1, train_loss_step=1.520, val_loss=3.080, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 79:  67%|█████▎  | 45/67 [00:13<00:06,  3.27it/s, loss=2.69, v_num=1, train_loss_step=1.520, val_loss=3.080, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 79:  69%|█████▍  | 46/67 [00:14<00:06,  3.26it/s, loss=2.69, v_num=1, train_loss_step=1.520, val_loss=3.080, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 79:  70%|█████▌  | 47/67 [00:14<00:06,  3.26it/s, loss=2.69, v_num=1, train_loss_step=1.520, val_loss=3.080, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 79:  72%|█████▋  | 48/67 [00:14<00:05,  3.25it/s, loss=2.69, v_num=1, train_loss_step=1.520, val_loss=3.080, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 79:  73%|█████▊  | 49/67 [00:15<00:05,  3.23it/s, loss=2.69, v_num=1, train_loss_step=1.520, val_loss=3.080, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 79:  75%|█████▉  | 50/67 [00:15<00:05,  3.23it/s, loss=2.69, v_num=1, train_loss_step=1.520, val_loss=3.080, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 79:  76%|██████  | 51/67 [00:15<00:05,  3.20it/s, loss=2.69, v_num=1, train_loss_step=1.520, val_loss=3.080, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 79:  78%|██████▏ | 52/67 [00:16<00:04,  3.20it/s, loss=2.69, v_num=1, train_loss_step=1.520, val_loss=3.080, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 79:  79%|██████▎ | 53/67 [00:16<00:04,  3.19it/s, loss=2.69, v_num=1, train_loss_step=1.520, val_loss=3.080, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 79:  81%|██████▍ | 54/67 [00:16<00:04,  3.19it/s, loss=2.69, v_num=1, train_loss_step=1.520, val_loss=3.080, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 79:  82%|██████▌ | 55/67 [00:17<00:03,  3.19it/s, loss=2.69, v_num=1, train_loss_step=1.520, val_loss=3.080, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 79:  84%|██████▋ | 56/67 [00:17<00:03,  3.19it/s, loss=2.69, v_num=1, train_loss_step=1.520, val_loss=3.080, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 79:  85%|██████▊ | 57/67 [00:17<00:03,  3.18it/s, loss=2.69, v_num=1, train_loss_step=1.520, val_loss=3.080, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 79:  87%|██████▉ | 58/67 [00:18<00:02,  3.18it/s, loss=2.69, v_num=1, train_loss_step=1.520, val_loss=3.080, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 79:  88%|███████ | 59/67 [00:18<00:02,  3.18it/s, loss=2.69, v_num=1, train_loss_step=1.520, val_loss=3.080, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 79:  90%|███████▏| 60/67 [00:18<00:02,  3.17it/s, loss=2.69, v_num=1, train_loss_step=1.520, val_loss=3.080, train_loss_epoch=1.940]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 79:  91%|███████▎| 61/67 [00:19<00:01,  3.15it/s, loss=2.69, v_num=1, train_loss_step=1.520, val_loss=3.080, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 79:  93%|███████▍| 62/67 [00:19<00:01,  3.15it/s, loss=2.69, v_num=1, train_loss_step=1.520, val_loss=3.080, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 79:  94%|███████▌| 63/67 [00:20<00:01,  3.14it/s, loss=2.69, v_num=1, train_loss_step=1.520, val_loss=3.080, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 79:  96%|███████▋| 64/67 [00:20<00:00,  3.14it/s, loss=2.69, v_num=1, train_loss_step=1.520, val_loss=3.080, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 79:  97%|███████▊| 65/67 [00:20<00:00,  3.14it/s, loss=2.69, v_num=1, train_loss_step=1.520, val_loss=3.080, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 79:  99%|███████▉| 66/67 [00:20<00:00,  3.14it/s, loss=2.69, v_num=1, train_loss_step=1.520, val_loss=3.080, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 79: 100%|████████| 67/67 [00:21<00:00,  3.09it/s, loss=2.69, v_num=1, train_loss_step=1.520, val_loss=3.080, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 80:  45%|███▌    | 30/67 [00:06<00:07,  4.98it/s, loss=2.48, v_num=1, train_loss_step=4.220, val_loss=3.080, train_loss_epoch=2.790]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 80:  46%|███▋    | 31/67 [00:08<00:09,  3.86it/s, loss=2.48, v_num=1, train_loss_step=4.220, val_loss=3.080, train_loss_epoch=2.790]\u001b[A\n",
      "Epoch 80:  48%|███▊    | 32/67 [00:08<00:09,  3.83it/s, loss=2.48, v_num=1, train_loss_step=4.220, val_loss=3.080, train_loss_epoch=2.790]\u001b[A\n",
      "Epoch 80:  49%|███▉    | 33/67 [00:08<00:08,  3.80it/s, loss=2.48, v_num=1, train_loss_step=4.220, val_loss=3.080, train_loss_epoch=2.790]\u001b[A\n",
      "Epoch 80:  51%|████    | 34/67 [00:09<00:08,  3.77it/s, loss=2.48, v_num=1, train_loss_step=4.220, val_loss=3.080, train_loss_epoch=2.790]\u001b[A\n",
      "Epoch 80:  52%|████▏   | 35/67 [00:09<00:08,  3.75it/s, loss=2.48, v_num=1, train_loss_step=4.220, val_loss=3.080, train_loss_epoch=2.790]\u001b[A\n",
      "Epoch 80:  54%|████▎   | 36/67 [00:09<00:08,  3.72it/s, loss=2.48, v_num=1, train_loss_step=4.220, val_loss=3.080, train_loss_epoch=2.790]\u001b[A\n",
      "Epoch 80:  55%|████▍   | 37/67 [00:09<00:08,  3.70it/s, loss=2.48, v_num=1, train_loss_step=4.220, val_loss=3.080, train_loss_epoch=2.790]\u001b[A\n",
      "Epoch 80:  57%|████▌   | 38/67 [00:10<00:07,  3.68it/s, loss=2.48, v_num=1, train_loss_step=4.220, val_loss=3.080, train_loss_epoch=2.790]\u001b[A\n",
      "Epoch 80:  58%|████▋   | 39/67 [00:10<00:07,  3.66it/s, loss=2.48, v_num=1, train_loss_step=4.220, val_loss=3.080, train_loss_epoch=2.790]\u001b[A\n",
      "Epoch 80:  60%|████▊   | 40/67 [00:10<00:07,  3.64it/s, loss=2.48, v_num=1, train_loss_step=4.220, val_loss=3.080, train_loss_epoch=2.790]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 80:  61%|████▉   | 41/67 [00:11<00:07,  3.59it/s, loss=2.48, v_num=1, train_loss_step=4.220, val_loss=3.080, train_loss_epoch=2.790]\u001b[A\n",
      "Epoch 80:  63%|█████   | 42/67 [00:11<00:06,  3.57it/s, loss=2.48, v_num=1, train_loss_step=4.220, val_loss=3.080, train_loss_epoch=2.790]\u001b[A\n",
      "Epoch 80:  64%|█████▏  | 43/67 [00:12<00:06,  3.56it/s, loss=2.48, v_num=1, train_loss_step=4.220, val_loss=3.080, train_loss_epoch=2.790]\u001b[A\n",
      "Epoch 80:  66%|█████▎  | 44/67 [00:12<00:06,  3.55it/s, loss=2.48, v_num=1, train_loss_step=4.220, val_loss=3.080, train_loss_epoch=2.790]\u001b[A\n",
      "Epoch 80:  67%|█████▎  | 45/67 [00:12<00:06,  3.54it/s, loss=2.48, v_num=1, train_loss_step=4.220, val_loss=3.080, train_loss_epoch=2.790]\u001b[A\n",
      "Epoch 80:  69%|█████▍  | 46/67 [00:13<00:05,  3.52it/s, loss=2.48, v_num=1, train_loss_step=4.220, val_loss=3.080, train_loss_epoch=2.790]\u001b[A\n",
      "Epoch 80:  70%|█████▌  | 47/67 [00:13<00:05,  3.51it/s, loss=2.48, v_num=1, train_loss_step=4.220, val_loss=3.080, train_loss_epoch=2.790]\u001b[A\n",
      "Epoch 80:  72%|█████▋  | 48/67 [00:13<00:05,  3.50it/s, loss=2.48, v_num=1, train_loss_step=4.220, val_loss=3.080, train_loss_epoch=2.790]\u001b[A\n",
      "Epoch 80:  73%|█████▊  | 49/67 [00:14<00:05,  3.49it/s, loss=2.48, v_num=1, train_loss_step=4.220, val_loss=3.080, train_loss_epoch=2.790]\u001b[A\n",
      "Epoch 80:  75%|█████▉  | 50/67 [00:14<00:04,  3.48it/s, loss=2.48, v_num=1, train_loss_step=4.220, val_loss=3.080, train_loss_epoch=2.790]\u001b[A\n",
      "Epoch 80:  76%|██████  | 51/67 [00:14<00:04,  3.44it/s, loss=2.48, v_num=1, train_loss_step=4.220, val_loss=3.080, train_loss_epoch=2.790]\u001b[A\n",
      "Epoch 80:  78%|██████▏ | 52/67 [00:15<00:04,  3.43it/s, loss=2.48, v_num=1, train_loss_step=4.220, val_loss=3.080, train_loss_epoch=2.790]\u001b[A\n",
      "Epoch 80:  79%|██████▎ | 53/67 [00:15<00:04,  3.42it/s, loss=2.48, v_num=1, train_loss_step=4.220, val_loss=3.080, train_loss_epoch=2.790]\u001b[A\n",
      "Epoch 80:  81%|██████▍ | 54/67 [00:15<00:03,  3.41it/s, loss=2.48, v_num=1, train_loss_step=4.220, val_loss=3.080, train_loss_epoch=2.790]\u001b[A\n",
      "Epoch 80:  82%|██████▌ | 55/67 [00:16<00:03,  3.40it/s, loss=2.48, v_num=1, train_loss_step=4.220, val_loss=3.080, train_loss_epoch=2.790]\u001b[A\n",
      "Epoch 80:  84%|██████▋ | 56/67 [00:16<00:03,  3.40it/s, loss=2.48, v_num=1, train_loss_step=4.220, val_loss=3.080, train_loss_epoch=2.790]\u001b[A\n",
      "Epoch 80:  85%|██████▊ | 57/67 [00:16<00:02,  3.39it/s, loss=2.48, v_num=1, train_loss_step=4.220, val_loss=3.080, train_loss_epoch=2.790]\u001b[A\n",
      "Epoch 80:  87%|██████▉ | 58/67 [00:17<00:02,  3.38it/s, loss=2.48, v_num=1, train_loss_step=4.220, val_loss=3.080, train_loss_epoch=2.790]\u001b[A\n",
      "Epoch 80:  88%|███████ | 59/67 [00:17<00:02,  3.37it/s, loss=2.48, v_num=1, train_loss_step=4.220, val_loss=3.080, train_loss_epoch=2.790]\u001b[A\n",
      "Epoch 80:  90%|███████▏| 60/67 [00:17<00:02,  3.37it/s, loss=2.48, v_num=1, train_loss_step=4.220, val_loss=3.080, train_loss_epoch=2.790]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 80:  91%|███████▎| 61/67 [00:18<00:01,  3.34it/s, loss=2.48, v_num=1, train_loss_step=4.220, val_loss=3.080, train_loss_epoch=2.790]\u001b[A\n",
      "Epoch 80:  93%|███████▍| 62/67 [00:18<00:01,  3.33it/s, loss=2.48, v_num=1, train_loss_step=4.220, val_loss=3.080, train_loss_epoch=2.790]\u001b[A\n",
      "Epoch 80:  94%|███████▌| 63/67 [00:18<00:01,  3.33it/s, loss=2.48, v_num=1, train_loss_step=4.220, val_loss=3.080, train_loss_epoch=2.790]\u001b[A\n",
      "Epoch 80:  96%|███████▋| 64/67 [00:19<00:00,  3.32it/s, loss=2.48, v_num=1, train_loss_step=4.220, val_loss=3.080, train_loss_epoch=2.790]\u001b[A\n",
      "Epoch 80:  97%|███████▊| 65/67 [00:19<00:00,  3.32it/s, loss=2.48, v_num=1, train_loss_step=4.220, val_loss=3.080, train_loss_epoch=2.790]\u001b[A\n",
      "Epoch 80:  99%|███████▉| 66/67 [00:19<00:00,  3.32it/s, loss=2.48, v_num=1, train_loss_step=4.220, val_loss=3.080, train_loss_epoch=2.790]\u001b[A\n",
      "Epoch 80: 100%|████████| 67/67 [00:20<00:00,  3.25it/s, loss=2.48, v_num=1, train_loss_step=4.220, val_loss=3.080, train_loss_epoch=2.790]\u001b[A\n",
      "Epoch 81:  45%|███▌    | 30/67 [00:06<00:07,  4.94it/s, loss=1.81, v_num=1, train_loss_step=2.420, val_loss=3.080, train_loss_epoch=2.300]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 81:  46%|███▋    | 31/67 [00:08<00:09,  3.85it/s, loss=1.81, v_num=1, train_loss_step=2.420, val_loss=3.080, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 81:  48%|███▊    | 32/67 [00:08<00:09,  3.82it/s, loss=1.81, v_num=1, train_loss_step=2.420, val_loss=3.080, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 81:  49%|███▉    | 33/67 [00:08<00:08,  3.79it/s, loss=1.81, v_num=1, train_loss_step=2.420, val_loss=3.080, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 81:  51%|████    | 34/67 [00:09<00:08,  3.76it/s, loss=1.81, v_num=1, train_loss_step=2.420, val_loss=3.080, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 81:  52%|████▏   | 35/67 [00:09<00:08,  3.73it/s, loss=1.81, v_num=1, train_loss_step=2.420, val_loss=3.080, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 81:  54%|████▎   | 36/67 [00:09<00:08,  3.71it/s, loss=1.81, v_num=1, train_loss_step=2.420, val_loss=3.080, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 81:  55%|████▍   | 37/67 [00:10<00:08,  3.69it/s, loss=1.81, v_num=1, train_loss_step=2.420, val_loss=3.080, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 81:  57%|████▌   | 38/67 [00:10<00:07,  3.66it/s, loss=1.81, v_num=1, train_loss_step=2.420, val_loss=3.080, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 81:  58%|████▋   | 39/67 [00:10<00:07,  3.64it/s, loss=1.81, v_num=1, train_loss_step=2.420, val_loss=3.080, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 81:  60%|████▊   | 40/67 [00:11<00:07,  3.62it/s, loss=1.81, v_num=1, train_loss_step=2.420, val_loss=3.080, train_loss_epoch=2.300]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 81:  61%|████▉   | 41/67 [00:11<00:07,  3.57it/s, loss=1.81, v_num=1, train_loss_step=2.420, val_loss=3.080, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 81:  63%|█████   | 42/67 [00:11<00:07,  3.55it/s, loss=1.81, v_num=1, train_loss_step=2.420, val_loss=3.080, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 81:  64%|█████▏  | 43/67 [00:12<00:06,  3.54it/s, loss=1.81, v_num=1, train_loss_step=2.420, val_loss=3.080, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 81:  66%|█████▎  | 44/67 [00:12<00:06,  3.52it/s, loss=1.81, v_num=1, train_loss_step=2.420, val_loss=3.080, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 81:  67%|█████▎  | 45/67 [00:12<00:06,  3.51it/s, loss=1.81, v_num=1, train_loss_step=2.420, val_loss=3.080, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 81:  69%|█████▍  | 46/67 [00:13<00:06,  3.49it/s, loss=1.81, v_num=1, train_loss_step=2.420, val_loss=3.080, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 81:  70%|█████▌  | 47/67 [00:13<00:05,  3.48it/s, loss=1.81, v_num=1, train_loss_step=2.420, val_loss=3.080, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 81:  72%|█████▋  | 48/67 [00:13<00:05,  3.47it/s, loss=1.81, v_num=1, train_loss_step=2.420, val_loss=3.080, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 81:  73%|█████▊  | 49/67 [00:14<00:05,  3.46it/s, loss=1.81, v_num=1, train_loss_step=2.420, val_loss=3.080, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 81:  75%|█████▉  | 50/67 [00:14<00:04,  3.45it/s, loss=1.81, v_num=1, train_loss_step=2.420, val_loss=3.080, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 81:  76%|██████  | 51/67 [00:14<00:04,  3.41it/s, loss=1.81, v_num=1, train_loss_step=2.420, val_loss=3.080, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 81:  78%|██████▏ | 52/67 [00:15<00:04,  3.40it/s, loss=1.81, v_num=1, train_loss_step=2.420, val_loss=3.080, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 81:  79%|██████▎ | 53/67 [00:15<00:04,  3.39it/s, loss=1.81, v_num=1, train_loss_step=2.420, val_loss=3.080, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 81:  81%|██████▍ | 54/67 [00:15<00:03,  3.39it/s, loss=1.81, v_num=1, train_loss_step=2.420, val_loss=3.080, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 81:  82%|██████▌ | 55/67 [00:16<00:03,  3.38it/s, loss=1.81, v_num=1, train_loss_step=2.420, val_loss=3.080, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 81:  84%|██████▋ | 56/67 [00:16<00:03,  3.37it/s, loss=1.81, v_num=1, train_loss_step=2.420, val_loss=3.080, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 81:  85%|██████▊ | 57/67 [00:16<00:02,  3.36it/s, loss=1.81, v_num=1, train_loss_step=2.420, val_loss=3.080, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 81:  87%|██████▉ | 58/67 [00:17<00:02,  3.35it/s, loss=1.81, v_num=1, train_loss_step=2.420, val_loss=3.080, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 81:  88%|███████ | 59/67 [00:17<00:02,  3.34it/s, loss=1.81, v_num=1, train_loss_step=2.420, val_loss=3.080, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 81:  90%|███████▏| 60/67 [00:17<00:02,  3.34it/s, loss=1.81, v_num=1, train_loss_step=2.420, val_loss=3.080, train_loss_epoch=2.300]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 81:  91%|███████▎| 61/67 [00:18<00:01,  3.31it/s, loss=1.81, v_num=1, train_loss_step=2.420, val_loss=3.080, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 81:  93%|███████▍| 62/67 [00:18<00:01,  3.30it/s, loss=1.81, v_num=1, train_loss_step=2.420, val_loss=3.080, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 81:  94%|███████▌| 63/67 [00:19<00:01,  3.30it/s, loss=1.81, v_num=1, train_loss_step=2.420, val_loss=3.080, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 81:  96%|███████▋| 64/67 [00:19<00:00,  3.29it/s, loss=1.81, v_num=1, train_loss_step=2.420, val_loss=3.080, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 81:  97%|███████▊| 65/67 [00:19<00:00,  3.29it/s, loss=1.81, v_num=1, train_loss_step=2.420, val_loss=3.080, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 81:  99%|███████▉| 66/67 [00:20<00:00,  3.28it/s, loss=1.81, v_num=1, train_loss_step=2.420, val_loss=3.080, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 81: 100%|████████| 67/67 [00:20<00:00,  3.22it/s, loss=1.81, v_num=1, train_loss_step=2.420, val_loss=3.080, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 82:  45%|███▌    | 30/67 [00:06<00:07,  4.98it/s, loss=2.29, v_num=1, train_loss_step=1.850, val_loss=3.080, train_loss_epoch=2.080]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 82:  46%|███▋    | 31/67 [00:08<00:09,  3.86it/s, loss=2.29, v_num=1, train_loss_step=1.850, val_loss=3.080, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 82:  48%|███▊    | 32/67 [00:08<00:09,  3.83it/s, loss=2.29, v_num=1, train_loss_step=1.850, val_loss=3.080, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 82:  49%|███▉    | 33/67 [00:08<00:08,  3.79it/s, loss=2.29, v_num=1, train_loss_step=1.850, val_loss=3.080, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 82:  51%|████    | 34/67 [00:09<00:08,  3.76it/s, loss=2.29, v_num=1, train_loss_step=1.850, val_loss=3.080, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 82:  52%|████▏   | 35/67 [00:09<00:08,  3.73it/s, loss=2.29, v_num=1, train_loss_step=1.850, val_loss=3.080, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 82:  54%|████▎   | 36/67 [00:09<00:08,  3.70it/s, loss=2.29, v_num=1, train_loss_step=1.850, val_loss=3.080, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 82:  55%|████▍   | 37/67 [00:10<00:08,  3.68it/s, loss=2.29, v_num=1, train_loss_step=1.850, val_loss=3.080, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 82:  57%|████▌   | 38/67 [00:10<00:07,  3.66it/s, loss=2.29, v_num=1, train_loss_step=1.850, val_loss=3.080, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 82:  58%|████▋   | 39/67 [00:10<00:07,  3.63it/s, loss=2.29, v_num=1, train_loss_step=1.850, val_loss=3.080, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 82:  60%|████▊   | 40/67 [00:11<00:07,  3.61it/s, loss=2.29, v_num=1, train_loss_step=1.850, val_loss=3.080, train_loss_epoch=2.080]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 82:  61%|████▉   | 41/67 [00:11<00:07,  3.56it/s, loss=2.29, v_num=1, train_loss_step=1.850, val_loss=3.080, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 82:  63%|█████   | 42/67 [00:11<00:07,  3.54it/s, loss=2.29, v_num=1, train_loss_step=1.850, val_loss=3.080, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 82:  64%|█████▏  | 43/67 [00:12<00:06,  3.52it/s, loss=2.29, v_num=1, train_loss_step=1.850, val_loss=3.080, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 82:  66%|█████▎  | 44/67 [00:12<00:06,  3.51it/s, loss=2.29, v_num=1, train_loss_step=1.850, val_loss=3.080, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 82:  67%|█████▎  | 45/67 [00:12<00:06,  3.50it/s, loss=2.29, v_num=1, train_loss_step=1.850, val_loss=3.080, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 82:  69%|█████▍  | 46/67 [00:13<00:06,  3.48it/s, loss=2.29, v_num=1, train_loss_step=1.850, val_loss=3.080, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 82:  70%|█████▌  | 47/67 [00:13<00:05,  3.47it/s, loss=2.29, v_num=1, train_loss_step=1.850, val_loss=3.080, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 82:  72%|█████▋  | 48/67 [00:13<00:05,  3.46it/s, loss=2.29, v_num=1, train_loss_step=1.850, val_loss=3.080, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 82:  73%|█████▊  | 49/67 [00:14<00:05,  3.45it/s, loss=2.29, v_num=1, train_loss_step=1.850, val_loss=3.080, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 82:  75%|█████▉  | 50/67 [00:14<00:04,  3.44it/s, loss=2.29, v_num=1, train_loss_step=1.850, val_loss=3.080, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 82:  76%|██████  | 51/67 [00:15<00:04,  3.40it/s, loss=2.29, v_num=1, train_loss_step=1.850, val_loss=3.080, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 82:  78%|██████▏ | 52/67 [00:15<00:04,  3.38it/s, loss=2.29, v_num=1, train_loss_step=1.850, val_loss=3.080, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 82:  79%|██████▎ | 53/67 [00:15<00:04,  3.38it/s, loss=2.29, v_num=1, train_loss_step=1.850, val_loss=3.080, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 82:  81%|██████▍ | 54/67 [00:16<00:03,  3.37it/s, loss=2.29, v_num=1, train_loss_step=1.850, val_loss=3.080, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 82:  82%|██████▌ | 55/67 [00:16<00:03,  3.36it/s, loss=2.29, v_num=1, train_loss_step=1.850, val_loss=3.080, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 82:  84%|██████▋ | 56/67 [00:16<00:03,  3.35it/s, loss=2.29, v_num=1, train_loss_step=1.850, val_loss=3.080, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 82:  85%|██████▊ | 57/67 [00:17<00:02,  3.35it/s, loss=2.29, v_num=1, train_loss_step=1.850, val_loss=3.080, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 82:  87%|██████▉ | 58/67 [00:17<00:02,  3.34it/s, loss=2.29, v_num=1, train_loss_step=1.850, val_loss=3.080, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 82:  88%|███████ | 59/67 [00:17<00:02,  3.33it/s, loss=2.29, v_num=1, train_loss_step=1.850, val_loss=3.080, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 82:  90%|███████▏| 60/67 [00:18<00:02,  3.33it/s, loss=2.29, v_num=1, train_loss_step=1.850, val_loss=3.080, train_loss_epoch=2.080]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 82:  91%|███████▎| 61/67 [00:18<00:01,  3.30it/s, loss=2.29, v_num=1, train_loss_step=1.850, val_loss=3.080, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 82:  93%|███████▍| 62/67 [00:18<00:01,  3.30it/s, loss=2.29, v_num=1, train_loss_step=1.850, val_loss=3.080, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 82:  94%|███████▌| 63/67 [00:19<00:01,  3.29it/s, loss=2.29, v_num=1, train_loss_step=1.850, val_loss=3.080, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 82:  96%|███████▋| 64/67 [00:19<00:00,  3.29it/s, loss=2.29, v_num=1, train_loss_step=1.850, val_loss=3.080, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 82:  97%|███████▊| 65/67 [00:19<00:00,  3.28it/s, loss=2.29, v_num=1, train_loss_step=1.850, val_loss=3.080, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 82:  99%|███████▉| 66/67 [00:20<00:00,  3.28it/s, loss=2.29, v_num=1, train_loss_step=1.850, val_loss=3.080, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 82: 100%|████████| 67/67 [00:20<00:00,  3.22it/s, loss=2.29, v_num=1, train_loss_step=1.850, val_loss=3.080, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 83:  45%|███▌    | 30/67 [00:06<00:07,  4.92it/s, loss=1.87, v_num=1, train_loss_step=0.598, val_loss=3.080, train_loss_epoch=2.470]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 83:  46%|███▋    | 31/67 [00:08<00:09,  3.82it/s, loss=1.87, v_num=1, train_loss_step=0.598, val_loss=3.080, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 83:  48%|███▊    | 32/67 [00:08<00:09,  3.79it/s, loss=1.87, v_num=1, train_loss_step=0.598, val_loss=3.080, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 83:  49%|███▉    | 33/67 [00:08<00:09,  3.76it/s, loss=1.87, v_num=1, train_loss_step=0.598, val_loss=3.080, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 83:  51%|████    | 34/67 [00:09<00:08,  3.73it/s, loss=1.87, v_num=1, train_loss_step=0.598, val_loss=3.080, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 83:  52%|████▏   | 35/67 [00:09<00:08,  3.70it/s, loss=1.87, v_num=1, train_loss_step=0.598, val_loss=3.080, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 83:  54%|████▎   | 36/67 [00:09<00:08,  3.68it/s, loss=1.87, v_num=1, train_loss_step=0.598, val_loss=3.080, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 83:  55%|████▍   | 37/67 [00:10<00:08,  3.66it/s, loss=1.87, v_num=1, train_loss_step=0.598, val_loss=3.080, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 83:  57%|████▌   | 38/67 [00:10<00:07,  3.64it/s, loss=1.87, v_num=1, train_loss_step=0.598, val_loss=3.080, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 83:  58%|████▋   | 39/67 [00:10<00:07,  3.62it/s, loss=1.87, v_num=1, train_loss_step=0.598, val_loss=3.080, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 83:  60%|████▊   | 40/67 [00:11<00:07,  3.60it/s, loss=1.87, v_num=1, train_loss_step=0.598, val_loss=3.080, train_loss_epoch=2.470]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 83:  61%|████▉   | 41/67 [00:11<00:07,  3.55it/s, loss=1.87, v_num=1, train_loss_step=0.598, val_loss=3.080, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 83:  63%|█████   | 42/67 [00:11<00:07,  3.53it/s, loss=1.87, v_num=1, train_loss_step=0.598, val_loss=3.080, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 83:  64%|█████▏  | 43/67 [00:12<00:06,  3.52it/s, loss=1.87, v_num=1, train_loss_step=0.598, val_loss=3.080, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 83:  66%|█████▎  | 44/67 [00:12<00:06,  3.51it/s, loss=1.87, v_num=1, train_loss_step=0.598, val_loss=3.080, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 83:  67%|█████▎  | 45/67 [00:12<00:06,  3.49it/s, loss=1.87, v_num=1, train_loss_step=0.598, val_loss=3.080, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 83:  69%|█████▍  | 46/67 [00:13<00:06,  3.48it/s, loss=1.87, v_num=1, train_loss_step=0.598, val_loss=3.080, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 83:  70%|█████▌  | 47/67 [00:13<00:05,  3.47it/s, loss=1.87, v_num=1, train_loss_step=0.598, val_loss=3.080, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 83:  72%|█████▋  | 48/67 [00:13<00:05,  3.46it/s, loss=1.87, v_num=1, train_loss_step=0.598, val_loss=3.080, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 83:  73%|█████▊  | 49/67 [00:14<00:05,  3.45it/s, loss=1.87, v_num=1, train_loss_step=0.598, val_loss=3.080, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 83:  75%|█████▉  | 50/67 [00:14<00:04,  3.44it/s, loss=1.87, v_num=1, train_loss_step=0.598, val_loss=3.080, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 83:  76%|██████  | 51/67 [00:14<00:04,  3.40it/s, loss=1.87, v_num=1, train_loss_step=0.598, val_loss=3.080, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 83:  78%|██████▏ | 52/67 [00:15<00:04,  3.39it/s, loss=1.87, v_num=1, train_loss_step=0.598, val_loss=3.080, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 83:  79%|██████▎ | 53/67 [00:15<00:04,  3.39it/s, loss=1.87, v_num=1, train_loss_step=0.598, val_loss=3.080, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 83:  81%|██████▍ | 54/67 [00:15<00:03,  3.38it/s, loss=1.87, v_num=1, train_loss_step=0.598, val_loss=3.080, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 83:  82%|██████▌ | 55/67 [00:16<00:03,  3.37it/s, loss=1.87, v_num=1, train_loss_step=0.598, val_loss=3.080, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 83:  84%|██████▋ | 56/67 [00:16<00:03,  3.37it/s, loss=1.87, v_num=1, train_loss_step=0.598, val_loss=3.080, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 83:  85%|██████▊ | 57/67 [00:16<00:02,  3.36it/s, loss=1.87, v_num=1, train_loss_step=0.598, val_loss=3.080, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 83:  87%|██████▉ | 58/67 [00:17<00:02,  3.35it/s, loss=1.87, v_num=1, train_loss_step=0.598, val_loss=3.080, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 83:  88%|███████ | 59/67 [00:17<00:02,  3.35it/s, loss=1.87, v_num=1, train_loss_step=0.598, val_loss=3.080, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 83:  90%|███████▏| 60/67 [00:17<00:02,  3.34it/s, loss=1.87, v_num=1, train_loss_step=0.598, val_loss=3.080, train_loss_epoch=2.470]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 83:  91%|███████▎| 61/67 [00:18<00:01,  3.31it/s, loss=1.87, v_num=1, train_loss_step=0.598, val_loss=3.080, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 83:  93%|███████▍| 62/67 [00:18<00:01,  3.31it/s, loss=1.87, v_num=1, train_loss_step=0.598, val_loss=3.080, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 83:  94%|███████▌| 63/67 [00:19<00:01,  3.31it/s, loss=1.87, v_num=1, train_loss_step=0.598, val_loss=3.080, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 83:  96%|███████▋| 64/67 [00:19<00:00,  3.30it/s, loss=1.87, v_num=1, train_loss_step=0.598, val_loss=3.080, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 83:  97%|███████▊| 65/67 [00:19<00:00,  3.30it/s, loss=1.87, v_num=1, train_loss_step=0.598, val_loss=3.080, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 83:  99%|███████▉| 66/67 [00:20<00:00,  3.30it/s, loss=1.87, v_num=1, train_loss_step=0.598, val_loss=3.080, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 83: 100%|████████| 67/67 [00:20<00:00,  3.23it/s, loss=1.87, v_num=1, train_loss_step=0.598, val_loss=3.070, train_loss_epoch=2.470]\u001b[A\n",
      "Epoch 84:  45%|███▌    | 30/67 [00:06<00:07,  5.00it/s, loss=2.63, v_num=1, train_loss_step=0.527, val_loss=3.070, train_loss_epoch=2.160]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 84:  46%|███▋    | 31/67 [00:08<00:09,  3.85it/s, loss=2.63, v_num=1, train_loss_step=0.527, val_loss=3.070, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 84:  48%|███▊    | 32/67 [00:08<00:09,  3.82it/s, loss=2.63, v_num=1, train_loss_step=0.527, val_loss=3.070, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 84:  49%|███▉    | 33/67 [00:08<00:08,  3.78it/s, loss=2.63, v_num=1, train_loss_step=0.527, val_loss=3.070, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 84:  51%|████    | 34/67 [00:09<00:08,  3.75it/s, loss=2.63, v_num=1, train_loss_step=0.527, val_loss=3.070, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 84:  52%|████▏   | 35/67 [00:09<00:08,  3.73it/s, loss=2.63, v_num=1, train_loss_step=0.527, val_loss=3.070, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 84:  54%|████▎   | 36/67 [00:09<00:08,  3.70it/s, loss=2.63, v_num=1, train_loss_step=0.527, val_loss=3.070, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 84:  55%|████▍   | 37/67 [00:10<00:08,  3.68it/s, loss=2.63, v_num=1, train_loss_step=0.527, val_loss=3.070, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 84:  57%|████▌   | 38/67 [00:10<00:07,  3.66it/s, loss=2.63, v_num=1, train_loss_step=0.527, val_loss=3.070, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 84:  58%|████▋   | 39/67 [00:10<00:07,  3.64it/s, loss=2.63, v_num=1, train_loss_step=0.527, val_loss=3.070, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 84:  60%|████▊   | 40/67 [00:11<00:07,  3.61it/s, loss=2.63, v_num=1, train_loss_step=0.527, val_loss=3.070, train_loss_epoch=2.160]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 84:  61%|████▉   | 41/67 [00:11<00:07,  3.56it/s, loss=2.63, v_num=1, train_loss_step=0.527, val_loss=3.070, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 84:  63%|█████   | 42/67 [00:11<00:07,  3.55it/s, loss=2.63, v_num=1, train_loss_step=0.527, val_loss=3.070, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 84:  64%|█████▏  | 43/67 [00:12<00:06,  3.53it/s, loss=2.63, v_num=1, train_loss_step=0.527, val_loss=3.070, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 84:  66%|█████▎  | 44/67 [00:12<00:06,  3.52it/s, loss=2.63, v_num=1, train_loss_step=0.527, val_loss=3.070, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 84:  67%|█████▎  | 45/67 [00:12<00:06,  3.50it/s, loss=2.63, v_num=1, train_loss_step=0.527, val_loss=3.070, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 84:  69%|█████▍  | 46/67 [00:13<00:06,  3.49it/s, loss=2.63, v_num=1, train_loss_step=0.527, val_loss=3.070, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 84:  70%|█████▌  | 47/67 [00:13<00:05,  3.47it/s, loss=2.63, v_num=1, train_loss_step=0.527, val_loss=3.070, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 84:  72%|█████▋  | 48/67 [00:13<00:05,  3.46it/s, loss=2.63, v_num=1, train_loss_step=0.527, val_loss=3.070, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 84:  73%|█████▊  | 49/67 [00:14<00:05,  3.45it/s, loss=2.63, v_num=1, train_loss_step=0.527, val_loss=3.070, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 84:  75%|█████▉  | 50/67 [00:14<00:04,  3.43it/s, loss=2.63, v_num=1, train_loss_step=0.527, val_loss=3.070, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 84:  76%|██████  | 51/67 [00:15<00:04,  3.40it/s, loss=2.63, v_num=1, train_loss_step=0.527, val_loss=3.070, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 84:  78%|██████▏ | 52/67 [00:15<00:04,  3.39it/s, loss=2.63, v_num=1, train_loss_step=0.527, val_loss=3.070, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 84:  79%|██████▎ | 53/67 [00:15<00:04,  3.38it/s, loss=2.63, v_num=1, train_loss_step=0.527, val_loss=3.070, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 84:  81%|██████▍ | 54/67 [00:16<00:03,  3.37it/s, loss=2.63, v_num=1, train_loss_step=0.527, val_loss=3.070, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 84:  82%|██████▌ | 55/67 [00:16<00:03,  3.36it/s, loss=2.63, v_num=1, train_loss_step=0.527, val_loss=3.070, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 84:  84%|██████▋ | 56/67 [00:16<00:03,  3.36it/s, loss=2.63, v_num=1, train_loss_step=0.527, val_loss=3.070, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 84:  85%|██████▊ | 57/67 [00:17<00:02,  3.35it/s, loss=2.63, v_num=1, train_loss_step=0.527, val_loss=3.070, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 84:  87%|██████▉ | 58/67 [00:17<00:02,  3.34it/s, loss=2.63, v_num=1, train_loss_step=0.527, val_loss=3.070, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 84:  88%|███████ | 59/67 [00:17<00:02,  3.33it/s, loss=2.63, v_num=1, train_loss_step=0.527, val_loss=3.070, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 84:  90%|███████▏| 60/67 [00:18<00:02,  3.33it/s, loss=2.63, v_num=1, train_loss_step=0.527, val_loss=3.070, train_loss_epoch=2.160]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 84:  91%|███████▎| 61/67 [00:18<00:01,  3.29it/s, loss=2.63, v_num=1, train_loss_step=0.527, val_loss=3.070, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 84:  93%|███████▍| 62/67 [00:18<00:01,  3.29it/s, loss=2.63, v_num=1, train_loss_step=0.527, val_loss=3.070, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 84:  94%|███████▌| 63/67 [00:19<00:01,  3.28it/s, loss=2.63, v_num=1, train_loss_step=0.527, val_loss=3.070, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 84:  96%|███████▋| 64/67 [00:19<00:00,  3.27it/s, loss=2.63, v_num=1, train_loss_step=0.527, val_loss=3.070, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 84:  97%|███████▊| 65/67 [00:19<00:00,  3.27it/s, loss=2.63, v_num=1, train_loss_step=0.527, val_loss=3.070, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 84:  99%|███████▉| 66/67 [00:20<00:00,  3.27it/s, loss=2.63, v_num=1, train_loss_step=0.527, val_loss=3.070, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 84: 100%|████████| 67/67 [00:20<00:00,  3.20it/s, loss=2.63, v_num=1, train_loss_step=0.527, val_loss=3.070, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 85:  45%|███▌    | 30/67 [00:05<00:07,  5.06it/s, loss=2.09, v_num=1, train_loss_step=2.500, val_loss=3.070, train_loss_epoch=2.270]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 85:  46%|███▋    | 31/67 [00:07<00:09,  3.92it/s, loss=2.09, v_num=1, train_loss_step=2.500, val_loss=3.070, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 85:  48%|███▊    | 32/67 [00:08<00:08,  3.89it/s, loss=2.09, v_num=1, train_loss_step=2.500, val_loss=3.070, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 85:  49%|███▉    | 33/67 [00:08<00:08,  3.85it/s, loss=2.09, v_num=1, train_loss_step=2.500, val_loss=3.070, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 85:  51%|████    | 34/67 [00:08<00:08,  3.82it/s, loss=2.09, v_num=1, train_loss_step=2.500, val_loss=3.070, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 85:  52%|████▏   | 35/67 [00:09<00:08,  3.79it/s, loss=2.09, v_num=1, train_loss_step=2.500, val_loss=3.070, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 85:  54%|████▎   | 36/67 [00:09<00:08,  3.76it/s, loss=2.09, v_num=1, train_loss_step=2.500, val_loss=3.070, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 85:  55%|████▍   | 37/67 [00:09<00:08,  3.74it/s, loss=2.09, v_num=1, train_loss_step=2.500, val_loss=3.070, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 85:  57%|████▌   | 38/67 [00:10<00:07,  3.71it/s, loss=2.09, v_num=1, train_loss_step=2.500, val_loss=3.070, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 85:  58%|████▋   | 39/67 [00:10<00:07,  3.69it/s, loss=2.09, v_num=1, train_loss_step=2.500, val_loss=3.070, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 85:  60%|████▊   | 40/67 [00:10<00:07,  3.67it/s, loss=2.09, v_num=1, train_loss_step=2.500, val_loss=3.070, train_loss_epoch=2.270]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 85:  61%|████▉   | 41/67 [00:11<00:07,  3.61it/s, loss=2.09, v_num=1, train_loss_step=2.500, val_loss=3.070, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 85:  63%|█████   | 42/67 [00:11<00:06,  3.59it/s, loss=2.09, v_num=1, train_loss_step=2.500, val_loss=3.070, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 85:  64%|█████▏  | 43/67 [00:12<00:06,  3.57it/s, loss=2.09, v_num=1, train_loss_step=2.500, val_loss=3.070, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 85:  66%|█████▎  | 44/67 [00:12<00:06,  3.56it/s, loss=2.09, v_num=1, train_loss_step=2.500, val_loss=3.070, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 85:  67%|█████▎  | 45/67 [00:12<00:06,  3.55it/s, loss=2.09, v_num=1, train_loss_step=2.500, val_loss=3.070, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 85:  69%|█████▍  | 46/67 [00:13<00:05,  3.53it/s, loss=2.09, v_num=1, train_loss_step=2.500, val_loss=3.070, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 85:  70%|█████▌  | 47/67 [00:13<00:05,  3.52it/s, loss=2.09, v_num=1, train_loss_step=2.500, val_loss=3.070, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 85:  72%|█████▋  | 48/67 [00:13<00:05,  3.50it/s, loss=2.09, v_num=1, train_loss_step=2.500, val_loss=3.070, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 85:  73%|█████▊  | 49/67 [00:14<00:05,  3.49it/s, loss=2.09, v_num=1, train_loss_step=2.500, val_loss=3.070, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 85:  75%|█████▉  | 50/67 [00:14<00:04,  3.48it/s, loss=2.09, v_num=1, train_loss_step=2.500, val_loss=3.070, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 85:  76%|██████  | 51/67 [00:14<00:04,  3.44it/s, loss=2.09, v_num=1, train_loss_step=2.500, val_loss=3.070, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 85:  78%|██████▏ | 52/67 [00:15<00:04,  3.43it/s, loss=2.09, v_num=1, train_loss_step=2.500, val_loss=3.070, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 85:  79%|██████▎ | 53/67 [00:15<00:04,  3.42it/s, loss=2.09, v_num=1, train_loss_step=2.500, val_loss=3.070, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 85:  81%|██████▍ | 54/67 [00:15<00:03,  3.41it/s, loss=2.09, v_num=1, train_loss_step=2.500, val_loss=3.070, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 85:  82%|██████▌ | 55/67 [00:16<00:03,  3.40it/s, loss=2.09, v_num=1, train_loss_step=2.500, val_loss=3.070, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 85:  84%|██████▋ | 56/67 [00:16<00:03,  3.40it/s, loss=2.09, v_num=1, train_loss_step=2.500, val_loss=3.070, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 85:  85%|██████▊ | 57/67 [00:16<00:02,  3.39it/s, loss=2.09, v_num=1, train_loss_step=2.500, val_loss=3.070, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 85:  87%|██████▉ | 58/67 [00:17<00:02,  3.38it/s, loss=2.09, v_num=1, train_loss_step=2.500, val_loss=3.070, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 85:  88%|███████ | 59/67 [00:17<00:02,  3.37it/s, loss=2.09, v_num=1, train_loss_step=2.500, val_loss=3.070, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 85:  90%|███████▏| 60/67 [00:17<00:02,  3.37it/s, loss=2.09, v_num=1, train_loss_step=2.500, val_loss=3.070, train_loss_epoch=2.270]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 85:  91%|███████▎| 61/67 [00:18<00:01,  3.34it/s, loss=2.09, v_num=1, train_loss_step=2.500, val_loss=3.070, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 85:  93%|███████▍| 62/67 [00:18<00:01,  3.33it/s, loss=2.09, v_num=1, train_loss_step=2.500, val_loss=3.070, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 85:  94%|███████▌| 63/67 [00:18<00:01,  3.33it/s, loss=2.09, v_num=1, train_loss_step=2.500, val_loss=3.070, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 85:  96%|███████▋| 64/67 [00:19<00:00,  3.32it/s, loss=2.09, v_num=1, train_loss_step=2.500, val_loss=3.070, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 85:  97%|███████▊| 65/67 [00:19<00:00,  3.32it/s, loss=2.09, v_num=1, train_loss_step=2.500, val_loss=3.070, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 85:  99%|███████▉| 66/67 [00:19<00:00,  3.31it/s, loss=2.09, v_num=1, train_loss_step=2.500, val_loss=3.070, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 85: 100%|████████| 67/67 [00:20<00:00,  3.25it/s, loss=2.09, v_num=1, train_loss_step=2.500, val_loss=3.070, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 86:  45%|███▌    | 30/67 [00:06<00:07,  4.90it/s, loss=2.53, v_num=1, train_loss_step=2.350, val_loss=3.070, train_loss_epoch=2.170]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 86:  46%|███▋    | 31/67 [00:08<00:09,  3.83it/s, loss=2.53, v_num=1, train_loss_step=2.350, val_loss=3.070, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 86:  48%|███▊    | 32/67 [00:08<00:09,  3.80it/s, loss=2.53, v_num=1, train_loss_step=2.350, val_loss=3.070, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 86:  49%|███▉    | 33/67 [00:08<00:09,  3.77it/s, loss=2.53, v_num=1, train_loss_step=2.350, val_loss=3.070, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 86:  51%|████    | 34/67 [00:09<00:08,  3.74it/s, loss=2.53, v_num=1, train_loss_step=2.350, val_loss=3.070, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 86:  52%|████▏   | 35/67 [00:09<00:08,  3.71it/s, loss=2.53, v_num=1, train_loss_step=2.350, val_loss=3.070, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 86:  54%|████▎   | 36/67 [00:09<00:08,  3.68it/s, loss=2.53, v_num=1, train_loss_step=2.350, val_loss=3.070, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 86:  55%|████▍   | 37/67 [00:10<00:08,  3.66it/s, loss=2.53, v_num=1, train_loss_step=2.350, val_loss=3.070, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 86:  57%|████▌   | 38/67 [00:10<00:07,  3.63it/s, loss=2.53, v_num=1, train_loss_step=2.350, val_loss=3.070, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 86:  58%|████▋   | 39/67 [00:10<00:07,  3.61it/s, loss=2.53, v_num=1, train_loss_step=2.350, val_loss=3.070, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 86:  60%|████▊   | 40/67 [00:11<00:07,  3.59it/s, loss=2.53, v_num=1, train_loss_step=2.350, val_loss=3.070, train_loss_epoch=2.170]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 86:  61%|████▉   | 41/67 [00:11<00:07,  3.54it/s, loss=2.53, v_num=1, train_loss_step=2.350, val_loss=3.070, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 86:  63%|█████   | 42/67 [00:11<00:07,  3.53it/s, loss=2.53, v_num=1, train_loss_step=2.350, val_loss=3.070, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 86:  64%|█████▏  | 43/67 [00:12<00:06,  3.51it/s, loss=2.53, v_num=1, train_loss_step=2.350, val_loss=3.070, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 86:  66%|█████▎  | 44/67 [00:12<00:06,  3.49it/s, loss=2.53, v_num=1, train_loss_step=2.350, val_loss=3.070, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 86:  67%|█████▎  | 45/67 [00:12<00:06,  3.48it/s, loss=2.53, v_num=1, train_loss_step=2.350, val_loss=3.070, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 86:  69%|█████▍  | 46/67 [00:13<00:06,  3.46it/s, loss=2.53, v_num=1, train_loss_step=2.350, val_loss=3.070, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 86:  70%|█████▌  | 47/67 [00:13<00:05,  3.45it/s, loss=2.53, v_num=1, train_loss_step=2.350, val_loss=3.070, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 86:  72%|█████▋  | 48/67 [00:13<00:05,  3.44it/s, loss=2.53, v_num=1, train_loss_step=2.350, val_loss=3.070, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 86:  73%|█████▊  | 49/67 [00:14<00:05,  3.43it/s, loss=2.53, v_num=1, train_loss_step=2.350, val_loss=3.070, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 86:  75%|█████▉  | 50/67 [00:14<00:04,  3.42it/s, loss=2.53, v_num=1, train_loss_step=2.350, val_loss=3.070, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 86:  76%|██████  | 51/67 [00:15<00:04,  3.37it/s, loss=2.53, v_num=1, train_loss_step=2.350, val_loss=3.070, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 86:  78%|██████▏ | 52/67 [00:15<00:04,  3.36it/s, loss=2.53, v_num=1, train_loss_step=2.350, val_loss=3.070, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 86:  79%|██████▎ | 53/67 [00:15<00:04,  3.35it/s, loss=2.53, v_num=1, train_loss_step=2.350, val_loss=3.070, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 86:  81%|██████▍ | 54/67 [00:16<00:03,  3.34it/s, loss=2.53, v_num=1, train_loss_step=2.350, val_loss=3.070, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 86:  82%|██████▌ | 55/67 [00:16<00:03,  3.34it/s, loss=2.53, v_num=1, train_loss_step=2.350, val_loss=3.070, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 86:  84%|██████▋ | 56/67 [00:16<00:03,  3.33it/s, loss=2.53, v_num=1, train_loss_step=2.350, val_loss=3.070, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 86:  85%|██████▊ | 57/67 [00:17<00:03,  3.33it/s, loss=2.53, v_num=1, train_loss_step=2.350, val_loss=3.070, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 86:  87%|██████▉ | 58/67 [00:17<00:02,  3.32it/s, loss=2.53, v_num=1, train_loss_step=2.350, val_loss=3.070, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 86:  88%|███████ | 59/67 [00:17<00:02,  3.31it/s, loss=2.53, v_num=1, train_loss_step=2.350, val_loss=3.070, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 86:  90%|███████▏| 60/67 [00:18<00:02,  3.31it/s, loss=2.53, v_num=1, train_loss_step=2.350, val_loss=3.070, train_loss_epoch=2.170]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 86:  91%|███████▎| 61/67 [00:18<00:01,  3.28it/s, loss=2.53, v_num=1, train_loss_step=2.350, val_loss=3.070, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 86:  93%|███████▍| 62/67 [00:18<00:01,  3.28it/s, loss=2.53, v_num=1, train_loss_step=2.350, val_loss=3.070, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 86:  94%|███████▌| 63/67 [00:19<00:01,  3.28it/s, loss=2.53, v_num=1, train_loss_step=2.350, val_loss=3.070, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 86:  96%|███████▋| 64/67 [00:19<00:00,  3.27it/s, loss=2.53, v_num=1, train_loss_step=2.350, val_loss=3.070, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 86:  97%|███████▊| 65/67 [00:19<00:00,  3.27it/s, loss=2.53, v_num=1, train_loss_step=2.350, val_loss=3.070, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 86:  99%|███████▉| 66/67 [00:20<00:00,  3.27it/s, loss=2.53, v_num=1, train_loss_step=2.350, val_loss=3.070, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 86: 100%|████████| 67/67 [00:20<00:00,  3.21it/s, loss=2.53, v_num=1, train_loss_step=2.350, val_loss=3.070, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 87:  45%|███▌    | 30/67 [00:05<00:07,  5.10it/s, loss=1.78, v_num=1, train_loss_step=0.789, val_loss=3.070, train_loss_epoch=2.310]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 87:  46%|███▋    | 31/67 [00:07<00:09,  3.95it/s, loss=1.78, v_num=1, train_loss_step=0.789, val_loss=3.070, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 87:  48%|███▊    | 32/67 [00:08<00:08,  3.91it/s, loss=1.78, v_num=1, train_loss_step=0.789, val_loss=3.070, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 87:  49%|███▉    | 33/67 [00:08<00:08,  3.88it/s, loss=1.78, v_num=1, train_loss_step=0.789, val_loss=3.070, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 87:  51%|████    | 34/67 [00:08<00:08,  3.84it/s, loss=1.78, v_num=1, train_loss_step=0.789, val_loss=3.070, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 87:  52%|████▏   | 35/67 [00:09<00:08,  3.82it/s, loss=1.78, v_num=1, train_loss_step=0.789, val_loss=3.070, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 87:  54%|████▎   | 36/67 [00:09<00:08,  3.79it/s, loss=1.78, v_num=1, train_loss_step=0.789, val_loss=3.070, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 87:  55%|████▍   | 37/67 [00:09<00:07,  3.77it/s, loss=1.78, v_num=1, train_loss_step=0.789, val_loss=3.070, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 87:  57%|████▌   | 38/67 [00:10<00:07,  3.74it/s, loss=1.78, v_num=1, train_loss_step=0.789, val_loss=3.070, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 87:  58%|████▋   | 39/67 [00:10<00:07,  3.72it/s, loss=1.78, v_num=1, train_loss_step=0.789, val_loss=3.070, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 87:  60%|████▊   | 40/67 [00:10<00:07,  3.70it/s, loss=1.78, v_num=1, train_loss_step=0.789, val_loss=3.070, train_loss_epoch=2.310]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 87:  61%|████▉   | 41/67 [00:11<00:07,  3.64it/s, loss=1.78, v_num=1, train_loss_step=0.789, val_loss=3.070, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 87:  63%|█████   | 42/67 [00:11<00:06,  3.63it/s, loss=1.78, v_num=1, train_loss_step=0.789, val_loss=3.070, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 87:  64%|█████▏  | 43/67 [00:11<00:06,  3.61it/s, loss=1.78, v_num=1, train_loss_step=0.789, val_loss=3.070, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 87:  66%|█████▎  | 44/67 [00:12<00:06,  3.59it/s, loss=1.78, v_num=1, train_loss_step=0.789, val_loss=3.070, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 87:  67%|█████▎  | 45/67 [00:12<00:06,  3.57it/s, loss=1.78, v_num=1, train_loss_step=0.789, val_loss=3.070, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 87:  69%|█████▍  | 46/67 [00:12<00:05,  3.56it/s, loss=1.78, v_num=1, train_loss_step=0.789, val_loss=3.070, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 87:  70%|█████▌  | 47/67 [00:13<00:05,  3.55it/s, loss=1.78, v_num=1, train_loss_step=0.789, val_loss=3.070, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 87:  72%|█████▋  | 48/67 [00:13<00:05,  3.53it/s, loss=1.78, v_num=1, train_loss_step=0.789, val_loss=3.070, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 87:  73%|█████▊  | 49/67 [00:13<00:05,  3.52it/s, loss=1.78, v_num=1, train_loss_step=0.789, val_loss=3.070, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 87:  75%|█████▉  | 50/67 [00:14<00:04,  3.51it/s, loss=1.78, v_num=1, train_loss_step=0.789, val_loss=3.070, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 87:  76%|██████  | 51/67 [00:14<00:04,  3.47it/s, loss=1.78, v_num=1, train_loss_step=0.789, val_loss=3.070, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 87:  78%|██████▏ | 52/67 [00:15<00:04,  3.46it/s, loss=1.78, v_num=1, train_loss_step=0.789, val_loss=3.070, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 87:  79%|██████▎ | 53/67 [00:15<00:04,  3.44it/s, loss=1.78, v_num=1, train_loss_step=0.789, val_loss=3.070, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 87:  81%|██████▍ | 54/67 [00:15<00:03,  3.43it/s, loss=1.78, v_num=1, train_loss_step=0.789, val_loss=3.070, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 87:  82%|██████▌ | 55/67 [00:16<00:03,  3.42it/s, loss=1.78, v_num=1, train_loss_step=0.789, val_loss=3.070, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 87:  84%|██████▋ | 56/67 [00:16<00:03,  3.41it/s, loss=1.78, v_num=1, train_loss_step=0.789, val_loss=3.070, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 87:  85%|██████▊ | 57/67 [00:16<00:02,  3.39it/s, loss=1.78, v_num=1, train_loss_step=0.789, val_loss=3.070, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 87:  87%|██████▉ | 58/67 [00:17<00:02,  3.38it/s, loss=1.78, v_num=1, train_loss_step=0.789, val_loss=3.070, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 87:  88%|███████ | 59/67 [00:17<00:02,  3.36it/s, loss=1.78, v_num=1, train_loss_step=0.789, val_loss=3.070, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 87:  90%|███████▏| 60/67 [00:17<00:02,  3.36it/s, loss=1.78, v_num=1, train_loss_step=0.789, val_loss=3.070, train_loss_epoch=2.310]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 87:  91%|███████▎| 61/67 [00:18<00:01,  3.32it/s, loss=1.78, v_num=1, train_loss_step=0.789, val_loss=3.070, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 87:  93%|███████▍| 62/67 [00:18<00:01,  3.31it/s, loss=1.78, v_num=1, train_loss_step=0.789, val_loss=3.070, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 87:  94%|███████▌| 63/67 [00:19<00:01,  3.30it/s, loss=1.78, v_num=1, train_loss_step=0.789, val_loss=3.070, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 87:  96%|███████▋| 64/67 [00:19<00:00,  3.30it/s, loss=1.78, v_num=1, train_loss_step=0.789, val_loss=3.070, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 87:  97%|███████▊| 65/67 [00:19<00:00,  3.30it/s, loss=1.78, v_num=1, train_loss_step=0.789, val_loss=3.070, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 87:  99%|███████▉| 66/67 [00:20<00:00,  3.29it/s, loss=1.78, v_num=1, train_loss_step=0.789, val_loss=3.070, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 87: 100%|████████| 67/67 [00:20<00:00,  3.23it/s, loss=1.78, v_num=1, train_loss_step=0.789, val_loss=3.070, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 88:  45%|███▌    | 30/67 [00:06<00:08,  4.57it/s, loss=1.81, v_num=1, train_loss_step=2.830, val_loss=3.070, train_loss_epoch=2.070]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 88:  46%|███▋    | 31/67 [00:08<00:09,  3.63it/s, loss=1.81, v_num=1, train_loss_step=2.830, val_loss=3.070, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 88:  48%|███▊    | 32/67 [00:08<00:09,  3.61it/s, loss=1.81, v_num=1, train_loss_step=2.830, val_loss=3.070, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 88:  49%|███▉    | 33/67 [00:09<00:09,  3.58it/s, loss=1.81, v_num=1, train_loss_step=2.830, val_loss=3.070, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 88:  51%|████    | 34/67 [00:09<00:09,  3.56it/s, loss=1.81, v_num=1, train_loss_step=2.830, val_loss=3.070, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 88:  52%|████▏   | 35/67 [00:09<00:09,  3.54it/s, loss=1.81, v_num=1, train_loss_step=2.830, val_loss=3.070, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 88:  54%|████▎   | 36/67 [00:10<00:08,  3.53it/s, loss=1.81, v_num=1, train_loss_step=2.830, val_loss=3.070, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 88:  55%|████▍   | 37/67 [00:10<00:08,  3.51it/s, loss=1.81, v_num=1, train_loss_step=2.830, val_loss=3.070, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 88:  57%|████▌   | 38/67 [00:10<00:08,  3.48it/s, loss=1.81, v_num=1, train_loss_step=2.830, val_loss=3.070, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 88:  58%|████▋   | 39/67 [00:11<00:08,  3.47it/s, loss=1.81, v_num=1, train_loss_step=2.830, val_loss=3.070, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 88:  60%|████▊   | 40/67 [00:11<00:07,  3.46it/s, loss=1.81, v_num=1, train_loss_step=2.830, val_loss=3.070, train_loss_epoch=2.070]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 88:  61%|████▉   | 41/67 [00:12<00:07,  3.41it/s, loss=1.81, v_num=1, train_loss_step=2.830, val_loss=3.070, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 88:  63%|█████   | 42/67 [00:12<00:07,  3.40it/s, loss=1.81, v_num=1, train_loss_step=2.830, val_loss=3.070, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 88:  64%|█████▏  | 43/67 [00:12<00:07,  3.38it/s, loss=1.81, v_num=1, train_loss_step=2.830, val_loss=3.070, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 88:  66%|█████▎  | 44/67 [00:13<00:06,  3.37it/s, loss=1.81, v_num=1, train_loss_step=2.830, val_loss=3.070, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 88:  67%|█████▎  | 45/67 [00:13<00:06,  3.35it/s, loss=1.81, v_num=1, train_loss_step=2.830, val_loss=3.070, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 88:  69%|█████▍  | 46/67 [00:13<00:06,  3.33it/s, loss=1.81, v_num=1, train_loss_step=2.830, val_loss=3.070, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 88:  70%|█████▌  | 47/67 [00:14<00:06,  3.32it/s, loss=1.81, v_num=1, train_loss_step=2.830, val_loss=3.070, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 88:  72%|█████▋  | 48/67 [00:14<00:05,  3.30it/s, loss=1.81, v_num=1, train_loss_step=2.830, val_loss=3.070, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 88:  73%|█████▊  | 49/67 [00:14<00:05,  3.29it/s, loss=1.81, v_num=1, train_loss_step=2.830, val_loss=3.070, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 88:  75%|█████▉  | 50/67 [00:15<00:05,  3.29it/s, loss=1.81, v_num=1, train_loss_step=2.830, val_loss=3.070, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 88:  76%|██████  | 51/67 [00:15<00:04,  3.25it/s, loss=1.81, v_num=1, train_loss_step=2.830, val_loss=3.070, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 88:  78%|██████▏ | 52/67 [00:16<00:04,  3.24it/s, loss=1.81, v_num=1, train_loss_step=2.830, val_loss=3.070, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 88:  79%|██████▎ | 53/67 [00:16<00:04,  3.23it/s, loss=1.81, v_num=1, train_loss_step=2.830, val_loss=3.070, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 88:  81%|██████▍ | 54/67 [00:16<00:04,  3.23it/s, loss=1.81, v_num=1, train_loss_step=2.830, val_loss=3.070, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 88:  82%|██████▌ | 55/67 [00:17<00:03,  3.22it/s, loss=1.81, v_num=1, train_loss_step=2.830, val_loss=3.070, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 88:  84%|██████▋ | 56/67 [00:17<00:03,  3.20it/s, loss=1.81, v_num=1, train_loss_step=2.830, val_loss=3.070, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 88:  85%|██████▊ | 57/67 [00:17<00:03,  3.20it/s, loss=1.81, v_num=1, train_loss_step=2.830, val_loss=3.070, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 88:  87%|██████▉ | 58/67 [00:18<00:02,  3.19it/s, loss=1.81, v_num=1, train_loss_step=2.830, val_loss=3.070, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 88:  88%|███████ | 59/67 [00:18<00:02,  3.18it/s, loss=1.81, v_num=1, train_loss_step=2.830, val_loss=3.070, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 88:  90%|███████▏| 60/67 [00:18<00:02,  3.18it/s, loss=1.81, v_num=1, train_loss_step=2.830, val_loss=3.070, train_loss_epoch=2.070]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 88:  91%|███████▎| 61/67 [00:19<00:01,  3.16it/s, loss=1.81, v_num=1, train_loss_step=2.830, val_loss=3.070, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 88:  93%|███████▍| 62/67 [00:19<00:01,  3.16it/s, loss=1.81, v_num=1, train_loss_step=2.830, val_loss=3.070, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 88:  94%|███████▌| 63/67 [00:19<00:01,  3.16it/s, loss=1.81, v_num=1, train_loss_step=2.830, val_loss=3.070, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 88:  96%|███████▋| 64/67 [00:20<00:00,  3.15it/s, loss=1.81, v_num=1, train_loss_step=2.830, val_loss=3.070, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 88:  97%|███████▊| 65/67 [00:20<00:00,  3.15it/s, loss=1.81, v_num=1, train_loss_step=2.830, val_loss=3.070, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 88:  99%|███████▉| 66/67 [00:20<00:00,  3.15it/s, loss=1.81, v_num=1, train_loss_step=2.830, val_loss=3.070, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 88: 100%|████████| 67/67 [00:21<00:00,  3.09it/s, loss=1.81, v_num=1, train_loss_step=2.830, val_loss=3.070, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 89:  45%|███▌    | 30/67 [00:06<00:07,  4.86it/s, loss=2.31, v_num=1, train_loss_step=4.300, val_loss=3.070, train_loss_epoch=1.810]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 89:  46%|███▋    | 31/67 [00:08<00:09,  3.81it/s, loss=2.31, v_num=1, train_loss_step=4.300, val_loss=3.070, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 89:  48%|███▊    | 32/67 [00:08<00:09,  3.78it/s, loss=2.31, v_num=1, train_loss_step=4.300, val_loss=3.070, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 89:  49%|███▉    | 33/67 [00:08<00:09,  3.76it/s, loss=2.31, v_num=1, train_loss_step=4.300, val_loss=3.070, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 89:  51%|████    | 34/67 [00:09<00:08,  3.73it/s, loss=2.31, v_num=1, train_loss_step=4.300, val_loss=3.070, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 89:  52%|████▏   | 35/67 [00:09<00:08,  3.71it/s, loss=2.31, v_num=1, train_loss_step=4.300, val_loss=3.070, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 89:  54%|████▎   | 36/67 [00:09<00:08,  3.69it/s, loss=2.31, v_num=1, train_loss_step=4.300, val_loss=3.070, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 89:  55%|████▍   | 37/67 [00:10<00:08,  3.67it/s, loss=2.31, v_num=1, train_loss_step=4.300, val_loss=3.070, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 89:  57%|████▌   | 38/67 [00:10<00:07,  3.65it/s, loss=2.31, v_num=1, train_loss_step=4.300, val_loss=3.070, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 89:  58%|████▋   | 39/67 [00:10<00:07,  3.63it/s, loss=2.31, v_num=1, train_loss_step=4.300, val_loss=3.070, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 89:  60%|████▊   | 40/67 [00:11<00:07,  3.62it/s, loss=2.31, v_num=1, train_loss_step=4.300, val_loss=3.070, train_loss_epoch=1.810]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 89:  61%|████▉   | 41/67 [00:11<00:07,  3.56it/s, loss=2.31, v_num=1, train_loss_step=4.300, val_loss=3.070, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 89:  63%|█████   | 42/67 [00:11<00:07,  3.55it/s, loss=2.31, v_num=1, train_loss_step=4.300, val_loss=3.070, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 89:  64%|█████▏  | 43/67 [00:12<00:06,  3.53it/s, loss=2.31, v_num=1, train_loss_step=4.300, val_loss=3.070, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 89:  66%|█████▎  | 44/67 [00:12<00:06,  3.52it/s, loss=2.31, v_num=1, train_loss_step=4.300, val_loss=3.070, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 89:  67%|█████▎  | 45/67 [00:12<00:06,  3.51it/s, loss=2.31, v_num=1, train_loss_step=4.300, val_loss=3.070, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 89:  69%|█████▍  | 46/67 [00:13<00:06,  3.50it/s, loss=2.31, v_num=1, train_loss_step=4.300, val_loss=3.070, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 89:  70%|█████▌  | 47/67 [00:13<00:05,  3.49it/s, loss=2.31, v_num=1, train_loss_step=4.300, val_loss=3.070, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 89:  72%|█████▋  | 48/67 [00:13<00:05,  3.48it/s, loss=2.31, v_num=1, train_loss_step=4.300, val_loss=3.070, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 89:  73%|█████▊  | 49/67 [00:14<00:05,  3.46it/s, loss=2.31, v_num=1, train_loss_step=4.300, val_loss=3.070, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 89:  75%|█████▉  | 50/67 [00:14<00:04,  3.45it/s, loss=2.31, v_num=1, train_loss_step=4.300, val_loss=3.070, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 89:  76%|██████  | 51/67 [00:14<00:04,  3.41it/s, loss=2.31, v_num=1, train_loss_step=4.300, val_loss=3.070, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 89:  78%|██████▏ | 52/67 [00:15<00:04,  3.40it/s, loss=2.31, v_num=1, train_loss_step=4.300, val_loss=3.070, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 89:  79%|██████▎ | 53/67 [00:15<00:04,  3.39it/s, loss=2.31, v_num=1, train_loss_step=4.300, val_loss=3.070, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 89:  81%|██████▍ | 54/67 [00:15<00:03,  3.39it/s, loss=2.31, v_num=1, train_loss_step=4.300, val_loss=3.070, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 89:  82%|██████▌ | 55/67 [00:16<00:03,  3.37it/s, loss=2.31, v_num=1, train_loss_step=4.300, val_loss=3.070, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 89:  84%|██████▋ | 56/67 [00:16<00:03,  3.37it/s, loss=2.31, v_num=1, train_loss_step=4.300, val_loss=3.070, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 89:  85%|██████▊ | 57/67 [00:16<00:02,  3.36it/s, loss=2.31, v_num=1, train_loss_step=4.300, val_loss=3.070, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 89:  87%|██████▉ | 58/67 [00:17<00:02,  3.35it/s, loss=2.31, v_num=1, train_loss_step=4.300, val_loss=3.070, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 89:  88%|███████ | 59/67 [00:17<00:02,  3.35it/s, loss=2.31, v_num=1, train_loss_step=4.300, val_loss=3.070, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 89:  90%|███████▏| 60/67 [00:17<00:02,  3.34it/s, loss=2.31, v_num=1, train_loss_step=4.300, val_loss=3.070, train_loss_epoch=1.810]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 89:  91%|███████▎| 61/67 [00:18<00:01,  3.31it/s, loss=2.31, v_num=1, train_loss_step=4.300, val_loss=3.070, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 89:  93%|███████▍| 62/67 [00:18<00:01,  3.31it/s, loss=2.31, v_num=1, train_loss_step=4.300, val_loss=3.070, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 89:  94%|███████▌| 63/67 [00:19<00:01,  3.31it/s, loss=2.31, v_num=1, train_loss_step=4.300, val_loss=3.070, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 89:  96%|███████▋| 64/67 [00:19<00:00,  3.30it/s, loss=2.31, v_num=1, train_loss_step=4.300, val_loss=3.070, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 89:  97%|███████▊| 65/67 [00:19<00:00,  3.30it/s, loss=2.31, v_num=1, train_loss_step=4.300, val_loss=3.070, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 89:  99%|███████▉| 66/67 [00:20<00:00,  3.30it/s, loss=2.31, v_num=1, train_loss_step=4.300, val_loss=3.070, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 89: 100%|████████| 67/67 [00:20<00:00,  3.24it/s, loss=2.31, v_num=1, train_loss_step=4.300, val_loss=3.070, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 90:  45%|███▌    | 30/67 [00:06<00:07,  4.98it/s, loss=1.53, v_num=1, train_loss_step=1.690, val_loss=3.070, train_loss_epoch=2.190]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 90:  46%|███▋    | 31/67 [00:08<00:09,  3.86it/s, loss=1.53, v_num=1, train_loss_step=1.690, val_loss=3.070, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 90:  48%|███▊    | 32/67 [00:08<00:09,  3.83it/s, loss=1.53, v_num=1, train_loss_step=1.690, val_loss=3.070, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 90:  49%|███▉    | 33/67 [00:08<00:08,  3.79it/s, loss=1.53, v_num=1, train_loss_step=1.690, val_loss=3.070, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 90:  51%|████    | 34/67 [00:09<00:08,  3.76it/s, loss=1.53, v_num=1, train_loss_step=1.690, val_loss=3.070, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 90:  52%|████▏   | 35/67 [00:09<00:08,  3.71it/s, loss=1.53, v_num=1, train_loss_step=1.690, val_loss=3.070, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 90:  54%|████▎   | 36/67 [00:09<00:08,  3.66it/s, loss=1.53, v_num=1, train_loss_step=1.690, val_loss=3.070, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 90:  55%|████▍   | 37/67 [00:10<00:08,  3.61it/s, loss=1.53, v_num=1, train_loss_step=1.690, val_loss=3.070, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 90:  57%|████▌   | 38/67 [00:10<00:08,  3.58it/s, loss=1.53, v_num=1, train_loss_step=1.690, val_loss=3.070, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 90:  58%|████▋   | 39/67 [00:10<00:07,  3.56it/s, loss=1.53, v_num=1, train_loss_step=1.690, val_loss=3.070, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 90:  60%|████▊   | 40/67 [00:11<00:07,  3.55it/s, loss=1.53, v_num=1, train_loss_step=1.690, val_loss=3.070, train_loss_epoch=2.190]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 90:  61%|████▉   | 41/67 [00:11<00:07,  3.49it/s, loss=1.53, v_num=1, train_loss_step=1.690, val_loss=3.070, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 90:  63%|█████   | 42/67 [00:12<00:07,  3.48it/s, loss=1.53, v_num=1, train_loss_step=1.690, val_loss=3.070, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 90:  64%|█████▏  | 43/67 [00:12<00:06,  3.46it/s, loss=1.53, v_num=1, train_loss_step=1.690, val_loss=3.070, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 90:  66%|█████▎  | 44/67 [00:12<00:06,  3.45it/s, loss=1.53, v_num=1, train_loss_step=1.690, val_loss=3.070, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 90:  67%|█████▎  | 45/67 [00:13<00:06,  3.43it/s, loss=1.53, v_num=1, train_loss_step=1.690, val_loss=3.070, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 90:  69%|█████▍  | 46/67 [00:13<00:06,  3.42it/s, loss=1.53, v_num=1, train_loss_step=1.690, val_loss=3.070, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 90:  70%|█████▌  | 47/67 [00:13<00:05,  3.41it/s, loss=1.53, v_num=1, train_loss_step=1.690, val_loss=3.070, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 90:  72%|█████▋  | 48/67 [00:14<00:05,  3.40it/s, loss=1.53, v_num=1, train_loss_step=1.690, val_loss=3.070, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 90:  73%|█████▊  | 49/67 [00:14<00:05,  3.39it/s, loss=1.53, v_num=1, train_loss_step=1.690, val_loss=3.070, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 90:  75%|█████▉  | 50/67 [00:14<00:05,  3.38it/s, loss=1.53, v_num=1, train_loss_step=1.690, val_loss=3.070, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 90:  76%|██████  | 51/67 [00:15<00:04,  3.34it/s, loss=1.53, v_num=1, train_loss_step=1.690, val_loss=3.070, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 90:  78%|██████▏ | 52/67 [00:15<00:04,  3.33it/s, loss=1.53, v_num=1, train_loss_step=1.690, val_loss=3.070, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 90:  79%|██████▎ | 53/67 [00:15<00:04,  3.32it/s, loss=1.53, v_num=1, train_loss_step=1.690, val_loss=3.070, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 90:  81%|██████▍ | 54/67 [00:16<00:03,  3.31it/s, loss=1.53, v_num=1, train_loss_step=1.690, val_loss=3.070, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 90:  82%|██████▌ | 55/67 [00:16<00:03,  3.30it/s, loss=1.53, v_num=1, train_loss_step=1.690, val_loss=3.070, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 90:  84%|██████▋ | 56/67 [00:16<00:03,  3.30it/s, loss=1.53, v_num=1, train_loss_step=1.690, val_loss=3.070, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 90:  85%|██████▊ | 57/67 [00:17<00:03,  3.29it/s, loss=1.53, v_num=1, train_loss_step=1.690, val_loss=3.070, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 90:  87%|██████▉ | 58/67 [00:17<00:02,  3.29it/s, loss=1.53, v_num=1, train_loss_step=1.690, val_loss=3.070, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 90:  88%|███████ | 59/67 [00:17<00:02,  3.28it/s, loss=1.53, v_num=1, train_loss_step=1.690, val_loss=3.070, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 90:  90%|███████▏| 60/67 [00:18<00:02,  3.28it/s, loss=1.53, v_num=1, train_loss_step=1.690, val_loss=3.070, train_loss_epoch=2.190]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 90:  91%|███████▎| 61/67 [00:18<00:01,  3.25it/s, loss=1.53, v_num=1, train_loss_step=1.690, val_loss=3.070, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 90:  93%|███████▍| 62/67 [00:19<00:01,  3.24it/s, loss=1.53, v_num=1, train_loss_step=1.690, val_loss=3.070, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 90:  94%|███████▌| 63/67 [00:19<00:01,  3.24it/s, loss=1.53, v_num=1, train_loss_step=1.690, val_loss=3.070, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 90:  96%|███████▋| 64/67 [00:19<00:00,  3.24it/s, loss=1.53, v_num=1, train_loss_step=1.690, val_loss=3.070, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 90:  97%|███████▊| 65/67 [00:20<00:00,  3.24it/s, loss=1.53, v_num=1, train_loss_step=1.690, val_loss=3.070, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 90:  99%|███████▉| 66/67 [00:20<00:00,  3.23it/s, loss=1.53, v_num=1, train_loss_step=1.690, val_loss=3.070, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 90: 100%|████████| 67/67 [00:21<00:00,  3.17it/s, loss=1.53, v_num=1, train_loss_step=1.690, val_loss=3.070, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 91:  45%|████▉      | 30/67 [00:06<00:07,  4.87it/s, loss=2, v_num=1, train_loss_step=3.210, val_loss=3.070, train_loss_epoch=1.720]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 91:  46%|█████      | 31/67 [00:08<00:09,  3.78it/s, loss=2, v_num=1, train_loss_step=3.210, val_loss=3.070, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 91:  48%|█████▎     | 32/67 [00:08<00:09,  3.76it/s, loss=2, v_num=1, train_loss_step=3.210, val_loss=3.070, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 91:  49%|█████▍     | 33/67 [00:08<00:09,  3.73it/s, loss=2, v_num=1, train_loss_step=3.210, val_loss=3.070, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 91:  51%|█████▌     | 34/67 [00:09<00:08,  3.70it/s, loss=2, v_num=1, train_loss_step=3.210, val_loss=3.070, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 91:  52%|█████▋     | 35/67 [00:09<00:08,  3.68it/s, loss=2, v_num=1, train_loss_step=3.210, val_loss=3.070, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 91:  54%|█████▉     | 36/67 [00:09<00:08,  3.65it/s, loss=2, v_num=1, train_loss_step=3.210, val_loss=3.070, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 91:  55%|██████     | 37/67 [00:10<00:08,  3.63it/s, loss=2, v_num=1, train_loss_step=3.210, val_loss=3.070, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 91:  57%|██████▏    | 38/67 [00:10<00:08,  3.62it/s, loss=2, v_num=1, train_loss_step=3.210, val_loss=3.070, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 91:  58%|██████▍    | 39/67 [00:10<00:07,  3.60it/s, loss=2, v_num=1, train_loss_step=3.210, val_loss=3.070, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 91:  60%|██████▌    | 40/67 [00:11<00:07,  3.58it/s, loss=2, v_num=1, train_loss_step=3.210, val_loss=3.070, train_loss_epoch=1.720]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 91:  61%|██████▋    | 41/67 [00:11<00:07,  3.53it/s, loss=2, v_num=1, train_loss_step=3.210, val_loss=3.070, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 91:  63%|██████▉    | 42/67 [00:11<00:07,  3.51it/s, loss=2, v_num=1, train_loss_step=3.210, val_loss=3.070, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 91:  64%|███████    | 43/67 [00:12<00:06,  3.49it/s, loss=2, v_num=1, train_loss_step=3.210, val_loss=3.070, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 91:  66%|███████▏   | 44/67 [00:12<00:06,  3.48it/s, loss=2, v_num=1, train_loss_step=3.210, val_loss=3.070, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 91:  67%|███████▍   | 45/67 [00:12<00:06,  3.47it/s, loss=2, v_num=1, train_loss_step=3.210, val_loss=3.070, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 91:  69%|███████▌   | 46/67 [00:13<00:06,  3.45it/s, loss=2, v_num=1, train_loss_step=3.210, val_loss=3.070, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 91:  70%|███████▋   | 47/67 [00:13<00:05,  3.44it/s, loss=2, v_num=1, train_loss_step=3.210, val_loss=3.070, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 91:  72%|███████▉   | 48/67 [00:13<00:05,  3.43it/s, loss=2, v_num=1, train_loss_step=3.210, val_loss=3.070, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 91:  73%|████████   | 49/67 [00:14<00:05,  3.42it/s, loss=2, v_num=1, train_loss_step=3.210, val_loss=3.070, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 91:  75%|████████▏  | 50/67 [00:14<00:04,  3.41it/s, loss=2, v_num=1, train_loss_step=3.210, val_loss=3.070, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 91:  76%|████████▎  | 51/67 [00:15<00:04,  3.36it/s, loss=2, v_num=1, train_loss_step=3.210, val_loss=3.070, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 91:  78%|████████▌  | 52/67 [00:15<00:04,  3.35it/s, loss=2, v_num=1, train_loss_step=3.210, val_loss=3.070, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 91:  79%|████████▋  | 53/67 [00:15<00:04,  3.34it/s, loss=2, v_num=1, train_loss_step=3.210, val_loss=3.070, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 91:  81%|████████▊  | 54/67 [00:16<00:03,  3.33it/s, loss=2, v_num=1, train_loss_step=3.210, val_loss=3.070, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 91:  82%|█████████  | 55/67 [00:16<00:03,  3.32it/s, loss=2, v_num=1, train_loss_step=3.210, val_loss=3.070, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 91:  84%|█████████▏ | 56/67 [00:16<00:03,  3.31it/s, loss=2, v_num=1, train_loss_step=3.210, val_loss=3.070, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 91:  85%|█████████▎ | 57/67 [00:17<00:03,  3.30it/s, loss=2, v_num=1, train_loss_step=3.210, val_loss=3.070, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 91:  87%|█████████▌ | 58/67 [00:17<00:02,  3.29it/s, loss=2, v_num=1, train_loss_step=3.210, val_loss=3.070, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 91:  88%|█████████▋ | 59/67 [00:17<00:02,  3.29it/s, loss=2, v_num=1, train_loss_step=3.210, val_loss=3.070, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 91:  90%|█████████▊ | 60/67 [00:18<00:02,  3.28it/s, loss=2, v_num=1, train_loss_step=3.210, val_loss=3.070, train_loss_epoch=1.720]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 91:  91%|██████████ | 61/67 [00:18<00:01,  3.25it/s, loss=2, v_num=1, train_loss_step=3.210, val_loss=3.070, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 91:  93%|██████████▏| 62/67 [00:19<00:01,  3.25it/s, loss=2, v_num=1, train_loss_step=3.210, val_loss=3.070, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 91:  94%|██████████▎| 63/67 [00:19<00:01,  3.24it/s, loss=2, v_num=1, train_loss_step=3.210, val_loss=3.070, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 91:  96%|██████████▌| 64/67 [00:19<00:00,  3.24it/s, loss=2, v_num=1, train_loss_step=3.210, val_loss=3.070, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 91:  97%|██████████▋| 65/67 [00:20<00:00,  3.23it/s, loss=2, v_num=1, train_loss_step=3.210, val_loss=3.070, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 91:  99%|██████████▊| 66/67 [00:20<00:00,  3.23it/s, loss=2, v_num=1, train_loss_step=3.210, val_loss=3.070, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 91: 100%|███████████| 67/67 [00:21<00:00,  3.17it/s, loss=2, v_num=1, train_loss_step=3.210, val_loss=3.070, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 92:  45%|███▌    | 30/67 [00:06<00:07,  4.73it/s, loss=2.04, v_num=1, train_loss_step=2.190, val_loss=3.070, train_loss_epoch=2.040]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 92:  46%|███▋    | 31/67 [00:08<00:09,  3.67it/s, loss=2.04, v_num=1, train_loss_step=2.190, val_loss=3.070, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 92:  48%|███▊    | 32/67 [00:08<00:09,  3.64it/s, loss=2.04, v_num=1, train_loss_step=2.190, val_loss=3.070, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 92:  49%|███▉    | 33/67 [00:09<00:09,  3.62it/s, loss=2.04, v_num=1, train_loss_step=2.190, val_loss=3.070, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 92:  51%|████    | 34/67 [00:09<00:09,  3.59it/s, loss=2.04, v_num=1, train_loss_step=2.190, val_loss=3.070, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 92:  52%|████▏   | 35/67 [00:09<00:08,  3.56it/s, loss=2.04, v_num=1, train_loss_step=2.190, val_loss=3.070, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 92:  54%|████▎   | 36/67 [00:10<00:08,  3.54it/s, loss=2.04, v_num=1, train_loss_step=2.190, val_loss=3.070, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 92:  55%|████▍   | 37/67 [00:10<00:08,  3.50it/s, loss=2.04, v_num=1, train_loss_step=2.190, val_loss=3.070, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 92:  57%|████▌   | 38/67 [00:10<00:08,  3.48it/s, loss=2.04, v_num=1, train_loss_step=2.190, val_loss=3.070, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 92:  58%|████▋   | 39/67 [00:11<00:08,  3.46it/s, loss=2.04, v_num=1, train_loss_step=2.190, val_loss=3.070, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 92:  60%|████▊   | 40/67 [00:11<00:07,  3.44it/s, loss=2.04, v_num=1, train_loss_step=2.190, val_loss=3.070, train_loss_epoch=2.040]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 92:  61%|████▉   | 41/67 [00:12<00:07,  3.39it/s, loss=2.04, v_num=1, train_loss_step=2.190, val_loss=3.070, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 92:  63%|█████   | 42/67 [00:12<00:07,  3.37it/s, loss=2.04, v_num=1, train_loss_step=2.190, val_loss=3.070, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 92:  64%|█████▏  | 43/67 [00:12<00:07,  3.36it/s, loss=2.04, v_num=1, train_loss_step=2.190, val_loss=3.070, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 92:  66%|█████▎  | 44/67 [00:13<00:06,  3.35it/s, loss=2.04, v_num=1, train_loss_step=2.190, val_loss=3.070, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 92:  67%|█████▎  | 45/67 [00:13<00:06,  3.34it/s, loss=2.04, v_num=1, train_loss_step=2.190, val_loss=3.070, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 92:  69%|█████▍  | 46/67 [00:13<00:06,  3.33it/s, loss=2.04, v_num=1, train_loss_step=2.190, val_loss=3.070, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 92:  70%|█████▌  | 47/67 [00:14<00:06,  3.32it/s, loss=2.04, v_num=1, train_loss_step=2.190, val_loss=3.070, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 92:  72%|█████▋  | 48/67 [00:14<00:05,  3.31it/s, loss=2.04, v_num=1, train_loss_step=2.190, val_loss=3.070, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 92:  73%|█████▊  | 49/67 [00:14<00:05,  3.30it/s, loss=2.04, v_num=1, train_loss_step=2.190, val_loss=3.070, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 92:  75%|█████▉  | 50/67 [00:15<00:05,  3.29it/s, loss=2.04, v_num=1, train_loss_step=2.190, val_loss=3.070, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 92:  76%|██████  | 51/67 [00:15<00:04,  3.26it/s, loss=2.04, v_num=1, train_loss_step=2.190, val_loss=3.070, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 92:  78%|██████▏ | 52/67 [00:16<00:04,  3.25it/s, loss=2.04, v_num=1, train_loss_step=2.190, val_loss=3.070, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 92:  79%|██████▎ | 53/67 [00:16<00:04,  3.24it/s, loss=2.04, v_num=1, train_loss_step=2.190, val_loss=3.070, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 92:  81%|██████▍ | 54/67 [00:16<00:04,  3.23it/s, loss=2.04, v_num=1, train_loss_step=2.190, val_loss=3.070, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 92:  82%|██████▌ | 55/67 [00:17<00:03,  3.21it/s, loss=2.04, v_num=1, train_loss_step=2.190, val_loss=3.070, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 92:  84%|██████▋ | 56/67 [00:17<00:03,  3.20it/s, loss=2.04, v_num=1, train_loss_step=2.190, val_loss=3.070, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 92:  85%|██████▊ | 57/67 [00:17<00:03,  3.19it/s, loss=2.04, v_num=1, train_loss_step=2.190, val_loss=3.070, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 92:  87%|██████▉ | 58/67 [00:18<00:02,  3.16it/s, loss=2.04, v_num=1, train_loss_step=2.190, val_loss=3.070, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 92:  88%|███████ | 59/67 [00:18<00:02,  3.16it/s, loss=2.04, v_num=1, train_loss_step=2.190, val_loss=3.070, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 92:  90%|███████▏| 60/67 [00:19<00:02,  3.15it/s, loss=2.04, v_num=1, train_loss_step=2.190, val_loss=3.070, train_loss_epoch=2.040]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 92:  91%|███████▎| 61/67 [00:19<00:01,  3.11it/s, loss=2.04, v_num=1, train_loss_step=2.190, val_loss=3.070, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 92:  93%|███████▍| 62/67 [00:19<00:01,  3.10it/s, loss=2.04, v_num=1, train_loss_step=2.190, val_loss=3.070, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 92:  94%|███████▌| 63/67 [00:20<00:01,  3.09it/s, loss=2.04, v_num=1, train_loss_step=2.190, val_loss=3.070, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 92:  96%|███████▋| 64/67 [00:20<00:00,  3.09it/s, loss=2.04, v_num=1, train_loss_step=2.190, val_loss=3.070, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 92:  97%|███████▊| 65/67 [00:21<00:00,  3.09it/s, loss=2.04, v_num=1, train_loss_step=2.190, val_loss=3.070, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 92:  99%|███████▉| 66/67 [00:21<00:00,  3.08it/s, loss=2.04, v_num=1, train_loss_step=2.190, val_loss=3.070, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 92: 100%|████████| 67/67 [00:22<00:00,  3.03it/s, loss=2.04, v_num=1, train_loss_step=2.190, val_loss=3.060, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 93:  45%|███▌    | 30/67 [00:06<00:07,  4.69it/s, loss=2.13, v_num=1, train_loss_step=2.980, val_loss=3.060, train_loss_epoch=2.310]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 93:  46%|███▋    | 31/67 [00:08<00:09,  3.62it/s, loss=2.13, v_num=1, train_loss_step=2.980, val_loss=3.060, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 93:  48%|███▊    | 32/67 [00:08<00:09,  3.58it/s, loss=2.13, v_num=1, train_loss_step=2.980, val_loss=3.060, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 93:  49%|███▉    | 33/67 [00:09<00:09,  3.56it/s, loss=2.13, v_num=1, train_loss_step=2.980, val_loss=3.060, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 93:  51%|████    | 34/67 [00:09<00:09,  3.53it/s, loss=2.13, v_num=1, train_loss_step=2.980, val_loss=3.060, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 93:  52%|████▏   | 35/67 [00:09<00:09,  3.52it/s, loss=2.13, v_num=1, train_loss_step=2.980, val_loss=3.060, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 93:  54%|████▎   | 36/67 [00:10<00:08,  3.49it/s, loss=2.13, v_num=1, train_loss_step=2.980, val_loss=3.060, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 93:  55%|████▍   | 37/67 [00:10<00:08,  3.45it/s, loss=2.13, v_num=1, train_loss_step=2.980, val_loss=3.060, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 93:  57%|████▌   | 38/67 [00:11<00:08,  3.43it/s, loss=2.13, v_num=1, train_loss_step=2.980, val_loss=3.060, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 93:  58%|████▋   | 39/67 [00:11<00:08,  3.41it/s, loss=2.13, v_num=1, train_loss_step=2.980, val_loss=3.060, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 93:  60%|████▊   | 40/67 [00:11<00:07,  3.39it/s, loss=2.13, v_num=1, train_loss_step=2.980, val_loss=3.060, train_loss_epoch=2.310]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 93:  61%|████▉   | 41/67 [00:12<00:07,  3.33it/s, loss=2.13, v_num=1, train_loss_step=2.980, val_loss=3.060, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 93:  63%|█████   | 42/67 [00:12<00:07,  3.32it/s, loss=2.13, v_num=1, train_loss_step=2.980, val_loss=3.060, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 93:  64%|█████▏  | 43/67 [00:13<00:07,  3.30it/s, loss=2.13, v_num=1, train_loss_step=2.980, val_loss=3.060, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 93:  66%|█████▎  | 44/67 [00:13<00:06,  3.29it/s, loss=2.13, v_num=1, train_loss_step=2.980, val_loss=3.060, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 93:  67%|█████▎  | 45/67 [00:13<00:06,  3.28it/s, loss=2.13, v_num=1, train_loss_step=2.980, val_loss=3.060, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 93:  69%|█████▍  | 46/67 [00:14<00:06,  3.26it/s, loss=2.13, v_num=1, train_loss_step=2.980, val_loss=3.060, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 93:  70%|█████▌  | 47/67 [00:14<00:06,  3.26it/s, loss=2.13, v_num=1, train_loss_step=2.980, val_loss=3.060, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 93:  72%|█████▋  | 48/67 [00:14<00:05,  3.25it/s, loss=2.13, v_num=1, train_loss_step=2.980, val_loss=3.060, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 93:  73%|█████▊  | 49/67 [00:15<00:05,  3.24it/s, loss=2.13, v_num=1, train_loss_step=2.980, val_loss=3.060, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 93:  75%|█████▉  | 50/67 [00:15<00:05,  3.23it/s, loss=2.13, v_num=1, train_loss_step=2.980, val_loss=3.060, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 93:  76%|██████  | 51/67 [00:15<00:05,  3.19it/s, loss=2.13, v_num=1, train_loss_step=2.980, val_loss=3.060, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 93:  78%|██████▏ | 52/67 [00:16<00:04,  3.18it/s, loss=2.13, v_num=1, train_loss_step=2.980, val_loss=3.060, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 93:  79%|██████▎ | 53/67 [00:16<00:04,  3.18it/s, loss=2.13, v_num=1, train_loss_step=2.980, val_loss=3.060, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 93:  81%|██████▍ | 54/67 [00:16<00:04,  3.18it/s, loss=2.13, v_num=1, train_loss_step=2.980, val_loss=3.060, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 93:  82%|██████▌ | 55/67 [00:17<00:03,  3.17it/s, loss=2.13, v_num=1, train_loss_step=2.980, val_loss=3.060, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 93:  84%|██████▋ | 56/67 [00:17<00:03,  3.16it/s, loss=2.13, v_num=1, train_loss_step=2.980, val_loss=3.060, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 93:  85%|██████▊ | 57/67 [00:18<00:03,  3.16it/s, loss=2.13, v_num=1, train_loss_step=2.980, val_loss=3.060, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 93:  87%|██████▉ | 58/67 [00:18<00:02,  3.15it/s, loss=2.13, v_num=1, train_loss_step=2.980, val_loss=3.060, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 93:  88%|███████ | 59/67 [00:18<00:02,  3.15it/s, loss=2.13, v_num=1, train_loss_step=2.980, val_loss=3.060, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 93:  90%|███████▏| 60/67 [00:19<00:02,  3.15it/s, loss=2.13, v_num=1, train_loss_step=2.980, val_loss=3.060, train_loss_epoch=2.310]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 93:  91%|███████▎| 61/67 [00:19<00:01,  3.12it/s, loss=2.13, v_num=1, train_loss_step=2.980, val_loss=3.060, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 93:  93%|███████▍| 62/67 [00:19<00:01,  3.12it/s, loss=2.13, v_num=1, train_loss_step=2.980, val_loss=3.060, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 93:  94%|███████▌| 63/67 [00:20<00:01,  3.12it/s, loss=2.13, v_num=1, train_loss_step=2.980, val_loss=3.060, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 93:  96%|███████▋| 64/67 [00:20<00:00,  3.11it/s, loss=2.13, v_num=1, train_loss_step=2.980, val_loss=3.060, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 93:  97%|███████▊| 65/67 [00:20<00:00,  3.11it/s, loss=2.13, v_num=1, train_loss_step=2.980, val_loss=3.060, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 93:  99%|███████▉| 66/67 [00:21<00:00,  3.11it/s, loss=2.13, v_num=1, train_loss_step=2.980, val_loss=3.060, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 93: 100%|████████| 67/67 [00:21<00:00,  3.06it/s, loss=2.13, v_num=1, train_loss_step=2.980, val_loss=3.060, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 94:  45%|███▌    | 30/67 [00:05<00:07,  5.17it/s, loss=1.71, v_num=1, train_loss_step=0.310, val_loss=3.060, train_loss_epoch=2.080]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 94:  46%|███▋    | 31/67 [00:07<00:09,  3.95it/s, loss=1.71, v_num=1, train_loss_step=0.310, val_loss=3.060, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 94:  48%|███▊    | 32/67 [00:08<00:08,  3.90it/s, loss=1.71, v_num=1, train_loss_step=0.310, val_loss=3.060, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 94:  49%|███▉    | 33/67 [00:08<00:08,  3.85it/s, loss=1.71, v_num=1, train_loss_step=0.310, val_loss=3.060, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 94:  51%|████    | 34/67 [00:08<00:08,  3.81it/s, loss=1.71, v_num=1, train_loss_step=0.310, val_loss=3.060, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 94:  52%|████▏   | 35/67 [00:09<00:08,  3.77it/s, loss=1.71, v_num=1, train_loss_step=0.310, val_loss=3.060, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 94:  54%|████▎   | 36/67 [00:09<00:08,  3.74it/s, loss=1.71, v_num=1, train_loss_step=0.310, val_loss=3.060, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 94:  55%|████▍   | 37/67 [00:10<00:08,  3.70it/s, loss=1.71, v_num=1, train_loss_step=0.310, val_loss=3.060, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 94:  57%|████▌   | 38/67 [00:10<00:07,  3.67it/s, loss=1.71, v_num=1, train_loss_step=0.310, val_loss=3.060, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 94:  58%|████▋   | 39/67 [00:10<00:07,  3.65it/s, loss=1.71, v_num=1, train_loss_step=0.310, val_loss=3.060, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 94:  60%|████▊   | 40/67 [00:11<00:07,  3.63it/s, loss=1.71, v_num=1, train_loss_step=0.310, val_loss=3.060, train_loss_epoch=2.080]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 94:  61%|████▉   | 41/67 [00:11<00:07,  3.56it/s, loss=1.71, v_num=1, train_loss_step=0.310, val_loss=3.060, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 94:  63%|█████   | 42/67 [00:11<00:07,  3.55it/s, loss=1.71, v_num=1, train_loss_step=0.310, val_loss=3.060, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 94:  64%|█████▏  | 43/67 [00:12<00:06,  3.53it/s, loss=1.71, v_num=1, train_loss_step=0.310, val_loss=3.060, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 94:  66%|█████▎  | 44/67 [00:12<00:06,  3.51it/s, loss=1.71, v_num=1, train_loss_step=0.310, val_loss=3.060, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 94:  67%|█████▎  | 45/67 [00:12<00:06,  3.50it/s, loss=1.71, v_num=1, train_loss_step=0.310, val_loss=3.060, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 94:  69%|█████▍  | 46/67 [00:13<00:06,  3.48it/s, loss=1.71, v_num=1, train_loss_step=0.310, val_loss=3.060, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 94:  70%|█████▌  | 47/67 [00:13<00:05,  3.47it/s, loss=1.71, v_num=1, train_loss_step=0.310, val_loss=3.060, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 94:  72%|█████▋  | 48/67 [00:13<00:05,  3.46it/s, loss=1.71, v_num=1, train_loss_step=0.310, val_loss=3.060, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 94:  73%|█████▊  | 49/67 [00:14<00:05,  3.44it/s, loss=1.71, v_num=1, train_loss_step=0.310, val_loss=3.060, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 94:  75%|█████▉  | 50/67 [00:14<00:04,  3.42it/s, loss=1.71, v_num=1, train_loss_step=0.310, val_loss=3.060, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 94:  76%|██████  | 51/67 [00:15<00:04,  3.38it/s, loss=1.71, v_num=1, train_loss_step=0.310, val_loss=3.060, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 94:  78%|██████▏ | 52/67 [00:15<00:04,  3.37it/s, loss=1.71, v_num=1, train_loss_step=0.310, val_loss=3.060, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 94:  79%|██████▎ | 53/67 [00:15<00:04,  3.36it/s, loss=1.71, v_num=1, train_loss_step=0.310, val_loss=3.060, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 94:  81%|██████▍ | 54/67 [00:16<00:03,  3.35it/s, loss=1.71, v_num=1, train_loss_step=0.310, val_loss=3.060, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 94:  82%|██████▌ | 55/67 [00:16<00:03,  3.35it/s, loss=1.71, v_num=1, train_loss_step=0.310, val_loss=3.060, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 94:  84%|██████▋ | 56/67 [00:16<00:03,  3.34it/s, loss=1.71, v_num=1, train_loss_step=0.310, val_loss=3.060, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 94:  85%|██████▊ | 57/67 [00:17<00:02,  3.33it/s, loss=1.71, v_num=1, train_loss_step=0.310, val_loss=3.060, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 94:  87%|██████▉ | 58/67 [00:17<00:02,  3.33it/s, loss=1.71, v_num=1, train_loss_step=0.310, val_loss=3.060, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 94:  88%|███████ | 59/67 [00:17<00:02,  3.32it/s, loss=1.71, v_num=1, train_loss_step=0.310, val_loss=3.060, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 94:  90%|███████▏| 60/67 [00:18<00:02,  3.31it/s, loss=1.71, v_num=1, train_loss_step=0.310, val_loss=3.060, train_loss_epoch=2.080]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 94:  91%|███████▎| 61/67 [00:18<00:01,  3.28it/s, loss=1.71, v_num=1, train_loss_step=0.310, val_loss=3.060, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 94:  93%|███████▍| 62/67 [00:18<00:01,  3.28it/s, loss=1.71, v_num=1, train_loss_step=0.310, val_loss=3.060, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 94:  94%|███████▌| 63/67 [00:19<00:01,  3.27it/s, loss=1.71, v_num=1, train_loss_step=0.310, val_loss=3.060, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 94:  96%|███████▋| 64/67 [00:19<00:00,  3.27it/s, loss=1.71, v_num=1, train_loss_step=0.310, val_loss=3.060, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 94:  97%|███████▊| 65/67 [00:19<00:00,  3.27it/s, loss=1.71, v_num=1, train_loss_step=0.310, val_loss=3.060, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 94:  99%|███████▉| 66/67 [00:20<00:00,  3.27it/s, loss=1.71, v_num=1, train_loss_step=0.310, val_loss=3.060, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 94: 100%|████████| 67/67 [00:20<00:00,  3.21it/s, loss=1.71, v_num=1, train_loss_step=0.310, val_loss=3.060, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 95:  45%|███▌    | 30/67 [00:05<00:06,  5.35it/s, loss=2.85, v_num=1, train_loss_step=3.650, val_loss=3.060, train_loss_epoch=2.110]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 95:  46%|███▋    | 31/67 [00:07<00:08,  4.10it/s, loss=2.85, v_num=1, train_loss_step=3.650, val_loss=3.060, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 95:  48%|███▊    | 32/67 [00:07<00:08,  4.05it/s, loss=2.85, v_num=1, train_loss_step=3.650, val_loss=3.060, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 95:  49%|███▉    | 33/67 [00:08<00:08,  4.01it/s, loss=2.85, v_num=1, train_loss_step=3.650, val_loss=3.060, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 95:  51%|████    | 34/67 [00:08<00:08,  3.96it/s, loss=2.85, v_num=1, train_loss_step=3.650, val_loss=3.060, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 95:  52%|████▏   | 35/67 [00:08<00:08,  3.93it/s, loss=2.85, v_num=1, train_loss_step=3.650, val_loss=3.060, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 95:  54%|████▎   | 36/67 [00:09<00:07,  3.90it/s, loss=2.85, v_num=1, train_loss_step=3.650, val_loss=3.060, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 95:  55%|████▍   | 37/67 [00:09<00:07,  3.86it/s, loss=2.85, v_num=1, train_loss_step=3.650, val_loss=3.060, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 95:  57%|████▌   | 38/67 [00:09<00:07,  3.83it/s, loss=2.85, v_num=1, train_loss_step=3.650, val_loss=3.060, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 95:  58%|████▋   | 39/67 [00:10<00:07,  3.81it/s, loss=2.85, v_num=1, train_loss_step=3.650, val_loss=3.060, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 95:  60%|████▊   | 40/67 [00:10<00:07,  3.78it/s, loss=2.85, v_num=1, train_loss_step=3.650, val_loss=3.060, train_loss_epoch=2.110]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 95:  61%|████▉   | 41/67 [00:11<00:07,  3.70it/s, loss=2.85, v_num=1, train_loss_step=3.650, val_loss=3.060, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 95:  63%|█████   | 42/67 [00:11<00:06,  3.68it/s, loss=2.85, v_num=1, train_loss_step=3.650, val_loss=3.060, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 95:  64%|█████▏  | 43/67 [00:11<00:06,  3.66it/s, loss=2.85, v_num=1, train_loss_step=3.650, val_loss=3.060, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 95:  66%|█████▎  | 44/67 [00:12<00:06,  3.64it/s, loss=2.85, v_num=1, train_loss_step=3.650, val_loss=3.060, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 95:  67%|█████▎  | 45/67 [00:12<00:06,  3.63it/s, loss=2.85, v_num=1, train_loss_step=3.650, val_loss=3.060, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 95:  69%|█████▍  | 46/67 [00:12<00:05,  3.60it/s, loss=2.85, v_num=1, train_loss_step=3.650, val_loss=3.060, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 95:  70%|█████▌  | 47/67 [00:13<00:05,  3.59it/s, loss=2.85, v_num=1, train_loss_step=3.650, val_loss=3.060, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 95:  72%|█████▋  | 48/67 [00:13<00:05,  3.58it/s, loss=2.85, v_num=1, train_loss_step=3.650, val_loss=3.060, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 95:  73%|█████▊  | 49/67 [00:13<00:05,  3.56it/s, loss=2.85, v_num=1, train_loss_step=3.650, val_loss=3.060, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 95:  75%|█████▉  | 50/67 [00:14<00:04,  3.55it/s, loss=2.85, v_num=1, train_loss_step=3.650, val_loss=3.060, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 95:  76%|██████  | 51/67 [00:14<00:04,  3.50it/s, loss=2.85, v_num=1, train_loss_step=3.650, val_loss=3.060, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 95:  78%|██████▏ | 52/67 [00:14<00:04,  3.49it/s, loss=2.85, v_num=1, train_loss_step=3.650, val_loss=3.060, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 95:  79%|██████▎ | 53/67 [00:15<00:04,  3.48it/s, loss=2.85, v_num=1, train_loss_step=3.650, val_loss=3.060, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 95:  81%|██████▍ | 54/67 [00:15<00:03,  3.47it/s, loss=2.85, v_num=1, train_loss_step=3.650, val_loss=3.060, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 95:  82%|██████▌ | 55/67 [00:15<00:03,  3.46it/s, loss=2.85, v_num=1, train_loss_step=3.650, val_loss=3.060, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 95:  84%|██████▋ | 56/67 [00:16<00:03,  3.45it/s, loss=2.85, v_num=1, train_loss_step=3.650, val_loss=3.060, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 95:  85%|██████▊ | 57/67 [00:16<00:02,  3.44it/s, loss=2.85, v_num=1, train_loss_step=3.650, val_loss=3.060, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 95:  87%|██████▉ | 58/67 [00:16<00:02,  3.43it/s, loss=2.85, v_num=1, train_loss_step=3.650, val_loss=3.060, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 95:  88%|███████ | 59/67 [00:17<00:02,  3.42it/s, loss=2.85, v_num=1, train_loss_step=3.650, val_loss=3.060, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 95:  90%|███████▏| 60/67 [00:17<00:02,  3.41it/s, loss=2.85, v_num=1, train_loss_step=3.650, val_loss=3.060, train_loss_epoch=2.110]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 95:  91%|███████▎| 61/67 [00:18<00:01,  3.38it/s, loss=2.85, v_num=1, train_loss_step=3.650, val_loss=3.060, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 95:  93%|███████▍| 62/67 [00:18<00:01,  3.37it/s, loss=2.85, v_num=1, train_loss_step=3.650, val_loss=3.060, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 95:  94%|███████▌| 63/67 [00:18<00:01,  3.37it/s, loss=2.85, v_num=1, train_loss_step=3.650, val_loss=3.060, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 95:  96%|███████▋| 64/67 [00:19<00:00,  3.36it/s, loss=2.85, v_num=1, train_loss_step=3.650, val_loss=3.060, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 95:  97%|███████▊| 65/67 [00:19<00:00,  3.36it/s, loss=2.85, v_num=1, train_loss_step=3.650, val_loss=3.060, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 95:  99%|███████▉| 66/67 [00:19<00:00,  3.35it/s, loss=2.85, v_num=1, train_loss_step=3.650, val_loss=3.060, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 95: 100%|████████| 67/67 [00:20<00:00,  3.29it/s, loss=2.85, v_num=1, train_loss_step=3.650, val_loss=3.060, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 96:  45%|███▌    | 30/67 [00:05<00:06,  5.34it/s, loss=2.28, v_num=1, train_loss_step=1.780, val_loss=3.060, train_loss_epoch=2.690]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 96:  46%|███▋    | 31/67 [00:07<00:08,  4.03it/s, loss=2.28, v_num=1, train_loss_step=1.780, val_loss=3.060, train_loss_epoch=2.690]\u001b[A\n",
      "Epoch 96:  48%|███▊    | 32/67 [00:08<00:08,  3.99it/s, loss=2.28, v_num=1, train_loss_step=1.780, val_loss=3.060, train_loss_epoch=2.690]\u001b[A\n",
      "Epoch 96:  49%|███▉    | 33/67 [00:08<00:08,  3.95it/s, loss=2.28, v_num=1, train_loss_step=1.780, val_loss=3.060, train_loss_epoch=2.690]\u001b[A\n",
      "Epoch 96:  51%|████    | 34/67 [00:08<00:08,  3.90it/s, loss=2.28, v_num=1, train_loss_step=1.780, val_loss=3.060, train_loss_epoch=2.690]\u001b[A\n",
      "Epoch 96:  52%|████▏   | 35/67 [00:09<00:08,  3.86it/s, loss=2.28, v_num=1, train_loss_step=1.780, val_loss=3.060, train_loss_epoch=2.690]\u001b[A\n",
      "Epoch 96:  54%|████▎   | 36/67 [00:09<00:08,  3.82it/s, loss=2.28, v_num=1, train_loss_step=1.780, val_loss=3.060, train_loss_epoch=2.690]\u001b[A\n",
      "Epoch 96:  55%|████▍   | 37/67 [00:09<00:07,  3.79it/s, loss=2.28, v_num=1, train_loss_step=1.780, val_loss=3.060, train_loss_epoch=2.690]\u001b[A\n",
      "Epoch 96:  57%|████▌   | 38/67 [00:10<00:07,  3.76it/s, loss=2.28, v_num=1, train_loss_step=1.780, val_loss=3.060, train_loss_epoch=2.690]\u001b[A\n",
      "Epoch 96:  58%|████▋   | 39/67 [00:10<00:07,  3.73it/s, loss=2.28, v_num=1, train_loss_step=1.780, val_loss=3.060, train_loss_epoch=2.690]\u001b[A\n",
      "Epoch 96:  60%|████▊   | 40/67 [00:10<00:07,  3.71it/s, loss=2.28, v_num=1, train_loss_step=1.780, val_loss=3.060, train_loss_epoch=2.690]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 96:  61%|████▉   | 41/67 [00:11<00:07,  3.64it/s, loss=2.28, v_num=1, train_loss_step=1.780, val_loss=3.060, train_loss_epoch=2.690]\u001b[A\n",
      "Epoch 96:  63%|█████   | 42/67 [00:11<00:06,  3.63it/s, loss=2.28, v_num=1, train_loss_step=1.780, val_loss=3.060, train_loss_epoch=2.690]\u001b[A\n",
      "Epoch 96:  64%|█████▏  | 43/67 [00:11<00:06,  3.60it/s, loss=2.28, v_num=1, train_loss_step=1.780, val_loss=3.060, train_loss_epoch=2.690]\u001b[A\n",
      "Epoch 96:  66%|█████▎  | 44/67 [00:12<00:06,  3.58it/s, loss=2.28, v_num=1, train_loss_step=1.780, val_loss=3.060, train_loss_epoch=2.690]\u001b[A\n",
      "Epoch 96:  67%|█████▎  | 45/67 [00:12<00:06,  3.55it/s, loss=2.28, v_num=1, train_loss_step=1.780, val_loss=3.060, train_loss_epoch=2.690]\u001b[A\n",
      "Epoch 96:  69%|█████▍  | 46/67 [00:13<00:05,  3.53it/s, loss=2.28, v_num=1, train_loss_step=1.780, val_loss=3.060, train_loss_epoch=2.690]\u001b[A\n",
      "Epoch 96:  70%|█████▌  | 47/67 [00:13<00:05,  3.52it/s, loss=2.28, v_num=1, train_loss_step=1.780, val_loss=3.060, train_loss_epoch=2.690]\u001b[A\n",
      "Epoch 96:  72%|█████▋  | 48/67 [00:13<00:05,  3.50it/s, loss=2.28, v_num=1, train_loss_step=1.780, val_loss=3.060, train_loss_epoch=2.690]\u001b[A\n",
      "Epoch 96:  73%|█████▊  | 49/67 [00:14<00:05,  3.49it/s, loss=2.28, v_num=1, train_loss_step=1.780, val_loss=3.060, train_loss_epoch=2.690]\u001b[A\n",
      "Epoch 96:  75%|█████▉  | 50/67 [00:14<00:04,  3.48it/s, loss=2.28, v_num=1, train_loss_step=1.780, val_loss=3.060, train_loss_epoch=2.690]\u001b[A\n",
      "Epoch 96:  76%|██████  | 51/67 [00:14<00:04,  3.43it/s, loss=2.28, v_num=1, train_loss_step=1.780, val_loss=3.060, train_loss_epoch=2.690]\u001b[A\n",
      "Epoch 96:  78%|██████▏ | 52/67 [00:15<00:04,  3.41it/s, loss=2.28, v_num=1, train_loss_step=1.780, val_loss=3.060, train_loss_epoch=2.690]\u001b[A\n",
      "Epoch 96:  79%|██████▎ | 53/67 [00:15<00:04,  3.41it/s, loss=2.28, v_num=1, train_loss_step=1.780, val_loss=3.060, train_loss_epoch=2.690]\u001b[A\n",
      "Epoch 96:  81%|██████▍ | 54/67 [00:15<00:03,  3.40it/s, loss=2.28, v_num=1, train_loss_step=1.780, val_loss=3.060, train_loss_epoch=2.690]\u001b[A\n",
      "Epoch 96:  82%|██████▌ | 55/67 [00:16<00:03,  3.39it/s, loss=2.28, v_num=1, train_loss_step=1.780, val_loss=3.060, train_loss_epoch=2.690]\u001b[A\n",
      "Epoch 96:  84%|██████▋ | 56/67 [00:16<00:03,  3.38it/s, loss=2.28, v_num=1, train_loss_step=1.780, val_loss=3.060, train_loss_epoch=2.690]\u001b[A\n",
      "Epoch 96:  85%|██████▊ | 57/67 [00:20<00:03,  2.75it/s, loss=2.28, v_num=1, train_loss_step=1.780, val_loss=3.060, train_loss_epoch=2.690]\u001b[A\n",
      "Epoch 96:  87%|██████▉ | 58/67 [00:21<00:03,  2.75it/s, loss=2.28, v_num=1, train_loss_step=1.780, val_loss=3.060, train_loss_epoch=2.690]\u001b[A\n",
      "Epoch 96:  88%|███████ | 59/67 [00:21<00:02,  2.76it/s, loss=2.28, v_num=1, train_loss_step=1.780, val_loss=3.060, train_loss_epoch=2.690]\u001b[A\n",
      "Epoch 96:  90%|███████▏| 60/67 [00:21<00:02,  2.76it/s, loss=2.28, v_num=1, train_loss_step=1.780, val_loss=3.060, train_loss_epoch=2.690]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 96:  91%|███████▎| 61/67 [00:22<00:02,  2.74it/s, loss=2.28, v_num=1, train_loss_step=1.780, val_loss=3.060, train_loss_epoch=2.690]\u001b[A\n",
      "Epoch 96:  93%|███████▍| 62/67 [00:22<00:01,  2.75it/s, loss=2.28, v_num=1, train_loss_step=1.780, val_loss=3.060, train_loss_epoch=2.690]\u001b[A\n",
      "Epoch 96:  94%|███████▌| 63/67 [00:22<00:01,  2.75it/s, loss=2.28, v_num=1, train_loss_step=1.780, val_loss=3.060, train_loss_epoch=2.690]\u001b[A\n",
      "Epoch 96:  96%|███████▋| 64/67 [00:23<00:01,  2.76it/s, loss=2.28, v_num=1, train_loss_step=1.780, val_loss=3.060, train_loss_epoch=2.690]\u001b[A\n",
      "Epoch 96:  97%|███████▊| 65/67 [00:23<00:00,  2.76it/s, loss=2.28, v_num=1, train_loss_step=1.780, val_loss=3.060, train_loss_epoch=2.690]\u001b[A\n",
      "Epoch 96:  99%|███████▉| 66/67 [00:23<00:00,  2.77it/s, loss=2.28, v_num=1, train_loss_step=1.780, val_loss=3.060, train_loss_epoch=2.690]\u001b[A\n",
      "Epoch 96: 100%|████████| 67/67 [00:24<00:00,  2.73it/s, loss=2.28, v_num=1, train_loss_step=1.780, val_loss=3.060, train_loss_epoch=2.690]\u001b[A\n",
      "Epoch 97:  45%|████     | 30/67 [00:05<00:07,  5.21it/s, loss=3.2, v_num=1, train_loss_step=14.30, val_loss=3.060, train_loss_epoch=2.140]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 97:  46%|████▏    | 31/67 [00:07<00:09,  3.98it/s, loss=3.2, v_num=1, train_loss_step=14.30, val_loss=3.060, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 97:  48%|████▎    | 32/67 [00:08<00:08,  3.94it/s, loss=3.2, v_num=1, train_loss_step=14.30, val_loss=3.060, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 97:  49%|████▍    | 33/67 [00:08<00:08,  3.90it/s, loss=3.2, v_num=1, train_loss_step=14.30, val_loss=3.060, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 97:  51%|████▌    | 34/67 [00:08<00:08,  3.86it/s, loss=3.2, v_num=1, train_loss_step=14.30, val_loss=3.060, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 97:  52%|████▋    | 35/67 [00:09<00:08,  3.83it/s, loss=3.2, v_num=1, train_loss_step=14.30, val_loss=3.060, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 97:  54%|████▊    | 36/67 [00:09<00:08,  3.80it/s, loss=3.2, v_num=1, train_loss_step=14.30, val_loss=3.060, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 97:  55%|████▉    | 37/67 [00:09<00:07,  3.77it/s, loss=3.2, v_num=1, train_loss_step=14.30, val_loss=3.060, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 97:  57%|█████    | 38/67 [00:10<00:07,  3.75it/s, loss=3.2, v_num=1, train_loss_step=14.30, val_loss=3.060, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 97:  58%|█████▏   | 39/67 [00:10<00:07,  3.73it/s, loss=3.2, v_num=1, train_loss_step=14.30, val_loss=3.060, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 97:  60%|█████▎   | 40/67 [00:10<00:07,  3.70it/s, loss=3.2, v_num=1, train_loss_step=14.30, val_loss=3.060, train_loss_epoch=2.140]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 97:  61%|█████▌   | 41/67 [00:11<00:07,  3.65it/s, loss=3.2, v_num=1, train_loss_step=14.30, val_loss=3.060, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 97:  63%|█████▋   | 42/67 [00:11<00:06,  3.63it/s, loss=3.2, v_num=1, train_loss_step=14.30, val_loss=3.060, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 97:  64%|█████▊   | 43/67 [00:11<00:06,  3.61it/s, loss=3.2, v_num=1, train_loss_step=14.30, val_loss=3.060, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 97:  66%|█████▉   | 44/67 [00:12<00:06,  3.60it/s, loss=3.2, v_num=1, train_loss_step=14.30, val_loss=3.060, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 97:  67%|██████   | 45/67 [00:12<00:06,  3.58it/s, loss=3.2, v_num=1, train_loss_step=14.30, val_loss=3.060, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 97:  69%|██████▏  | 46/67 [00:14<00:06,  3.09it/s, loss=3.2, v_num=1, train_loss_step=14.30, val_loss=3.060, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 97:  70%|██████▎  | 47/67 [00:15<00:06,  3.09it/s, loss=3.2, v_num=1, train_loss_step=14.30, val_loss=3.060, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 97:  72%|██████▍  | 48/67 [00:15<00:06,  3.09it/s, loss=3.2, v_num=1, train_loss_step=14.30, val_loss=3.060, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 97:  73%|██████▌  | 49/67 [00:15<00:05,  3.09it/s, loss=3.2, v_num=1, train_loss_step=14.30, val_loss=3.060, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 97:  75%|██████▋  | 50/67 [00:16<00:05,  3.09it/s, loss=3.2, v_num=1, train_loss_step=14.30, val_loss=3.060, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 97:  76%|██████▊  | 51/67 [00:16<00:05,  3.06it/s, loss=3.2, v_num=1, train_loss_step=14.30, val_loss=3.060, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 97:  78%|██████▉  | 52/67 [00:16<00:04,  3.06it/s, loss=3.2, v_num=1, train_loss_step=14.30, val_loss=3.060, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 97:  79%|███████  | 53/67 [00:17<00:04,  3.06it/s, loss=3.2, v_num=1, train_loss_step=14.30, val_loss=3.060, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 97:  81%|███████▎ | 54/67 [00:17<00:04,  3.06it/s, loss=3.2, v_num=1, train_loss_step=14.30, val_loss=3.060, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 97:  82%|███████▍ | 55/67 [00:17<00:03,  3.06it/s, loss=3.2, v_num=1, train_loss_step=14.30, val_loss=3.060, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 97:  84%|███████▌ | 56/67 [00:18<00:03,  3.06it/s, loss=3.2, v_num=1, train_loss_step=14.30, val_loss=3.060, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 97:  85%|███████▋ | 57/67 [00:18<00:03,  3.06it/s, loss=3.2, v_num=1, train_loss_step=14.30, val_loss=3.060, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 97:  87%|███████▊ | 58/67 [00:18<00:02,  3.06it/s, loss=3.2, v_num=1, train_loss_step=14.30, val_loss=3.060, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 97:  88%|███████▉ | 59/67 [00:19<00:02,  3.06it/s, loss=3.2, v_num=1, train_loss_step=14.30, val_loss=3.060, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 97:  90%|████████ | 60/67 [00:19<00:02,  3.06it/s, loss=3.2, v_num=1, train_loss_step=14.30, val_loss=3.060, train_loss_epoch=2.140]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 97:  91%|████████▏| 61/67 [00:20<00:01,  3.04it/s, loss=3.2, v_num=1, train_loss_step=14.30, val_loss=3.060, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 97:  93%|████████▎| 62/67 [00:20<00:01,  3.04it/s, loss=3.2, v_num=1, train_loss_step=14.30, val_loss=3.060, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 97:  94%|████████▍| 63/67 [00:20<00:01,  3.04it/s, loss=3.2, v_num=1, train_loss_step=14.30, val_loss=3.060, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 97:  96%|████████▌| 64/67 [00:21<00:00,  3.04it/s, loss=3.2, v_num=1, train_loss_step=14.30, val_loss=3.060, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 97:  97%|████████▋| 65/67 [00:21<00:00,  3.04it/s, loss=3.2, v_num=1, train_loss_step=14.30, val_loss=3.060, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 97:  99%|████████▊| 66/67 [00:21<00:00,  3.04it/s, loss=3.2, v_num=1, train_loss_step=14.30, val_loss=3.060, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 97: 100%|█████████| 67/67 [00:22<00:00,  2.99it/s, loss=3.2, v_num=1, train_loss_step=14.30, val_loss=3.050, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 98:  45%|███▌    | 30/67 [00:05<00:07,  5.22it/s, loss=1.89, v_num=1, train_loss_step=5.820, val_loss=3.050, train_loss_epoch=2.680]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 98:  46%|███▋    | 31/67 [00:07<00:09,  3.99it/s, loss=1.89, v_num=1, train_loss_step=5.820, val_loss=3.050, train_loss_epoch=2.680]\u001b[A\n",
      "Epoch 98:  48%|███▊    | 32/67 [00:08<00:08,  3.95it/s, loss=1.89, v_num=1, train_loss_step=5.820, val_loss=3.050, train_loss_epoch=2.680]\u001b[A\n",
      "Epoch 98:  49%|███▉    | 33/67 [00:08<00:08,  3.91it/s, loss=1.89, v_num=1, train_loss_step=5.820, val_loss=3.050, train_loss_epoch=2.680]\u001b[A\n",
      "Epoch 98:  51%|████    | 34/67 [00:08<00:08,  3.87it/s, loss=1.89, v_num=1, train_loss_step=5.820, val_loss=3.050, train_loss_epoch=2.680]\u001b[A\n",
      "Epoch 98:  52%|████▏   | 35/67 [00:09<00:08,  3.84it/s, loss=1.89, v_num=1, train_loss_step=5.820, val_loss=3.050, train_loss_epoch=2.680]\u001b[A\n",
      "Epoch 98:  54%|████▎   | 36/67 [00:09<00:08,  3.81it/s, loss=1.89, v_num=1, train_loss_step=5.820, val_loss=3.050, train_loss_epoch=2.680]\u001b[A\n",
      "Epoch 98:  55%|████▍   | 37/67 [00:11<00:09,  3.21it/s, loss=1.89, v_num=1, train_loss_step=5.820, val_loss=3.050, train_loss_epoch=2.680]\u001b[A\n",
      "Epoch 98:  57%|████▌   | 38/67 [00:11<00:09,  3.21it/s, loss=1.89, v_num=1, train_loss_step=5.820, val_loss=3.050, train_loss_epoch=2.680]\u001b[A\n",
      "Epoch 98:  58%|████▋   | 39/67 [00:12<00:08,  3.20it/s, loss=1.89, v_num=1, train_loss_step=5.820, val_loss=3.050, train_loss_epoch=2.680]\u001b[A\n",
      "Epoch 98:  60%|████▊   | 40/67 [00:12<00:08,  3.20it/s, loss=1.89, v_num=1, train_loss_step=5.820, val_loss=3.050, train_loss_epoch=2.680]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 98:  61%|████▉   | 41/67 [00:12<00:08,  3.17it/s, loss=1.89, v_num=1, train_loss_step=5.820, val_loss=3.050, train_loss_epoch=2.680]\u001b[A\n",
      "Epoch 98:  63%|█████   | 42/67 [00:13<00:07,  3.16it/s, loss=1.89, v_num=1, train_loss_step=5.820, val_loss=3.050, train_loss_epoch=2.680]\u001b[A\n",
      "Epoch 98:  64%|█████▏  | 43/67 [00:13<00:07,  3.16it/s, loss=1.89, v_num=1, train_loss_step=5.820, val_loss=3.050, train_loss_epoch=2.680]\u001b[A\n",
      "Epoch 98:  66%|█████▎  | 44/67 [00:13<00:07,  3.16it/s, loss=1.89, v_num=1, train_loss_step=5.820, val_loss=3.050, train_loss_epoch=2.680]\u001b[A\n",
      "Epoch 98:  67%|█████▎  | 45/67 [00:14<00:06,  3.15it/s, loss=1.89, v_num=1, train_loss_step=5.820, val_loss=3.050, train_loss_epoch=2.680]\u001b[A\n",
      "Epoch 98:  69%|█████▍  | 46/67 [00:14<00:06,  3.15it/s, loss=1.89, v_num=1, train_loss_step=5.820, val_loss=3.050, train_loss_epoch=2.680]\u001b[A\n",
      "Epoch 98:  70%|█████▌  | 47/67 [00:14<00:06,  3.15it/s, loss=1.89, v_num=1, train_loss_step=5.820, val_loss=3.050, train_loss_epoch=2.680]\u001b[A\n",
      "Epoch 98:  72%|█████▋  | 48/67 [00:15<00:06,  3.14it/s, loss=1.89, v_num=1, train_loss_step=5.820, val_loss=3.050, train_loss_epoch=2.680]\u001b[A\n",
      "Epoch 98:  73%|█████▊  | 49/67 [00:15<00:05,  3.14it/s, loss=1.89, v_num=1, train_loss_step=5.820, val_loss=3.050, train_loss_epoch=2.680]\u001b[A\n",
      "Epoch 98:  75%|█████▉  | 50/67 [00:15<00:05,  3.14it/s, loss=1.89, v_num=1, train_loss_step=5.820, val_loss=3.050, train_loss_epoch=2.680]\u001b[A\n",
      "Epoch 98:  76%|██████  | 51/67 [00:16<00:05,  3.11it/s, loss=1.89, v_num=1, train_loss_step=5.820, val_loss=3.050, train_loss_epoch=2.680]\u001b[A\n",
      "Epoch 98:  78%|██████▏ | 52/67 [00:16<00:04,  3.11it/s, loss=1.89, v_num=1, train_loss_step=5.820, val_loss=3.050, train_loss_epoch=2.680]\u001b[A\n",
      "Epoch 98:  79%|██████▎ | 53/67 [00:17<00:04,  3.11it/s, loss=1.89, v_num=1, train_loss_step=5.820, val_loss=3.050, train_loss_epoch=2.680]\u001b[A\n",
      "Epoch 98:  81%|██████▍ | 54/67 [00:17<00:04,  3.11it/s, loss=1.89, v_num=1, train_loss_step=5.820, val_loss=3.050, train_loss_epoch=2.680]\u001b[A\n",
      "Epoch 98:  82%|██████▌ | 55/67 [00:17<00:03,  3.10it/s, loss=1.89, v_num=1, train_loss_step=5.820, val_loss=3.050, train_loss_epoch=2.680]\u001b[A\n",
      "Epoch 98:  84%|██████▋ | 56/67 [00:18<00:03,  3.10it/s, loss=1.89, v_num=1, train_loss_step=5.820, val_loss=3.050, train_loss_epoch=2.680]\u001b[A\n",
      "Epoch 98:  85%|██████▊ | 57/67 [00:18<00:03,  3.10it/s, loss=1.89, v_num=1, train_loss_step=5.820, val_loss=3.050, train_loss_epoch=2.680]\u001b[A\n",
      "Epoch 98:  87%|██████▉ | 58/67 [00:18<00:02,  3.09it/s, loss=1.89, v_num=1, train_loss_step=5.820, val_loss=3.050, train_loss_epoch=2.680]\u001b[A\n",
      "Epoch 98:  88%|███████ | 59/67 [00:19<00:02,  3.09it/s, loss=1.89, v_num=1, train_loss_step=5.820, val_loss=3.050, train_loss_epoch=2.680]\u001b[A\n",
      "Epoch 98:  90%|███████▏| 60/67 [00:19<00:02,  3.09it/s, loss=1.89, v_num=1, train_loss_step=5.820, val_loss=3.050, train_loss_epoch=2.680]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 98:  91%|███████▎| 61/67 [00:19<00:01,  3.07it/s, loss=1.89, v_num=1, train_loss_step=5.820, val_loss=3.050, train_loss_epoch=2.680]\u001b[A\n",
      "Epoch 98:  93%|███████▍| 62/67 [00:20<00:01,  3.07it/s, loss=1.89, v_num=1, train_loss_step=5.820, val_loss=3.050, train_loss_epoch=2.680]\u001b[A\n",
      "Epoch 98:  94%|███████▌| 63/67 [00:20<00:01,  3.07it/s, loss=1.89, v_num=1, train_loss_step=5.820, val_loss=3.050, train_loss_epoch=2.680]\u001b[A\n",
      "Epoch 98:  96%|███████▋| 64/67 [00:20<00:00,  3.07it/s, loss=1.89, v_num=1, train_loss_step=5.820, val_loss=3.050, train_loss_epoch=2.680]\u001b[A\n",
      "Epoch 98:  97%|███████▊| 65/67 [00:21<00:00,  3.07it/s, loss=1.89, v_num=1, train_loss_step=5.820, val_loss=3.050, train_loss_epoch=2.680]\u001b[A\n",
      "Epoch 98:  99%|███████▉| 66/67 [00:21<00:00,  3.07it/s, loss=1.89, v_num=1, train_loss_step=5.820, val_loss=3.050, train_loss_epoch=2.680]\u001b[A\n",
      "Epoch 98: 100%|████████| 67/67 [00:22<00:00,  3.02it/s, loss=1.89, v_num=1, train_loss_step=5.820, val_loss=3.050, train_loss_epoch=2.680]\u001b[A\n",
      "Epoch 99:  45%|███▌    | 30/67 [00:05<00:07,  5.20it/s, loss=2.49, v_num=1, train_loss_step=3.280, val_loss=3.050, train_loss_epoch=2.090]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 99:  46%|███▋    | 31/67 [00:07<00:09,  3.99it/s, loss=2.49, v_num=1, train_loss_step=3.280, val_loss=3.050, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 99:  48%|███▊    | 32/67 [00:08<00:08,  3.95it/s, loss=2.49, v_num=1, train_loss_step=3.280, val_loss=3.050, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 99:  49%|███▉    | 33/67 [00:08<00:08,  3.91it/s, loss=2.49, v_num=1, train_loss_step=3.280, val_loss=3.050, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 99:  51%|████    | 34/67 [00:08<00:08,  3.87it/s, loss=2.49, v_num=1, train_loss_step=3.280, val_loss=3.050, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 99:  52%|████▏   | 35/67 [00:09<00:08,  3.84it/s, loss=2.49, v_num=1, train_loss_step=3.280, val_loss=3.050, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 99:  54%|████▎   | 36/67 [00:09<00:08,  3.81it/s, loss=2.49, v_num=1, train_loss_step=3.280, val_loss=3.050, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 99:  55%|████▍   | 37/67 [00:09<00:07,  3.78it/s, loss=2.49, v_num=1, train_loss_step=3.280, val_loss=3.050, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 99:  57%|████▌   | 38/67 [00:10<00:07,  3.76it/s, loss=2.49, v_num=1, train_loss_step=3.280, val_loss=3.050, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 99:  58%|████▋   | 39/67 [00:10<00:07,  3.74it/s, loss=2.49, v_num=1, train_loss_step=3.280, val_loss=3.050, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 99:  60%|████▊   | 40/67 [00:10<00:07,  3.71it/s, loss=2.49, v_num=1, train_loss_step=3.280, val_loss=3.050, train_loss_epoch=2.090]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 99:  61%|████▉   | 41/67 [00:11<00:07,  3.65it/s, loss=2.49, v_num=1, train_loss_step=3.280, val_loss=3.050, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 99:  63%|█████   | 42/67 [00:11<00:06,  3.63it/s, loss=2.49, v_num=1, train_loss_step=3.280, val_loss=3.050, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 99:  64%|█████▏  | 43/67 [00:11<00:06,  3.61it/s, loss=2.49, v_num=1, train_loss_step=3.280, val_loss=3.050, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 99:  66%|█████▎  | 44/67 [00:12<00:06,  3.60it/s, loss=2.49, v_num=1, train_loss_step=3.280, val_loss=3.050, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 99:  67%|█████▎  | 45/67 [00:12<00:06,  3.58it/s, loss=2.49, v_num=1, train_loss_step=3.280, val_loss=3.050, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 99:  69%|█████▍  | 46/67 [00:12<00:05,  3.57it/s, loss=2.49, v_num=1, train_loss_step=3.280, val_loss=3.050, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 99:  70%|█████▌  | 47/67 [00:13<00:05,  3.56it/s, loss=2.49, v_num=1, train_loss_step=3.280, val_loss=3.050, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 99:  72%|█████▋  | 48/67 [00:13<00:05,  3.54it/s, loss=2.49, v_num=1, train_loss_step=3.280, val_loss=3.050, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 99:  73%|█████▊  | 49/67 [00:13<00:05,  3.53it/s, loss=2.49, v_num=1, train_loss_step=3.280, val_loss=3.050, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 99:  75%|█████▉  | 50/67 [00:14<00:04,  3.52it/s, loss=2.49, v_num=1, train_loss_step=3.280, val_loss=3.050, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 99:  76%|██████  | 51/67 [00:14<00:04,  3.48it/s, loss=2.49, v_num=1, train_loss_step=3.280, val_loss=3.050, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 99:  78%|██████▏ | 52/67 [00:15<00:04,  3.46it/s, loss=2.49, v_num=1, train_loss_step=3.280, val_loss=3.050, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 99:  79%|██████▎ | 53/67 [00:15<00:04,  3.46it/s, loss=2.49, v_num=1, train_loss_step=3.280, val_loss=3.050, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 99:  81%|██████▍ | 54/67 [00:15<00:03,  3.45it/s, loss=2.49, v_num=1, train_loss_step=3.280, val_loss=3.050, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 99:  82%|██████▌ | 55/67 [00:15<00:03,  3.44it/s, loss=2.49, v_num=1, train_loss_step=3.280, val_loss=3.050, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 99:  84%|██████▋ | 56/67 [00:16<00:03,  3.43it/s, loss=2.49, v_num=1, train_loss_step=3.280, val_loss=3.050, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 99:  85%|██████▊ | 57/67 [00:16<00:02,  3.42it/s, loss=2.49, v_num=1, train_loss_step=3.280, val_loss=3.050, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 99:  87%|██████▉ | 58/67 [00:16<00:02,  3.42it/s, loss=2.49, v_num=1, train_loss_step=3.280, val_loss=3.050, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 99:  88%|███████ | 59/67 [00:17<00:02,  3.41it/s, loss=2.49, v_num=1, train_loss_step=3.280, val_loss=3.050, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 99:  90%|███████▏| 60/67 [00:17<00:02,  3.40it/s, loss=2.49, v_num=1, train_loss_step=3.280, val_loss=3.050, train_loss_epoch=2.090]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 99:  91%|███████▎| 61/67 [00:18<00:01,  3.37it/s, loss=2.49, v_num=1, train_loss_step=3.280, val_loss=3.050, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 99:  93%|███████▍| 62/67 [00:18<00:01,  3.37it/s, loss=2.49, v_num=1, train_loss_step=3.280, val_loss=3.050, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 99:  94%|███████▌| 63/67 [00:18<00:01,  3.36it/s, loss=2.49, v_num=1, train_loss_step=3.280, val_loss=3.050, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 99:  96%|███████▋| 64/67 [00:19<00:00,  3.35it/s, loss=2.49, v_num=1, train_loss_step=3.280, val_loss=3.050, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 99:  97%|███████▊| 65/67 [00:19<00:00,  3.35it/s, loss=2.49, v_num=1, train_loss_step=3.280, val_loss=3.050, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 99:  99%|███████▉| 66/67 [00:19<00:00,  3.35it/s, loss=2.49, v_num=1, train_loss_step=3.280, val_loss=3.050, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 99: 100%|████████| 67/67 [00:20<00:00,  3.28it/s, loss=2.49, v_num=1, train_loss_step=3.280, val_loss=3.050, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 100:  45%|███▏   | 30/67 [00:05<00:07,  5.28it/s, loss=1.67, v_num=1, train_loss_step=0.621, val_loss=3.050, train_loss_epoch=2.110]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 100:  46%|███▏   | 31/67 [00:07<00:09,  3.93it/s, loss=1.67, v_num=1, train_loss_step=0.621, val_loss=3.050, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 100:  48%|███▎   | 32/67 [00:08<00:08,  3.89it/s, loss=1.67, v_num=1, train_loss_step=0.621, val_loss=3.050, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 100:  49%|███▍   | 33/67 [00:08<00:08,  3.85it/s, loss=1.67, v_num=1, train_loss_step=0.621, val_loss=3.050, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 100:  51%|███▌   | 34/67 [00:08<00:08,  3.81it/s, loss=1.67, v_num=1, train_loss_step=0.621, val_loss=3.050, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 100:  52%|███▋   | 35/67 [00:09<00:08,  3.78it/s, loss=1.67, v_num=1, train_loss_step=0.621, val_loss=3.050, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 100:  54%|███▊   | 36/67 [00:09<00:08,  3.76it/s, loss=1.67, v_num=1, train_loss_step=0.621, val_loss=3.050, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 100:  55%|███▊   | 37/67 [00:09<00:08,  3.73it/s, loss=1.67, v_num=1, train_loss_step=0.621, val_loss=3.050, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 100:  57%|███▉   | 38/67 [00:10<00:07,  3.71it/s, loss=1.67, v_num=1, train_loss_step=0.621, val_loss=3.050, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 100:  58%|████   | 39/67 [00:10<00:07,  3.69it/s, loss=1.67, v_num=1, train_loss_step=0.621, val_loss=3.050, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 100:  60%|████▏  | 40/67 [00:10<00:07,  3.67it/s, loss=1.67, v_num=1, train_loss_step=0.621, val_loss=3.050, train_loss_epoch=2.110]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 100:  61%|████▎  | 41/67 [00:11<00:07,  3.60it/s, loss=1.67, v_num=1, train_loss_step=0.621, val_loss=3.050, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 100:  63%|████▍  | 42/67 [00:11<00:06,  3.59it/s, loss=1.67, v_num=1, train_loss_step=0.621, val_loss=3.050, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 100:  64%|████▍  | 43/67 [00:12<00:06,  3.57it/s, loss=1.67, v_num=1, train_loss_step=0.621, val_loss=3.050, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 100:  66%|████▌  | 44/67 [00:12<00:06,  3.55it/s, loss=1.67, v_num=1, train_loss_step=0.621, val_loss=3.050, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 100:  67%|████▋  | 45/67 [00:12<00:06,  3.54it/s, loss=1.67, v_num=1, train_loss_step=0.621, val_loss=3.050, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 100:  69%|████▊  | 46/67 [00:13<00:05,  3.53it/s, loss=1.67, v_num=1, train_loss_step=0.621, val_loss=3.050, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 100:  70%|████▉  | 47/67 [00:13<00:05,  3.51it/s, loss=1.67, v_num=1, train_loss_step=0.621, val_loss=3.050, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 100:  72%|█████  | 48/67 [00:13<00:05,  3.50it/s, loss=1.67, v_num=1, train_loss_step=0.621, val_loss=3.050, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 100:  73%|█████  | 49/67 [00:14<00:05,  3.49it/s, loss=1.67, v_num=1, train_loss_step=0.621, val_loss=3.050, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 100:  75%|█████▏ | 50/67 [00:14<00:04,  3.48it/s, loss=1.67, v_num=1, train_loss_step=0.621, val_loss=3.050, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 100:  76%|█████▎ | 51/67 [00:14<00:04,  3.44it/s, loss=1.67, v_num=1, train_loss_step=0.621, val_loss=3.050, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 100:  78%|█████▍ | 52/67 [00:15<00:04,  3.43it/s, loss=1.67, v_num=1, train_loss_step=0.621, val_loss=3.050, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 100:  79%|█████▌ | 53/67 [00:15<00:04,  3.42it/s, loss=1.67, v_num=1, train_loss_step=0.621, val_loss=3.050, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 100:  81%|█████▋ | 54/67 [00:15<00:03,  3.41it/s, loss=1.67, v_num=1, train_loss_step=0.621, val_loss=3.050, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 100:  82%|█████▋ | 55/67 [00:16<00:03,  3.40it/s, loss=1.67, v_num=1, train_loss_step=0.621, val_loss=3.050, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 100:  84%|█████▊ | 56/67 [00:16<00:03,  3.40it/s, loss=1.67, v_num=1, train_loss_step=0.621, val_loss=3.050, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 100:  85%|█████▉ | 57/67 [00:16<00:02,  3.39it/s, loss=1.67, v_num=1, train_loss_step=0.621, val_loss=3.050, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 100:  87%|██████ | 58/67 [00:17<00:02,  3.38it/s, loss=1.67, v_num=1, train_loss_step=0.621, val_loss=3.050, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 100:  88%|██████▏| 59/67 [00:17<00:02,  3.38it/s, loss=1.67, v_num=1, train_loss_step=0.621, val_loss=3.050, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 100:  90%|██████▎| 60/67 [00:17<00:02,  3.37it/s, loss=1.67, v_num=1, train_loss_step=0.621, val_loss=3.050, train_loss_epoch=2.110]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 100:  91%|██████▎| 61/67 [00:18<00:01,  3.34it/s, loss=1.67, v_num=1, train_loss_step=0.621, val_loss=3.050, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 100:  93%|██████▍| 62/67 [00:18<00:01,  3.33it/s, loss=1.67, v_num=1, train_loss_step=0.621, val_loss=3.050, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 100:  94%|██████▌| 63/67 [00:18<00:01,  3.33it/s, loss=1.67, v_num=1, train_loss_step=0.621, val_loss=3.050, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 100:  96%|██████▋| 64/67 [00:19<00:00,  3.32it/s, loss=1.67, v_num=1, train_loss_step=0.621, val_loss=3.050, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 100:  97%|██████▊| 65/67 [00:19<00:00,  3.32it/s, loss=1.67, v_num=1, train_loss_step=0.621, val_loss=3.050, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 100:  99%|██████▉| 66/67 [00:19<00:00,  3.32it/s, loss=1.67, v_num=1, train_loss_step=0.621, val_loss=3.050, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 100: 100%|███████| 67/67 [00:20<00:00,  3.25it/s, loss=1.67, v_num=1, train_loss_step=0.621, val_loss=3.050, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 101:  45%|███▏   | 30/67 [00:05<00:06,  5.38it/s, loss=1.94, v_num=1, train_loss_step=1.090, val_loss=3.050, train_loss_epoch=1.660]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 101:  46%|███▏   | 31/67 [00:07<00:08,  4.15it/s, loss=1.94, v_num=1, train_loss_step=1.090, val_loss=3.050, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 101:  48%|███▎   | 32/67 [00:07<00:08,  4.10it/s, loss=1.94, v_num=1, train_loss_step=1.090, val_loss=3.050, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 101:  49%|███▍   | 33/67 [00:08<00:08,  4.06it/s, loss=1.94, v_num=1, train_loss_step=1.090, val_loss=3.050, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 101:  51%|███▌   | 34/67 [00:08<00:08,  4.02it/s, loss=1.94, v_num=1, train_loss_step=1.090, val_loss=3.050, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 101:  52%|███▋   | 35/67 [00:08<00:08,  3.98it/s, loss=1.94, v_num=1, train_loss_step=1.090, val_loss=3.050, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 101:  54%|███▊   | 36/67 [00:09<00:07,  3.94it/s, loss=1.94, v_num=1, train_loss_step=1.090, val_loss=3.050, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 101:  55%|███▊   | 37/67 [00:09<00:07,  3.90it/s, loss=1.94, v_num=1, train_loss_step=1.090, val_loss=3.050, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 101:  57%|███▉   | 38/67 [00:09<00:07,  3.88it/s, loss=1.94, v_num=1, train_loss_step=1.090, val_loss=3.050, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 101:  58%|████   | 39/67 [00:10<00:07,  3.85it/s, loss=1.94, v_num=1, train_loss_step=1.090, val_loss=3.050, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 101:  60%|████▏  | 40/67 [00:10<00:07,  3.82it/s, loss=1.94, v_num=1, train_loss_step=1.090, val_loss=3.050, train_loss_epoch=1.660]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 101:  61%|████▎  | 41/67 [00:10<00:06,  3.76it/s, loss=1.94, v_num=1, train_loss_step=1.090, val_loss=3.050, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 101:  63%|████▍  | 42/67 [00:11<00:06,  3.74it/s, loss=1.94, v_num=1, train_loss_step=1.090, val_loss=3.050, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 101:  64%|████▍  | 43/67 [00:11<00:06,  3.71it/s, loss=1.94, v_num=1, train_loss_step=1.090, val_loss=3.050, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 101:  66%|████▌  | 44/67 [00:11<00:06,  3.70it/s, loss=1.94, v_num=1, train_loss_step=1.090, val_loss=3.050, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 101:  67%|████▋  | 45/67 [00:12<00:05,  3.68it/s, loss=1.94, v_num=1, train_loss_step=1.090, val_loss=3.050, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 101:  69%|████▊  | 46/67 [00:12<00:05,  3.66it/s, loss=1.94, v_num=1, train_loss_step=1.090, val_loss=3.050, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 101:  70%|████▉  | 47/67 [00:12<00:05,  3.65it/s, loss=1.94, v_num=1, train_loss_step=1.090, val_loss=3.050, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 101:  72%|█████  | 48/67 [00:13<00:05,  3.63it/s, loss=1.94, v_num=1, train_loss_step=1.090, val_loss=3.050, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 101:  73%|█████  | 49/67 [00:13<00:04,  3.61it/s, loss=1.94, v_num=1, train_loss_step=1.090, val_loss=3.050, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 101:  75%|█████▏ | 50/67 [00:13<00:04,  3.60it/s, loss=1.94, v_num=1, train_loss_step=1.090, val_loss=3.050, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 101:  76%|█████▎ | 51/67 [00:14<00:04,  3.56it/s, loss=1.94, v_num=1, train_loss_step=1.090, val_loss=3.050, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 101:  78%|█████▍ | 52/67 [00:14<00:04,  3.54it/s, loss=1.94, v_num=1, train_loss_step=1.090, val_loss=3.050, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 101:  79%|█████▌ | 53/67 [00:15<00:03,  3.53it/s, loss=1.94, v_num=1, train_loss_step=1.090, val_loss=3.050, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 101:  81%|█████▋ | 54/67 [00:15<00:03,  3.52it/s, loss=1.94, v_num=1, train_loss_step=1.090, val_loss=3.050, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 101:  82%|█████▋ | 55/67 [00:15<00:03,  3.51it/s, loss=1.94, v_num=1, train_loss_step=1.090, val_loss=3.050, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 101:  84%|█████▊ | 56/67 [00:16<00:03,  3.50it/s, loss=1.94, v_num=1, train_loss_step=1.090, val_loss=3.050, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 101:  85%|█████▉ | 57/67 [00:16<00:02,  3.49it/s, loss=1.94, v_num=1, train_loss_step=1.090, val_loss=3.050, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 101:  87%|██████ | 58/67 [00:16<00:02,  3.48it/s, loss=1.94, v_num=1, train_loss_step=1.090, val_loss=3.050, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 101:  88%|██████▏| 59/67 [00:16<00:02,  3.47it/s, loss=1.94, v_num=1, train_loss_step=1.090, val_loss=3.050, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 101:  90%|██████▎| 60/67 [00:17<00:02,  3.46it/s, loss=1.94, v_num=1, train_loss_step=1.090, val_loss=3.050, train_loss_epoch=1.660]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 101:  91%|██████▎| 61/67 [00:17<00:01,  3.43it/s, loss=1.94, v_num=1, train_loss_step=1.090, val_loss=3.050, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 101:  93%|██████▍| 62/67 [00:18<00:01,  3.43it/s, loss=1.94, v_num=1, train_loss_step=1.090, val_loss=3.050, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 101:  94%|██████▌| 63/67 [00:18<00:01,  3.42it/s, loss=1.94, v_num=1, train_loss_step=1.090, val_loss=3.050, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 101:  96%|██████▋| 64/67 [00:18<00:00,  3.41it/s, loss=1.94, v_num=1, train_loss_step=1.090, val_loss=3.050, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 101:  97%|██████▊| 65/67 [00:19<00:00,  3.41it/s, loss=1.94, v_num=1, train_loss_step=1.090, val_loss=3.050, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 101:  99%|██████▉| 66/67 [00:19<00:00,  3.40it/s, loss=1.94, v_num=1, train_loss_step=1.090, val_loss=3.050, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 101: 100%|███████| 67/67 [00:20<00:00,  3.34it/s, loss=1.94, v_num=1, train_loss_step=1.090, val_loss=3.050, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 102:  45%|███▏   | 30/67 [00:05<00:07,  5.27it/s, loss=2.33, v_num=1, train_loss_step=2.090, val_loss=3.050, train_loss_epoch=2.060]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 102:  46%|███▏   | 31/67 [00:07<00:08,  4.01it/s, loss=2.33, v_num=1, train_loss_step=2.090, val_loss=3.050, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 102:  48%|███▎   | 32/67 [00:08<00:08,  3.96it/s, loss=2.33, v_num=1, train_loss_step=2.090, val_loss=3.050, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 102:  49%|███▍   | 33/67 [00:08<00:08,  3.92it/s, loss=2.33, v_num=1, train_loss_step=2.090, val_loss=3.050, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 102:  51%|███▌   | 34/67 [00:08<00:08,  3.88it/s, loss=2.33, v_num=1, train_loss_step=2.090, val_loss=3.050, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 102:  52%|███▋   | 35/67 [00:09<00:08,  3.85it/s, loss=2.33, v_num=1, train_loss_step=2.090, val_loss=3.050, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 102:  54%|███▊   | 36/67 [00:09<00:08,  3.82it/s, loss=2.33, v_num=1, train_loss_step=2.090, val_loss=3.050, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 102:  55%|███▊   | 37/67 [00:09<00:07,  3.79it/s, loss=2.33, v_num=1, train_loss_step=2.090, val_loss=3.050, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 102:  57%|███▉   | 38/67 [00:10<00:07,  3.76it/s, loss=2.33, v_num=1, train_loss_step=2.090, val_loss=3.050, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 102:  58%|████   | 39/67 [00:10<00:07,  3.74it/s, loss=2.33, v_num=1, train_loss_step=2.090, val_loss=3.050, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 102:  60%|████▏  | 40/67 [00:10<00:07,  3.71it/s, loss=2.33, v_num=1, train_loss_step=2.090, val_loss=3.050, train_loss_epoch=2.060]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 102:  61%|████▎  | 41/67 [00:11<00:07,  3.65it/s, loss=2.33, v_num=1, train_loss_step=2.090, val_loss=3.050, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 102:  63%|████▍  | 42/67 [00:11<00:06,  3.63it/s, loss=2.33, v_num=1, train_loss_step=2.090, val_loss=3.050, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 102:  64%|████▍  | 43/67 [00:11<00:06,  3.61it/s, loss=2.33, v_num=1, train_loss_step=2.090, val_loss=3.050, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 102:  66%|████▌  | 44/67 [00:12<00:06,  3.60it/s, loss=2.33, v_num=1, train_loss_step=2.090, val_loss=3.050, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 102:  67%|████▋  | 45/67 [00:12<00:06,  3.58it/s, loss=2.33, v_num=1, train_loss_step=2.090, val_loss=3.050, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 102:  69%|████▊  | 46/67 [00:12<00:05,  3.57it/s, loss=2.33, v_num=1, train_loss_step=2.090, val_loss=3.050, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 102:  70%|████▉  | 47/67 [00:13<00:05,  3.55it/s, loss=2.33, v_num=1, train_loss_step=2.090, val_loss=3.050, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 102:  72%|█████  | 48/67 [00:13<00:05,  3.54it/s, loss=2.33, v_num=1, train_loss_step=2.090, val_loss=3.050, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 102:  73%|█████  | 49/67 [00:13<00:05,  3.53it/s, loss=2.33, v_num=1, train_loss_step=2.090, val_loss=3.050, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 102:  75%|█████▏ | 50/67 [00:14<00:04,  3.52it/s, loss=2.33, v_num=1, train_loss_step=2.090, val_loss=3.050, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 102:  76%|█████▎ | 51/67 [00:14<00:04,  3.48it/s, loss=2.33, v_num=1, train_loss_step=2.090, val_loss=3.050, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 102:  78%|█████▍ | 52/67 [00:15<00:04,  3.47it/s, loss=2.33, v_num=1, train_loss_step=2.090, val_loss=3.050, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 102:  79%|█████▌ | 53/67 [00:15<00:04,  3.46it/s, loss=2.33, v_num=1, train_loss_step=2.090, val_loss=3.050, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 102:  81%|█████▋ | 54/67 [00:15<00:03,  3.45it/s, loss=2.33, v_num=1, train_loss_step=2.090, val_loss=3.050, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 102:  82%|█████▋ | 55/67 [00:16<00:03,  3.44it/s, loss=2.33, v_num=1, train_loss_step=2.090, val_loss=3.050, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 102:  84%|█████▊ | 56/67 [00:16<00:03,  3.43it/s, loss=2.33, v_num=1, train_loss_step=2.090, val_loss=3.050, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 102:  85%|█████▉ | 57/67 [00:16<00:02,  3.42it/s, loss=2.33, v_num=1, train_loss_step=2.090, val_loss=3.050, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 102:  87%|██████ | 58/67 [00:17<00:02,  3.41it/s, loss=2.33, v_num=1, train_loss_step=2.090, val_loss=3.050, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 102:  88%|██████▏| 59/67 [00:17<00:02,  3.40it/s, loss=2.33, v_num=1, train_loss_step=2.090, val_loss=3.050, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 102:  90%|██████▎| 60/67 [00:17<00:02,  3.40it/s, loss=2.33, v_num=1, train_loss_step=2.090, val_loss=3.050, train_loss_epoch=2.060]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 102:  91%|██████▎| 61/67 [00:18<00:01,  3.37it/s, loss=2.33, v_num=1, train_loss_step=2.090, val_loss=3.050, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 102:  93%|██████▍| 62/67 [00:18<00:01,  3.36it/s, loss=2.33, v_num=1, train_loss_step=2.090, val_loss=3.050, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 102:  94%|██████▌| 63/67 [00:18<00:01,  3.36it/s, loss=2.33, v_num=1, train_loss_step=2.090, val_loss=3.050, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 102:  96%|██████▋| 64/67 [00:19<00:00,  3.35it/s, loss=2.33, v_num=1, train_loss_step=2.090, val_loss=3.050, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 102:  97%|██████▊| 65/67 [00:19<00:00,  3.35it/s, loss=2.33, v_num=1, train_loss_step=2.090, val_loss=3.050, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 102:  99%|██████▉| 66/67 [00:19<00:00,  3.35it/s, loss=2.33, v_num=1, train_loss_step=2.090, val_loss=3.050, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 102: 100%|███████| 67/67 [00:20<00:00,  3.28it/s, loss=2.33, v_num=1, train_loss_step=2.090, val_loss=3.050, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 103:  45%|███▌    | 30/67 [00:05<00:06,  5.41it/s, loss=2.6, v_num=1, train_loss_step=0.654, val_loss=3.050, train_loss_epoch=2.140]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 103:  46%|███▋    | 31/67 [00:07<00:08,  4.13it/s, loss=2.6, v_num=1, train_loss_step=0.654, val_loss=3.050, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 103:  48%|███▊    | 32/67 [00:07<00:08,  4.08it/s, loss=2.6, v_num=1, train_loss_step=0.654, val_loss=3.050, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 103:  49%|███▉    | 33/67 [00:08<00:08,  4.04it/s, loss=2.6, v_num=1, train_loss_step=0.654, val_loss=3.050, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 103:  51%|████    | 34/67 [00:08<00:08,  3.99it/s, loss=2.6, v_num=1, train_loss_step=0.654, val_loss=3.050, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 103:  52%|████▏   | 35/67 [00:08<00:08,  3.96it/s, loss=2.6, v_num=1, train_loss_step=0.654, val_loss=3.050, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 103:  54%|████▎   | 36/67 [00:09<00:07,  3.93it/s, loss=2.6, v_num=1, train_loss_step=0.654, val_loss=3.050, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 103:  55%|████▍   | 37/67 [00:09<00:07,  3.89it/s, loss=2.6, v_num=1, train_loss_step=0.654, val_loss=3.050, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 103:  57%|████▌   | 38/67 [00:09<00:07,  3.86it/s, loss=2.6, v_num=1, train_loss_step=0.654, val_loss=3.050, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 103:  58%|████▋   | 39/67 [00:10<00:07,  3.83it/s, loss=2.6, v_num=1, train_loss_step=0.654, val_loss=3.050, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 103:  60%|████▊   | 40/67 [00:10<00:07,  3.80it/s, loss=2.6, v_num=1, train_loss_step=0.654, val_loss=3.050, train_loss_epoch=2.140]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 103:  61%|████▉   | 41/67 [00:10<00:06,  3.74it/s, loss=2.6, v_num=1, train_loss_step=0.654, val_loss=3.050, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 103:  63%|█████   | 42/67 [00:11<00:06,  3.72it/s, loss=2.6, v_num=1, train_loss_step=0.654, val_loss=3.050, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 103:  64%|█████▏  | 43/67 [00:11<00:06,  3.70it/s, loss=2.6, v_num=1, train_loss_step=0.654, val_loss=3.050, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 103:  66%|█████▎  | 44/67 [00:11<00:06,  3.68it/s, loss=2.6, v_num=1, train_loss_step=0.654, val_loss=3.050, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 103:  67%|█████▎  | 45/67 [00:12<00:06,  3.66it/s, loss=2.6, v_num=1, train_loss_step=0.654, val_loss=3.050, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 103:  69%|█████▍  | 46/67 [00:12<00:05,  3.64it/s, loss=2.6, v_num=1, train_loss_step=0.654, val_loss=3.050, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 103:  70%|█████▌  | 47/67 [00:12<00:05,  3.63it/s, loss=2.6, v_num=1, train_loss_step=0.654, val_loss=3.050, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 103:  72%|█████▋  | 48/67 [00:13<00:05,  3.61it/s, loss=2.6, v_num=1, train_loss_step=0.654, val_loss=3.050, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 103:  73%|█████▊  | 49/67 [00:13<00:05,  3.60it/s, loss=2.6, v_num=1, train_loss_step=0.654, val_loss=3.050, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 103:  75%|█████▉  | 50/67 [00:13<00:04,  3.58it/s, loss=2.6, v_num=1, train_loss_step=0.654, val_loss=3.050, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 103:  76%|██████  | 51/67 [00:14<00:04,  3.54it/s, loss=2.6, v_num=1, train_loss_step=0.654, val_loss=3.050, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 103:  78%|██████▏ | 52/67 [00:14<00:04,  3.52it/s, loss=2.6, v_num=1, train_loss_step=0.654, val_loss=3.050, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 103:  79%|██████▎ | 53/67 [00:15<00:03,  3.51it/s, loss=2.6, v_num=1, train_loss_step=0.654, val_loss=3.050, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 103:  81%|██████▍ | 54/67 [00:15<00:03,  3.50it/s, loss=2.6, v_num=1, train_loss_step=0.654, val_loss=3.050, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 103:  82%|██████▌ | 55/67 [00:15<00:03,  3.49it/s, loss=2.6, v_num=1, train_loss_step=0.654, val_loss=3.050, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 103:  84%|██████▋ | 56/67 [00:16<00:03,  3.48it/s, loss=2.6, v_num=1, train_loss_step=0.654, val_loss=3.050, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 103:  85%|██████▊ | 57/67 [00:16<00:02,  3.47it/s, loss=2.6, v_num=1, train_loss_step=0.654, val_loss=3.050, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 103:  87%|██████▉ | 58/67 [00:16<00:02,  3.46it/s, loss=2.6, v_num=1, train_loss_step=0.654, val_loss=3.050, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 103:  88%|███████ | 59/67 [00:17<00:02,  3.46it/s, loss=2.6, v_num=1, train_loss_step=0.654, val_loss=3.050, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 103:  90%|███████▏| 60/67 [00:17<00:02,  3.45it/s, loss=2.6, v_num=1, train_loss_step=0.654, val_loss=3.050, train_loss_epoch=2.140]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 103:  91%|███████▎| 61/67 [00:17<00:01,  3.42it/s, loss=2.6, v_num=1, train_loss_step=0.654, val_loss=3.050, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 103:  93%|███████▍| 62/67 [00:18<00:01,  3.41it/s, loss=2.6, v_num=1, train_loss_step=0.654, val_loss=3.050, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 103:  94%|███████▌| 63/67 [00:18<00:01,  3.40it/s, loss=2.6, v_num=1, train_loss_step=0.654, val_loss=3.050, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 103:  96%|███████▋| 64/67 [00:18<00:00,  3.40it/s, loss=2.6, v_num=1, train_loss_step=0.654, val_loss=3.050, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 103:  97%|███████▊| 65/67 [00:19<00:00,  3.39it/s, loss=2.6, v_num=1, train_loss_step=0.654, val_loss=3.050, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 103:  99%|███████▉| 66/67 [00:19<00:00,  3.39it/s, loss=2.6, v_num=1, train_loss_step=0.654, val_loss=3.050, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 103: 100%|████████| 67/67 [00:20<00:00,  3.32it/s, loss=2.6, v_num=1, train_loss_step=0.654, val_loss=3.050, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 104:  45%|███▏   | 30/67 [00:05<00:06,  5.45it/s, loss=2.07, v_num=1, train_loss_step=1.600, val_loss=3.050, train_loss_epoch=2.380]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 104:  46%|███▏   | 31/67 [00:07<00:08,  4.15it/s, loss=2.07, v_num=1, train_loss_step=1.600, val_loss=3.050, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 104:  48%|███▎   | 32/67 [00:07<00:08,  4.10it/s, loss=2.07, v_num=1, train_loss_step=1.600, val_loss=3.050, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 104:  49%|███▍   | 33/67 [00:08<00:08,  4.06it/s, loss=2.07, v_num=1, train_loss_step=1.600, val_loss=3.050, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 104:  51%|███▌   | 34/67 [00:08<00:08,  4.01it/s, loss=2.07, v_num=1, train_loss_step=1.600, val_loss=3.050, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 104:  52%|███▋   | 35/67 [00:08<00:08,  3.98it/s, loss=2.07, v_num=1, train_loss_step=1.600, val_loss=3.050, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 104:  54%|███▊   | 36/67 [00:09<00:07,  3.94it/s, loss=2.07, v_num=1, train_loss_step=1.600, val_loss=3.050, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 104:  55%|███▊   | 37/67 [00:09<00:07,  3.90it/s, loss=2.07, v_num=1, train_loss_step=1.600, val_loss=3.050, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 104:  57%|███▉   | 38/67 [00:09<00:07,  3.87it/s, loss=2.07, v_num=1, train_loss_step=1.600, val_loss=3.050, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 104:  58%|████   | 39/67 [00:10<00:07,  3.85it/s, loss=2.07, v_num=1, train_loss_step=1.600, val_loss=3.050, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 104:  60%|████▏  | 40/67 [00:10<00:07,  3.82it/s, loss=2.07, v_num=1, train_loss_step=1.600, val_loss=3.050, train_loss_epoch=2.380]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 104:  61%|████▎  | 41/67 [00:10<00:06,  3.74it/s, loss=2.07, v_num=1, train_loss_step=1.600, val_loss=3.050, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 104:  63%|████▍  | 42/67 [00:11<00:06,  3.72it/s, loss=2.07, v_num=1, train_loss_step=1.600, val_loss=3.050, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 104:  64%|████▍  | 43/67 [00:11<00:06,  3.70it/s, loss=2.07, v_num=1, train_loss_step=1.600, val_loss=3.050, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 104:  66%|████▌  | 44/67 [00:11<00:06,  3.68it/s, loss=2.07, v_num=1, train_loss_step=1.600, val_loss=3.050, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 104:  67%|████▋  | 45/67 [00:12<00:06,  3.66it/s, loss=2.07, v_num=1, train_loss_step=1.600, val_loss=3.050, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 104:  69%|████▊  | 46/67 [00:12<00:05,  3.64it/s, loss=2.07, v_num=1, train_loss_step=1.600, val_loss=3.050, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 104:  70%|████▉  | 47/67 [00:12<00:05,  3.63it/s, loss=2.07, v_num=1, train_loss_step=1.600, val_loss=3.050, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 104:  72%|█████  | 48/67 [00:13<00:05,  3.61it/s, loss=2.07, v_num=1, train_loss_step=1.600, val_loss=3.050, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 104:  73%|█████  | 49/67 [00:13<00:05,  3.59it/s, loss=2.07, v_num=1, train_loss_step=1.600, val_loss=3.050, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 104:  75%|█████▏ | 50/67 [00:13<00:04,  3.58it/s, loss=2.07, v_num=1, train_loss_step=1.600, val_loss=3.050, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 104:  76%|█████▎ | 51/67 [00:14<00:04,  3.53it/s, loss=2.07, v_num=1, train_loss_step=1.600, val_loss=3.050, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 104:  78%|█████▍ | 52/67 [00:14<00:04,  3.52it/s, loss=2.07, v_num=1, train_loss_step=1.600, val_loss=3.050, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 104:  79%|█████▌ | 53/67 [00:15<00:03,  3.51it/s, loss=2.07, v_num=1, train_loss_step=1.600, val_loss=3.050, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 104:  81%|█████▋ | 54/67 [00:15<00:03,  3.50it/s, loss=2.07, v_num=1, train_loss_step=1.600, val_loss=3.050, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 104:  82%|█████▋ | 55/67 [00:15<00:03,  3.49it/s, loss=2.07, v_num=1, train_loss_step=1.600, val_loss=3.050, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 104:  84%|█████▊ | 56/67 [00:16<00:03,  3.48it/s, loss=2.07, v_num=1, train_loss_step=1.600, val_loss=3.050, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 104:  85%|█████▉ | 57/67 [00:16<00:02,  3.47it/s, loss=2.07, v_num=1, train_loss_step=1.600, val_loss=3.050, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 104:  87%|██████ | 58/67 [00:16<00:02,  3.46it/s, loss=2.07, v_num=1, train_loss_step=1.600, val_loss=3.050, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 104:  88%|██████▏| 59/67 [00:17<00:02,  3.45it/s, loss=2.07, v_num=1, train_loss_step=1.600, val_loss=3.050, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 104:  90%|██████▎| 60/67 [00:17<00:02,  3.45it/s, loss=2.07, v_num=1, train_loss_step=1.600, val_loss=3.050, train_loss_epoch=2.380]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 104:  91%|██████▎| 61/67 [00:17<00:01,  3.41it/s, loss=2.07, v_num=1, train_loss_step=1.600, val_loss=3.050, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 104:  93%|██████▍| 62/67 [00:18<00:01,  3.41it/s, loss=2.07, v_num=1, train_loss_step=1.600, val_loss=3.050, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 104:  94%|██████▌| 63/67 [00:18<00:01,  3.40it/s, loss=2.07, v_num=1, train_loss_step=1.600, val_loss=3.050, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 104:  96%|██████▋| 64/67 [00:18<00:00,  3.40it/s, loss=2.07, v_num=1, train_loss_step=1.600, val_loss=3.050, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 104:  97%|██████▊| 65/67 [00:19<00:00,  3.39it/s, loss=2.07, v_num=1, train_loss_step=1.600, val_loss=3.050, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 104:  99%|██████▉| 66/67 [00:19<00:00,  3.39it/s, loss=2.07, v_num=1, train_loss_step=1.600, val_loss=3.050, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 104: 100%|███████| 67/67 [00:20<00:00,  3.31it/s, loss=2.07, v_num=1, train_loss_step=1.600, val_loss=3.040, train_loss_epoch=2.380]\u001b[A\n",
      "Epoch 105:  45%|███▏   | 30/67 [00:05<00:06,  5.37it/s, loss=1.61, v_num=1, train_loss_step=1.190, val_loss=3.040, train_loss_epoch=2.190]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 105:  46%|███▏   | 31/67 [00:07<00:08,  4.06it/s, loss=1.61, v_num=1, train_loss_step=1.190, val_loss=3.040, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 105:  48%|███▎   | 32/67 [00:07<00:08,  4.02it/s, loss=1.61, v_num=1, train_loss_step=1.190, val_loss=3.040, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 105:  49%|███▍   | 33/67 [00:08<00:08,  3.97it/s, loss=1.61, v_num=1, train_loss_step=1.190, val_loss=3.040, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 105:  51%|███▌   | 34/67 [00:08<00:08,  3.93it/s, loss=1.61, v_num=1, train_loss_step=1.190, val_loss=3.040, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 105:  52%|███▋   | 35/67 [00:08<00:08,  3.90it/s, loss=1.61, v_num=1, train_loss_step=1.190, val_loss=3.040, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 105:  54%|███▊   | 36/67 [00:09<00:08,  3.87it/s, loss=1.61, v_num=1, train_loss_step=1.190, val_loss=3.040, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 105:  55%|███▊   | 37/67 [00:09<00:07,  3.83it/s, loss=1.61, v_num=1, train_loss_step=1.190, val_loss=3.040, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 105:  57%|███▉   | 38/67 [00:09<00:07,  3.81it/s, loss=1.61, v_num=1, train_loss_step=1.190, val_loss=3.040, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 105:  58%|████   | 39/67 [00:10<00:07,  3.78it/s, loss=1.61, v_num=1, train_loss_step=1.190, val_loss=3.040, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 105:  60%|████▏  | 40/67 [00:10<00:07,  3.75it/s, loss=1.61, v_num=1, train_loss_step=1.190, val_loss=3.040, train_loss_epoch=2.190]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 105:  61%|████▎  | 41/67 [00:11<00:07,  3.68it/s, loss=1.61, v_num=1, train_loss_step=1.190, val_loss=3.040, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 105:  63%|████▍  | 42/67 [00:11<00:06,  3.67it/s, loss=1.61, v_num=1, train_loss_step=1.190, val_loss=3.040, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 105:  64%|████▍  | 43/67 [00:11<00:06,  3.65it/s, loss=1.61, v_num=1, train_loss_step=1.190, val_loss=3.040, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 105:  66%|████▌  | 44/67 [00:12<00:06,  3.63it/s, loss=1.61, v_num=1, train_loss_step=1.190, val_loss=3.040, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 105:  67%|████▋  | 45/67 [00:12<00:06,  3.62it/s, loss=1.61, v_num=1, train_loss_step=1.190, val_loss=3.040, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 105:  69%|████▊  | 46/67 [00:12<00:05,  3.60it/s, loss=1.61, v_num=1, train_loss_step=1.190, val_loss=3.040, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 105:  70%|████▉  | 47/67 [00:13<00:05,  3.58it/s, loss=1.61, v_num=1, train_loss_step=1.190, val_loss=3.040, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 105:  72%|█████  | 48/67 [00:13<00:05,  3.57it/s, loss=1.61, v_num=1, train_loss_step=1.190, val_loss=3.040, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 105:  73%|█████  | 49/67 [00:13<00:05,  3.55it/s, loss=1.61, v_num=1, train_loss_step=1.190, val_loss=3.040, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 105:  75%|█████▏ | 50/67 [00:14<00:04,  3.54it/s, loss=1.61, v_num=1, train_loss_step=1.190, val_loss=3.040, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 105:  76%|█████▎ | 51/67 [00:14<00:04,  3.49it/s, loss=1.61, v_num=1, train_loss_step=1.190, val_loss=3.040, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 105:  78%|█████▍ | 52/67 [00:14<00:04,  3.48it/s, loss=1.61, v_num=1, train_loss_step=1.190, val_loss=3.040, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 105:  79%|█████▌ | 53/67 [00:15<00:04,  3.47it/s, loss=1.61, v_num=1, train_loss_step=1.190, val_loss=3.040, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 105:  81%|█████▋ | 54/67 [00:15<00:03,  3.46it/s, loss=1.61, v_num=1, train_loss_step=1.190, val_loss=3.040, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 105:  82%|█████▋ | 55/67 [00:15<00:03,  3.45it/s, loss=1.61, v_num=1, train_loss_step=1.190, val_loss=3.040, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 105:  84%|█████▊ | 56/67 [00:16<00:03,  3.44it/s, loss=1.61, v_num=1, train_loss_step=1.190, val_loss=3.040, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 105:  85%|█████▉ | 57/67 [00:16<00:02,  3.43it/s, loss=1.61, v_num=1, train_loss_step=1.190, val_loss=3.040, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 105:  87%|██████ | 58/67 [00:16<00:02,  3.42it/s, loss=1.61, v_num=1, train_loss_step=1.190, val_loss=3.040, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 105:  88%|██████▏| 59/67 [00:17<00:02,  3.41it/s, loss=1.61, v_num=1, train_loss_step=1.190, val_loss=3.040, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 105:  90%|██████▎| 60/67 [00:17<00:02,  3.40it/s, loss=1.61, v_num=1, train_loss_step=1.190, val_loss=3.040, train_loss_epoch=2.190]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 105:  91%|██████▎| 61/67 [00:18<00:01,  3.37it/s, loss=1.61, v_num=1, train_loss_step=1.190, val_loss=3.040, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 105:  93%|██████▍| 62/67 [00:18<00:01,  3.36it/s, loss=1.61, v_num=1, train_loss_step=1.190, val_loss=3.040, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 105:  94%|██████▌| 63/67 [00:18<00:01,  3.36it/s, loss=1.61, v_num=1, train_loss_step=1.190, val_loss=3.040, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 105:  96%|██████▋| 64/67 [00:19<00:00,  3.35it/s, loss=1.61, v_num=1, train_loss_step=1.190, val_loss=3.040, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 105:  97%|██████▊| 65/67 [00:19<00:00,  3.35it/s, loss=1.61, v_num=1, train_loss_step=1.190, val_loss=3.040, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 105:  99%|██████▉| 66/67 [00:19<00:00,  3.34it/s, loss=1.61, v_num=1, train_loss_step=1.190, val_loss=3.040, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 105: 100%|███████| 67/67 [00:20<00:00,  3.28it/s, loss=1.61, v_num=1, train_loss_step=1.190, val_loss=3.040, train_loss_epoch=2.190]\u001b[A\n",
      "Epoch 106:  45%|███▏   | 30/67 [00:05<00:06,  5.37it/s, loss=2.39, v_num=1, train_loss_step=4.150, val_loss=3.040, train_loss_epoch=1.660]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 106:  46%|███▏   | 31/67 [00:07<00:08,  4.09it/s, loss=2.39, v_num=1, train_loss_step=4.150, val_loss=3.040, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 106:  48%|███▎   | 32/67 [00:07<00:08,  4.04it/s, loss=2.39, v_num=1, train_loss_step=4.150, val_loss=3.040, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 106:  49%|███▍   | 33/67 [00:08<00:08,  4.00it/s, loss=2.39, v_num=1, train_loss_step=4.150, val_loss=3.040, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 106:  51%|███▌   | 34/67 [00:08<00:08,  3.96it/s, loss=2.39, v_num=1, train_loss_step=4.150, val_loss=3.040, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 106:  52%|███▋   | 35/67 [00:08<00:08,  3.92it/s, loss=2.39, v_num=1, train_loss_step=4.150, val_loss=3.040, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 106:  54%|███▊   | 36/67 [00:09<00:07,  3.89it/s, loss=2.39, v_num=1, train_loss_step=4.150, val_loss=3.040, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 106:  55%|███▊   | 37/67 [00:09<00:07,  3.85it/s, loss=2.39, v_num=1, train_loss_step=4.150, val_loss=3.040, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 106:  57%|███▉   | 38/67 [00:09<00:07,  3.83it/s, loss=2.39, v_num=1, train_loss_step=4.150, val_loss=3.040, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 106:  58%|████   | 39/67 [00:10<00:07,  3.80it/s, loss=2.39, v_num=1, train_loss_step=4.150, val_loss=3.040, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 106:  60%|████▏  | 40/67 [00:10<00:07,  3.78it/s, loss=2.39, v_num=1, train_loss_step=4.150, val_loss=3.040, train_loss_epoch=1.660]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 106:  61%|████▎  | 41/67 [00:11<00:07,  3.71it/s, loss=2.39, v_num=1, train_loss_step=4.150, val_loss=3.040, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 106:  63%|████▍  | 42/67 [00:11<00:06,  3.69it/s, loss=2.39, v_num=1, train_loss_step=4.150, val_loss=3.040, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 106:  64%|████▍  | 43/67 [00:11<00:06,  3.66it/s, loss=2.39, v_num=1, train_loss_step=4.150, val_loss=3.040, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 106:  66%|████▌  | 44/67 [00:12<00:06,  3.65it/s, loss=2.39, v_num=1, train_loss_step=4.150, val_loss=3.040, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 106:  67%|████▋  | 45/67 [00:12<00:06,  3.63it/s, loss=2.39, v_num=1, train_loss_step=4.150, val_loss=3.040, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 106:  69%|████▊  | 46/67 [00:12<00:05,  3.61it/s, loss=2.39, v_num=1, train_loss_step=4.150, val_loss=3.040, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 106:  70%|████▉  | 47/67 [00:13<00:05,  3.60it/s, loss=2.39, v_num=1, train_loss_step=4.150, val_loss=3.040, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 106:  72%|█████  | 48/67 [00:13<00:05,  3.58it/s, loss=2.39, v_num=1, train_loss_step=4.150, val_loss=3.040, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 106:  73%|█████  | 49/67 [00:13<00:05,  3.57it/s, loss=2.39, v_num=1, train_loss_step=4.150, val_loss=3.040, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 106:  75%|█████▏ | 50/67 [00:14<00:04,  3.55it/s, loss=2.39, v_num=1, train_loss_step=4.150, val_loss=3.040, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 106:  76%|█████▎ | 51/67 [00:14<00:04,  3.51it/s, loss=2.39, v_num=1, train_loss_step=4.150, val_loss=3.040, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 106:  78%|█████▍ | 52/67 [00:14<00:04,  3.49it/s, loss=2.39, v_num=1, train_loss_step=4.150, val_loss=3.040, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 106:  79%|█████▌ | 53/67 [00:15<00:04,  3.49it/s, loss=2.39, v_num=1, train_loss_step=4.150, val_loss=3.040, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 106:  81%|█████▋ | 54/67 [00:15<00:03,  3.48it/s, loss=2.39, v_num=1, train_loss_step=4.150, val_loss=3.040, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 106:  82%|█████▋ | 55/67 [00:15<00:03,  3.46it/s, loss=2.39, v_num=1, train_loss_step=4.150, val_loss=3.040, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 106:  84%|█████▊ | 56/67 [00:16<00:03,  3.45it/s, loss=2.39, v_num=1, train_loss_step=4.150, val_loss=3.040, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 106:  85%|█████▉ | 57/67 [00:16<00:02,  3.44it/s, loss=2.39, v_num=1, train_loss_step=4.150, val_loss=3.040, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 106:  87%|██████ | 58/67 [00:16<00:02,  3.43it/s, loss=2.39, v_num=1, train_loss_step=4.150, val_loss=3.040, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 106:  88%|██████▏| 59/67 [00:17<00:02,  3.43it/s, loss=2.39, v_num=1, train_loss_step=4.150, val_loss=3.040, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 106:  90%|██████▎| 60/67 [00:17<00:02,  3.42it/s, loss=2.39, v_num=1, train_loss_step=4.150, val_loss=3.040, train_loss_epoch=1.660]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 106:  91%|██████▎| 61/67 [00:18<00:01,  3.39it/s, loss=2.39, v_num=1, train_loss_step=4.150, val_loss=3.040, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 106:  93%|██████▍| 62/67 [00:18<00:01,  3.38it/s, loss=2.39, v_num=1, train_loss_step=4.150, val_loss=3.040, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 106:  94%|██████▌| 63/67 [00:18<00:01,  3.37it/s, loss=2.39, v_num=1, train_loss_step=4.150, val_loss=3.040, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 106:  96%|██████▋| 64/67 [00:18<00:00,  3.37it/s, loss=2.39, v_num=1, train_loss_step=4.150, val_loss=3.040, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 106:  97%|██████▊| 65/67 [00:19<00:00,  3.37it/s, loss=2.39, v_num=1, train_loss_step=4.150, val_loss=3.040, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 106:  99%|██████▉| 66/67 [00:19<00:00,  3.36it/s, loss=2.39, v_num=1, train_loss_step=4.150, val_loss=3.040, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 106: 100%|███████| 67/67 [00:20<00:00,  3.29it/s, loss=2.39, v_num=1, train_loss_step=4.150, val_loss=3.040, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 107:  45%|███▏   | 30/67 [00:05<00:06,  5.39it/s, loss=2.39, v_num=1, train_loss_step=2.820, val_loss=3.040, train_loss_epoch=2.340]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 107:  46%|███▏   | 31/67 [00:07<00:08,  4.16it/s, loss=2.39, v_num=1, train_loss_step=2.820, val_loss=3.040, train_loss_epoch=2.340]\u001b[A\n",
      "Epoch 107:  48%|███▎   | 32/67 [00:07<00:08,  4.11it/s, loss=2.39, v_num=1, train_loss_step=2.820, val_loss=3.040, train_loss_epoch=2.340]\u001b[A\n",
      "Epoch 107:  49%|███▍   | 33/67 [00:08<00:08,  4.07it/s, loss=2.39, v_num=1, train_loss_step=2.820, val_loss=3.040, train_loss_epoch=2.340]\u001b[A\n",
      "Epoch 107:  51%|███▌   | 34/67 [00:08<00:08,  4.03it/s, loss=2.39, v_num=1, train_loss_step=2.820, val_loss=3.040, train_loss_epoch=2.340]\u001b[A\n",
      "Epoch 107:  52%|███▋   | 35/67 [00:08<00:08,  3.98it/s, loss=2.39, v_num=1, train_loss_step=2.820, val_loss=3.040, train_loss_epoch=2.340]\u001b[A\n",
      "Epoch 107:  54%|███▊   | 36/67 [00:09<00:07,  3.95it/s, loss=2.39, v_num=1, train_loss_step=2.820, val_loss=3.040, train_loss_epoch=2.340]\u001b[A\n",
      "Epoch 107:  55%|███▊   | 37/67 [00:09<00:07,  3.91it/s, loss=2.39, v_num=1, train_loss_step=2.820, val_loss=3.040, train_loss_epoch=2.340]\u001b[A\n",
      "Epoch 107:  57%|███▉   | 38/67 [00:09<00:07,  3.88it/s, loss=2.39, v_num=1, train_loss_step=2.820, val_loss=3.040, train_loss_epoch=2.340]\u001b[A\n",
      "Epoch 107:  58%|████   | 39/67 [00:10<00:07,  3.85it/s, loss=2.39, v_num=1, train_loss_step=2.820, val_loss=3.040, train_loss_epoch=2.340]\u001b[A\n",
      "Epoch 107:  60%|████▏  | 40/67 [00:10<00:07,  3.83it/s, loss=2.39, v_num=1, train_loss_step=2.820, val_loss=3.040, train_loss_epoch=2.340]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 107:  61%|████▎  | 41/67 [00:10<00:06,  3.76it/s, loss=2.39, v_num=1, train_loss_step=2.820, val_loss=3.040, train_loss_epoch=2.340]\u001b[A\n",
      "Epoch 107:  63%|████▍  | 42/67 [00:11<00:06,  3.74it/s, loss=2.39, v_num=1, train_loss_step=2.820, val_loss=3.040, train_loss_epoch=2.340]\u001b[A\n",
      "Epoch 107:  64%|████▍  | 43/67 [00:11<00:06,  3.71it/s, loss=2.39, v_num=1, train_loss_step=2.820, val_loss=3.040, train_loss_epoch=2.340]\u001b[A\n",
      "Epoch 107:  66%|████▌  | 44/67 [00:11<00:06,  3.69it/s, loss=2.39, v_num=1, train_loss_step=2.820, val_loss=3.040, train_loss_epoch=2.340]\u001b[A\n",
      "Epoch 107:  67%|████▋  | 45/67 [00:12<00:05,  3.67it/s, loss=2.39, v_num=1, train_loss_step=2.820, val_loss=3.040, train_loss_epoch=2.340]\u001b[A\n",
      "Epoch 107:  69%|████▊  | 46/67 [00:12<00:05,  3.66it/s, loss=2.39, v_num=1, train_loss_step=2.820, val_loss=3.040, train_loss_epoch=2.340]\u001b[A\n",
      "Epoch 107:  70%|████▉  | 47/67 [00:12<00:05,  3.64it/s, loss=2.39, v_num=1, train_loss_step=2.820, val_loss=3.040, train_loss_epoch=2.340]\u001b[A\n",
      "Epoch 107:  72%|█████  | 48/67 [00:13<00:05,  3.62it/s, loss=2.39, v_num=1, train_loss_step=2.820, val_loss=3.040, train_loss_epoch=2.340]\u001b[A\n",
      "Epoch 107:  73%|█████  | 49/67 [00:13<00:04,  3.61it/s, loss=2.39, v_num=1, train_loss_step=2.820, val_loss=3.040, train_loss_epoch=2.340]\u001b[A\n",
      "Epoch 107:  75%|█████▏ | 50/67 [00:13<00:04,  3.59it/s, loss=2.39, v_num=1, train_loss_step=2.820, val_loss=3.040, train_loss_epoch=2.340]\u001b[A\n",
      "Epoch 107:  76%|█████▎ | 51/67 [00:14<00:04,  3.55it/s, loss=2.39, v_num=1, train_loss_step=2.820, val_loss=3.040, train_loss_epoch=2.340]\u001b[A\n",
      "Epoch 107:  78%|█████▍ | 52/67 [00:14<00:04,  3.53it/s, loss=2.39, v_num=1, train_loss_step=2.820, val_loss=3.040, train_loss_epoch=2.340]\u001b[A\n",
      "Epoch 107:  79%|█████▌ | 53/67 [00:15<00:03,  3.52it/s, loss=2.39, v_num=1, train_loss_step=2.820, val_loss=3.040, train_loss_epoch=2.340]\u001b[A\n",
      "Epoch 107:  81%|█████▋ | 54/67 [00:15<00:03,  3.51it/s, loss=2.39, v_num=1, train_loss_step=2.820, val_loss=3.040, train_loss_epoch=2.340]\u001b[A\n",
      "Epoch 107:  82%|█████▋ | 55/67 [00:15<00:03,  3.50it/s, loss=2.39, v_num=1, train_loss_step=2.820, val_loss=3.040, train_loss_epoch=2.340]\u001b[A\n",
      "Epoch 107:  84%|█████▊ | 56/67 [00:16<00:03,  3.49it/s, loss=2.39, v_num=1, train_loss_step=2.820, val_loss=3.040, train_loss_epoch=2.340]\u001b[A\n",
      "Epoch 107:  85%|█████▉ | 57/67 [00:16<00:02,  3.48it/s, loss=2.39, v_num=1, train_loss_step=2.820, val_loss=3.040, train_loss_epoch=2.340]\u001b[A\n",
      "Epoch 107:  87%|██████ | 58/67 [00:16<00:02,  3.47it/s, loss=2.39, v_num=1, train_loss_step=2.820, val_loss=3.040, train_loss_epoch=2.340]\u001b[A\n",
      "Epoch 107:  88%|██████▏| 59/67 [00:17<00:02,  3.46it/s, loss=2.39, v_num=1, train_loss_step=2.820, val_loss=3.040, train_loss_epoch=2.340]\u001b[A\n",
      "Epoch 107:  90%|██████▎| 60/67 [00:17<00:02,  3.45it/s, loss=2.39, v_num=1, train_loss_step=2.820, val_loss=3.040, train_loss_epoch=2.340]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 107:  91%|██████▎| 61/67 [00:17<00:01,  3.42it/s, loss=2.39, v_num=1, train_loss_step=2.820, val_loss=3.040, train_loss_epoch=2.340]\u001b[A\n",
      "Epoch 107:  93%|██████▍| 62/67 [00:18<00:01,  3.41it/s, loss=2.39, v_num=1, train_loss_step=2.820, val_loss=3.040, train_loss_epoch=2.340]\u001b[A\n",
      "Epoch 107:  94%|██████▌| 63/67 [00:18<00:01,  3.40it/s, loss=2.39, v_num=1, train_loss_step=2.820, val_loss=3.040, train_loss_epoch=2.340]\u001b[A\n",
      "Epoch 107:  96%|██████▋| 64/67 [00:18<00:00,  3.40it/s, loss=2.39, v_num=1, train_loss_step=2.820, val_loss=3.040, train_loss_epoch=2.340]\u001b[A\n",
      "Epoch 107:  97%|██████▊| 65/67 [00:19<00:00,  3.39it/s, loss=2.39, v_num=1, train_loss_step=2.820, val_loss=3.040, train_loss_epoch=2.340]\u001b[A\n",
      "Epoch 107:  99%|██████▉| 66/67 [00:19<00:00,  3.39it/s, loss=2.39, v_num=1, train_loss_step=2.820, val_loss=3.040, train_loss_epoch=2.340]\u001b[A\n",
      "Epoch 107: 100%|███████| 67/67 [00:20<00:00,  3.32it/s, loss=2.39, v_num=1, train_loss_step=2.820, val_loss=3.040, train_loss_epoch=2.340]\u001b[A\n",
      "Epoch 108:  45%|███▏   | 30/67 [00:05<00:06,  5.38it/s, loss=2.16, v_num=1, train_loss_step=0.951, val_loss=3.040, train_loss_epoch=2.390]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 108:  46%|███▏   | 31/67 [00:07<00:08,  4.04it/s, loss=2.16, v_num=1, train_loss_step=0.951, val_loss=3.040, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 108:  48%|███▎   | 32/67 [00:07<00:08,  4.00it/s, loss=2.16, v_num=1, train_loss_step=0.951, val_loss=3.040, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 108:  49%|███▍   | 33/67 [00:08<00:08,  3.96it/s, loss=2.16, v_num=1, train_loss_step=0.951, val_loss=3.040, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 108:  51%|███▌   | 34/67 [00:08<00:08,  3.92it/s, loss=2.16, v_num=1, train_loss_step=0.951, val_loss=3.040, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 108:  52%|███▋   | 35/67 [00:09<00:08,  3.89it/s, loss=2.16, v_num=1, train_loss_step=0.951, val_loss=3.040, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 108:  54%|███▊   | 36/67 [00:09<00:08,  3.86it/s, loss=2.16, v_num=1, train_loss_step=0.951, val_loss=3.040, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 108:  55%|███▊   | 37/67 [00:09<00:07,  3.83it/s, loss=2.16, v_num=1, train_loss_step=0.951, val_loss=3.040, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 108:  57%|███▉   | 38/67 [00:09<00:07,  3.80it/s, loss=2.16, v_num=1, train_loss_step=0.951, val_loss=3.040, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 108:  58%|████   | 39/67 [00:10<00:07,  3.77it/s, loss=2.16, v_num=1, train_loss_step=0.951, val_loss=3.040, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 108:  60%|████▏  | 40/67 [00:10<00:07,  3.75it/s, loss=2.16, v_num=1, train_loss_step=0.951, val_loss=3.040, train_loss_epoch=2.390]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 108:  61%|████▎  | 41/67 [00:11<00:07,  3.69it/s, loss=2.16, v_num=1, train_loss_step=0.951, val_loss=3.040, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 108:  63%|████▍  | 42/67 [00:11<00:06,  3.67it/s, loss=2.16, v_num=1, train_loss_step=0.951, val_loss=3.040, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 108:  64%|████▍  | 43/67 [00:11<00:06,  3.65it/s, loss=2.16, v_num=1, train_loss_step=0.951, val_loss=3.040, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 108:  66%|████▌  | 44/67 [00:12<00:06,  3.63it/s, loss=2.16, v_num=1, train_loss_step=0.951, val_loss=3.040, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 108:  67%|████▋  | 45/67 [00:12<00:06,  3.61it/s, loss=2.16, v_num=1, train_loss_step=0.951, val_loss=3.040, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 108:  69%|████▊  | 46/67 [00:12<00:05,  3.60it/s, loss=2.16, v_num=1, train_loss_step=0.951, val_loss=3.040, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 108:  70%|████▉  | 47/67 [00:13<00:05,  3.58it/s, loss=2.16, v_num=1, train_loss_step=0.951, val_loss=3.040, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 108:  72%|█████  | 48/67 [00:13<00:05,  3.57it/s, loss=2.16, v_num=1, train_loss_step=0.951, val_loss=3.040, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 108:  73%|█████  | 49/67 [00:13<00:05,  3.56it/s, loss=2.16, v_num=1, train_loss_step=0.951, val_loss=3.040, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 108:  75%|█████▏ | 50/67 [00:14<00:04,  3.54it/s, loss=2.16, v_num=1, train_loss_step=0.951, val_loss=3.040, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 108:  76%|█████▎ | 51/67 [00:14<00:04,  3.49it/s, loss=2.16, v_num=1, train_loss_step=0.951, val_loss=3.040, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 108:  78%|█████▍ | 52/67 [00:14<00:04,  3.48it/s, loss=2.16, v_num=1, train_loss_step=0.951, val_loss=3.040, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 108:  79%|█████▌ | 53/67 [00:15<00:04,  3.47it/s, loss=2.16, v_num=1, train_loss_step=0.951, val_loss=3.040, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 108:  81%|█████▋ | 54/67 [00:15<00:03,  3.46it/s, loss=2.16, v_num=1, train_loss_step=0.951, val_loss=3.040, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 108:  82%|█████▋ | 55/67 [00:15<00:03,  3.45it/s, loss=2.16, v_num=1, train_loss_step=0.951, val_loss=3.040, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 108:  84%|█████▊ | 56/67 [00:16<00:03,  3.44it/s, loss=2.16, v_num=1, train_loss_step=0.951, val_loss=3.040, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 108:  85%|█████▉ | 57/67 [00:16<00:02,  3.43it/s, loss=2.16, v_num=1, train_loss_step=0.951, val_loss=3.040, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 108:  87%|██████ | 58/67 [00:16<00:02,  3.42it/s, loss=2.16, v_num=1, train_loss_step=0.951, val_loss=3.040, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 108:  88%|██████▏| 59/67 [00:17<00:02,  3.41it/s, loss=2.16, v_num=1, train_loss_step=0.951, val_loss=3.040, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 108:  90%|██████▎| 60/67 [00:17<00:02,  3.41it/s, loss=2.16, v_num=1, train_loss_step=0.951, val_loss=3.040, train_loss_epoch=2.390]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 108:  91%|██████▎| 61/67 [00:18<00:01,  3.38it/s, loss=2.16, v_num=1, train_loss_step=0.951, val_loss=3.040, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 108:  93%|██████▍| 62/67 [00:18<00:01,  3.37it/s, loss=2.16, v_num=1, train_loss_step=0.951, val_loss=3.040, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 108:  94%|██████▌| 63/67 [00:18<00:01,  3.36it/s, loss=2.16, v_num=1, train_loss_step=0.951, val_loss=3.040, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 108:  96%|██████▋| 64/67 [00:19<00:00,  3.36it/s, loss=2.16, v_num=1, train_loss_step=0.951, val_loss=3.040, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 108:  97%|██████▊| 65/67 [00:19<00:00,  3.35it/s, loss=2.16, v_num=1, train_loss_step=0.951, val_loss=3.040, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 108:  99%|██████▉| 66/67 [00:19<00:00,  3.35it/s, loss=2.16, v_num=1, train_loss_step=0.951, val_loss=3.040, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 108: 100%|███████| 67/67 [00:20<00:00,  3.29it/s, loss=2.16, v_num=1, train_loss_step=0.951, val_loss=3.040, train_loss_epoch=2.390]\u001b[A\n",
      "Epoch 109:  45%|███▏   | 30/67 [00:05<00:06,  5.40it/s, loss=2.35, v_num=1, train_loss_step=2.960, val_loss=3.040, train_loss_epoch=2.280]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 109:  46%|███▏   | 31/67 [00:07<00:08,  4.10it/s, loss=2.35, v_num=1, train_loss_step=2.960, val_loss=3.040, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 109:  48%|███▎   | 32/67 [00:07<00:08,  4.06it/s, loss=2.35, v_num=1, train_loss_step=2.960, val_loss=3.040, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 109:  49%|███▍   | 33/67 [00:08<00:08,  4.02it/s, loss=2.35, v_num=1, train_loss_step=2.960, val_loss=3.040, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 109:  51%|███▌   | 34/67 [00:08<00:08,  3.97it/s, loss=2.35, v_num=1, train_loss_step=2.960, val_loss=3.040, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 109:  52%|███▋   | 35/67 [00:08<00:08,  3.94it/s, loss=2.35, v_num=1, train_loss_step=2.960, val_loss=3.040, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 109:  54%|███▊   | 36/67 [00:09<00:07,  3.91it/s, loss=2.35, v_num=1, train_loss_step=2.960, val_loss=3.040, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 109:  55%|███▊   | 37/67 [00:09<00:07,  3.87it/s, loss=2.35, v_num=1, train_loss_step=2.960, val_loss=3.040, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 109:  57%|███▉   | 38/67 [00:09<00:07,  3.84it/s, loss=2.35, v_num=1, train_loss_step=2.960, val_loss=3.040, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 109:  58%|████   | 39/67 [00:10<00:07,  3.82it/s, loss=2.35, v_num=1, train_loss_step=2.960, val_loss=3.040, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 109:  60%|████▏  | 40/67 [00:10<00:07,  3.79it/s, loss=2.35, v_num=1, train_loss_step=2.960, val_loss=3.040, train_loss_epoch=2.280]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 109:  61%|████▎  | 41/67 [00:11<00:07,  3.71it/s, loss=2.35, v_num=1, train_loss_step=2.960, val_loss=3.040, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 109:  63%|████▍  | 42/67 [00:11<00:06,  3.69it/s, loss=2.35, v_num=1, train_loss_step=2.960, val_loss=3.040, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 109:  64%|████▍  | 43/67 [00:11<00:06,  3.67it/s, loss=2.35, v_num=1, train_loss_step=2.960, val_loss=3.040, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 109:  66%|████▌  | 44/67 [00:12<00:06,  3.65it/s, loss=2.35, v_num=1, train_loss_step=2.960, val_loss=3.040, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 109:  67%|████▋  | 45/67 [00:12<00:06,  3.63it/s, loss=2.35, v_num=1, train_loss_step=2.960, val_loss=3.040, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 109:  69%|████▊  | 46/67 [00:12<00:05,  3.61it/s, loss=2.35, v_num=1, train_loss_step=2.960, val_loss=3.040, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 109:  70%|████▉  | 47/67 [00:13<00:05,  3.60it/s, loss=2.35, v_num=1, train_loss_step=2.960, val_loss=3.040, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 109:  72%|█████  | 48/67 [00:13<00:05,  3.59it/s, loss=2.35, v_num=1, train_loss_step=2.960, val_loss=3.040, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 109:  73%|█████  | 49/67 [00:13<00:05,  3.57it/s, loss=2.35, v_num=1, train_loss_step=2.960, val_loss=3.040, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 109:  75%|█████▏ | 50/67 [00:14<00:04,  3.56it/s, loss=2.35, v_num=1, train_loss_step=2.960, val_loss=3.040, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 109:  76%|█████▎ | 51/67 [00:14<00:04,  3.51it/s, loss=2.35, v_num=1, train_loss_step=2.960, val_loss=3.040, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 109:  78%|█████▍ | 52/67 [00:14<00:04,  3.50it/s, loss=2.35, v_num=1, train_loss_step=2.960, val_loss=3.040, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 109:  79%|█████▌ | 53/67 [00:15<00:04,  3.49it/s, loss=2.35, v_num=1, train_loss_step=2.960, val_loss=3.040, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 109:  81%|█████▋ | 54/67 [00:15<00:03,  3.48it/s, loss=2.35, v_num=1, train_loss_step=2.960, val_loss=3.040, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 109:  82%|█████▋ | 55/67 [00:15<00:03,  3.47it/s, loss=2.35, v_num=1, train_loss_step=2.960, val_loss=3.040, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 109:  84%|█████▊ | 56/67 [00:16<00:03,  3.46it/s, loss=2.35, v_num=1, train_loss_step=2.960, val_loss=3.040, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 109:  85%|█████▉ | 57/67 [00:16<00:02,  3.46it/s, loss=2.35, v_num=1, train_loss_step=2.960, val_loss=3.040, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 109:  87%|██████ | 58/67 [00:16<00:02,  3.45it/s, loss=2.35, v_num=1, train_loss_step=2.960, val_loss=3.040, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 109:  88%|██████▏| 59/67 [00:17<00:02,  3.44it/s, loss=2.35, v_num=1, train_loss_step=2.960, val_loss=3.040, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 109:  90%|██████▎| 60/67 [00:17<00:02,  3.43it/s, loss=2.35, v_num=1, train_loss_step=2.960, val_loss=3.040, train_loss_epoch=2.280]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 109:  91%|██████▎| 61/67 [00:17<00:01,  3.39it/s, loss=2.35, v_num=1, train_loss_step=2.960, val_loss=3.040, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 109:  93%|██████▍| 62/67 [00:18<00:01,  3.39it/s, loss=2.35, v_num=1, train_loss_step=2.960, val_loss=3.040, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 109:  94%|██████▌| 63/67 [00:18<00:01,  3.38it/s, loss=2.35, v_num=1, train_loss_step=2.960, val_loss=3.040, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 109:  96%|██████▋| 64/67 [00:18<00:00,  3.38it/s, loss=2.35, v_num=1, train_loss_step=2.960, val_loss=3.040, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 109:  97%|██████▊| 65/67 [00:19<00:00,  3.37it/s, loss=2.35, v_num=1, train_loss_step=2.960, val_loss=3.040, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 109:  99%|██████▉| 66/67 [00:19<00:00,  3.37it/s, loss=2.35, v_num=1, train_loss_step=2.960, val_loss=3.040, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 109: 100%|███████| 67/67 [00:20<00:00,  3.31it/s, loss=2.35, v_num=1, train_loss_step=2.960, val_loss=3.030, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 110:  45%|███▏   | 30/67 [00:05<00:06,  5.32it/s, loss=2.19, v_num=1, train_loss_step=0.531, val_loss=3.030, train_loss_epoch=2.200]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 110:  46%|███▏   | 31/67 [00:07<00:08,  4.01it/s, loss=2.19, v_num=1, train_loss_step=0.531, val_loss=3.030, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 110:  48%|███▎   | 32/67 [00:08<00:08,  3.97it/s, loss=2.19, v_num=1, train_loss_step=0.531, val_loss=3.030, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 110:  49%|███▍   | 33/67 [00:08<00:08,  3.93it/s, loss=2.19, v_num=1, train_loss_step=0.531, val_loss=3.030, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 110:  51%|███▌   | 34/67 [00:08<00:08,  3.89it/s, loss=2.19, v_num=1, train_loss_step=0.531, val_loss=3.030, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 110:  52%|███▋   | 35/67 [00:09<00:08,  3.86it/s, loss=2.19, v_num=1, train_loss_step=0.531, val_loss=3.030, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 110:  54%|███▊   | 36/67 [00:09<00:08,  3.83it/s, loss=2.19, v_num=1, train_loss_step=0.531, val_loss=3.030, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 110:  55%|███▊   | 37/67 [00:09<00:07,  3.80it/s, loss=2.19, v_num=1, train_loss_step=0.531, val_loss=3.030, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 110:  57%|███▉   | 38/67 [00:10<00:07,  3.77it/s, loss=2.19, v_num=1, train_loss_step=0.531, val_loss=3.030, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 110:  58%|████   | 39/67 [00:10<00:07,  3.74it/s, loss=2.19, v_num=1, train_loss_step=0.531, val_loss=3.030, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 110:  60%|████▏  | 40/67 [00:10<00:07,  3.71it/s, loss=2.19, v_num=1, train_loss_step=0.531, val_loss=3.030, train_loss_epoch=2.200]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 110:  61%|████▎  | 41/67 [00:11<00:07,  3.65it/s, loss=2.19, v_num=1, train_loss_step=0.531, val_loss=3.030, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 110:  63%|████▍  | 42/67 [00:11<00:06,  3.63it/s, loss=2.19, v_num=1, train_loss_step=0.531, val_loss=3.030, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 110:  64%|████▍  | 43/67 [00:11<00:06,  3.61it/s, loss=2.19, v_num=1, train_loss_step=0.531, val_loss=3.030, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 110:  66%|████▌  | 44/67 [00:12<00:06,  3.59it/s, loss=2.19, v_num=1, train_loss_step=0.531, val_loss=3.030, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 110:  67%|████▋  | 45/67 [00:12<00:06,  3.58it/s, loss=2.19, v_num=1, train_loss_step=0.531, val_loss=3.030, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 110:  69%|████▊  | 46/67 [00:12<00:05,  3.56it/s, loss=2.19, v_num=1, train_loss_step=0.531, val_loss=3.030, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 110:  70%|████▉  | 47/67 [00:13<00:05,  3.54it/s, loss=2.19, v_num=1, train_loss_step=0.531, val_loss=3.030, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 110:  72%|█████  | 48/67 [00:13<00:05,  3.53it/s, loss=2.19, v_num=1, train_loss_step=0.531, val_loss=3.030, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 110:  73%|█████  | 49/67 [00:13<00:05,  3.52it/s, loss=2.19, v_num=1, train_loss_step=0.531, val_loss=3.030, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 110:  75%|█████▏ | 50/67 [00:14<00:04,  3.51it/s, loss=2.19, v_num=1, train_loss_step=0.531, val_loss=3.030, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 110:  76%|█████▎ | 51/67 [00:14<00:04,  3.46it/s, loss=2.19, v_num=1, train_loss_step=0.531, val_loss=3.030, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 110:  78%|█████▍ | 52/67 [00:15<00:04,  3.45it/s, loss=2.19, v_num=1, train_loss_step=0.531, val_loss=3.030, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 110:  79%|█████▌ | 53/67 [00:15<00:04,  3.44it/s, loss=2.19, v_num=1, train_loss_step=0.531, val_loss=3.030, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 110:  81%|█████▋ | 54/67 [00:15<00:03,  3.43it/s, loss=2.19, v_num=1, train_loss_step=0.531, val_loss=3.030, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 110:  82%|█████▋ | 55/67 [00:16<00:03,  3.42it/s, loss=2.19, v_num=1, train_loss_step=0.531, val_loss=3.030, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 110:  84%|█████▊ | 56/67 [00:16<00:03,  3.41it/s, loss=2.19, v_num=1, train_loss_step=0.531, val_loss=3.030, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 110:  85%|█████▉ | 57/67 [00:16<00:02,  3.41it/s, loss=2.19, v_num=1, train_loss_step=0.531, val_loss=3.030, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 110:  87%|██████ | 58/67 [00:17<00:02,  3.40it/s, loss=2.19, v_num=1, train_loss_step=0.531, val_loss=3.030, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 110:  88%|██████▏| 59/67 [00:17<00:02,  3.39it/s, loss=2.19, v_num=1, train_loss_step=0.531, val_loss=3.030, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 110:  90%|██████▎| 60/67 [00:17<00:02,  3.38it/s, loss=2.19, v_num=1, train_loss_step=0.531, val_loss=3.030, train_loss_epoch=2.200]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 110:  91%|██████▎| 61/67 [00:18<00:01,  3.35it/s, loss=2.19, v_num=1, train_loss_step=0.531, val_loss=3.030, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 110:  93%|██████▍| 62/67 [00:18<00:01,  3.35it/s, loss=2.19, v_num=1, train_loss_step=0.531, val_loss=3.030, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 110:  94%|██████▌| 63/67 [00:18<00:01,  3.34it/s, loss=2.19, v_num=1, train_loss_step=0.531, val_loss=3.030, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 110:  96%|██████▋| 64/67 [00:19<00:00,  3.33it/s, loss=2.19, v_num=1, train_loss_step=0.531, val_loss=3.030, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 110:  97%|██████▊| 65/67 [00:19<00:00,  3.33it/s, loss=2.19, v_num=1, train_loss_step=0.531, val_loss=3.030, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 110:  99%|██████▉| 66/67 [00:19<00:00,  3.33it/s, loss=2.19, v_num=1, train_loss_step=0.531, val_loss=3.030, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 110: 100%|███████| 67/67 [00:20<00:00,  3.26it/s, loss=2.19, v_num=1, train_loss_step=0.531, val_loss=3.030, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 111:  45%|███▏   | 30/67 [00:05<00:07,  5.21it/s, loss=2.19, v_num=1, train_loss_step=0.900, val_loss=3.030, train_loss_epoch=2.080]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 111:  46%|███▏   | 31/67 [00:07<00:09,  4.00it/s, loss=2.19, v_num=1, train_loss_step=0.900, val_loss=3.030, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 111:  48%|███▎   | 32/67 [00:08<00:08,  3.96it/s, loss=2.19, v_num=1, train_loss_step=0.900, val_loss=3.030, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 111:  49%|███▍   | 33/67 [00:08<00:08,  3.92it/s, loss=2.19, v_num=1, train_loss_step=0.900, val_loss=3.030, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 111:  51%|███▌   | 34/67 [00:08<00:08,  3.88it/s, loss=2.19, v_num=1, train_loss_step=0.900, val_loss=3.030, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 111:  52%|███▋   | 35/67 [00:09<00:08,  3.85it/s, loss=2.19, v_num=1, train_loss_step=0.900, val_loss=3.030, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 111:  54%|███▊   | 36/67 [00:09<00:08,  3.81it/s, loss=2.19, v_num=1, train_loss_step=0.900, val_loss=3.030, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 111:  55%|███▊   | 37/67 [00:09<00:07,  3.78it/s, loss=2.19, v_num=1, train_loss_step=0.900, val_loss=3.030, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 111:  57%|███▉   | 38/67 [00:10<00:07,  3.75it/s, loss=2.19, v_num=1, train_loss_step=0.900, val_loss=3.030, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 111:  58%|████   | 39/67 [00:10<00:07,  3.73it/s, loss=2.19, v_num=1, train_loss_step=0.900, val_loss=3.030, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 111:  60%|████▏  | 40/67 [00:10<00:07,  3.71it/s, loss=2.19, v_num=1, train_loss_step=0.900, val_loss=3.030, train_loss_epoch=2.080]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 111:  61%|████▎  | 41/67 [00:11<00:07,  3.63it/s, loss=2.19, v_num=1, train_loss_step=0.900, val_loss=3.030, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 111:  63%|████▍  | 42/67 [00:11<00:06,  3.61it/s, loss=2.19, v_num=1, train_loss_step=0.900, val_loss=3.030, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 111:  64%|████▍  | 43/67 [00:11<00:06,  3.59it/s, loss=2.19, v_num=1, train_loss_step=0.900, val_loss=3.030, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 111:  66%|████▌  | 44/67 [00:12<00:06,  3.57it/s, loss=2.19, v_num=1, train_loss_step=0.900, val_loss=3.030, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 111:  67%|████▋  | 45/67 [00:12<00:06,  3.56it/s, loss=2.19, v_num=1, train_loss_step=0.900, val_loss=3.030, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 111:  69%|████▊  | 46/67 [00:12<00:05,  3.54it/s, loss=2.19, v_num=1, train_loss_step=0.900, val_loss=3.030, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 111:  70%|████▉  | 47/67 [00:13<00:05,  3.53it/s, loss=2.19, v_num=1, train_loss_step=0.900, val_loss=3.030, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 111:  72%|█████  | 48/67 [00:13<00:05,  3.51it/s, loss=2.19, v_num=1, train_loss_step=0.900, val_loss=3.030, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 111:  73%|█████  | 49/67 [00:14<00:05,  3.50it/s, loss=2.19, v_num=1, train_loss_step=0.900, val_loss=3.030, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 111:  75%|█████▏ | 50/67 [00:14<00:04,  3.48it/s, loss=2.19, v_num=1, train_loss_step=0.900, val_loss=3.030, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 111:  76%|█████▎ | 51/67 [00:14<00:04,  3.43it/s, loss=2.19, v_num=1, train_loss_step=0.900, val_loss=3.030, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 111:  78%|█████▍ | 52/67 [00:15<00:04,  3.42it/s, loss=2.19, v_num=1, train_loss_step=0.900, val_loss=3.030, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 111:  79%|█████▌ | 53/67 [00:15<00:04,  3.41it/s, loss=2.19, v_num=1, train_loss_step=0.900, val_loss=3.030, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 111:  81%|█████▋ | 54/67 [00:15<00:03,  3.40it/s, loss=2.19, v_num=1, train_loss_step=0.900, val_loss=3.030, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 111:  82%|█████▋ | 55/67 [00:16<00:03,  3.39it/s, loss=2.19, v_num=1, train_loss_step=0.900, val_loss=3.030, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 111:  84%|█████▊ | 56/67 [00:16<00:03,  3.38it/s, loss=2.19, v_num=1, train_loss_step=0.900, val_loss=3.030, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 111:  85%|█████▉ | 57/67 [00:16<00:02,  3.37it/s, loss=2.19, v_num=1, train_loss_step=0.900, val_loss=3.030, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 111:  87%|██████ | 58/67 [00:17<00:02,  3.37it/s, loss=2.19, v_num=1, train_loss_step=0.900, val_loss=3.030, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 111:  88%|██████▏| 59/67 [00:17<00:02,  3.36it/s, loss=2.19, v_num=1, train_loss_step=0.900, val_loss=3.030, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 111:  90%|██████▎| 60/67 [00:17<00:02,  3.35it/s, loss=2.19, v_num=1, train_loss_step=0.900, val_loss=3.030, train_loss_epoch=2.080]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 111:  91%|██████▎| 61/67 [00:18<00:01,  3.32it/s, loss=2.19, v_num=1, train_loss_step=0.900, val_loss=3.030, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 111:  93%|██████▍| 62/67 [00:18<00:01,  3.32it/s, loss=2.19, v_num=1, train_loss_step=0.900, val_loss=3.030, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 111:  94%|██████▌| 63/67 [00:19<00:01,  3.31it/s, loss=2.19, v_num=1, train_loss_step=0.900, val_loss=3.030, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 111:  96%|██████▋| 64/67 [00:19<00:00,  3.31it/s, loss=2.19, v_num=1, train_loss_step=0.900, val_loss=3.030, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 111:  97%|██████▊| 65/67 [00:19<00:00,  3.30it/s, loss=2.19, v_num=1, train_loss_step=0.900, val_loss=3.030, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 111:  99%|██████▉| 66/67 [00:20<00:00,  3.29it/s, loss=2.19, v_num=1, train_loss_step=0.900, val_loss=3.030, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 111: 100%|███████| 67/67 [00:20<00:00,  3.23it/s, loss=2.19, v_num=1, train_loss_step=0.900, val_loss=3.030, train_loss_epoch=2.080]\u001b[A\n",
      "Epoch 112:  45%|███▏   | 30/67 [00:05<00:07,  5.15it/s, loss=4.27, v_num=1, train_loss_step=1.910, val_loss=3.030, train_loss_epoch=2.490]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 112:  46%|███▏   | 31/67 [00:07<00:09,  3.95it/s, loss=4.27, v_num=1, train_loss_step=1.910, val_loss=3.030, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 112:  48%|███▎   | 32/67 [00:08<00:08,  3.91it/s, loss=4.27, v_num=1, train_loss_step=1.910, val_loss=3.030, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 112:  49%|███▍   | 33/67 [00:08<00:08,  3.86it/s, loss=4.27, v_num=1, train_loss_step=1.910, val_loss=3.030, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 112:  51%|███▌   | 34/67 [00:08<00:08,  3.82it/s, loss=4.27, v_num=1, train_loss_step=1.910, val_loss=3.030, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 112:  52%|███▋   | 35/67 [00:09<00:08,  3.78it/s, loss=4.27, v_num=1, train_loss_step=1.910, val_loss=3.030, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 112:  54%|███▊   | 36/67 [00:09<00:08,  3.75it/s, loss=4.27, v_num=1, train_loss_step=1.910, val_loss=3.030, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 112:  55%|███▊   | 37/67 [00:09<00:08,  3.71it/s, loss=4.27, v_num=1, train_loss_step=1.910, val_loss=3.030, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 112:  57%|███▉   | 38/67 [00:10<00:07,  3.69it/s, loss=4.27, v_num=1, train_loss_step=1.910, val_loss=3.030, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 112:  58%|████   | 39/67 [00:10<00:07,  3.67it/s, loss=4.27, v_num=1, train_loss_step=1.910, val_loss=3.030, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 112:  60%|████▏  | 40/67 [00:10<00:07,  3.65it/s, loss=4.27, v_num=1, train_loss_step=1.910, val_loss=3.030, train_loss_epoch=2.490]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 112:  61%|████▎  | 41/67 [00:11<00:07,  3.58it/s, loss=4.27, v_num=1, train_loss_step=1.910, val_loss=3.030, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 112:  63%|████▍  | 42/67 [00:11<00:07,  3.56it/s, loss=4.27, v_num=1, train_loss_step=1.910, val_loss=3.030, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 112:  64%|████▍  | 43/67 [00:12<00:06,  3.53it/s, loss=4.27, v_num=1, train_loss_step=1.910, val_loss=3.030, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 112:  66%|████▌  | 44/67 [00:12<00:06,  3.52it/s, loss=4.27, v_num=1, train_loss_step=1.910, val_loss=3.030, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 112:  67%|████▋  | 45/67 [00:12<00:06,  3.50it/s, loss=4.27, v_num=1, train_loss_step=1.910, val_loss=3.030, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 112:  69%|████▊  | 46/67 [00:13<00:06,  3.48it/s, loss=4.27, v_num=1, train_loss_step=1.910, val_loss=3.030, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 112:  70%|████▉  | 47/67 [00:13<00:05,  3.47it/s, loss=4.27, v_num=1, train_loss_step=1.910, val_loss=3.030, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 112:  72%|█████  | 48/67 [00:13<00:05,  3.46it/s, loss=4.27, v_num=1, train_loss_step=1.910, val_loss=3.030, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 112:  73%|█████  | 49/67 [00:14<00:05,  3.44it/s, loss=4.27, v_num=1, train_loss_step=1.910, val_loss=3.030, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 112:  75%|█████▏ | 50/67 [00:14<00:04,  3.43it/s, loss=4.27, v_num=1, train_loss_step=1.910, val_loss=3.030, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 112:  76%|█████▎ | 51/67 [00:15<00:04,  3.38it/s, loss=4.27, v_num=1, train_loss_step=1.910, val_loss=3.030, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 112:  78%|█████▍ | 52/67 [00:15<00:04,  3.36it/s, loss=4.27, v_num=1, train_loss_step=1.910, val_loss=3.030, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 112:  79%|█████▌ | 53/67 [00:15<00:04,  3.35it/s, loss=4.27, v_num=1, train_loss_step=1.910, val_loss=3.030, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 112:  81%|█████▋ | 54/67 [00:16<00:03,  3.34it/s, loss=4.27, v_num=1, train_loss_step=1.910, val_loss=3.030, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 112:  82%|█████▋ | 55/67 [00:16<00:03,  3.33it/s, loss=4.27, v_num=1, train_loss_step=1.910, val_loss=3.030, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 112:  84%|█████▊ | 56/67 [00:16<00:03,  3.32it/s, loss=4.27, v_num=1, train_loss_step=1.910, val_loss=3.030, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 112:  85%|█████▉ | 57/67 [00:17<00:03,  3.31it/s, loss=4.27, v_num=1, train_loss_step=1.910, val_loss=3.030, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 112:  87%|██████ | 58/67 [00:17<00:02,  3.30it/s, loss=4.27, v_num=1, train_loss_step=1.910, val_loss=3.030, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 112:  88%|██████▏| 59/67 [00:17<00:02,  3.29it/s, loss=4.27, v_num=1, train_loss_step=1.910, val_loss=3.030, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 112:  90%|██████▎| 60/67 [00:18<00:02,  3.29it/s, loss=4.27, v_num=1, train_loss_step=1.910, val_loss=3.030, train_loss_epoch=2.490]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 112:  91%|██████▎| 61/67 [00:18<00:01,  3.26it/s, loss=4.27, v_num=1, train_loss_step=1.910, val_loss=3.030, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 112:  93%|██████▍| 62/67 [00:19<00:01,  3.26it/s, loss=4.27, v_num=1, train_loss_step=1.910, val_loss=3.030, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 112:  94%|██████▌| 63/67 [00:19<00:01,  3.26it/s, loss=4.27, v_num=1, train_loss_step=1.910, val_loss=3.030, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 112:  96%|██████▋| 64/67 [00:19<00:00,  3.25it/s, loss=4.27, v_num=1, train_loss_step=1.910, val_loss=3.030, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 112:  97%|██████▊| 65/67 [00:20<00:00,  3.25it/s, loss=4.27, v_num=1, train_loss_step=1.910, val_loss=3.030, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 112:  99%|██████▉| 66/67 [00:20<00:00,  3.24it/s, loss=4.27, v_num=1, train_loss_step=1.910, val_loss=3.030, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 112: 100%|███████| 67/67 [00:21<00:00,  3.18it/s, loss=4.27, v_num=1, train_loss_step=1.910, val_loss=3.030, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 113:  45%|███▏   | 30/67 [00:05<00:06,  5.39it/s, loss=1.94, v_num=1, train_loss_step=1.710, val_loss=3.030, train_loss_epoch=3.840]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 113:  46%|███▏   | 31/67 [00:07<00:08,  4.02it/s, loss=1.94, v_num=1, train_loss_step=1.710, val_loss=3.030, train_loss_epoch=3.840]\u001b[A\n",
      "Epoch 113:  48%|███▎   | 32/67 [00:08<00:08,  3.98it/s, loss=1.94, v_num=1, train_loss_step=1.710, val_loss=3.030, train_loss_epoch=3.840]\u001b[A\n",
      "Epoch 113:  49%|███▍   | 33/67 [00:08<00:08,  3.94it/s, loss=1.94, v_num=1, train_loss_step=1.710, val_loss=3.030, train_loss_epoch=3.840]\u001b[A\n",
      "Epoch 113:  51%|███▌   | 34/67 [00:08<00:08,  3.90it/s, loss=1.94, v_num=1, train_loss_step=1.710, val_loss=3.030, train_loss_epoch=3.840]\u001b[A\n",
      "Epoch 113:  52%|███▋   | 35/67 [00:09<00:08,  3.87it/s, loss=1.94, v_num=1, train_loss_step=1.710, val_loss=3.030, train_loss_epoch=3.840]\u001b[A\n",
      "Epoch 113:  54%|███▊   | 36/67 [00:09<00:08,  3.84it/s, loss=1.94, v_num=1, train_loss_step=1.710, val_loss=3.030, train_loss_epoch=3.840]\u001b[A\n",
      "Epoch 113:  55%|███▊   | 37/67 [00:09<00:07,  3.80it/s, loss=1.94, v_num=1, train_loss_step=1.710, val_loss=3.030, train_loss_epoch=3.840]\u001b[A\n",
      "Epoch 113:  57%|███▉   | 38/67 [00:10<00:07,  3.76it/s, loss=1.94, v_num=1, train_loss_step=1.710, val_loss=3.030, train_loss_epoch=3.840]\u001b[A\n",
      "Epoch 113:  58%|████   | 39/67 [00:10<00:07,  3.74it/s, loss=1.94, v_num=1, train_loss_step=1.710, val_loss=3.030, train_loss_epoch=3.840]\u001b[A\n",
      "Epoch 113:  60%|████▏  | 40/67 [00:10<00:07,  3.70it/s, loss=1.94, v_num=1, train_loss_step=1.710, val_loss=3.030, train_loss_epoch=3.840]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 113:  61%|████▎  | 41/67 [00:11<00:07,  3.63it/s, loss=1.94, v_num=1, train_loss_step=1.710, val_loss=3.030, train_loss_epoch=3.840]\u001b[A\n",
      "Epoch 113:  63%|████▍  | 42/67 [00:11<00:06,  3.61it/s, loss=1.94, v_num=1, train_loss_step=1.710, val_loss=3.030, train_loss_epoch=3.840]\u001b[A\n",
      "Epoch 113:  64%|████▍  | 43/67 [00:11<00:06,  3.59it/s, loss=1.94, v_num=1, train_loss_step=1.710, val_loss=3.030, train_loss_epoch=3.840]\u001b[A\n",
      "Epoch 113:  66%|████▌  | 44/67 [00:12<00:06,  3.57it/s, loss=1.94, v_num=1, train_loss_step=1.710, val_loss=3.030, train_loss_epoch=3.840]\u001b[A\n",
      "Epoch 113:  67%|████▋  | 45/67 [00:12<00:06,  3.56it/s, loss=1.94, v_num=1, train_loss_step=1.710, val_loss=3.030, train_loss_epoch=3.840]\u001b[A\n",
      "Epoch 113:  69%|████▊  | 46/67 [00:13<00:05,  3.54it/s, loss=1.94, v_num=1, train_loss_step=1.710, val_loss=3.030, train_loss_epoch=3.840]\u001b[A\n",
      "Epoch 113:  70%|████▉  | 47/67 [00:13<00:05,  3.53it/s, loss=1.94, v_num=1, train_loss_step=1.710, val_loss=3.030, train_loss_epoch=3.840]\u001b[A\n",
      "Epoch 113:  72%|█████  | 48/67 [00:13<00:05,  3.51it/s, loss=1.94, v_num=1, train_loss_step=1.710, val_loss=3.030, train_loss_epoch=3.840]\u001b[A\n",
      "Epoch 113:  73%|█████  | 49/67 [00:14<00:05,  3.50it/s, loss=1.94, v_num=1, train_loss_step=1.710, val_loss=3.030, train_loss_epoch=3.840]\u001b[A\n",
      "Epoch 113:  75%|█████▏ | 50/67 [00:14<00:04,  3.48it/s, loss=1.94, v_num=1, train_loss_step=1.710, val_loss=3.030, train_loss_epoch=3.840]\u001b[A\n",
      "Epoch 113:  76%|█████▎ | 51/67 [00:14<00:04,  3.42it/s, loss=1.94, v_num=1, train_loss_step=1.710, val_loss=3.030, train_loss_epoch=3.840]\u001b[A\n",
      "Epoch 113:  78%|█████▍ | 52/67 [00:15<00:04,  3.41it/s, loss=1.94, v_num=1, train_loss_step=1.710, val_loss=3.030, train_loss_epoch=3.840]\u001b[A\n",
      "Epoch 113:  79%|█████▌ | 53/67 [00:15<00:04,  3.40it/s, loss=1.94, v_num=1, train_loss_step=1.710, val_loss=3.030, train_loss_epoch=3.840]\u001b[A\n",
      "Epoch 113:  81%|█████▋ | 54/67 [00:15<00:03,  3.39it/s, loss=1.94, v_num=1, train_loss_step=1.710, val_loss=3.030, train_loss_epoch=3.840]\u001b[A\n",
      "Epoch 113:  82%|█████▋ | 55/67 [00:16<00:03,  3.37it/s, loss=1.94, v_num=1, train_loss_step=1.710, val_loss=3.030, train_loss_epoch=3.840]\u001b[A\n",
      "Epoch 113:  84%|█████▊ | 56/67 [00:16<00:03,  3.37it/s, loss=1.94, v_num=1, train_loss_step=1.710, val_loss=3.030, train_loss_epoch=3.840]\u001b[A\n",
      "Epoch 113:  85%|█████▉ | 57/67 [00:16<00:02,  3.36it/s, loss=1.94, v_num=1, train_loss_step=1.710, val_loss=3.030, train_loss_epoch=3.840]\u001b[A\n",
      "Epoch 113:  87%|██████ | 58/67 [00:17<00:02,  3.35it/s, loss=1.94, v_num=1, train_loss_step=1.710, val_loss=3.030, train_loss_epoch=3.840]\u001b[A\n",
      "Epoch 113:  88%|██████▏| 59/67 [00:17<00:02,  3.34it/s, loss=1.94, v_num=1, train_loss_step=1.710, val_loss=3.030, train_loss_epoch=3.840]\u001b[A\n",
      "Epoch 113:  90%|██████▎| 60/67 [00:17<00:02,  3.33it/s, loss=1.94, v_num=1, train_loss_step=1.710, val_loss=3.030, train_loss_epoch=3.840]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 113:  91%|██████▎| 61/67 [00:18<00:01,  3.30it/s, loss=1.94, v_num=1, train_loss_step=1.710, val_loss=3.030, train_loss_epoch=3.840]\u001b[A\n",
      "Epoch 113:  93%|██████▍| 62/67 [00:18<00:01,  3.30it/s, loss=1.94, v_num=1, train_loss_step=1.710, val_loss=3.030, train_loss_epoch=3.840]\u001b[A\n",
      "Epoch 113:  94%|██████▌| 63/67 [00:19<00:01,  3.29it/s, loss=1.94, v_num=1, train_loss_step=1.710, val_loss=3.030, train_loss_epoch=3.840]\u001b[A\n",
      "Epoch 113:  96%|██████▋| 64/67 [00:19<00:00,  3.28it/s, loss=1.94, v_num=1, train_loss_step=1.710, val_loss=3.030, train_loss_epoch=3.840]\u001b[A\n",
      "Epoch 113:  97%|██████▊| 65/67 [00:19<00:00,  3.28it/s, loss=1.94, v_num=1, train_loss_step=1.710, val_loss=3.030, train_loss_epoch=3.840]\u001b[A\n",
      "Epoch 113:  99%|██████▉| 66/67 [00:20<00:00,  3.27it/s, loss=1.94, v_num=1, train_loss_step=1.710, val_loss=3.030, train_loss_epoch=3.840]\u001b[A\n",
      "Epoch 113: 100%|███████| 67/67 [00:20<00:00,  3.20it/s, loss=1.94, v_num=1, train_loss_step=1.710, val_loss=3.030, train_loss_epoch=3.840]\u001b[A\n",
      "Epoch 114:  45%|███▏   | 30/67 [00:05<00:07,  5.18it/s, loss=1.92, v_num=1, train_loss_step=3.210, val_loss=3.030, train_loss_epoch=1.740]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 114:  46%|███▏   | 31/67 [00:07<00:09,  3.91it/s, loss=1.92, v_num=1, train_loss_step=3.210, val_loss=3.030, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 114:  48%|███▎   | 32/67 [00:08<00:09,  3.87it/s, loss=1.92, v_num=1, train_loss_step=3.210, val_loss=3.030, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 114:  49%|███▍   | 33/67 [00:08<00:08,  3.84it/s, loss=1.92, v_num=1, train_loss_step=3.210, val_loss=3.030, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 114:  51%|███▌   | 34/67 [00:08<00:08,  3.81it/s, loss=1.92, v_num=1, train_loss_step=3.210, val_loss=3.030, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 114:  52%|███▋   | 35/67 [00:09<00:08,  3.77it/s, loss=1.92, v_num=1, train_loss_step=3.210, val_loss=3.030, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 114:  54%|███▊   | 36/67 [00:09<00:08,  3.75it/s, loss=1.92, v_num=1, train_loss_step=3.210, val_loss=3.030, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 114:  55%|███▊   | 37/67 [00:09<00:08,  3.72it/s, loss=1.92, v_num=1, train_loss_step=3.210, val_loss=3.030, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 114:  57%|███▉   | 38/67 [00:10<00:07,  3.70it/s, loss=1.92, v_num=1, train_loss_step=3.210, val_loss=3.030, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 114:  58%|████   | 39/67 [00:10<00:07,  3.68it/s, loss=1.92, v_num=1, train_loss_step=3.210, val_loss=3.030, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 114:  60%|████▏  | 40/67 [00:10<00:07,  3.66it/s, loss=1.92, v_num=1, train_loss_step=3.210, val_loss=3.030, train_loss_epoch=1.740]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 114:  61%|████▎  | 41/67 [00:11<00:07,  3.59it/s, loss=1.92, v_num=1, train_loss_step=3.210, val_loss=3.030, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 114:  63%|████▍  | 42/67 [00:11<00:06,  3.58it/s, loss=1.92, v_num=1, train_loss_step=3.210, val_loss=3.030, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 114:  64%|████▍  | 43/67 [00:12<00:06,  3.56it/s, loss=1.92, v_num=1, train_loss_step=3.210, val_loss=3.030, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 114:  66%|████▌  | 44/67 [00:12<00:06,  3.55it/s, loss=1.92, v_num=1, train_loss_step=3.210, val_loss=3.030, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 114:  67%|████▋  | 45/67 [00:12<00:06,  3.54it/s, loss=1.92, v_num=1, train_loss_step=3.210, val_loss=3.030, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 114:  69%|████▊  | 46/67 [00:13<00:05,  3.52it/s, loss=1.92, v_num=1, train_loss_step=3.210, val_loss=3.030, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 114:  70%|████▉  | 47/67 [00:13<00:05,  3.51it/s, loss=1.92, v_num=1, train_loss_step=3.210, val_loss=3.030, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 114:  72%|█████  | 48/67 [00:13<00:05,  3.50it/s, loss=1.92, v_num=1, train_loss_step=3.210, val_loss=3.030, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 114:  73%|█████  | 49/67 [00:14<00:05,  3.49it/s, loss=1.92, v_num=1, train_loss_step=3.210, val_loss=3.030, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 114:  75%|█████▏ | 50/67 [00:14<00:04,  3.48it/s, loss=1.92, v_num=1, train_loss_step=3.210, val_loss=3.030, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 114:  76%|█████▎ | 51/67 [00:14<00:04,  3.43it/s, loss=1.92, v_num=1, train_loss_step=3.210, val_loss=3.030, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 114:  78%|█████▍ | 52/67 [00:15<00:04,  3.42it/s, loss=1.92, v_num=1, train_loss_step=3.210, val_loss=3.030, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 114:  79%|█████▌ | 53/67 [00:15<00:04,  3.41it/s, loss=1.92, v_num=1, train_loss_step=3.210, val_loss=3.030, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 114:  81%|█████▋ | 54/67 [00:15<00:03,  3.41it/s, loss=1.92, v_num=1, train_loss_step=3.210, val_loss=3.030, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 114:  82%|█████▋ | 55/67 [00:16<00:03,  3.40it/s, loss=1.92, v_num=1, train_loss_step=3.210, val_loss=3.030, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 114:  84%|█████▊ | 56/67 [00:16<00:03,  3.39it/s, loss=1.92, v_num=1, train_loss_step=3.210, val_loss=3.030, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 114:  85%|█████▉ | 57/67 [00:16<00:02,  3.38it/s, loss=1.92, v_num=1, train_loss_step=3.210, val_loss=3.030, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 114:  87%|██████ | 58/67 [00:17<00:02,  3.37it/s, loss=1.92, v_num=1, train_loss_step=3.210, val_loss=3.030, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 114:  88%|██████▏| 59/67 [00:17<00:02,  3.37it/s, loss=1.92, v_num=1, train_loss_step=3.210, val_loss=3.030, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 114:  90%|██████▎| 60/67 [00:17<00:02,  3.36it/s, loss=1.92, v_num=1, train_loss_step=3.210, val_loss=3.030, train_loss_epoch=1.740]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 114:  91%|██████▎| 61/67 [00:18<00:01,  3.33it/s, loss=1.92, v_num=1, train_loss_step=3.210, val_loss=3.030, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 114:  93%|██████▍| 62/67 [00:18<00:01,  3.32it/s, loss=1.92, v_num=1, train_loss_step=3.210, val_loss=3.030, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 114:  94%|██████▌| 63/67 [00:18<00:01,  3.32it/s, loss=1.92, v_num=1, train_loss_step=3.210, val_loss=3.030, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 114:  96%|██████▋| 64/67 [00:19<00:00,  3.32it/s, loss=1.92, v_num=1, train_loss_step=3.210, val_loss=3.030, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 114:  97%|██████▊| 65/67 [00:19<00:00,  3.31it/s, loss=1.92, v_num=1, train_loss_step=3.210, val_loss=3.030, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 114:  99%|██████▉| 66/67 [00:19<00:00,  3.30it/s, loss=1.92, v_num=1, train_loss_step=3.210, val_loss=3.030, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 114: 100%|███████| 67/67 [00:20<00:00,  3.24it/s, loss=1.92, v_num=1, train_loss_step=3.210, val_loss=3.030, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 115:  45%|███▏   | 30/67 [00:05<00:06,  5.31it/s, loss=2.67, v_num=1, train_loss_step=0.419, val_loss=3.030, train_loss_epoch=1.870]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 115:  46%|███▏   | 31/67 [00:07<00:09,  3.98it/s, loss=2.67, v_num=1, train_loss_step=0.419, val_loss=3.030, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 115:  48%|███▎   | 32/67 [00:08<00:08,  3.94it/s, loss=2.67, v_num=1, train_loss_step=0.419, val_loss=3.030, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 115:  49%|███▍   | 33/67 [00:08<00:08,  3.90it/s, loss=2.67, v_num=1, train_loss_step=0.419, val_loss=3.030, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 115:  51%|███▌   | 34/67 [00:08<00:08,  3.86it/s, loss=2.67, v_num=1, train_loss_step=0.419, val_loss=3.030, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 115:  52%|███▋   | 35/67 [00:09<00:08,  3.83it/s, loss=2.67, v_num=1, train_loss_step=0.419, val_loss=3.030, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 115:  54%|███▊   | 36/67 [00:09<00:08,  3.80it/s, loss=2.67, v_num=1, train_loss_step=0.419, val_loss=3.030, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 115:  55%|███▊   | 37/67 [00:09<00:07,  3.77it/s, loss=2.67, v_num=1, train_loss_step=0.419, val_loss=3.030, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 115:  57%|███▉   | 38/67 [00:10<00:07,  3.74it/s, loss=2.67, v_num=1, train_loss_step=0.419, val_loss=3.030, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 115:  58%|████   | 39/67 [00:10<00:07,  3.72it/s, loss=2.67, v_num=1, train_loss_step=0.419, val_loss=3.030, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 115:  60%|████▏  | 40/67 [00:10<00:07,  3.69it/s, loss=2.67, v_num=1, train_loss_step=0.419, val_loss=3.030, train_loss_epoch=1.870]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 115:  61%|████▎  | 41/67 [00:11<00:07,  3.63it/s, loss=2.67, v_num=1, train_loss_step=0.419, val_loss=3.030, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 115:  63%|████▍  | 42/67 [00:11<00:06,  3.61it/s, loss=2.67, v_num=1, train_loss_step=0.419, val_loss=3.030, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 115:  64%|████▍  | 43/67 [00:11<00:06,  3.59it/s, loss=2.67, v_num=1, train_loss_step=0.419, val_loss=3.030, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 115:  66%|████▌  | 44/67 [00:12<00:06,  3.57it/s, loss=2.67, v_num=1, train_loss_step=0.419, val_loss=3.030, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 115:  67%|████▋  | 45/67 [00:12<00:06,  3.56it/s, loss=2.67, v_num=1, train_loss_step=0.419, val_loss=3.030, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 115:  69%|████▊  | 46/67 [00:12<00:05,  3.54it/s, loss=2.67, v_num=1, train_loss_step=0.419, val_loss=3.030, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 115:  70%|████▉  | 47/67 [00:13<00:05,  3.53it/s, loss=2.67, v_num=1, train_loss_step=0.419, val_loss=3.030, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 115:  72%|█████  | 48/67 [00:13<00:05,  3.51it/s, loss=2.67, v_num=1, train_loss_step=0.419, val_loss=3.030, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 115:  73%|█████  | 49/67 [00:14<00:05,  3.49it/s, loss=2.67, v_num=1, train_loss_step=0.419, val_loss=3.030, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 115:  75%|█████▏ | 50/67 [00:14<00:04,  3.48it/s, loss=2.67, v_num=1, train_loss_step=0.419, val_loss=3.030, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 115:  76%|█████▎ | 51/67 [00:14<00:04,  3.44it/s, loss=2.67, v_num=1, train_loss_step=0.419, val_loss=3.030, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 115:  78%|█████▍ | 52/67 [00:15<00:04,  3.43it/s, loss=2.67, v_num=1, train_loss_step=0.419, val_loss=3.030, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 115:  79%|█████▌ | 53/67 [00:15<00:04,  3.42it/s, loss=2.67, v_num=1, train_loss_step=0.419, val_loss=3.030, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 115:  81%|█████▋ | 54/67 [00:15<00:03,  3.41it/s, loss=2.67, v_num=1, train_loss_step=0.419, val_loss=3.030, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 115:  82%|█████▋ | 55/67 [00:16<00:03,  3.40it/s, loss=2.67, v_num=1, train_loss_step=0.419, val_loss=3.030, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 115:  84%|█████▊ | 56/67 [00:16<00:03,  3.39it/s, loss=2.67, v_num=1, train_loss_step=0.419, val_loss=3.030, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 115:  85%|█████▉ | 57/67 [00:16<00:02,  3.38it/s, loss=2.67, v_num=1, train_loss_step=0.419, val_loss=3.030, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 115:  87%|██████ | 58/67 [00:17<00:02,  3.37it/s, loss=2.67, v_num=1, train_loss_step=0.419, val_loss=3.030, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 115:  88%|██████▏| 59/67 [00:17<00:02,  3.36it/s, loss=2.67, v_num=1, train_loss_step=0.419, val_loss=3.030, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 115:  90%|██████▎| 60/67 [00:17<00:02,  3.36it/s, loss=2.67, v_num=1, train_loss_step=0.419, val_loss=3.030, train_loss_epoch=1.870]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 115:  91%|██████▎| 61/67 [00:18<00:01,  3.33it/s, loss=2.67, v_num=1, train_loss_step=0.419, val_loss=3.030, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 115:  93%|██████▍| 62/67 [00:18<00:01,  3.32it/s, loss=2.67, v_num=1, train_loss_step=0.419, val_loss=3.030, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 115:  94%|██████▌| 63/67 [00:18<00:01,  3.32it/s, loss=2.67, v_num=1, train_loss_step=0.419, val_loss=3.030, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 115:  96%|██████▋| 64/67 [00:19<00:00,  3.32it/s, loss=2.67, v_num=1, train_loss_step=0.419, val_loss=3.030, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 115:  97%|██████▊| 65/67 [00:19<00:00,  3.31it/s, loss=2.67, v_num=1, train_loss_step=0.419, val_loss=3.030, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 115:  99%|██████▉| 66/67 [00:19<00:00,  3.31it/s, loss=2.67, v_num=1, train_loss_step=0.419, val_loss=3.030, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 115: 100%|███████| 67/67 [00:20<00:00,  3.25it/s, loss=2.67, v_num=1, train_loss_step=0.419, val_loss=3.020, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 116:  45%|███▌    | 30/67 [00:05<00:06,  5.30it/s, loss=1.4, v_num=1, train_loss_step=1.580, val_loss=3.020, train_loss_epoch=3.000]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 116:  46%|███▋    | 31/67 [00:07<00:09,  3.94it/s, loss=1.4, v_num=1, train_loss_step=1.580, val_loss=3.020, train_loss_epoch=3.000]\u001b[A\n",
      "Epoch 116:  48%|███▊    | 32/67 [00:08<00:08,  3.90it/s, loss=1.4, v_num=1, train_loss_step=1.580, val_loss=3.020, train_loss_epoch=3.000]\u001b[A\n",
      "Epoch 116:  49%|███▉    | 33/67 [00:08<00:08,  3.87it/s, loss=1.4, v_num=1, train_loss_step=1.580, val_loss=3.020, train_loss_epoch=3.000]\u001b[A\n",
      "Epoch 116:  51%|████    | 34/67 [00:08<00:08,  3.83it/s, loss=1.4, v_num=1, train_loss_step=1.580, val_loss=3.020, train_loss_epoch=3.000]\u001b[A\n",
      "Epoch 116:  52%|████▏   | 35/67 [00:09<00:08,  3.80it/s, loss=1.4, v_num=1, train_loss_step=1.580, val_loss=3.020, train_loss_epoch=3.000]\u001b[A\n",
      "Epoch 116:  54%|████▎   | 36/67 [00:09<00:08,  3.77it/s, loss=1.4, v_num=1, train_loss_step=1.580, val_loss=3.020, train_loss_epoch=3.000]\u001b[A\n",
      "Epoch 116:  55%|████▍   | 37/67 [00:09<00:08,  3.74it/s, loss=1.4, v_num=1, train_loss_step=1.580, val_loss=3.020, train_loss_epoch=3.000]\u001b[A\n",
      "Epoch 116:  57%|████▌   | 38/67 [00:10<00:07,  3.71it/s, loss=1.4, v_num=1, train_loss_step=1.580, val_loss=3.020, train_loss_epoch=3.000]\u001b[A\n",
      "Epoch 116:  58%|████▋   | 39/67 [00:10<00:07,  3.69it/s, loss=1.4, v_num=1, train_loss_step=1.580, val_loss=3.020, train_loss_epoch=3.000]\u001b[A\n",
      "Epoch 116:  60%|████▊   | 40/67 [00:10<00:07,  3.66it/s, loss=1.4, v_num=1, train_loss_step=1.580, val_loss=3.020, train_loss_epoch=3.000]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 116:  61%|████▉   | 41/67 [00:11<00:07,  3.61it/s, loss=1.4, v_num=1, train_loss_step=1.580, val_loss=3.020, train_loss_epoch=3.000]\u001b[A\n",
      "Epoch 116:  63%|█████   | 42/67 [00:11<00:06,  3.59it/s, loss=1.4, v_num=1, train_loss_step=1.580, val_loss=3.020, train_loss_epoch=3.000]\u001b[A\n",
      "Epoch 116:  64%|█████▏  | 43/67 [00:12<00:06,  3.57it/s, loss=1.4, v_num=1, train_loss_step=1.580, val_loss=3.020, train_loss_epoch=3.000]\u001b[A\n",
      "Epoch 116:  66%|█████▎  | 44/67 [00:12<00:06,  3.55it/s, loss=1.4, v_num=1, train_loss_step=1.580, val_loss=3.020, train_loss_epoch=3.000]\u001b[A\n",
      "Epoch 116:  67%|█████▎  | 45/67 [00:12<00:06,  3.54it/s, loss=1.4, v_num=1, train_loss_step=1.580, val_loss=3.020, train_loss_epoch=3.000]\u001b[A\n",
      "Epoch 116:  69%|█████▍  | 46/67 [00:13<00:05,  3.53it/s, loss=1.4, v_num=1, train_loss_step=1.580, val_loss=3.020, train_loss_epoch=3.000]\u001b[A\n",
      "Epoch 116:  70%|█████▌  | 47/67 [00:13<00:05,  3.52it/s, loss=1.4, v_num=1, train_loss_step=1.580, val_loss=3.020, train_loss_epoch=3.000]\u001b[A\n",
      "Epoch 116:  72%|█████▋  | 48/67 [00:13<00:05,  3.51it/s, loss=1.4, v_num=1, train_loss_step=1.580, val_loss=3.020, train_loss_epoch=3.000]\u001b[A\n",
      "Epoch 116:  73%|█████▊  | 49/67 [00:14<00:05,  3.49it/s, loss=1.4, v_num=1, train_loss_step=1.580, val_loss=3.020, train_loss_epoch=3.000]\u001b[A\n",
      "Epoch 116:  75%|█████▉  | 50/67 [00:14<00:04,  3.49it/s, loss=1.4, v_num=1, train_loss_step=1.580, val_loss=3.020, train_loss_epoch=3.000]\u001b[A\n",
      "Epoch 116:  76%|██████  | 51/67 [00:14<00:04,  3.45it/s, loss=1.4, v_num=1, train_loss_step=1.580, val_loss=3.020, train_loss_epoch=3.000]\u001b[A\n",
      "Epoch 116:  78%|██████▏ | 52/67 [00:15<00:04,  3.44it/s, loss=1.4, v_num=1, train_loss_step=1.580, val_loss=3.020, train_loss_epoch=3.000]\u001b[A\n",
      "Epoch 116:  79%|██████▎ | 53/67 [00:15<00:04,  3.43it/s, loss=1.4, v_num=1, train_loss_step=1.580, val_loss=3.020, train_loss_epoch=3.000]\u001b[A\n",
      "Epoch 116:  81%|██████▍ | 54/67 [00:15<00:03,  3.42it/s, loss=1.4, v_num=1, train_loss_step=1.580, val_loss=3.020, train_loss_epoch=3.000]\u001b[A\n",
      "Epoch 116:  82%|██████▌ | 55/67 [00:16<00:03,  3.42it/s, loss=1.4, v_num=1, train_loss_step=1.580, val_loss=3.020, train_loss_epoch=3.000]\u001b[A\n",
      "Epoch 116:  84%|██████▋ | 56/67 [00:16<00:03,  3.41it/s, loss=1.4, v_num=1, train_loss_step=1.580, val_loss=3.020, train_loss_epoch=3.000]\u001b[A\n",
      "Epoch 116:  85%|██████▊ | 57/67 [00:16<00:02,  3.40it/s, loss=1.4, v_num=1, train_loss_step=1.580, val_loss=3.020, train_loss_epoch=3.000]\u001b[A\n",
      "Epoch 116:  87%|██████▉ | 58/67 [00:17<00:02,  3.40it/s, loss=1.4, v_num=1, train_loss_step=1.580, val_loss=3.020, train_loss_epoch=3.000]\u001b[A\n",
      "Epoch 116:  88%|███████ | 59/67 [00:17<00:02,  3.39it/s, loss=1.4, v_num=1, train_loss_step=1.580, val_loss=3.020, train_loss_epoch=3.000]\u001b[A\n",
      "Epoch 116:  90%|███████▏| 60/67 [00:17<00:02,  3.38it/s, loss=1.4, v_num=1, train_loss_step=1.580, val_loss=3.020, train_loss_epoch=3.000]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 116:  91%|███████▎| 61/67 [00:18<00:01,  3.35it/s, loss=1.4, v_num=1, train_loss_step=1.580, val_loss=3.020, train_loss_epoch=3.000]\u001b[A\n",
      "Epoch 116:  93%|███████▍| 62/67 [00:18<00:01,  3.35it/s, loss=1.4, v_num=1, train_loss_step=1.580, val_loss=3.020, train_loss_epoch=3.000]\u001b[A\n",
      "Epoch 116:  94%|███████▌| 63/67 [00:18<00:01,  3.34it/s, loss=1.4, v_num=1, train_loss_step=1.580, val_loss=3.020, train_loss_epoch=3.000]\u001b[A\n",
      "Epoch 116:  96%|███████▋| 64/67 [00:19<00:00,  3.34it/s, loss=1.4, v_num=1, train_loss_step=1.580, val_loss=3.020, train_loss_epoch=3.000]\u001b[A\n",
      "Epoch 116:  97%|███████▊| 65/67 [00:19<00:00,  3.34it/s, loss=1.4, v_num=1, train_loss_step=1.580, val_loss=3.020, train_loss_epoch=3.000]\u001b[A\n",
      "Epoch 116:  99%|███████▉| 66/67 [00:19<00:00,  3.33it/s, loss=1.4, v_num=1, train_loss_step=1.580, val_loss=3.020, train_loss_epoch=3.000]\u001b[A\n",
      "Epoch 116: 100%|████████| 67/67 [00:20<00:00,  3.27it/s, loss=1.4, v_num=1, train_loss_step=1.580, val_loss=3.020, train_loss_epoch=3.000]\u001b[A\n",
      "Epoch 117:  45%|███▏   | 30/67 [00:05<00:06,  5.36it/s, loss=2.22, v_num=1, train_loss_step=1.440, val_loss=3.020, train_loss_epoch=1.690]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 117:  46%|███▏   | 31/67 [00:07<00:08,  4.05it/s, loss=2.22, v_num=1, train_loss_step=1.440, val_loss=3.020, train_loss_epoch=1.690]\u001b[A\n",
      "Epoch 117:  48%|███▎   | 32/67 [00:07<00:08,  4.02it/s, loss=2.22, v_num=1, train_loss_step=1.440, val_loss=3.020, train_loss_epoch=1.690]\u001b[A\n",
      "Epoch 117:  49%|███▍   | 33/67 [00:08<00:08,  3.98it/s, loss=2.22, v_num=1, train_loss_step=1.440, val_loss=3.020, train_loss_epoch=1.690]\u001b[A\n",
      "Epoch 117:  51%|███▌   | 34/67 [00:08<00:08,  3.94it/s, loss=2.22, v_num=1, train_loss_step=1.440, val_loss=3.020, train_loss_epoch=1.690]\u001b[A\n",
      "Epoch 117:  52%|███▋   | 35/67 [00:08<00:08,  3.91it/s, loss=2.22, v_num=1, train_loss_step=1.440, val_loss=3.020, train_loss_epoch=1.690]\u001b[A\n",
      "Epoch 117:  54%|███▊   | 36/67 [00:09<00:07,  3.88it/s, loss=2.22, v_num=1, train_loss_step=1.440, val_loss=3.020, train_loss_epoch=1.690]\u001b[A\n",
      "Epoch 117:  55%|███▊   | 37/67 [00:09<00:07,  3.85it/s, loss=2.22, v_num=1, train_loss_step=1.440, val_loss=3.020, train_loss_epoch=1.690]\u001b[A\n",
      "Epoch 117:  57%|███▉   | 38/67 [00:09<00:07,  3.83it/s, loss=2.22, v_num=1, train_loss_step=1.440, val_loss=3.020, train_loss_epoch=1.690]\u001b[A\n",
      "Epoch 117:  58%|████   | 39/67 [00:10<00:07,  3.80it/s, loss=2.22, v_num=1, train_loss_step=1.440, val_loss=3.020, train_loss_epoch=1.690]\u001b[A\n",
      "Epoch 117:  60%|████▏  | 40/67 [00:10<00:07,  3.78it/s, loss=2.22, v_num=1, train_loss_step=1.440, val_loss=3.020, train_loss_epoch=1.690]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 117:  61%|████▎  | 41/67 [00:11<00:07,  3.70it/s, loss=2.22, v_num=1, train_loss_step=1.440, val_loss=3.020, train_loss_epoch=1.690]\u001b[A\n",
      "Epoch 117:  63%|████▍  | 42/67 [00:11<00:06,  3.68it/s, loss=2.22, v_num=1, train_loss_step=1.440, val_loss=3.020, train_loss_epoch=1.690]\u001b[A\n",
      "Epoch 117:  64%|████▍  | 43/67 [00:11<00:06,  3.66it/s, loss=2.22, v_num=1, train_loss_step=1.440, val_loss=3.020, train_loss_epoch=1.690]\u001b[A\n",
      "Epoch 117:  66%|████▌  | 44/67 [00:12<00:06,  3.64it/s, loss=2.22, v_num=1, train_loss_step=1.440, val_loss=3.020, train_loss_epoch=1.690]\u001b[A\n",
      "Epoch 117:  67%|████▋  | 45/67 [00:12<00:06,  3.62it/s, loss=2.22, v_num=1, train_loss_step=1.440, val_loss=3.020, train_loss_epoch=1.690]\u001b[A\n",
      "Epoch 117:  69%|████▊  | 46/67 [00:12<00:05,  3.61it/s, loss=2.22, v_num=1, train_loss_step=1.440, val_loss=3.020, train_loss_epoch=1.690]\u001b[A\n",
      "Epoch 117:  70%|████▉  | 47/67 [00:13<00:05,  3.60it/s, loss=2.22, v_num=1, train_loss_step=1.440, val_loss=3.020, train_loss_epoch=1.690]\u001b[A\n",
      "Epoch 117:  72%|█████  | 48/67 [00:13<00:05,  3.58it/s, loss=2.22, v_num=1, train_loss_step=1.440, val_loss=3.020, train_loss_epoch=1.690]\u001b[A\n",
      "Epoch 117:  73%|█████  | 49/67 [00:13<00:05,  3.57it/s, loss=2.22, v_num=1, train_loss_step=1.440, val_loss=3.020, train_loss_epoch=1.690]\u001b[A\n",
      "Epoch 117:  75%|█████▏ | 50/67 [00:14<00:04,  3.55it/s, loss=2.22, v_num=1, train_loss_step=1.440, val_loss=3.020, train_loss_epoch=1.690]\u001b[A\n",
      "Epoch 117:  76%|█████▎ | 51/67 [00:14<00:04,  3.51it/s, loss=2.22, v_num=1, train_loss_step=1.440, val_loss=3.020, train_loss_epoch=1.690]\u001b[A\n",
      "Epoch 117:  78%|█████▍ | 52/67 [00:14<00:04,  3.50it/s, loss=2.22, v_num=1, train_loss_step=1.440, val_loss=3.020, train_loss_epoch=1.690]\u001b[A\n",
      "Epoch 117:  79%|█████▌ | 53/67 [00:15<00:04,  3.49it/s, loss=2.22, v_num=1, train_loss_step=1.440, val_loss=3.020, train_loss_epoch=1.690]\u001b[A\n",
      "Epoch 117:  81%|█████▋ | 54/67 [00:15<00:03,  3.48it/s, loss=2.22, v_num=1, train_loss_step=1.440, val_loss=3.020, train_loss_epoch=1.690]\u001b[A\n",
      "Epoch 117:  82%|█████▋ | 55/67 [00:15<00:03,  3.47it/s, loss=2.22, v_num=1, train_loss_step=1.440, val_loss=3.020, train_loss_epoch=1.690]\u001b[A\n",
      "Epoch 117:  84%|█████▊ | 56/67 [00:16<00:03,  3.46it/s, loss=2.22, v_num=1, train_loss_step=1.440, val_loss=3.020, train_loss_epoch=1.690]\u001b[A\n",
      "Epoch 117:  85%|█████▉ | 57/67 [00:16<00:02,  3.45it/s, loss=2.22, v_num=1, train_loss_step=1.440, val_loss=3.020, train_loss_epoch=1.690]\u001b[A\n",
      "Epoch 117:  87%|██████ | 58/67 [00:16<00:02,  3.45it/s, loss=2.22, v_num=1, train_loss_step=1.440, val_loss=3.020, train_loss_epoch=1.690]\u001b[A\n",
      "Epoch 117:  88%|██████▏| 59/67 [00:17<00:02,  3.44it/s, loss=2.22, v_num=1, train_loss_step=1.440, val_loss=3.020, train_loss_epoch=1.690]\u001b[A\n",
      "Epoch 117:  90%|██████▎| 60/67 [00:17<00:02,  3.43it/s, loss=2.22, v_num=1, train_loss_step=1.440, val_loss=3.020, train_loss_epoch=1.690]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 117:  91%|██████▎| 61/67 [00:17<00:01,  3.40it/s, loss=2.22, v_num=1, train_loss_step=1.440, val_loss=3.020, train_loss_epoch=1.690]\u001b[A\n",
      "Epoch 117:  93%|██████▍| 62/67 [00:18<00:01,  3.40it/s, loss=2.22, v_num=1, train_loss_step=1.440, val_loss=3.020, train_loss_epoch=1.690]\u001b[A\n",
      "Epoch 117:  94%|██████▌| 63/67 [00:18<00:01,  3.39it/s, loss=2.22, v_num=1, train_loss_step=1.440, val_loss=3.020, train_loss_epoch=1.690]\u001b[A\n",
      "Epoch 117:  96%|██████▋| 64/67 [00:18<00:00,  3.38it/s, loss=2.22, v_num=1, train_loss_step=1.440, val_loss=3.020, train_loss_epoch=1.690]\u001b[A\n",
      "Epoch 117:  97%|██████▊| 65/67 [00:19<00:00,  3.38it/s, loss=2.22, v_num=1, train_loss_step=1.440, val_loss=3.020, train_loss_epoch=1.690]\u001b[A\n",
      "Epoch 117:  99%|██████▉| 66/67 [00:19<00:00,  3.37it/s, loss=2.22, v_num=1, train_loss_step=1.440, val_loss=3.020, train_loss_epoch=1.690]\u001b[A\n",
      "Epoch 117: 100%|███████| 67/67 [00:20<00:00,  3.30it/s, loss=2.22, v_num=1, train_loss_step=1.440, val_loss=3.020, train_loss_epoch=1.690]\u001b[A\n",
      "Epoch 118:  45%|███▏   | 30/67 [00:05<00:06,  5.34it/s, loss=3.05, v_num=1, train_loss_step=11.00, val_loss=3.020, train_loss_epoch=1.960]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 118:  46%|███▏   | 31/67 [00:07<00:08,  4.08it/s, loss=3.05, v_num=1, train_loss_step=11.00, val_loss=3.020, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 118:  48%|███▎   | 32/67 [00:07<00:08,  4.03it/s, loss=3.05, v_num=1, train_loss_step=11.00, val_loss=3.020, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 118:  49%|███▍   | 33/67 [00:08<00:08,  3.99it/s, loss=3.05, v_num=1, train_loss_step=11.00, val_loss=3.020, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 118:  51%|███▌   | 34/67 [00:08<00:08,  3.94it/s, loss=3.05, v_num=1, train_loss_step=11.00, val_loss=3.020, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 118:  52%|███▋   | 35/67 [00:08<00:08,  3.91it/s, loss=3.05, v_num=1, train_loss_step=11.00, val_loss=3.020, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 118:  54%|███▊   | 36/67 [00:09<00:08,  3.86it/s, loss=3.05, v_num=1, train_loss_step=11.00, val_loss=3.020, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 118:  55%|███▊   | 37/67 [00:09<00:07,  3.82it/s, loss=3.05, v_num=1, train_loss_step=11.00, val_loss=3.020, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 118:  57%|███▉   | 38/67 [00:10<00:07,  3.79it/s, loss=3.05, v_num=1, train_loss_step=11.00, val_loss=3.020, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 118:  58%|████   | 39/67 [00:10<00:07,  3.77it/s, loss=3.05, v_num=1, train_loss_step=11.00, val_loss=3.020, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 118:  60%|████▏  | 40/67 [00:10<00:07,  3.74it/s, loss=3.05, v_num=1, train_loss_step=11.00, val_loss=3.020, train_loss_epoch=1.960]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 118:  61%|████▎  | 41/67 [00:11<00:07,  3.66it/s, loss=3.05, v_num=1, train_loss_step=11.00, val_loss=3.020, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 118:  63%|████▍  | 42/67 [00:11<00:06,  3.64it/s, loss=3.05, v_num=1, train_loss_step=11.00, val_loss=3.020, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 118:  64%|████▍  | 43/67 [00:11<00:06,  3.62it/s, loss=3.05, v_num=1, train_loss_step=11.00, val_loss=3.020, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 118:  66%|████▌  | 44/67 [00:12<00:06,  3.61it/s, loss=3.05, v_num=1, train_loss_step=11.00, val_loss=3.020, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 118:  67%|████▋  | 45/67 [00:12<00:06,  3.59it/s, loss=3.05, v_num=1, train_loss_step=11.00, val_loss=3.020, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 118:  69%|████▊  | 46/67 [00:12<00:05,  3.58it/s, loss=3.05, v_num=1, train_loss_step=11.00, val_loss=3.020, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 118:  70%|████▉  | 47/67 [00:13<00:05,  3.56it/s, loss=3.05, v_num=1, train_loss_step=11.00, val_loss=3.020, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 118:  72%|█████  | 48/67 [00:13<00:05,  3.55it/s, loss=3.05, v_num=1, train_loss_step=11.00, val_loss=3.020, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 118:  73%|█████  | 49/67 [00:13<00:05,  3.54it/s, loss=3.05, v_num=1, train_loss_step=11.00, val_loss=3.020, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 118:  75%|█████▏ | 50/67 [00:14<00:04,  3.52it/s, loss=3.05, v_num=1, train_loss_step=11.00, val_loss=3.020, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 118:  76%|█████▎ | 51/67 [00:14<00:04,  3.48it/s, loss=3.05, v_num=1, train_loss_step=11.00, val_loss=3.020, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 118:  78%|█████▍ | 52/67 [00:14<00:04,  3.47it/s, loss=3.05, v_num=1, train_loss_step=11.00, val_loss=3.020, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 118:  79%|█████▌ | 53/67 [00:15<00:04,  3.46it/s, loss=3.05, v_num=1, train_loss_step=11.00, val_loss=3.020, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 118:  81%|█████▋ | 54/67 [00:15<00:03,  3.45it/s, loss=3.05, v_num=1, train_loss_step=11.00, val_loss=3.020, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 118:  82%|█████▋ | 55/67 [00:15<00:03,  3.45it/s, loss=3.05, v_num=1, train_loss_step=11.00, val_loss=3.020, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 118:  84%|█████▊ | 56/67 [00:16<00:03,  3.44it/s, loss=3.05, v_num=1, train_loss_step=11.00, val_loss=3.020, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 118:  85%|█████▉ | 57/67 [00:16<00:02,  3.43it/s, loss=3.05, v_num=1, train_loss_step=11.00, val_loss=3.020, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 118:  87%|██████ | 58/67 [00:16<00:02,  3.42it/s, loss=3.05, v_num=1, train_loss_step=11.00, val_loss=3.020, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 118:  88%|██████▏| 59/67 [00:17<00:02,  3.41it/s, loss=3.05, v_num=1, train_loss_step=11.00, val_loss=3.020, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 118:  90%|██████▎| 60/67 [00:17<00:02,  3.41it/s, loss=3.05, v_num=1, train_loss_step=11.00, val_loss=3.020, train_loss_epoch=1.960]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 118:  91%|██████▎| 61/67 [00:18<00:01,  3.37it/s, loss=3.05, v_num=1, train_loss_step=11.00, val_loss=3.020, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 118:  93%|██████▍| 62/67 [00:18<00:01,  3.36it/s, loss=3.05, v_num=1, train_loss_step=11.00, val_loss=3.020, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 118:  94%|██████▌| 63/67 [00:21<00:01,  2.98it/s, loss=3.05, v_num=1, train_loss_step=11.00, val_loss=3.020, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 118:  96%|██████▋| 64/67 [00:21<00:01,  2.99it/s, loss=3.05, v_num=1, train_loss_step=11.00, val_loss=3.020, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 118:  97%|██████▊| 65/67 [00:21<00:00,  2.99it/s, loss=3.05, v_num=1, train_loss_step=11.00, val_loss=3.020, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 118:  99%|██████▉| 66/67 [00:22<00:00,  2.99it/s, loss=3.05, v_num=1, train_loss_step=11.00, val_loss=3.020, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 118: 100%|███████| 67/67 [00:22<00:00,  2.94it/s, loss=3.05, v_num=1, train_loss_step=11.00, val_loss=3.020, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 119:  45%|███▏   | 30/67 [00:05<00:07,  5.12it/s, loss=2.01, v_num=1, train_loss_step=1.780, val_loss=3.020, train_loss_epoch=2.900]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 119:  46%|███▏   | 31/67 [00:08<00:09,  3.85it/s, loss=2.01, v_num=1, train_loss_step=1.780, val_loss=3.020, train_loss_epoch=2.900]\u001b[A\n",
      "Epoch 119:  48%|███▎   | 32/67 [00:08<00:09,  3.81it/s, loss=2.01, v_num=1, train_loss_step=1.780, val_loss=3.020, train_loss_epoch=2.900]\u001b[A\n",
      "Epoch 119:  49%|███▍   | 33/67 [00:08<00:08,  3.78it/s, loss=2.01, v_num=1, train_loss_step=1.780, val_loss=3.020, train_loss_epoch=2.900]\u001b[A\n",
      "Epoch 119:  51%|███▌   | 34/67 [00:09<00:08,  3.75it/s, loss=2.01, v_num=1, train_loss_step=1.780, val_loss=3.020, train_loss_epoch=2.900]\u001b[A\n",
      "Epoch 119:  52%|███▋   | 35/67 [00:09<00:08,  3.72it/s, loss=2.01, v_num=1, train_loss_step=1.780, val_loss=3.020, train_loss_epoch=2.900]\u001b[A\n",
      "Epoch 119:  54%|███▊   | 36/67 [00:09<00:08,  3.69it/s, loss=2.01, v_num=1, train_loss_step=1.780, val_loss=3.020, train_loss_epoch=2.900]\u001b[A\n",
      "Epoch 119:  55%|███▊   | 37/67 [00:10<00:08,  3.67it/s, loss=2.01, v_num=1, train_loss_step=1.780, val_loss=3.020, train_loss_epoch=2.900]\u001b[A\n",
      "Epoch 119:  57%|███▉   | 38/67 [00:10<00:07,  3.64it/s, loss=2.01, v_num=1, train_loss_step=1.780, val_loss=3.020, train_loss_epoch=2.900]\u001b[A\n",
      "Epoch 119:  58%|████   | 39/67 [00:10<00:07,  3.62it/s, loss=2.01, v_num=1, train_loss_step=1.780, val_loss=3.020, train_loss_epoch=2.900]\u001b[A\n",
      "Epoch 119:  60%|████▏  | 40/67 [00:11<00:07,  3.59it/s, loss=2.01, v_num=1, train_loss_step=1.780, val_loss=3.020, train_loss_epoch=2.900]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 119:  61%|████▎  | 41/67 [00:11<00:07,  3.54it/s, loss=2.01, v_num=1, train_loss_step=1.780, val_loss=3.020, train_loss_epoch=2.900]\u001b[A\n",
      "Epoch 119:  63%|████▍  | 42/67 [00:11<00:07,  3.52it/s, loss=2.01, v_num=1, train_loss_step=1.780, val_loss=3.020, train_loss_epoch=2.900]\u001b[A\n",
      "Epoch 119:  64%|████▍  | 43/67 [00:12<00:06,  3.50it/s, loss=2.01, v_num=1, train_loss_step=1.780, val_loss=3.020, train_loss_epoch=2.900]\u001b[A\n",
      "Epoch 119:  66%|████▌  | 44/67 [00:12<00:06,  3.48it/s, loss=2.01, v_num=1, train_loss_step=1.780, val_loss=3.020, train_loss_epoch=2.900]\u001b[A\n",
      "Epoch 119:  67%|████▋  | 45/67 [00:12<00:06,  3.47it/s, loss=2.01, v_num=1, train_loss_step=1.780, val_loss=3.020, train_loss_epoch=2.900]\u001b[A\n",
      "Epoch 119:  69%|████▊  | 46/67 [00:13<00:06,  3.45it/s, loss=2.01, v_num=1, train_loss_step=1.780, val_loss=3.020, train_loss_epoch=2.900]\u001b[A\n",
      "Epoch 119:  70%|████▉  | 47/67 [00:13<00:05,  3.43it/s, loss=2.01, v_num=1, train_loss_step=1.780, val_loss=3.020, train_loss_epoch=2.900]\u001b[A\n",
      "Epoch 119:  72%|█████  | 48/67 [00:14<00:05,  3.42it/s, loss=2.01, v_num=1, train_loss_step=1.780, val_loss=3.020, train_loss_epoch=2.900]\u001b[A\n",
      "Epoch 119:  73%|█████  | 49/67 [00:14<00:05,  3.41it/s, loss=2.01, v_num=1, train_loss_step=1.780, val_loss=3.020, train_loss_epoch=2.900]\u001b[A\n",
      "Epoch 119:  75%|█████▏ | 50/67 [00:14<00:05,  3.40it/s, loss=2.01, v_num=1, train_loss_step=1.780, val_loss=3.020, train_loss_epoch=2.900]\u001b[A\n",
      "Epoch 119:  76%|█████▎ | 51/67 [00:15<00:04,  3.34it/s, loss=2.01, v_num=1, train_loss_step=1.780, val_loss=3.020, train_loss_epoch=2.900]\u001b[A\n",
      "Epoch 119:  78%|█████▍ | 52/67 [00:17<00:05,  2.91it/s, loss=2.01, v_num=1, train_loss_step=1.780, val_loss=3.020, train_loss_epoch=2.900]\u001b[A\n",
      "Epoch 119:  79%|█████▌ | 53/67 [00:18<00:04,  2.91it/s, loss=2.01, v_num=1, train_loss_step=1.780, val_loss=3.020, train_loss_epoch=2.900]\u001b[A\n",
      "Epoch 119:  81%|█████▋ | 54/67 [00:18<00:04,  2.91it/s, loss=2.01, v_num=1, train_loss_step=1.780, val_loss=3.020, train_loss_epoch=2.900]\u001b[A\n",
      "Epoch 119:  82%|█████▋ | 55/67 [00:18<00:04,  2.91it/s, loss=2.01, v_num=1, train_loss_step=1.780, val_loss=3.020, train_loss_epoch=2.900]\u001b[A\n",
      "Epoch 119:  84%|█████▊ | 56/67 [00:19<00:03,  2.91it/s, loss=2.01, v_num=1, train_loss_step=1.780, val_loss=3.020, train_loss_epoch=2.900]\u001b[A\n",
      "Epoch 119:  85%|█████▉ | 57/67 [00:19<00:03,  2.91it/s, loss=2.01, v_num=1, train_loss_step=1.780, val_loss=3.020, train_loss_epoch=2.900]\u001b[A\n",
      "Epoch 119:  87%|██████ | 58/67 [00:19<00:03,  2.91it/s, loss=2.01, v_num=1, train_loss_step=1.780, val_loss=3.020, train_loss_epoch=2.900]\u001b[A\n",
      "Epoch 119:  88%|██████▏| 59/67 [00:20<00:02,  2.92it/s, loss=2.01, v_num=1, train_loss_step=1.780, val_loss=3.020, train_loss_epoch=2.900]\u001b[A\n",
      "Epoch 119:  90%|██████▎| 60/67 [00:20<00:02,  2.92it/s, loss=2.01, v_num=1, train_loss_step=1.780, val_loss=3.020, train_loss_epoch=2.900]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 119:  91%|██████▎| 61/67 [00:21<00:02,  2.89it/s, loss=2.01, v_num=1, train_loss_step=1.780, val_loss=3.020, train_loss_epoch=2.900]\u001b[A\n",
      "Epoch 119:  93%|██████▍| 62/67 [00:21<00:01,  2.89it/s, loss=2.01, v_num=1, train_loss_step=1.780, val_loss=3.020, train_loss_epoch=2.900]\u001b[A\n",
      "Epoch 119:  94%|██████▌| 63/67 [00:21<00:01,  2.89it/s, loss=2.01, v_num=1, train_loss_step=1.780, val_loss=3.020, train_loss_epoch=2.900]\u001b[A\n",
      "Epoch 119:  96%|██████▋| 64/67 [00:22<00:01,  2.90it/s, loss=2.01, v_num=1, train_loss_step=1.780, val_loss=3.020, train_loss_epoch=2.900]\u001b[A\n",
      "Epoch 119:  97%|██████▊| 65/67 [00:22<00:00,  2.90it/s, loss=2.01, v_num=1, train_loss_step=1.780, val_loss=3.020, train_loss_epoch=2.900]\u001b[A\n",
      "Epoch 119:  99%|██████▉| 66/67 [00:22<00:00,  2.90it/s, loss=2.01, v_num=1, train_loss_step=1.780, val_loss=3.020, train_loss_epoch=2.900]\u001b[A\n",
      "Epoch 119: 100%|███████| 67/67 [00:23<00:00,  2.85it/s, loss=2.01, v_num=1, train_loss_step=1.780, val_loss=3.020, train_loss_epoch=2.900]\u001b[A\n",
      "Epoch 120:  45%|███▏   | 30/67 [00:05<00:07,  5.13it/s, loss=2.21, v_num=1, train_loss_step=3.760, val_loss=3.020, train_loss_epoch=1.910]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 120:  46%|███▏   | 31/67 [00:07<00:09,  3.90it/s, loss=2.21, v_num=1, train_loss_step=3.760, val_loss=3.020, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 120:  48%|███▎   | 32/67 [00:08<00:09,  3.87it/s, loss=2.21, v_num=1, train_loss_step=3.760, val_loss=3.020, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 120:  49%|███▍   | 33/67 [00:08<00:08,  3.83it/s, loss=2.21, v_num=1, train_loss_step=3.760, val_loss=3.020, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 120:  51%|███▌   | 34/67 [00:08<00:08,  3.80it/s, loss=2.21, v_num=1, train_loss_step=3.760, val_loss=3.020, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 120:  52%|███▋   | 35/67 [00:09<00:08,  3.76it/s, loss=2.21, v_num=1, train_loss_step=3.760, val_loss=3.020, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 120:  54%|███▊   | 36/67 [00:09<00:08,  3.73it/s, loss=2.21, v_num=1, train_loss_step=3.760, val_loss=3.020, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 120:  55%|███▊   | 37/67 [00:09<00:08,  3.71it/s, loss=2.21, v_num=1, train_loss_step=3.760, val_loss=3.020, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 120:  57%|███▉   | 38/67 [00:10<00:07,  3.68it/s, loss=2.21, v_num=1, train_loss_step=3.760, val_loss=3.020, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 120:  58%|████   | 39/67 [00:10<00:07,  3.66it/s, loss=2.21, v_num=1, train_loss_step=3.760, val_loss=3.020, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 120:  60%|████▏  | 40/67 [00:10<00:07,  3.64it/s, loss=2.21, v_num=1, train_loss_step=3.760, val_loss=3.020, train_loss_epoch=1.910]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 120:  61%|████▎  | 41/67 [00:11<00:07,  3.57it/s, loss=2.21, v_num=1, train_loss_step=3.760, val_loss=3.020, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 120:  63%|████▍  | 42/67 [00:11<00:07,  3.55it/s, loss=2.21, v_num=1, train_loss_step=3.760, val_loss=3.020, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 120:  64%|████▍  | 43/67 [00:14<00:08,  2.95it/s, loss=2.21, v_num=1, train_loss_step=3.760, val_loss=3.020, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 120:  66%|████▌  | 44/67 [00:14<00:07,  2.95it/s, loss=2.21, v_num=1, train_loss_step=3.760, val_loss=3.020, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 120:  67%|████▋  | 45/67 [00:15<00:07,  2.95it/s, loss=2.21, v_num=1, train_loss_step=3.760, val_loss=3.020, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 120:  69%|████▊  | 46/67 [00:15<00:07,  2.95it/s, loss=2.21, v_num=1, train_loss_step=3.760, val_loss=3.020, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 120:  70%|████▉  | 47/67 [00:15<00:06,  2.95it/s, loss=2.21, v_num=1, train_loss_step=3.760, val_loss=3.020, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 120:  72%|█████  | 48/67 [00:16<00:06,  2.95it/s, loss=2.21, v_num=1, train_loss_step=3.760, val_loss=3.020, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 120:  73%|█████  | 49/67 [00:16<00:06,  2.95it/s, loss=2.21, v_num=1, train_loss_step=3.760, val_loss=3.020, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 120:  75%|█████▏ | 50/67 [00:16<00:05,  2.95it/s, loss=2.21, v_num=1, train_loss_step=3.760, val_loss=3.020, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 120:  76%|█████▎ | 51/67 [00:17<00:05,  2.92it/s, loss=2.21, v_num=1, train_loss_step=3.760, val_loss=3.020, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 120:  78%|█████▍ | 52/67 [00:17<00:05,  2.92it/s, loss=2.21, v_num=1, train_loss_step=3.760, val_loss=3.020, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 120:  79%|█████▌ | 53/67 [00:18<00:04,  2.92it/s, loss=2.21, v_num=1, train_loss_step=3.760, val_loss=3.020, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 120:  81%|█████▋ | 54/67 [00:18<00:04,  2.93it/s, loss=2.21, v_num=1, train_loss_step=3.760, val_loss=3.020, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 120:  82%|█████▋ | 55/67 [00:18<00:04,  2.93it/s, loss=2.21, v_num=1, train_loss_step=3.760, val_loss=3.020, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 120:  84%|█████▊ | 56/67 [00:19<00:03,  2.93it/s, loss=2.21, v_num=1, train_loss_step=3.760, val_loss=3.020, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 120:  85%|█████▉ | 57/67 [00:19<00:03,  2.93it/s, loss=2.21, v_num=1, train_loss_step=3.760, val_loss=3.020, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 120:  87%|██████ | 58/67 [00:19<00:03,  2.94it/s, loss=2.21, v_num=1, train_loss_step=3.760, val_loss=3.020, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 120:  88%|██████▏| 59/67 [00:20<00:02,  2.94it/s, loss=2.21, v_num=1, train_loss_step=3.760, val_loss=3.020, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 120:  90%|██████▎| 60/67 [00:20<00:02,  2.94it/s, loss=2.21, v_num=1, train_loss_step=3.760, val_loss=3.020, train_loss_epoch=1.910]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 120:  91%|██████▎| 61/67 [00:20<00:02,  2.92it/s, loss=2.21, v_num=1, train_loss_step=3.760, val_loss=3.020, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 120:  93%|██████▍| 62/67 [00:21<00:01,  2.93it/s, loss=2.21, v_num=1, train_loss_step=3.760, val_loss=3.020, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 120:  94%|██████▌| 63/67 [00:21<00:01,  2.93it/s, loss=2.21, v_num=1, train_loss_step=3.760, val_loss=3.020, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 120:  96%|██████▋| 64/67 [00:21<00:01,  2.93it/s, loss=2.21, v_num=1, train_loss_step=3.760, val_loss=3.020, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 120:  97%|██████▊| 65/67 [00:22<00:00,  2.94it/s, loss=2.21, v_num=1, train_loss_step=3.760, val_loss=3.020, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 120:  99%|██████▉| 66/67 [00:22<00:00,  2.94it/s, loss=2.21, v_num=1, train_loss_step=3.760, val_loss=3.020, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 120: 100%|███████| 67/67 [00:23<00:00,  2.89it/s, loss=2.21, v_num=1, train_loss_step=3.760, val_loss=3.020, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 121:  45%|███▏   | 30/67 [00:05<00:07,  5.15it/s, loss=1.93, v_num=1, train_loss_step=1.010, val_loss=3.020, train_loss_epoch=2.170]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 121:  46%|███▏   | 31/67 [00:07<00:09,  3.97it/s, loss=1.93, v_num=1, train_loss_step=1.010, val_loss=3.020, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 121:  48%|███▎   | 32/67 [00:08<00:08,  3.93it/s, loss=1.93, v_num=1, train_loss_step=1.010, val_loss=3.020, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 121:  49%|███▍   | 33/67 [00:08<00:08,  3.90it/s, loss=1.93, v_num=1, train_loss_step=1.010, val_loss=3.020, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 121:  51%|███▌   | 34/67 [00:11<00:10,  3.07it/s, loss=1.93, v_num=1, train_loss_step=1.010, val_loss=3.020, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 121:  52%|███▋   | 35/67 [00:11<00:10,  3.07it/s, loss=1.93, v_num=1, train_loss_step=1.010, val_loss=3.020, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 121:  54%|███▊   | 36/67 [00:11<00:10,  3.07it/s, loss=1.93, v_num=1, train_loss_step=1.010, val_loss=3.020, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 121:  55%|███▊   | 37/67 [00:12<00:09,  3.06it/s, loss=1.93, v_num=1, train_loss_step=1.010, val_loss=3.020, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 121:  57%|███▉   | 38/67 [00:12<00:09,  3.06it/s, loss=1.93, v_num=1, train_loss_step=1.010, val_loss=3.020, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 121:  58%|████   | 39/67 [00:12<00:09,  3.07it/s, loss=1.93, v_num=1, train_loss_step=1.010, val_loss=3.020, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 121:  60%|████▏  | 40/67 [00:13<00:08,  3.06it/s, loss=1.93, v_num=1, train_loss_step=1.010, val_loss=3.020, train_loss_epoch=2.170]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 121:  61%|████▎  | 41/67 [00:13<00:08,  3.04it/s, loss=1.93, v_num=1, train_loss_step=1.010, val_loss=3.020, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 121:  63%|████▍  | 42/67 [00:13<00:08,  3.04it/s, loss=1.93, v_num=1, train_loss_step=1.010, val_loss=3.020, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 121:  64%|████▍  | 43/67 [00:14<00:07,  3.03it/s, loss=1.93, v_num=1, train_loss_step=1.010, val_loss=3.020, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 121:  66%|████▌  | 44/67 [00:14<00:07,  3.04it/s, loss=1.93, v_num=1, train_loss_step=1.010, val_loss=3.020, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 121:  67%|████▋  | 45/67 [00:14<00:07,  3.04it/s, loss=1.93, v_num=1, train_loss_step=1.010, val_loss=3.020, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 121:  69%|████▊  | 46/67 [00:15<00:06,  3.03it/s, loss=1.93, v_num=1, train_loss_step=1.010, val_loss=3.020, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 121:  70%|████▉  | 47/67 [00:15<00:06,  3.03it/s, loss=1.93, v_num=1, train_loss_step=1.010, val_loss=3.020, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 121:  72%|█████  | 48/67 [00:15<00:06,  3.03it/s, loss=1.93, v_num=1, train_loss_step=1.010, val_loss=3.020, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 121:  73%|█████  | 49/67 [00:16<00:05,  3.03it/s, loss=1.93, v_num=1, train_loss_step=1.010, val_loss=3.020, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 121:  75%|█████▏ | 50/67 [00:16<00:05,  3.03it/s, loss=1.93, v_num=1, train_loss_step=1.010, val_loss=3.020, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 121:  76%|█████▎ | 51/67 [00:16<00:05,  3.00it/s, loss=1.93, v_num=1, train_loss_step=1.010, val_loss=3.020, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 121:  78%|█████▍ | 52/67 [00:17<00:04,  3.00it/s, loss=1.93, v_num=1, train_loss_step=1.010, val_loss=3.020, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 121:  79%|█████▌ | 53/67 [00:17<00:04,  3.01it/s, loss=1.93, v_num=1, train_loss_step=1.010, val_loss=3.020, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 121:  81%|█████▋ | 54/67 [00:17<00:04,  3.01it/s, loss=1.93, v_num=1, train_loss_step=1.010, val_loss=3.020, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 121:  82%|█████▋ | 55/67 [00:18<00:03,  3.01it/s, loss=1.93, v_num=1, train_loss_step=1.010, val_loss=3.020, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 121:  84%|█████▊ | 56/67 [00:18<00:03,  3.01it/s, loss=1.93, v_num=1, train_loss_step=1.010, val_loss=3.020, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 121:  85%|█████▉ | 57/67 [00:18<00:03,  3.01it/s, loss=1.93, v_num=1, train_loss_step=1.010, val_loss=3.020, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 121:  87%|██████ | 58/67 [00:19<00:02,  3.01it/s, loss=1.93, v_num=1, train_loss_step=1.010, val_loss=3.020, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 121:  88%|██████▏| 59/67 [00:19<00:02,  3.01it/s, loss=1.93, v_num=1, train_loss_step=1.010, val_loss=3.020, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 121:  90%|██████▎| 60/67 [00:19<00:02,  3.01it/s, loss=1.93, v_num=1, train_loss_step=1.010, val_loss=3.020, train_loss_epoch=2.170]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 121:  91%|██████▎| 61/67 [00:20<00:02,  2.99it/s, loss=1.93, v_num=1, train_loss_step=1.010, val_loss=3.020, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 121:  93%|██████▍| 62/67 [00:20<00:01,  3.00it/s, loss=1.93, v_num=1, train_loss_step=1.010, val_loss=3.020, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 121:  94%|██████▌| 63/67 [00:21<00:01,  3.00it/s, loss=1.93, v_num=1, train_loss_step=1.010, val_loss=3.020, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 121:  96%|██████▋| 64/67 [00:21<00:01,  3.00it/s, loss=1.93, v_num=1, train_loss_step=1.010, val_loss=3.020, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 121:  97%|██████▊| 65/67 [00:21<00:00,  3.00it/s, loss=1.93, v_num=1, train_loss_step=1.010, val_loss=3.020, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 121:  99%|██████▉| 66/67 [00:22<00:00,  3.00it/s, loss=1.93, v_num=1, train_loss_step=1.010, val_loss=3.020, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 121: 100%|███████| 67/67 [00:25<00:00,  2.63it/s, loss=1.93, v_num=1, train_loss_step=1.010, val_loss=3.020, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 122:  45%|███▏   | 30/67 [00:05<00:07,  5.14it/s, loss=3.73, v_num=1, train_loss_step=3.870, val_loss=3.020, train_loss_epoch=1.930]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 122:  46%|███▏   | 31/67 [00:07<00:09,  3.88it/s, loss=3.73, v_num=1, train_loss_step=3.870, val_loss=3.020, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 122:  48%|███▎   | 32/67 [00:08<00:09,  3.85it/s, loss=3.73, v_num=1, train_loss_step=3.870, val_loss=3.020, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 122:  49%|███▍   | 33/67 [00:08<00:08,  3.81it/s, loss=3.73, v_num=1, train_loss_step=3.870, val_loss=3.020, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 122:  51%|███▌   | 34/67 [00:09<00:08,  3.78it/s, loss=3.73, v_num=1, train_loss_step=3.870, val_loss=3.020, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 122:  52%|███▋   | 35/67 [00:09<00:08,  3.75it/s, loss=3.73, v_num=1, train_loss_step=3.870, val_loss=3.020, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 122:  54%|███▊   | 36/67 [00:09<00:08,  3.72it/s, loss=3.73, v_num=1, train_loss_step=3.870, val_loss=3.020, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 122:  55%|███▊   | 37/67 [00:10<00:08,  3.69it/s, loss=3.73, v_num=1, train_loss_step=3.870, val_loss=3.020, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 122:  57%|███▉   | 38/67 [00:10<00:07,  3.67it/s, loss=3.73, v_num=1, train_loss_step=3.870, val_loss=3.020, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 122:  58%|████   | 39/67 [00:10<00:07,  3.65it/s, loss=3.73, v_num=1, train_loss_step=3.870, val_loss=3.020, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 122:  60%|████▏  | 40/67 [00:11<00:07,  3.63it/s, loss=3.73, v_num=1, train_loss_step=3.870, val_loss=3.020, train_loss_epoch=1.930]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 122:  61%|████▎  | 41/67 [00:11<00:07,  3.57it/s, loss=3.73, v_num=1, train_loss_step=3.870, val_loss=3.020, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 122:  63%|████▍  | 42/67 [00:11<00:07,  3.55it/s, loss=3.73, v_num=1, train_loss_step=3.870, val_loss=3.020, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 122:  64%|████▍  | 43/67 [00:12<00:06,  3.54it/s, loss=3.73, v_num=1, train_loss_step=3.870, val_loss=3.020, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 122:  66%|████▌  | 44/67 [00:12<00:06,  3.52it/s, loss=3.73, v_num=1, train_loss_step=3.870, val_loss=3.020, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 122:  67%|████▋  | 45/67 [00:12<00:06,  3.51it/s, loss=3.73, v_num=1, train_loss_step=3.870, val_loss=3.020, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 122:  69%|████▊  | 46/67 [00:13<00:06,  3.49it/s, loss=3.73, v_num=1, train_loss_step=3.870, val_loss=3.020, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 122:  70%|████▉  | 47/67 [00:13<00:05,  3.48it/s, loss=3.73, v_num=1, train_loss_step=3.870, val_loss=3.020, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 122:  72%|█████  | 48/67 [00:13<00:05,  3.48it/s, loss=3.73, v_num=1, train_loss_step=3.870, val_loss=3.020, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 122:  73%|█████  | 49/67 [00:14<00:05,  3.46it/s, loss=3.73, v_num=1, train_loss_step=3.870, val_loss=3.020, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 122:  75%|█████▏ | 50/67 [00:14<00:04,  3.45it/s, loss=3.73, v_num=1, train_loss_step=3.870, val_loss=3.020, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 122:  76%|█████▎ | 51/67 [00:14<00:04,  3.41it/s, loss=3.73, v_num=1, train_loss_step=3.870, val_loss=3.020, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 122:  78%|█████▍ | 52/67 [00:15<00:04,  3.41it/s, loss=3.73, v_num=1, train_loss_step=3.870, val_loss=3.020, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 122:  79%|█████▌ | 53/67 [00:15<00:04,  3.40it/s, loss=3.73, v_num=1, train_loss_step=3.870, val_loss=3.020, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 122:  81%|█████▋ | 54/67 [00:15<00:03,  3.39it/s, loss=3.73, v_num=1, train_loss_step=3.870, val_loss=3.020, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 122:  82%|█████▋ | 55/67 [00:16<00:03,  3.39it/s, loss=3.73, v_num=1, train_loss_step=3.870, val_loss=3.020, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 122:  84%|█████▊ | 56/67 [00:16<00:03,  3.38it/s, loss=3.73, v_num=1, train_loss_step=3.870, val_loss=3.020, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 122:  85%|█████▉ | 57/67 [00:16<00:02,  3.37it/s, loss=3.73, v_num=1, train_loss_step=3.870, val_loss=3.020, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 122:  87%|██████ | 58/67 [00:17<00:02,  3.37it/s, loss=3.73, v_num=1, train_loss_step=3.870, val_loss=3.020, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 122:  88%|██████▏| 59/67 [00:17<00:02,  3.36it/s, loss=3.73, v_num=1, train_loss_step=3.870, val_loss=3.020, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 122:  90%|██████▎| 60/67 [00:17<00:02,  3.35it/s, loss=3.73, v_num=1, train_loss_step=3.870, val_loss=3.020, train_loss_epoch=1.930]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 122:  91%|██████▎| 61/67 [00:18<00:01,  3.33it/s, loss=3.73, v_num=1, train_loss_step=3.870, val_loss=3.020, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 122:  93%|██████▍| 62/67 [00:18<00:01,  3.32it/s, loss=3.73, v_num=1, train_loss_step=3.870, val_loss=3.020, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 122:  94%|██████▌| 63/67 [00:18<00:01,  3.32it/s, loss=3.73, v_num=1, train_loss_step=3.870, val_loss=3.020, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 122:  96%|██████▋| 64/67 [00:19<00:00,  3.32it/s, loss=3.73, v_num=1, train_loss_step=3.870, val_loss=3.020, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 122:  97%|██████▊| 65/67 [00:19<00:00,  3.31it/s, loss=3.73, v_num=1, train_loss_step=3.870, val_loss=3.020, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 122:  99%|██████▉| 66/67 [00:19<00:00,  3.31it/s, loss=3.73, v_num=1, train_loss_step=3.870, val_loss=3.020, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 122: 100%|███████| 67/67 [00:20<00:00,  3.24it/s, loss=3.73, v_num=1, train_loss_step=3.870, val_loss=3.010, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 123:  45%|███▏   | 30/67 [00:05<00:07,  5.27it/s, loss=1.85, v_num=1, train_loss_step=2.960, val_loss=3.010, train_loss_epoch=3.140]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 123:  46%|███▏   | 31/67 [00:07<00:08,  4.03it/s, loss=1.85, v_num=1, train_loss_step=2.960, val_loss=3.010, train_loss_epoch=3.140]\u001b[A\n",
      "Epoch 123:  48%|███▎   | 32/67 [00:08<00:08,  4.00it/s, loss=1.85, v_num=1, train_loss_step=2.960, val_loss=3.010, train_loss_epoch=3.140]\u001b[A\n",
      "Epoch 123:  49%|███▍   | 33/67 [00:08<00:08,  3.95it/s, loss=1.85, v_num=1, train_loss_step=2.960, val_loss=3.010, train_loss_epoch=3.140]\u001b[A\n",
      "Epoch 123:  51%|███▌   | 34/67 [00:08<00:08,  3.91it/s, loss=1.85, v_num=1, train_loss_step=2.960, val_loss=3.010, train_loss_epoch=3.140]\u001b[A\n",
      "Epoch 123:  52%|███▋   | 35/67 [00:09<00:08,  3.87it/s, loss=1.85, v_num=1, train_loss_step=2.960, val_loss=3.010, train_loss_epoch=3.140]\u001b[A\n",
      "Epoch 123:  54%|███▊   | 36/67 [00:09<00:08,  3.85it/s, loss=1.85, v_num=1, train_loss_step=2.960, val_loss=3.010, train_loss_epoch=3.140]\u001b[A\n",
      "Epoch 123:  55%|███▊   | 37/67 [00:09<00:07,  3.82it/s, loss=1.85, v_num=1, train_loss_step=2.960, val_loss=3.010, train_loss_epoch=3.140]\u001b[A\n",
      "Epoch 123:  57%|███▉   | 38/67 [00:10<00:07,  3.80it/s, loss=1.85, v_num=1, train_loss_step=2.960, val_loss=3.010, train_loss_epoch=3.140]\u001b[A\n",
      "Epoch 123:  58%|████   | 39/67 [00:10<00:07,  3.77it/s, loss=1.85, v_num=1, train_loss_step=2.960, val_loss=3.010, train_loss_epoch=3.140]\u001b[A\n",
      "Epoch 123:  60%|████▏  | 40/67 [00:10<00:07,  3.75it/s, loss=1.85, v_num=1, train_loss_step=2.960, val_loss=3.010, train_loss_epoch=3.140]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 123:  61%|████▎  | 41/67 [00:11<00:07,  3.68it/s, loss=1.85, v_num=1, train_loss_step=2.960, val_loss=3.010, train_loss_epoch=3.140]\u001b[A\n",
      "Epoch 123:  63%|████▍  | 42/67 [00:11<00:06,  3.66it/s, loss=1.85, v_num=1, train_loss_step=2.960, val_loss=3.010, train_loss_epoch=3.140]\u001b[A\n",
      "Epoch 123:  64%|████▍  | 43/67 [00:11<00:06,  3.63it/s, loss=1.85, v_num=1, train_loss_step=2.960, val_loss=3.010, train_loss_epoch=3.140]\u001b[A\n",
      "Epoch 123:  66%|████▌  | 44/67 [00:12<00:06,  3.61it/s, loss=1.85, v_num=1, train_loss_step=2.960, val_loss=3.010, train_loss_epoch=3.140]\u001b[A\n",
      "Epoch 123:  67%|████▋  | 45/67 [00:12<00:06,  3.60it/s, loss=1.85, v_num=1, train_loss_step=2.960, val_loss=3.010, train_loss_epoch=3.140]\u001b[A\n",
      "Epoch 123:  69%|████▊  | 46/67 [00:12<00:05,  3.58it/s, loss=1.85, v_num=1, train_loss_step=2.960, val_loss=3.010, train_loss_epoch=3.140]\u001b[A\n",
      "Epoch 123:  70%|████▉  | 47/67 [00:13<00:05,  3.57it/s, loss=1.85, v_num=1, train_loss_step=2.960, val_loss=3.010, train_loss_epoch=3.140]\u001b[A\n",
      "Epoch 123:  72%|█████  | 48/67 [00:13<00:05,  3.55it/s, loss=1.85, v_num=1, train_loss_step=2.960, val_loss=3.010, train_loss_epoch=3.140]\u001b[A\n",
      "Epoch 123:  73%|█████  | 49/67 [00:13<00:05,  3.54it/s, loss=1.85, v_num=1, train_loss_step=2.960, val_loss=3.010, train_loss_epoch=3.140]\u001b[A\n",
      "Epoch 123:  75%|█████▏ | 50/67 [00:14<00:04,  3.52it/s, loss=1.85, v_num=1, train_loss_step=2.960, val_loss=3.010, train_loss_epoch=3.140]\u001b[A\n",
      "Epoch 123:  76%|█████▎ | 51/67 [00:14<00:04,  3.47it/s, loss=1.85, v_num=1, train_loss_step=2.960, val_loss=3.010, train_loss_epoch=3.140]\u001b[A\n",
      "Epoch 123:  78%|█████▍ | 52/67 [00:15<00:04,  3.46it/s, loss=1.85, v_num=1, train_loss_step=2.960, val_loss=3.010, train_loss_epoch=3.140]\u001b[A\n",
      "Epoch 123:  79%|█████▌ | 53/67 [00:15<00:04,  3.45it/s, loss=1.85, v_num=1, train_loss_step=2.960, val_loss=3.010, train_loss_epoch=3.140]\u001b[A\n",
      "Epoch 123:  81%|█████▋ | 54/67 [00:15<00:03,  3.45it/s, loss=1.85, v_num=1, train_loss_step=2.960, val_loss=3.010, train_loss_epoch=3.140]\u001b[A\n",
      "Epoch 123:  82%|█████▋ | 55/67 [00:16<00:03,  3.44it/s, loss=1.85, v_num=1, train_loss_step=2.960, val_loss=3.010, train_loss_epoch=3.140]\u001b[A\n",
      "Epoch 123:  84%|█████▊ | 56/67 [00:16<00:03,  3.43it/s, loss=1.85, v_num=1, train_loss_step=2.960, val_loss=3.010, train_loss_epoch=3.140]\u001b[A\n",
      "Epoch 123:  85%|█████▉ | 57/67 [00:16<00:02,  3.42it/s, loss=1.85, v_num=1, train_loss_step=2.960, val_loss=3.010, train_loss_epoch=3.140]\u001b[A\n",
      "Epoch 123:  87%|██████ | 58/67 [00:16<00:02,  3.41it/s, loss=1.85, v_num=1, train_loss_step=2.960, val_loss=3.010, train_loss_epoch=3.140]\u001b[A\n",
      "Epoch 123:  88%|██████▏| 59/67 [00:17<00:02,  3.41it/s, loss=1.85, v_num=1, train_loss_step=2.960, val_loss=3.010, train_loss_epoch=3.140]\u001b[A\n",
      "Epoch 123:  90%|██████▎| 60/67 [00:17<00:02,  3.40it/s, loss=1.85, v_num=1, train_loss_step=2.960, val_loss=3.010, train_loss_epoch=3.140]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 123:  91%|██████▎| 61/67 [00:18<00:01,  3.37it/s, loss=1.85, v_num=1, train_loss_step=2.960, val_loss=3.010, train_loss_epoch=3.140]\u001b[A\n",
      "Epoch 123:  93%|██████▍| 62/67 [00:18<00:01,  3.36it/s, loss=1.85, v_num=1, train_loss_step=2.960, val_loss=3.010, train_loss_epoch=3.140]\u001b[A\n",
      "Epoch 123:  94%|██████▌| 63/67 [00:18<00:01,  3.36it/s, loss=1.85, v_num=1, train_loss_step=2.960, val_loss=3.010, train_loss_epoch=3.140]\u001b[A\n",
      "Epoch 123:  96%|██████▋| 64/67 [00:19<00:00,  3.36it/s, loss=1.85, v_num=1, train_loss_step=2.960, val_loss=3.010, train_loss_epoch=3.140]\u001b[A\n",
      "Epoch 123:  97%|██████▊| 65/67 [00:19<00:00,  3.35it/s, loss=1.85, v_num=1, train_loss_step=2.960, val_loss=3.010, train_loss_epoch=3.140]\u001b[A\n",
      "Epoch 123:  99%|██████▉| 66/67 [00:19<00:00,  3.35it/s, loss=1.85, v_num=1, train_loss_step=2.960, val_loss=3.010, train_loss_epoch=3.140]\u001b[A\n",
      "Epoch 123: 100%|███████| 67/67 [00:20<00:00,  3.28it/s, loss=1.85, v_num=1, train_loss_step=2.960, val_loss=3.020, train_loss_epoch=3.140]\u001b[A\n",
      "Epoch 124:  45%|███▏   | 30/67 [00:05<00:07,  5.19it/s, loss=1.72, v_num=1, train_loss_step=2.380, val_loss=3.020, train_loss_epoch=2.060]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 124:  46%|███▏   | 31/67 [00:07<00:09,  3.93it/s, loss=1.72, v_num=1, train_loss_step=2.380, val_loss=3.020, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 124:  48%|███▎   | 32/67 [00:08<00:08,  3.90it/s, loss=1.72, v_num=1, train_loss_step=2.380, val_loss=3.020, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 124:  49%|███▍   | 33/67 [00:08<00:08,  3.87it/s, loss=1.72, v_num=1, train_loss_step=2.380, val_loss=3.020, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 124:  51%|███▌   | 34/67 [00:08<00:08,  3.84it/s, loss=1.72, v_num=1, train_loss_step=2.380, val_loss=3.020, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 124:  52%|███▋   | 35/67 [00:09<00:08,  3.81it/s, loss=1.72, v_num=1, train_loss_step=2.380, val_loss=3.020, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 124:  54%|███▊   | 36/67 [00:09<00:08,  3.78it/s, loss=1.72, v_num=1, train_loss_step=2.380, val_loss=3.020, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 124:  55%|███▊   | 37/67 [00:09<00:07,  3.76it/s, loss=1.72, v_num=1, train_loss_step=2.380, val_loss=3.020, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 124:  57%|███▉   | 38/67 [00:10<00:07,  3.73it/s, loss=1.72, v_num=1, train_loss_step=2.380, val_loss=3.020, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 124:  58%|████   | 39/67 [00:10<00:07,  3.71it/s, loss=1.72, v_num=1, train_loss_step=2.380, val_loss=3.020, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 124:  60%|████▏  | 40/67 [00:10<00:07,  3.69it/s, loss=1.72, v_num=1, train_loss_step=2.380, val_loss=3.020, train_loss_epoch=2.060]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 124:  61%|████▎  | 41/67 [00:11<00:07,  3.61it/s, loss=1.72, v_num=1, train_loss_step=2.380, val_loss=3.020, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 124:  63%|████▍  | 42/67 [00:11<00:06,  3.60it/s, loss=1.72, v_num=1, train_loss_step=2.380, val_loss=3.020, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 124:  64%|████▍  | 43/67 [00:12<00:06,  3.58it/s, loss=1.72, v_num=1, train_loss_step=2.380, val_loss=3.020, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 124:  66%|████▌  | 44/67 [00:12<00:06,  3.57it/s, loss=1.72, v_num=1, train_loss_step=2.380, val_loss=3.020, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 124:  67%|████▋  | 45/67 [00:12<00:06,  3.55it/s, loss=1.72, v_num=1, train_loss_step=2.380, val_loss=3.020, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 124:  69%|████▊  | 46/67 [00:12<00:05,  3.54it/s, loss=1.72, v_num=1, train_loss_step=2.380, val_loss=3.020, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 124:  70%|████▉  | 47/67 [00:13<00:05,  3.53it/s, loss=1.72, v_num=1, train_loss_step=2.380, val_loss=3.020, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 124:  72%|█████  | 48/67 [00:13<00:05,  3.52it/s, loss=1.72, v_num=1, train_loss_step=2.380, val_loss=3.020, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 124:  73%|█████  | 49/67 [00:13<00:05,  3.51it/s, loss=1.72, v_num=1, train_loss_step=2.380, val_loss=3.020, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 124:  75%|█████▏ | 50/67 [00:14<00:04,  3.49it/s, loss=1.72, v_num=1, train_loss_step=2.380, val_loss=3.020, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 124:  76%|█████▎ | 51/67 [00:14<00:04,  3.45it/s, loss=1.72, v_num=1, train_loss_step=2.380, val_loss=3.020, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 124:  78%|█████▍ | 52/67 [00:15<00:04,  3.44it/s, loss=1.72, v_num=1, train_loss_step=2.380, val_loss=3.020, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 124:  79%|█████▌ | 53/67 [00:15<00:04,  3.43it/s, loss=1.72, v_num=1, train_loss_step=2.380, val_loss=3.020, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 124:  81%|█████▋ | 54/67 [00:15<00:03,  3.42it/s, loss=1.72, v_num=1, train_loss_step=2.380, val_loss=3.020, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 124:  82%|█████▋ | 55/67 [00:16<00:03,  3.42it/s, loss=1.72, v_num=1, train_loss_step=2.380, val_loss=3.020, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 124:  84%|█████▊ | 56/67 [00:16<00:03,  3.41it/s, loss=1.72, v_num=1, train_loss_step=2.380, val_loss=3.020, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 124:  85%|█████▉ | 57/67 [00:16<00:02,  3.41it/s, loss=1.72, v_num=1, train_loss_step=2.380, val_loss=3.020, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 124:  87%|██████ | 58/67 [00:17<00:02,  3.40it/s, loss=1.72, v_num=1, train_loss_step=2.380, val_loss=3.020, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 124:  88%|██████▏| 59/67 [00:17<00:02,  3.39it/s, loss=1.72, v_num=1, train_loss_step=2.380, val_loss=3.020, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 124:  90%|██████▎| 60/67 [00:17<00:02,  3.38it/s, loss=1.72, v_num=1, train_loss_step=2.380, val_loss=3.020, train_loss_epoch=2.060]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 124:  91%|██████▎| 61/67 [00:18<00:01,  3.35it/s, loss=1.72, v_num=1, train_loss_step=2.380, val_loss=3.020, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 124:  93%|██████▍| 62/67 [00:18<00:01,  3.35it/s, loss=1.72, v_num=1, train_loss_step=2.380, val_loss=3.020, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 124:  94%|██████▌| 63/67 [00:18<00:01,  3.34it/s, loss=1.72, v_num=1, train_loss_step=2.380, val_loss=3.020, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 124:  96%|██████▋| 64/67 [00:19<00:00,  3.34it/s, loss=1.72, v_num=1, train_loss_step=2.380, val_loss=3.020, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 124:  97%|██████▊| 65/67 [00:19<00:00,  3.33it/s, loss=1.72, v_num=1, train_loss_step=2.380, val_loss=3.020, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 124:  99%|██████▉| 66/67 [00:19<00:00,  3.33it/s, loss=1.72, v_num=1, train_loss_step=2.380, val_loss=3.020, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 124: 100%|███████| 67/67 [00:20<00:00,  3.27it/s, loss=1.72, v_num=1, train_loss_step=2.380, val_loss=3.010, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 125:  45%|███▏   | 30/67 [00:05<00:07,  5.21it/s, loss=2.48, v_num=1, train_loss_step=1.990, val_loss=3.010, train_loss_epoch=1.660]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 125:  46%|███▏   | 31/67 [00:07<00:09,  3.96it/s, loss=2.48, v_num=1, train_loss_step=1.990, val_loss=3.010, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 125:  48%|███▎   | 32/67 [00:08<00:08,  3.93it/s, loss=2.48, v_num=1, train_loss_step=1.990, val_loss=3.010, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 125:  49%|███▍   | 33/67 [00:08<00:08,  3.90it/s, loss=2.48, v_num=1, train_loss_step=1.990, val_loss=3.010, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 125:  51%|███▌   | 34/67 [00:08<00:08,  3.86it/s, loss=2.48, v_num=1, train_loss_step=1.990, val_loss=3.010, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 125:  52%|███▋   | 35/67 [00:09<00:08,  3.83it/s, loss=2.48, v_num=1, train_loss_step=1.990, val_loss=3.010, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 125:  54%|███▊   | 36/67 [00:09<00:08,  3.81it/s, loss=2.48, v_num=1, train_loss_step=1.990, val_loss=3.010, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 125:  55%|███▊   | 37/67 [00:09<00:07,  3.78it/s, loss=2.48, v_num=1, train_loss_step=1.990, val_loss=3.010, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 125:  57%|███▉   | 38/67 [00:10<00:07,  3.75it/s, loss=2.48, v_num=1, train_loss_step=1.990, val_loss=3.010, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 125:  58%|████   | 39/67 [00:10<00:07,  3.73it/s, loss=2.48, v_num=1, train_loss_step=1.990, val_loss=3.010, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 125:  60%|████▏  | 40/67 [00:10<00:07,  3.71it/s, loss=2.48, v_num=1, train_loss_step=1.990, val_loss=3.010, train_loss_epoch=1.660]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 125:  61%|████▎  | 41/67 [00:11<00:07,  3.66it/s, loss=2.48, v_num=1, train_loss_step=1.990, val_loss=3.010, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 125:  63%|████▍  | 42/67 [00:11<00:06,  3.64it/s, loss=2.48, v_num=1, train_loss_step=1.990, val_loss=3.010, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 125:  64%|████▍  | 43/67 [00:11<00:06,  3.62it/s, loss=2.48, v_num=1, train_loss_step=1.990, val_loss=3.010, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 125:  66%|████▌  | 44/67 [00:12<00:06,  3.60it/s, loss=2.48, v_num=1, train_loss_step=1.990, val_loss=3.010, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 125:  67%|████▋  | 45/67 [00:12<00:06,  3.59it/s, loss=2.48, v_num=1, train_loss_step=1.990, val_loss=3.010, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 125:  69%|████▊  | 46/67 [00:12<00:05,  3.58it/s, loss=2.48, v_num=1, train_loss_step=1.990, val_loss=3.010, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 125:  70%|████▉  | 47/67 [00:13<00:05,  3.56it/s, loss=2.48, v_num=1, train_loss_step=1.990, val_loss=3.010, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 125:  72%|█████  | 48/67 [00:13<00:05,  3.55it/s, loss=2.48, v_num=1, train_loss_step=1.990, val_loss=3.010, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 125:  73%|█████  | 49/67 [00:13<00:05,  3.54it/s, loss=2.48, v_num=1, train_loss_step=1.990, val_loss=3.010, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 125:  75%|█████▏ | 50/67 [00:14<00:04,  3.52it/s, loss=2.48, v_num=1, train_loss_step=1.990, val_loss=3.010, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 125:  76%|█████▎ | 51/67 [00:14<00:04,  3.48it/s, loss=2.48, v_num=1, train_loss_step=1.990, val_loss=3.010, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 125:  78%|█████▍ | 52/67 [00:14<00:04,  3.47it/s, loss=2.48, v_num=1, train_loss_step=1.990, val_loss=3.010, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 125:  79%|█████▌ | 53/67 [00:15<00:04,  3.46it/s, loss=2.48, v_num=1, train_loss_step=1.990, val_loss=3.010, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 125:  81%|█████▋ | 54/67 [00:15<00:03,  3.45it/s, loss=2.48, v_num=1, train_loss_step=1.990, val_loss=3.010, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 125:  82%|█████▋ | 55/67 [00:15<00:03,  3.44it/s, loss=2.48, v_num=1, train_loss_step=1.990, val_loss=3.010, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 125:  84%|█████▊ | 56/67 [00:16<00:03,  3.43it/s, loss=2.48, v_num=1, train_loss_step=1.990, val_loss=3.010, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 125:  85%|█████▉ | 57/67 [00:16<00:02,  3.43it/s, loss=2.48, v_num=1, train_loss_step=1.990, val_loss=3.010, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 125:  87%|██████ | 58/67 [00:16<00:02,  3.42it/s, loss=2.48, v_num=1, train_loss_step=1.990, val_loss=3.010, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 125:  88%|██████▏| 59/67 [00:17<00:02,  3.41it/s, loss=2.48, v_num=1, train_loss_step=1.990, val_loss=3.010, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 125:  90%|██████▎| 60/67 [00:17<00:02,  3.41it/s, loss=2.48, v_num=1, train_loss_step=1.990, val_loss=3.010, train_loss_epoch=1.660]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 125:  91%|██████▎| 61/67 [00:18<00:01,  3.36it/s, loss=2.48, v_num=1, train_loss_step=1.990, val_loss=3.010, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 125:  93%|██████▍| 62/67 [00:18<00:01,  3.36it/s, loss=2.48, v_num=1, train_loss_step=1.990, val_loss=3.010, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 125:  94%|██████▌| 63/67 [00:18<00:01,  3.36it/s, loss=2.48, v_num=1, train_loss_step=1.990, val_loss=3.010, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 125:  96%|██████▋| 64/67 [00:19<00:00,  3.35it/s, loss=2.48, v_num=1, train_loss_step=1.990, val_loss=3.010, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 125:  97%|██████▊| 65/67 [00:19<00:00,  3.35it/s, loss=2.48, v_num=1, train_loss_step=1.990, val_loss=3.010, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 125:  99%|██████▉| 66/67 [00:19<00:00,  3.34it/s, loss=2.48, v_num=1, train_loss_step=1.990, val_loss=3.010, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 125: 100%|███████| 67/67 [00:20<00:00,  3.28it/s, loss=2.48, v_num=1, train_loss_step=1.990, val_loss=3.000, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 126:  45%|███▏   | 30/67 [00:05<00:06,  5.32it/s, loss=2.69, v_num=1, train_loss_step=1.580, val_loss=3.000, train_loss_epoch=2.320]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 126:  46%|███▏   | 31/67 [00:07<00:08,  4.01it/s, loss=2.69, v_num=1, train_loss_step=1.580, val_loss=3.000, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 126:  48%|███▎   | 32/67 [00:08<00:08,  3.97it/s, loss=2.69, v_num=1, train_loss_step=1.580, val_loss=3.000, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 126:  49%|███▍   | 33/67 [00:08<00:08,  3.93it/s, loss=2.69, v_num=1, train_loss_step=1.580, val_loss=3.000, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 126:  51%|███▌   | 34/67 [00:08<00:08,  3.90it/s, loss=2.69, v_num=1, train_loss_step=1.580, val_loss=3.000, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 126:  52%|███▋   | 35/67 [00:09<00:08,  3.87it/s, loss=2.69, v_num=1, train_loss_step=1.580, val_loss=3.000, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 126:  54%|███▊   | 36/67 [00:09<00:08,  3.84it/s, loss=2.69, v_num=1, train_loss_step=1.580, val_loss=3.000, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 126:  55%|███▊   | 37/67 [00:09<00:07,  3.81it/s, loss=2.69, v_num=1, train_loss_step=1.580, val_loss=3.000, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 126:  57%|███▉   | 38/67 [00:10<00:07,  3.79it/s, loss=2.69, v_num=1, train_loss_step=1.580, val_loss=3.000, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 126:  58%|████   | 39/67 [00:10<00:07,  3.77it/s, loss=2.69, v_num=1, train_loss_step=1.580, val_loss=3.000, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 126:  60%|████▏  | 40/67 [00:10<00:07,  3.74it/s, loss=2.69, v_num=1, train_loss_step=1.580, val_loss=3.000, train_loss_epoch=2.320]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 126:  61%|████▎  | 41/67 [00:11<00:07,  3.67it/s, loss=2.69, v_num=1, train_loss_step=1.580, val_loss=3.000, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 126:  63%|████▍  | 42/67 [00:11<00:06,  3.66it/s, loss=2.69, v_num=1, train_loss_step=1.580, val_loss=3.000, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 126:  64%|████▍  | 43/67 [00:11<00:06,  3.64it/s, loss=2.69, v_num=1, train_loss_step=1.580, val_loss=3.000, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 126:  66%|████▌  | 44/67 [00:12<00:06,  3.62it/s, loss=2.69, v_num=1, train_loss_step=1.580, val_loss=3.000, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 126:  67%|████▋  | 45/67 [00:12<00:06,  3.61it/s, loss=2.69, v_num=1, train_loss_step=1.580, val_loss=3.000, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 126:  69%|████▊  | 46/67 [00:12<00:05,  3.59it/s, loss=2.69, v_num=1, train_loss_step=1.580, val_loss=3.000, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 126:  70%|████▉  | 47/67 [00:13<00:05,  3.58it/s, loss=2.69, v_num=1, train_loss_step=1.580, val_loss=3.000, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 126:  72%|█████  | 48/67 [00:13<00:05,  3.57it/s, loss=2.69, v_num=1, train_loss_step=1.580, val_loss=3.000, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 126:  73%|█████  | 49/67 [00:13<00:05,  3.55it/s, loss=2.69, v_num=1, train_loss_step=1.580, val_loss=3.000, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 126:  75%|█████▏ | 50/67 [00:14<00:04,  3.54it/s, loss=2.69, v_num=1, train_loss_step=1.580, val_loss=3.000, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 126:  76%|█████▎ | 51/67 [00:14<00:04,  3.49it/s, loss=2.69, v_num=1, train_loss_step=1.580, val_loss=3.000, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 126:  78%|█████▍ | 52/67 [00:14<00:04,  3.48it/s, loss=2.69, v_num=1, train_loss_step=1.580, val_loss=3.000, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 126:  79%|█████▌ | 53/67 [00:15<00:04,  3.47it/s, loss=2.69, v_num=1, train_loss_step=1.580, val_loss=3.000, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 126:  81%|█████▋ | 54/67 [00:15<00:03,  3.46it/s, loss=2.69, v_num=1, train_loss_step=1.580, val_loss=3.000, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 126:  82%|█████▋ | 55/67 [00:15<00:03,  3.45it/s, loss=2.69, v_num=1, train_loss_step=1.580, val_loss=3.000, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 126:  84%|█████▊ | 56/67 [00:16<00:03,  3.44it/s, loss=2.69, v_num=1, train_loss_step=1.580, val_loss=3.000, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 126:  85%|█████▉ | 57/67 [00:16<00:02,  3.43it/s, loss=2.69, v_num=1, train_loss_step=1.580, val_loss=3.000, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 126:  87%|██████ | 58/67 [00:16<00:02,  3.42it/s, loss=2.69, v_num=1, train_loss_step=1.580, val_loss=3.000, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 126:  88%|██████▏| 59/67 [00:17<00:02,  3.42it/s, loss=2.69, v_num=1, train_loss_step=1.580, val_loss=3.000, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 126:  90%|██████▎| 60/67 [00:17<00:02,  3.41it/s, loss=2.69, v_num=1, train_loss_step=1.580, val_loss=3.000, train_loss_epoch=2.320]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 126:  91%|██████▎| 61/67 [00:18<00:01,  3.37it/s, loss=2.69, v_num=1, train_loss_step=1.580, val_loss=3.000, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 126:  93%|██████▍| 62/67 [00:18<00:01,  3.36it/s, loss=2.69, v_num=1, train_loss_step=1.580, val_loss=3.000, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 126:  94%|██████▌| 63/67 [00:18<00:01,  3.36it/s, loss=2.69, v_num=1, train_loss_step=1.580, val_loss=3.000, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 126:  96%|██████▋| 64/67 [00:19<00:00,  3.35it/s, loss=2.69, v_num=1, train_loss_step=1.580, val_loss=3.000, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 126:  97%|██████▊| 65/67 [00:19<00:00,  3.35it/s, loss=2.69, v_num=1, train_loss_step=1.580, val_loss=3.000, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 126:  99%|██████▉| 66/67 [00:19<00:00,  3.34it/s, loss=2.69, v_num=1, train_loss_step=1.580, val_loss=3.000, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 126: 100%|███████| 67/67 [00:20<00:00,  3.27it/s, loss=2.69, v_num=1, train_loss_step=1.580, val_loss=3.000, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 127:  45%|███▏   | 30/67 [00:05<00:07,  5.27it/s, loss=2.51, v_num=1, train_loss_step=2.630, val_loss=3.000, train_loss_epoch=2.370]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 127:  46%|███▏   | 31/67 [00:07<00:09,  3.96it/s, loss=2.51, v_num=1, train_loss_step=2.630, val_loss=3.000, train_loss_epoch=2.370]\u001b[A\n",
      "Epoch 127:  48%|███▎   | 32/67 [00:08<00:08,  3.92it/s, loss=2.51, v_num=1, train_loss_step=2.630, val_loss=3.000, train_loss_epoch=2.370]\u001b[A\n",
      "Epoch 127:  49%|███▍   | 33/67 [00:08<00:08,  3.89it/s, loss=2.51, v_num=1, train_loss_step=2.630, val_loss=3.000, train_loss_epoch=2.370]\u001b[A\n",
      "Epoch 127:  51%|███▌   | 34/67 [00:08<00:08,  3.85it/s, loss=2.51, v_num=1, train_loss_step=2.630, val_loss=3.000, train_loss_epoch=2.370]\u001b[A\n",
      "Epoch 127:  52%|███▋   | 35/67 [00:09<00:08,  3.82it/s, loss=2.51, v_num=1, train_loss_step=2.630, val_loss=3.000, train_loss_epoch=2.370]\u001b[A\n",
      "Epoch 127:  54%|███▊   | 36/67 [00:09<00:08,  3.79it/s, loss=2.51, v_num=1, train_loss_step=2.630, val_loss=3.000, train_loss_epoch=2.370]\u001b[A\n",
      "Epoch 127:  55%|███▊   | 37/67 [00:09<00:07,  3.76it/s, loss=2.51, v_num=1, train_loss_step=2.630, val_loss=3.000, train_loss_epoch=2.370]\u001b[A\n",
      "Epoch 127:  57%|███▉   | 38/67 [00:10<00:07,  3.73it/s, loss=2.51, v_num=1, train_loss_step=2.630, val_loss=3.000, train_loss_epoch=2.370]\u001b[A\n",
      "Epoch 127:  58%|████   | 39/67 [00:10<00:07,  3.71it/s, loss=2.51, v_num=1, train_loss_step=2.630, val_loss=3.000, train_loss_epoch=2.370]\u001b[A\n",
      "Epoch 127:  60%|████▏  | 40/67 [00:10<00:07,  3.68it/s, loss=2.51, v_num=1, train_loss_step=2.630, val_loss=3.000, train_loss_epoch=2.370]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 127:  61%|████▎  | 41/67 [00:11<00:07,  3.61it/s, loss=2.51, v_num=1, train_loss_step=2.630, val_loss=3.000, train_loss_epoch=2.370]\u001b[A\n",
      "Epoch 127:  63%|████▍  | 42/67 [00:11<00:06,  3.59it/s, loss=2.51, v_num=1, train_loss_step=2.630, val_loss=3.000, train_loss_epoch=2.370]\u001b[A\n",
      "Epoch 127:  64%|████▍  | 43/67 [00:12<00:06,  3.57it/s, loss=2.51, v_num=1, train_loss_step=2.630, val_loss=3.000, train_loss_epoch=2.370]\u001b[A\n",
      "Epoch 127:  66%|████▌  | 44/67 [00:12<00:06,  3.55it/s, loss=2.51, v_num=1, train_loss_step=2.630, val_loss=3.000, train_loss_epoch=2.370]\u001b[A\n",
      "Epoch 127:  67%|████▋  | 45/67 [00:12<00:06,  3.54it/s, loss=2.51, v_num=1, train_loss_step=2.630, val_loss=3.000, train_loss_epoch=2.370]\u001b[A\n",
      "Epoch 127:  69%|████▊  | 46/67 [00:13<00:05,  3.51it/s, loss=2.51, v_num=1, train_loss_step=2.630, val_loss=3.000, train_loss_epoch=2.370]\u001b[A\n",
      "Epoch 127:  70%|████▉  | 47/67 [00:13<00:05,  3.50it/s, loss=2.51, v_num=1, train_loss_step=2.630, val_loss=3.000, train_loss_epoch=2.370]\u001b[A\n",
      "Epoch 127:  72%|█████  | 48/67 [00:13<00:05,  3.49it/s, loss=2.51, v_num=1, train_loss_step=2.630, val_loss=3.000, train_loss_epoch=2.370]\u001b[A\n",
      "Epoch 127:  73%|█████  | 49/67 [00:14<00:05,  3.47it/s, loss=2.51, v_num=1, train_loss_step=2.630, val_loss=3.000, train_loss_epoch=2.370]\u001b[A\n",
      "Epoch 127:  75%|█████▏ | 50/67 [00:14<00:04,  3.46it/s, loss=2.51, v_num=1, train_loss_step=2.630, val_loss=3.000, train_loss_epoch=2.370]\u001b[A\n",
      "Epoch 127:  76%|█████▎ | 51/67 [00:14<00:04,  3.41it/s, loss=2.51, v_num=1, train_loss_step=2.630, val_loss=3.000, train_loss_epoch=2.370]\u001b[A\n",
      "Epoch 127:  78%|█████▍ | 52/67 [00:15<00:04,  3.40it/s, loss=2.51, v_num=1, train_loss_step=2.630, val_loss=3.000, train_loss_epoch=2.370]\u001b[A\n",
      "Epoch 127:  79%|█████▌ | 53/67 [00:15<00:04,  3.39it/s, loss=2.51, v_num=1, train_loss_step=2.630, val_loss=3.000, train_loss_epoch=2.370]\u001b[A\n",
      "Epoch 127:  81%|█████▋ | 54/67 [00:15<00:03,  3.38it/s, loss=2.51, v_num=1, train_loss_step=2.630, val_loss=3.000, train_loss_epoch=2.370]\u001b[A\n",
      "Epoch 127:  82%|█████▋ | 55/67 [00:16<00:03,  3.37it/s, loss=2.51, v_num=1, train_loss_step=2.630, val_loss=3.000, train_loss_epoch=2.370]\u001b[A\n",
      "Epoch 127:  84%|█████▊ | 56/67 [00:16<00:03,  3.37it/s, loss=2.51, v_num=1, train_loss_step=2.630, val_loss=3.000, train_loss_epoch=2.370]\u001b[A\n",
      "Epoch 127:  85%|█████▉ | 57/67 [00:16<00:02,  3.36it/s, loss=2.51, v_num=1, train_loss_step=2.630, val_loss=3.000, train_loss_epoch=2.370]\u001b[A\n",
      "Epoch 127:  87%|██████ | 58/67 [00:17<00:02,  3.35it/s, loss=2.51, v_num=1, train_loss_step=2.630, val_loss=3.000, train_loss_epoch=2.370]\u001b[A\n",
      "Epoch 127:  88%|██████▏| 59/67 [00:17<00:02,  3.34it/s, loss=2.51, v_num=1, train_loss_step=2.630, val_loss=3.000, train_loss_epoch=2.370]\u001b[A\n",
      "Epoch 127:  90%|██████▎| 60/67 [00:17<00:02,  3.34it/s, loss=2.51, v_num=1, train_loss_step=2.630, val_loss=3.000, train_loss_epoch=2.370]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 127:  91%|██████▎| 61/67 [00:18<00:01,  3.30it/s, loss=2.51, v_num=1, train_loss_step=2.630, val_loss=3.000, train_loss_epoch=2.370]\u001b[A\n",
      "Epoch 127:  93%|██████▍| 62/67 [00:18<00:01,  3.30it/s, loss=2.51, v_num=1, train_loss_step=2.630, val_loss=3.000, train_loss_epoch=2.370]\u001b[A\n",
      "Epoch 127:  94%|██████▌| 63/67 [00:19<00:01,  3.29it/s, loss=2.51, v_num=1, train_loss_step=2.630, val_loss=3.000, train_loss_epoch=2.370]\u001b[A\n",
      "Epoch 127:  96%|██████▋| 64/67 [00:19<00:00,  3.29it/s, loss=2.51, v_num=1, train_loss_step=2.630, val_loss=3.000, train_loss_epoch=2.370]\u001b[A\n",
      "Epoch 127:  97%|██████▊| 65/67 [00:19<00:00,  3.29it/s, loss=2.51, v_num=1, train_loss_step=2.630, val_loss=3.000, train_loss_epoch=2.370]\u001b[A\n",
      "Epoch 127:  99%|██████▉| 66/67 [00:20<00:00,  3.29it/s, loss=2.51, v_num=1, train_loss_step=2.630, val_loss=3.000, train_loss_epoch=2.370]\u001b[A\n",
      "Epoch 127: 100%|███████| 67/67 [00:20<00:00,  3.22it/s, loss=2.51, v_num=1, train_loss_step=2.630, val_loss=3.000, train_loss_epoch=2.370]\u001b[A\n",
      "Epoch 128:  45%|████▍     | 30/67 [00:05<00:07,  5.05it/s, loss=2, v_num=1, train_loss_step=4.130, val_loss=3.000, train_loss_epoch=2.160]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 128:  46%|████▋     | 31/67 [00:08<00:09,  3.84it/s, loss=2, v_num=1, train_loss_step=4.130, val_loss=3.000, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 128:  48%|████▊     | 32/67 [00:08<00:09,  3.80it/s, loss=2, v_num=1, train_loss_step=4.130, val_loss=3.000, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 128:  49%|████▉     | 33/67 [00:08<00:09,  3.77it/s, loss=2, v_num=1, train_loss_step=4.130, val_loss=3.000, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 128:  51%|█████     | 34/67 [00:09<00:08,  3.73it/s, loss=2, v_num=1, train_loss_step=4.130, val_loss=3.000, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 128:  52%|█████▏    | 35/67 [00:09<00:08,  3.70it/s, loss=2, v_num=1, train_loss_step=4.130, val_loss=3.000, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 128:  54%|█████▎    | 36/67 [00:09<00:08,  3.67it/s, loss=2, v_num=1, train_loss_step=4.130, val_loss=3.000, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 128:  55%|█████▌    | 37/67 [00:10<00:08,  3.65it/s, loss=2, v_num=1, train_loss_step=4.130, val_loss=3.000, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 128:  57%|█████▋    | 38/67 [00:10<00:07,  3.63it/s, loss=2, v_num=1, train_loss_step=4.130, val_loss=3.000, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 128:  58%|█████▊    | 39/67 [00:10<00:07,  3.60it/s, loss=2, v_num=1, train_loss_step=4.130, val_loss=3.000, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 128:  60%|█████▉    | 40/67 [00:11<00:07,  3.58it/s, loss=2, v_num=1, train_loss_step=4.130, val_loss=3.000, train_loss_epoch=2.160]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 128:  61%|██████    | 41/67 [00:11<00:07,  3.51it/s, loss=2, v_num=1, train_loss_step=4.130, val_loss=3.000, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 128:  63%|██████▎   | 42/67 [00:12<00:07,  3.49it/s, loss=2, v_num=1, train_loss_step=4.130, val_loss=3.000, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 128:  64%|██████▍   | 43/67 [00:12<00:06,  3.48it/s, loss=2, v_num=1, train_loss_step=4.130, val_loss=3.000, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 128:  66%|██████▌   | 44/67 [00:12<00:06,  3.47it/s, loss=2, v_num=1, train_loss_step=4.130, val_loss=3.000, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 128:  67%|██████▋   | 45/67 [00:13<00:06,  3.45it/s, loss=2, v_num=1, train_loss_step=4.130, val_loss=3.000, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 128:  69%|██████▊   | 46/67 [00:13<00:06,  3.44it/s, loss=2, v_num=1, train_loss_step=4.130, val_loss=3.000, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 128:  70%|███████   | 47/67 [00:13<00:05,  3.42it/s, loss=2, v_num=1, train_loss_step=4.130, val_loss=3.000, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 128:  72%|███████▏  | 48/67 [00:14<00:05,  3.41it/s, loss=2, v_num=1, train_loss_step=4.130, val_loss=3.000, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 128:  73%|███████▎  | 49/67 [00:14<00:05,  3.40it/s, loss=2, v_num=1, train_loss_step=4.130, val_loss=3.000, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 128:  75%|███████▍  | 50/67 [00:14<00:05,  3.39it/s, loss=2, v_num=1, train_loss_step=4.130, val_loss=3.000, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 128:  76%|███████▌  | 51/67 [00:15<00:04,  3.35it/s, loss=2, v_num=1, train_loss_step=4.130, val_loss=3.000, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 128:  78%|███████▊  | 52/67 [00:15<00:04,  3.34it/s, loss=2, v_num=1, train_loss_step=4.130, val_loss=3.000, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 128:  79%|███████▉  | 53/67 [00:15<00:04,  3.34it/s, loss=2, v_num=1, train_loss_step=4.130, val_loss=3.000, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 128:  81%|████████  | 54/67 [00:16<00:03,  3.33it/s, loss=2, v_num=1, train_loss_step=4.130, val_loss=3.000, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 128:  82%|████████▏ | 55/67 [00:16<00:03,  3.32it/s, loss=2, v_num=1, train_loss_step=4.130, val_loss=3.000, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 128:  84%|████████▎ | 56/67 [00:16<00:03,  3.31it/s, loss=2, v_num=1, train_loss_step=4.130, val_loss=3.000, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 128:  85%|████████▌ | 57/67 [00:17<00:03,  3.30it/s, loss=2, v_num=1, train_loss_step=4.130, val_loss=3.000, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 128:  87%|████████▋ | 58/67 [00:17<00:02,  3.29it/s, loss=2, v_num=1, train_loss_step=4.130, val_loss=3.000, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 128:  88%|████████▊ | 59/67 [00:17<00:02,  3.29it/s, loss=2, v_num=1, train_loss_step=4.130, val_loss=3.000, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 128:  90%|████████▉ | 60/67 [00:18<00:02,  3.28it/s, loss=2, v_num=1, train_loss_step=4.130, val_loss=3.000, train_loss_epoch=2.160]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 128:  91%|█████████ | 61/67 [00:18<00:01,  3.25it/s, loss=2, v_num=1, train_loss_step=4.130, val_loss=3.000, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 128:  93%|█████████▎| 62/67 [00:19<00:01,  3.24it/s, loss=2, v_num=1, train_loss_step=4.130, val_loss=3.000, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 128:  94%|█████████▍| 63/67 [00:19<00:01,  3.23it/s, loss=2, v_num=1, train_loss_step=4.130, val_loss=3.000, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 128:  96%|█████████▌| 64/67 [00:19<00:00,  3.23it/s, loss=2, v_num=1, train_loss_step=4.130, val_loss=3.000, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 128:  97%|█████████▋| 65/67 [00:20<00:00,  3.23it/s, loss=2, v_num=1, train_loss_step=4.130, val_loss=3.000, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 128:  99%|█████████▊| 66/67 [00:20<00:00,  3.23it/s, loss=2, v_num=1, train_loss_step=4.130, val_loss=3.000, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 128: 100%|██████████| 67/67 [00:21<00:00,  3.17it/s, loss=2, v_num=1, train_loss_step=4.130, val_loss=2.990, train_loss_epoch=2.160]\u001b[A\n",
      "Epoch 129:  45%|████▍     | 30/67 [00:05<00:07,  5.12it/s, loss=2, v_num=1, train_loss_step=1.910, val_loss=2.990, train_loss_epoch=2.000]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 129:  46%|████▋     | 31/67 [00:07<00:09,  3.91it/s, loss=2, v_num=1, train_loss_step=1.910, val_loss=2.990, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 129:  48%|████▊     | 32/67 [00:08<00:09,  3.87it/s, loss=2, v_num=1, train_loss_step=1.910, val_loss=2.990, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 129:  49%|████▉     | 33/67 [00:08<00:08,  3.84it/s, loss=2, v_num=1, train_loss_step=1.910, val_loss=2.990, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 129:  51%|█████     | 34/67 [00:08<00:08,  3.81it/s, loss=2, v_num=1, train_loss_step=1.910, val_loss=2.990, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 129:  52%|█████▏    | 35/67 [00:09<00:08,  3.78it/s, loss=2, v_num=1, train_loss_step=1.910, val_loss=2.990, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 129:  54%|█████▎    | 36/67 [00:09<00:08,  3.75it/s, loss=2, v_num=1, train_loss_step=1.910, val_loss=2.990, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 129:  55%|█████▌    | 37/67 [00:09<00:08,  3.72it/s, loss=2, v_num=1, train_loss_step=1.910, val_loss=2.990, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 129:  57%|█████▋    | 38/67 [00:10<00:07,  3.69it/s, loss=2, v_num=1, train_loss_step=1.910, val_loss=2.990, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 129:  58%|█████▊    | 39/67 [00:10<00:07,  3.67it/s, loss=2, v_num=1, train_loss_step=1.910, val_loss=2.990, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 129:  60%|█████▉    | 40/67 [00:10<00:07,  3.64it/s, loss=2, v_num=1, train_loss_step=1.910, val_loss=2.990, train_loss_epoch=2.000]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 129:  61%|██████    | 41/67 [00:11<00:07,  3.58it/s, loss=2, v_num=1, train_loss_step=1.910, val_loss=2.990, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 129:  63%|██████▎   | 42/67 [00:11<00:07,  3.57it/s, loss=2, v_num=1, train_loss_step=1.910, val_loss=2.990, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 129:  64%|██████▍   | 43/67 [00:12<00:06,  3.55it/s, loss=2, v_num=1, train_loss_step=1.910, val_loss=2.990, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 129:  66%|██████▌   | 44/67 [00:12<00:06,  3.54it/s, loss=2, v_num=1, train_loss_step=1.910, val_loss=2.990, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 129:  67%|██████▋   | 45/67 [00:12<00:06,  3.52it/s, loss=2, v_num=1, train_loss_step=1.910, val_loss=2.990, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 129:  69%|██████▊   | 46/67 [00:13<00:05,  3.51it/s, loss=2, v_num=1, train_loss_step=1.910, val_loss=2.990, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 129:  70%|███████   | 47/67 [00:13<00:05,  3.50it/s, loss=2, v_num=1, train_loss_step=1.910, val_loss=2.990, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 129:  72%|███████▏  | 48/67 [00:13<00:05,  3.49it/s, loss=2, v_num=1, train_loss_step=1.910, val_loss=2.990, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 129:  73%|███████▎  | 49/67 [00:14<00:05,  3.47it/s, loss=2, v_num=1, train_loss_step=1.910, val_loss=2.990, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 129:  75%|███████▍  | 50/67 [00:14<00:04,  3.45it/s, loss=2, v_num=1, train_loss_step=1.910, val_loss=2.990, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 129:  76%|███████▌  | 51/67 [00:14<00:04,  3.41it/s, loss=2, v_num=1, train_loss_step=1.910, val_loss=2.990, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 129:  78%|███████▊  | 52/67 [00:15<00:04,  3.40it/s, loss=2, v_num=1, train_loss_step=1.910, val_loss=2.990, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 129:  79%|███████▉  | 53/67 [00:15<00:04,  3.39it/s, loss=2, v_num=1, train_loss_step=1.910, val_loss=2.990, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 129:  81%|████████  | 54/67 [00:16<00:03,  3.37it/s, loss=2, v_num=1, train_loss_step=1.910, val_loss=2.990, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 129:  82%|████████▏ | 55/67 [00:16<00:03,  3.37it/s, loss=2, v_num=1, train_loss_step=1.910, val_loss=2.990, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 129:  84%|████████▎ | 56/67 [00:16<00:03,  3.36it/s, loss=2, v_num=1, train_loss_step=1.910, val_loss=2.990, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 129:  85%|████████▌ | 57/67 [00:16<00:02,  3.35it/s, loss=2, v_num=1, train_loss_step=1.910, val_loss=2.990, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 129:  87%|████████▋ | 58/67 [00:17<00:02,  3.35it/s, loss=2, v_num=1, train_loss_step=1.910, val_loss=2.990, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 129:  88%|████████▊ | 59/67 [00:17<00:02,  3.34it/s, loss=2, v_num=1, train_loss_step=1.910, val_loss=2.990, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 129:  90%|████████▉ | 60/67 [00:18<00:02,  3.33it/s, loss=2, v_num=1, train_loss_step=1.910, val_loss=2.990, train_loss_epoch=2.000]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 129:  91%|█████████ | 61/67 [00:18<00:01,  3.29it/s, loss=2, v_num=1, train_loss_step=1.910, val_loss=2.990, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 129:  93%|█████████▎| 62/67 [00:18<00:01,  3.28it/s, loss=2, v_num=1, train_loss_step=1.910, val_loss=2.990, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 129:  94%|█████████▍| 63/67 [00:19<00:01,  3.27it/s, loss=2, v_num=1, train_loss_step=1.910, val_loss=2.990, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 129:  96%|█████████▌| 64/67 [00:19<00:00,  3.27it/s, loss=2, v_num=1, train_loss_step=1.910, val_loss=2.990, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 129:  97%|█████████▋| 65/67 [00:19<00:00,  3.26it/s, loss=2, v_num=1, train_loss_step=1.910, val_loss=2.990, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 129:  99%|█████████▊| 66/67 [00:20<00:00,  3.25it/s, loss=2, v_num=1, train_loss_step=1.910, val_loss=2.990, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 129: 100%|██████████| 67/67 [00:21<00:00,  3.18it/s, loss=2, v_num=1, train_loss_step=1.910, val_loss=2.990, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 130:  45%|███▏   | 30/67 [00:06<00:07,  4.84it/s, loss=2.57, v_num=1, train_loss_step=6.220, val_loss=2.990, train_loss_epoch=1.820]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 130:  46%|███▏   | 31/67 [00:08<00:09,  3.73it/s, loss=2.57, v_num=1, train_loss_step=6.220, val_loss=2.990, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 130:  48%|███▎   | 32/67 [00:08<00:09,  3.69it/s, loss=2.57, v_num=1, train_loss_step=6.220, val_loss=2.990, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 130:  49%|███▍   | 33/67 [00:09<00:09,  3.66it/s, loss=2.57, v_num=1, train_loss_step=6.220, val_loss=2.990, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 130:  51%|███▌   | 34/67 [00:09<00:09,  3.63it/s, loss=2.57, v_num=1, train_loss_step=6.220, val_loss=2.990, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 130:  52%|███▋   | 35/67 [00:09<00:08,  3.59it/s, loss=2.57, v_num=1, train_loss_step=6.220, val_loss=2.990, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 130:  54%|███▊   | 36/67 [00:10<00:08,  3.57it/s, loss=2.57, v_num=1, train_loss_step=6.220, val_loss=2.990, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 130:  55%|███▊   | 37/67 [00:10<00:08,  3.55it/s, loss=2.57, v_num=1, train_loss_step=6.220, val_loss=2.990, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 130:  57%|███▉   | 38/67 [00:10<00:08,  3.52it/s, loss=2.57, v_num=1, train_loss_step=6.220, val_loss=2.990, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 130:  58%|████   | 39/67 [00:11<00:08,  3.50it/s, loss=2.57, v_num=1, train_loss_step=6.220, val_loss=2.990, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 130:  60%|████▏  | 40/67 [00:11<00:07,  3.47it/s, loss=2.57, v_num=1, train_loss_step=6.220, val_loss=2.990, train_loss_epoch=1.820]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 130:  61%|████▎  | 41/67 [00:12<00:07,  3.40it/s, loss=2.57, v_num=1, train_loss_step=6.220, val_loss=2.990, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 130:  63%|████▍  | 42/67 [00:12<00:07,  3.38it/s, loss=2.57, v_num=1, train_loss_step=6.220, val_loss=2.990, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 130:  64%|████▍  | 43/67 [00:12<00:07,  3.36it/s, loss=2.57, v_num=1, train_loss_step=6.220, val_loss=2.990, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 130:  66%|████▌  | 44/67 [00:13<00:06,  3.35it/s, loss=2.57, v_num=1, train_loss_step=6.220, val_loss=2.990, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 130:  67%|████▋  | 45/67 [00:13<00:06,  3.34it/s, loss=2.57, v_num=1, train_loss_step=6.220, val_loss=2.990, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 130:  69%|████▊  | 46/67 [00:13<00:06,  3.33it/s, loss=2.57, v_num=1, train_loss_step=6.220, val_loss=2.990, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 130:  70%|████▉  | 47/67 [00:14<00:06,  3.32it/s, loss=2.57, v_num=1, train_loss_step=6.220, val_loss=2.990, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 130:  72%|█████  | 48/67 [00:14<00:05,  3.31it/s, loss=2.57, v_num=1, train_loss_step=6.220, val_loss=2.990, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 130:  73%|█████  | 49/67 [00:14<00:05,  3.30it/s, loss=2.57, v_num=1, train_loss_step=6.220, val_loss=2.990, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 130:  75%|█████▏ | 50/67 [00:15<00:05,  3.29it/s, loss=2.57, v_num=1, train_loss_step=6.220, val_loss=2.990, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 130:  76%|█████▎ | 51/67 [00:15<00:04,  3.26it/s, loss=2.57, v_num=1, train_loss_step=6.220, val_loss=2.990, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 130:  78%|█████▍ | 52/67 [00:15<00:04,  3.25it/s, loss=2.57, v_num=1, train_loss_step=6.220, val_loss=2.990, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 130:  79%|█████▌ | 53/67 [00:16<00:04,  3.24it/s, loss=2.57, v_num=1, train_loss_step=6.220, val_loss=2.990, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 130:  81%|█████▋ | 54/67 [00:16<00:04,  3.23it/s, loss=2.57, v_num=1, train_loss_step=6.220, val_loss=2.990, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 130:  82%|█████▋ | 55/67 [00:17<00:03,  3.22it/s, loss=2.57, v_num=1, train_loss_step=6.220, val_loss=2.990, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 130:  84%|█████▊ | 56/67 [00:17<00:03,  3.21it/s, loss=2.57, v_num=1, train_loss_step=6.220, val_loss=2.990, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 130:  85%|█████▉ | 57/67 [00:17<00:03,  3.21it/s, loss=2.57, v_num=1, train_loss_step=6.220, val_loss=2.990, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 130:  87%|██████ | 58/67 [00:18<00:02,  3.19it/s, loss=2.57, v_num=1, train_loss_step=6.220, val_loss=2.990, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 130:  88%|██████▏| 59/67 [00:18<00:02,  3.18it/s, loss=2.57, v_num=1, train_loss_step=6.220, val_loss=2.990, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 130:  90%|██████▎| 60/67 [00:18<00:02,  3.18it/s, loss=2.57, v_num=1, train_loss_step=6.220, val_loss=2.990, train_loss_epoch=1.820]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 130:  91%|██████▎| 61/67 [00:19<00:01,  3.14it/s, loss=2.57, v_num=1, train_loss_step=6.220, val_loss=2.990, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 130:  93%|██████▍| 62/67 [00:19<00:01,  3.14it/s, loss=2.57, v_num=1, train_loss_step=6.220, val_loss=2.990, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 130:  94%|██████▌| 63/67 [00:20<00:01,  3.14it/s, loss=2.57, v_num=1, train_loss_step=6.220, val_loss=2.990, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 130:  96%|██████▋| 64/67 [00:20<00:00,  3.14it/s, loss=2.57, v_num=1, train_loss_step=6.220, val_loss=2.990, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 130:  97%|██████▊| 65/67 [00:20<00:00,  3.14it/s, loss=2.57, v_num=1, train_loss_step=6.220, val_loss=2.990, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 130:  99%|██████▉| 66/67 [00:21<00:00,  3.14it/s, loss=2.57, v_num=1, train_loss_step=6.220, val_loss=2.990, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 130: 100%|███████| 67/67 [00:21<00:00,  3.08it/s, loss=2.57, v_num=1, train_loss_step=6.220, val_loss=2.990, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 131:  45%|███▏   | 30/67 [00:05<00:07,  5.05it/s, loss=1.96, v_num=1, train_loss_step=1.130, val_loss=2.990, train_loss_epoch=2.500]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 131:  46%|███▏   | 31/67 [00:07<00:09,  3.88it/s, loss=1.96, v_num=1, train_loss_step=1.130, val_loss=2.990, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 131:  48%|███▎   | 32/67 [00:08<00:09,  3.83it/s, loss=1.96, v_num=1, train_loss_step=1.130, val_loss=2.990, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 131:  49%|███▍   | 33/67 [00:08<00:08,  3.80it/s, loss=1.96, v_num=1, train_loss_step=1.130, val_loss=2.990, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 131:  51%|███▌   | 34/67 [00:09<00:08,  3.75it/s, loss=1.96, v_num=1, train_loss_step=1.130, val_loss=2.990, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 131:  52%|███▋   | 35/67 [00:09<00:08,  3.72it/s, loss=1.96, v_num=1, train_loss_step=1.130, val_loss=2.990, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 131:  54%|███▊   | 36/67 [00:09<00:08,  3.69it/s, loss=1.96, v_num=1, train_loss_step=1.130, val_loss=2.990, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 131:  55%|███▊   | 37/67 [00:10<00:08,  3.66it/s, loss=1.96, v_num=1, train_loss_step=1.130, val_loss=2.990, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 131:  57%|███▉   | 38/67 [00:10<00:08,  3.62it/s, loss=1.96, v_num=1, train_loss_step=1.130, val_loss=2.990, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 131:  58%|████   | 39/67 [00:10<00:07,  3.59it/s, loss=1.96, v_num=1, train_loss_step=1.130, val_loss=2.990, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 131:  60%|████▏  | 40/67 [00:11<00:07,  3.57it/s, loss=1.96, v_num=1, train_loss_step=1.130, val_loss=2.990, train_loss_epoch=2.500]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 131:  61%|████▎  | 41/67 [00:11<00:07,  3.49it/s, loss=1.96, v_num=1, train_loss_step=1.130, val_loss=2.990, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 131:  63%|████▍  | 42/67 [00:12<00:07,  3.47it/s, loss=1.96, v_num=1, train_loss_step=1.130, val_loss=2.990, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 131:  64%|████▍  | 43/67 [00:12<00:06,  3.45it/s, loss=1.96, v_num=1, train_loss_step=1.130, val_loss=2.990, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 131:  66%|████▌  | 44/67 [00:12<00:06,  3.42it/s, loss=1.96, v_num=1, train_loss_step=1.130, val_loss=2.990, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 131:  67%|████▋  | 45/67 [00:13<00:06,  3.40it/s, loss=1.96, v_num=1, train_loss_step=1.130, val_loss=2.990, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 131:  69%|████▊  | 46/67 [00:13<00:06,  3.39it/s, loss=1.96, v_num=1, train_loss_step=1.130, val_loss=2.990, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 131:  70%|████▉  | 47/67 [00:13<00:05,  3.37it/s, loss=1.96, v_num=1, train_loss_step=1.130, val_loss=2.990, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 131:  72%|█████  | 48/67 [00:14<00:05,  3.36it/s, loss=1.96, v_num=1, train_loss_step=1.130, val_loss=2.990, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 131:  73%|█████  | 49/67 [00:14<00:05,  3.34it/s, loss=1.96, v_num=1, train_loss_step=1.130, val_loss=2.990, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 131:  75%|█████▏ | 50/67 [00:15<00:05,  3.33it/s, loss=1.96, v_num=1, train_loss_step=1.130, val_loss=2.990, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 131:  76%|█████▎ | 51/67 [00:15<00:04,  3.28it/s, loss=1.96, v_num=1, train_loss_step=1.130, val_loss=2.990, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 131:  78%|█████▍ | 52/67 [00:15<00:04,  3.27it/s, loss=1.96, v_num=1, train_loss_step=1.130, val_loss=2.990, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 131:  79%|█████▌ | 53/67 [00:16<00:04,  3.26it/s, loss=1.96, v_num=1, train_loss_step=1.130, val_loss=2.990, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 131:  81%|█████▋ | 54/67 [00:16<00:04,  3.25it/s, loss=1.96, v_num=1, train_loss_step=1.130, val_loss=2.990, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 131:  82%|█████▋ | 55/67 [00:16<00:03,  3.24it/s, loss=1.96, v_num=1, train_loss_step=1.130, val_loss=2.990, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 131:  84%|█████▊ | 56/67 [00:17<00:03,  3.23it/s, loss=1.96, v_num=1, train_loss_step=1.130, val_loss=2.990, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 131:  85%|█████▉ | 57/67 [00:17<00:03,  3.22it/s, loss=1.96, v_num=1, train_loss_step=1.130, val_loss=2.990, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 131:  87%|██████ | 58/67 [00:18<00:02,  3.21it/s, loss=1.96, v_num=1, train_loss_step=1.130, val_loss=2.990, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 131:  88%|██████▏| 59/67 [00:18<00:02,  3.21it/s, loss=1.96, v_num=1, train_loss_step=1.130, val_loss=2.990, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 131:  90%|██████▎| 60/67 [00:18<00:02,  3.20it/s, loss=1.96, v_num=1, train_loss_step=1.130, val_loss=2.990, train_loss_epoch=2.500]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 131:  91%|██████▎| 61/67 [00:19<00:01,  3.17it/s, loss=1.96, v_num=1, train_loss_step=1.130, val_loss=2.990, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 131:  93%|██████▍| 62/67 [00:19<00:01,  3.17it/s, loss=1.96, v_num=1, train_loss_step=1.130, val_loss=2.990, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 131:  94%|██████▌| 63/67 [00:19<00:01,  3.16it/s, loss=1.96, v_num=1, train_loss_step=1.130, val_loss=2.990, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 131:  96%|██████▋| 64/67 [00:20<00:00,  3.15it/s, loss=1.96, v_num=1, train_loss_step=1.130, val_loss=2.990, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 131:  97%|██████▊| 65/67 [00:20<00:00,  3.15it/s, loss=1.96, v_num=1, train_loss_step=1.130, val_loss=2.990, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 131:  99%|██████▉| 66/67 [00:20<00:00,  3.15it/s, loss=1.96, v_num=1, train_loss_step=1.130, val_loss=2.990, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 131: 100%|███████| 67/67 [00:21<00:00,  3.08it/s, loss=1.96, v_num=1, train_loss_step=1.130, val_loss=2.990, train_loss_epoch=2.500]\u001b[A\n",
      "Epoch 132:  45%|███▏   | 30/67 [00:06<00:07,  4.64it/s, loss=1.76, v_num=1, train_loss_step=0.811, val_loss=2.990, train_loss_epoch=1.870]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 132:  46%|███▏   | 31/67 [00:08<00:10,  3.55it/s, loss=1.76, v_num=1, train_loss_step=0.811, val_loss=2.990, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 132:  48%|███▎   | 32/67 [00:09<00:09,  3.52it/s, loss=1.76, v_num=1, train_loss_step=0.811, val_loss=2.990, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 132:  49%|███▍   | 33/67 [00:09<00:09,  3.51it/s, loss=1.76, v_num=1, train_loss_step=0.811, val_loss=2.990, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 132:  51%|███▌   | 34/67 [00:09<00:09,  3.49it/s, loss=1.76, v_num=1, train_loss_step=0.811, val_loss=2.990, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 132:  52%|███▋   | 35/67 [00:10<00:09,  3.47it/s, loss=1.76, v_num=1, train_loss_step=0.811, val_loss=2.990, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 132:  54%|███▊   | 36/67 [00:10<00:08,  3.45it/s, loss=1.76, v_num=1, train_loss_step=0.811, val_loss=2.990, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 132:  55%|███▊   | 37/67 [00:10<00:08,  3.43it/s, loss=1.76, v_num=1, train_loss_step=0.811, val_loss=2.990, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 132:  57%|███▉   | 38/67 [00:11<00:08,  3.42it/s, loss=1.76, v_num=1, train_loss_step=0.811, val_loss=2.990, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 132:  58%|████   | 39/67 [00:11<00:08,  3.41it/s, loss=1.76, v_num=1, train_loss_step=0.811, val_loss=2.990, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 132:  60%|████▏  | 40/67 [00:11<00:07,  3.40it/s, loss=1.76, v_num=1, train_loss_step=0.811, val_loss=2.990, train_loss_epoch=1.870]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 132:  61%|████▎  | 41/67 [00:12<00:07,  3.35it/s, loss=1.76, v_num=1, train_loss_step=0.811, val_loss=2.990, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 132:  63%|████▍  | 42/67 [00:12<00:07,  3.33it/s, loss=1.76, v_num=1, train_loss_step=0.811, val_loss=2.990, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 132:  64%|████▍  | 43/67 [00:12<00:07,  3.32it/s, loss=1.76, v_num=1, train_loss_step=0.811, val_loss=2.990, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 132:  66%|████▌  | 44/67 [00:13<00:06,  3.32it/s, loss=1.76, v_num=1, train_loss_step=0.811, val_loss=2.990, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 132:  67%|████▋  | 45/67 [00:13<00:06,  3.31it/s, loss=1.76, v_num=1, train_loss_step=0.811, val_loss=2.990, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 132:  69%|████▊  | 46/67 [00:13<00:06,  3.30it/s, loss=1.76, v_num=1, train_loss_step=0.811, val_loss=2.990, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 132:  70%|████▉  | 47/67 [00:14<00:06,  3.29it/s, loss=1.76, v_num=1, train_loss_step=0.811, val_loss=2.990, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 132:  72%|█████  | 48/67 [00:14<00:05,  3.28it/s, loss=1.76, v_num=1, train_loss_step=0.811, val_loss=2.990, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 132:  73%|█████  | 49/67 [00:14<00:05,  3.28it/s, loss=1.76, v_num=1, train_loss_step=0.811, val_loss=2.990, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 132:  75%|█████▏ | 50/67 [00:15<00:05,  3.27it/s, loss=1.76, v_num=1, train_loss_step=0.811, val_loss=2.990, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 132:  76%|█████▎ | 51/67 [00:15<00:04,  3.23it/s, loss=1.76, v_num=1, train_loss_step=0.811, val_loss=2.990, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 132:  78%|█████▍ | 52/67 [00:16<00:04,  3.22it/s, loss=1.76, v_num=1, train_loss_step=0.811, val_loss=2.990, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 132:  79%|█████▌ | 53/67 [00:16<00:04,  3.22it/s, loss=1.76, v_num=1, train_loss_step=0.811, val_loss=2.990, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 132:  81%|█████▋ | 54/67 [00:16<00:04,  3.21it/s, loss=1.76, v_num=1, train_loss_step=0.811, val_loss=2.990, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 132:  82%|█████▋ | 55/67 [00:17<00:03,  3.20it/s, loss=1.76, v_num=1, train_loss_step=0.811, val_loss=2.990, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 132:  84%|█████▊ | 56/67 [00:17<00:03,  3.20it/s, loss=1.76, v_num=1, train_loss_step=0.811, val_loss=2.990, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 132:  85%|█████▉ | 57/67 [00:17<00:03,  3.19it/s, loss=1.76, v_num=1, train_loss_step=0.811, val_loss=2.990, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 132:  87%|██████ | 58/67 [00:18<00:02,  3.19it/s, loss=1.76, v_num=1, train_loss_step=0.811, val_loss=2.990, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 132:  88%|██████▏| 59/67 [00:18<00:02,  3.19it/s, loss=1.76, v_num=1, train_loss_step=0.811, val_loss=2.990, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 132:  90%|██████▎| 60/67 [00:18<00:02,  3.18it/s, loss=1.76, v_num=1, train_loss_step=0.811, val_loss=2.990, train_loss_epoch=1.870]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 132:  91%|██████▎| 61/67 [00:19<00:01,  3.15it/s, loss=1.76, v_num=1, train_loss_step=0.811, val_loss=2.990, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 132:  93%|██████▍| 62/67 [00:19<00:01,  3.15it/s, loss=1.76, v_num=1, train_loss_step=0.811, val_loss=2.990, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 132:  94%|██████▌| 63/67 [00:20<00:01,  3.14it/s, loss=1.76, v_num=1, train_loss_step=0.811, val_loss=2.990, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 132:  96%|██████▋| 64/67 [00:20<00:00,  3.14it/s, loss=1.76, v_num=1, train_loss_step=0.811, val_loss=2.990, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 132:  97%|██████▊| 65/67 [00:20<00:00,  3.14it/s, loss=1.76, v_num=1, train_loss_step=0.811, val_loss=2.990, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 132:  99%|██████▉| 66/67 [00:21<00:00,  3.14it/s, loss=1.76, v_num=1, train_loss_step=0.811, val_loss=2.990, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 132: 100%|███████| 67/67 [00:21<00:00,  3.08it/s, loss=1.76, v_num=1, train_loss_step=0.811, val_loss=2.990, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 133:  45%|███▏   | 30/67 [00:05<00:07,  5.18it/s, loss=1.88, v_num=1, train_loss_step=1.260, val_loss=2.990, train_loss_epoch=1.840]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 133:  46%|███▏   | 31/67 [00:07<00:09,  3.89it/s, loss=1.88, v_num=1, train_loss_step=1.260, val_loss=2.990, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 133:  48%|███▎   | 32/67 [00:08<00:09,  3.85it/s, loss=1.88, v_num=1, train_loss_step=1.260, val_loss=2.990, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 133:  49%|███▍   | 33/67 [00:08<00:08,  3.82it/s, loss=1.88, v_num=1, train_loss_step=1.260, val_loss=2.990, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 133:  51%|███▌   | 34/67 [00:08<00:08,  3.78it/s, loss=1.88, v_num=1, train_loss_step=1.260, val_loss=2.990, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 133:  52%|███▋   | 35/67 [00:09<00:08,  3.75it/s, loss=1.88, v_num=1, train_loss_step=1.260, val_loss=2.990, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 133:  54%|███▊   | 36/67 [00:09<00:08,  3.73it/s, loss=1.88, v_num=1, train_loss_step=1.260, val_loss=2.990, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 133:  55%|███▊   | 37/67 [00:10<00:08,  3.70it/s, loss=1.88, v_num=1, train_loss_step=1.260, val_loss=2.990, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 133:  57%|███▉   | 38/67 [00:10<00:07,  3.68it/s, loss=1.88, v_num=1, train_loss_step=1.260, val_loss=2.990, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 133:  58%|████   | 39/67 [00:10<00:07,  3.66it/s, loss=1.88, v_num=1, train_loss_step=1.260, val_loss=2.990, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 133:  60%|████▏  | 40/67 [00:11<00:07,  3.63it/s, loss=1.88, v_num=1, train_loss_step=1.260, val_loss=2.990, train_loss_epoch=1.840]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 133:  61%|████▎  | 41/67 [00:11<00:07,  3.57it/s, loss=1.88, v_num=1, train_loss_step=1.260, val_loss=2.990, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 133:  63%|████▍  | 42/67 [00:11<00:07,  3.55it/s, loss=1.88, v_num=1, train_loss_step=1.260, val_loss=2.990, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 133:  64%|████▍  | 43/67 [00:12<00:06,  3.53it/s, loss=1.88, v_num=1, train_loss_step=1.260, val_loss=2.990, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 133:  66%|████▌  | 44/67 [00:12<00:06,  3.52it/s, loss=1.88, v_num=1, train_loss_step=1.260, val_loss=2.990, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 133:  67%|████▋  | 45/67 [00:12<00:06,  3.51it/s, loss=1.88, v_num=1, train_loss_step=1.260, val_loss=2.990, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 133:  69%|████▊  | 46/67 [00:13<00:06,  3.50it/s, loss=1.88, v_num=1, train_loss_step=1.260, val_loss=2.990, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 133:  70%|████▉  | 47/67 [00:13<00:05,  3.48it/s, loss=1.88, v_num=1, train_loss_step=1.260, val_loss=2.990, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 133:  72%|█████  | 48/67 [00:13<00:05,  3.47it/s, loss=1.88, v_num=1, train_loss_step=1.260, val_loss=2.990, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 133:  73%|█████  | 49/67 [00:14<00:05,  3.46it/s, loss=1.88, v_num=1, train_loss_step=1.260, val_loss=2.990, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 133:  75%|█████▏ | 50/67 [00:14<00:04,  3.45it/s, loss=1.88, v_num=1, train_loss_step=1.260, val_loss=2.990, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 133:  76%|█████▎ | 51/67 [00:14<00:04,  3.40it/s, loss=1.88, v_num=1, train_loss_step=1.260, val_loss=2.990, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 133:  78%|█████▍ | 52/67 [00:15<00:04,  3.39it/s, loss=1.88, v_num=1, train_loss_step=1.260, val_loss=2.990, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 133:  79%|█████▌ | 53/67 [00:15<00:04,  3.38it/s, loss=1.88, v_num=1, train_loss_step=1.260, val_loss=2.990, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 133:  81%|█████▋ | 54/67 [00:16<00:03,  3.37it/s, loss=1.88, v_num=1, train_loss_step=1.260, val_loss=2.990, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 133:  82%|█████▋ | 55/67 [00:16<00:03,  3.36it/s, loss=1.88, v_num=1, train_loss_step=1.260, val_loss=2.990, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 133:  84%|█████▊ | 56/67 [00:16<00:03,  3.35it/s, loss=1.88, v_num=1, train_loss_step=1.260, val_loss=2.990, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 133:  85%|█████▉ | 57/67 [00:17<00:02,  3.34it/s, loss=1.88, v_num=1, train_loss_step=1.260, val_loss=2.990, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 133:  87%|██████ | 58/67 [00:17<00:02,  3.34it/s, loss=1.88, v_num=1, train_loss_step=1.260, val_loss=2.990, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 133:  88%|██████▏| 59/67 [00:17<00:02,  3.33it/s, loss=1.88, v_num=1, train_loss_step=1.260, val_loss=2.990, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 133:  90%|██████▎| 60/67 [00:18<00:02,  3.32it/s, loss=1.88, v_num=1, train_loss_step=1.260, val_loss=2.990, train_loss_epoch=1.840]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 133:  91%|██████▎| 61/67 [00:18<00:01,  3.29it/s, loss=1.88, v_num=1, train_loss_step=1.260, val_loss=2.990, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 133:  93%|██████▍| 62/67 [00:18<00:01,  3.28it/s, loss=1.88, v_num=1, train_loss_step=1.260, val_loss=2.990, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 133:  94%|██████▌| 63/67 [00:19<00:01,  3.28it/s, loss=1.88, v_num=1, train_loss_step=1.260, val_loss=2.990, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 133:  96%|██████▋| 64/67 [00:19<00:00,  3.27it/s, loss=1.88, v_num=1, train_loss_step=1.260, val_loss=2.990, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 133:  97%|██████▊| 65/67 [00:19<00:00,  3.27it/s, loss=1.88, v_num=1, train_loss_step=1.260, val_loss=2.990, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 133:  99%|██████▉| 66/67 [00:20<00:00,  3.26it/s, loss=1.88, v_num=1, train_loss_step=1.260, val_loss=2.990, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 133: 100%|███████| 67/67 [00:20<00:00,  3.20it/s, loss=1.88, v_num=1, train_loss_step=1.260, val_loss=2.980, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 134:  45%|███▏   | 30/67 [00:05<00:07,  5.07it/s, loss=2.15, v_num=1, train_loss_step=2.050, val_loss=2.980, train_loss_epoch=1.890]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 134:  46%|███▏   | 31/67 [00:08<00:09,  3.78it/s, loss=2.15, v_num=1, train_loss_step=2.050, val_loss=2.980, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 134:  48%|███▎   | 32/67 [00:08<00:09,  3.75it/s, loss=2.15, v_num=1, train_loss_step=2.050, val_loss=2.980, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 134:  49%|███▍   | 33/67 [00:08<00:09,  3.72it/s, loss=2.15, v_num=1, train_loss_step=2.050, val_loss=2.980, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 134:  51%|███▌   | 34/67 [00:09<00:08,  3.69it/s, loss=2.15, v_num=1, train_loss_step=2.050, val_loss=2.980, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 134:  52%|███▋   | 35/67 [00:09<00:08,  3.66it/s, loss=2.15, v_num=1, train_loss_step=2.050, val_loss=2.980, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 134:  54%|███▊   | 36/67 [00:09<00:08,  3.65it/s, loss=2.15, v_num=1, train_loss_step=2.050, val_loss=2.980, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 134:  55%|███▊   | 37/67 [00:10<00:08,  3.62it/s, loss=2.15, v_num=1, train_loss_step=2.050, val_loss=2.980, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 134:  57%|███▉   | 38/67 [00:10<00:08,  3.60it/s, loss=2.15, v_num=1, train_loss_step=2.050, val_loss=2.980, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 134:  58%|████   | 39/67 [00:10<00:07,  3.58it/s, loss=2.15, v_num=1, train_loss_step=2.050, val_loss=2.980, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 134:  60%|████▏  | 40/67 [00:11<00:07,  3.56it/s, loss=2.15, v_num=1, train_loss_step=2.050, val_loss=2.980, train_loss_epoch=1.890]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 134:  61%|████▎  | 41/67 [00:11<00:07,  3.50it/s, loss=2.15, v_num=1, train_loss_step=2.050, val_loss=2.980, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 134:  63%|████▍  | 42/67 [00:12<00:07,  3.49it/s, loss=2.15, v_num=1, train_loss_step=2.050, val_loss=2.980, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 134:  64%|████▍  | 43/67 [00:12<00:06,  3.47it/s, loss=2.15, v_num=1, train_loss_step=2.050, val_loss=2.980, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 134:  66%|████▌  | 44/67 [00:12<00:06,  3.46it/s, loss=2.15, v_num=1, train_loss_step=2.050, val_loss=2.980, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 134:  67%|████▋  | 45/67 [00:13<00:06,  3.45it/s, loss=2.15, v_num=1, train_loss_step=2.050, val_loss=2.980, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 134:  69%|████▊  | 46/67 [00:13<00:06,  3.43it/s, loss=2.15, v_num=1, train_loss_step=2.050, val_loss=2.980, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 134:  70%|████▉  | 47/67 [00:13<00:05,  3.42it/s, loss=2.15, v_num=1, train_loss_step=2.050, val_loss=2.980, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 134:  72%|█████  | 48/67 [00:14<00:05,  3.41it/s, loss=2.15, v_num=1, train_loss_step=2.050, val_loss=2.980, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 134:  73%|█████  | 49/67 [00:14<00:05,  3.40it/s, loss=2.15, v_num=1, train_loss_step=2.050, val_loss=2.980, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 134:  75%|█████▏ | 50/67 [00:14<00:05,  3.38it/s, loss=2.15, v_num=1, train_loss_step=2.050, val_loss=2.980, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 134:  76%|█████▎ | 51/67 [00:15<00:04,  3.34it/s, loss=2.15, v_num=1, train_loss_step=2.050, val_loss=2.980, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 134:  78%|█████▍ | 52/67 [00:15<00:04,  3.33it/s, loss=2.15, v_num=1, train_loss_step=2.050, val_loss=2.980, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 134:  79%|█████▌ | 53/67 [00:15<00:04,  3.33it/s, loss=2.15, v_num=1, train_loss_step=2.050, val_loss=2.980, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 134:  81%|█████▋ | 54/67 [00:16<00:03,  3.32it/s, loss=2.15, v_num=1, train_loss_step=2.050, val_loss=2.980, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 134:  82%|█████▋ | 55/67 [00:16<00:03,  3.31it/s, loss=2.15, v_num=1, train_loss_step=2.050, val_loss=2.980, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 134:  84%|█████▊ | 56/67 [00:16<00:03,  3.30it/s, loss=2.15, v_num=1, train_loss_step=2.050, val_loss=2.980, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 134:  85%|█████▉ | 57/67 [00:17<00:03,  3.30it/s, loss=2.15, v_num=1, train_loss_step=2.050, val_loss=2.980, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 134:  87%|██████ | 58/67 [00:17<00:02,  3.29it/s, loss=2.15, v_num=1, train_loss_step=2.050, val_loss=2.980, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 134:  88%|██████▏| 59/67 [00:17<00:02,  3.28it/s, loss=2.15, v_num=1, train_loss_step=2.050, val_loss=2.980, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 134:  90%|██████▎| 60/67 [00:18<00:02,  3.28it/s, loss=2.15, v_num=1, train_loss_step=2.050, val_loss=2.980, train_loss_epoch=1.890]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 134:  91%|██████▎| 61/67 [00:18<00:01,  3.24it/s, loss=2.15, v_num=1, train_loss_step=2.050, val_loss=2.980, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 134:  93%|██████▍| 62/67 [00:19<00:01,  3.24it/s, loss=2.15, v_num=1, train_loss_step=2.050, val_loss=2.980, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 134:  94%|██████▌| 63/67 [00:19<00:01,  3.23it/s, loss=2.15, v_num=1, train_loss_step=2.050, val_loss=2.980, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 134:  96%|██████▋| 64/67 [00:19<00:00,  3.23it/s, loss=2.15, v_num=1, train_loss_step=2.050, val_loss=2.980, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 134:  97%|██████▊| 65/67 [00:20<00:00,  3.23it/s, loss=2.15, v_num=1, train_loss_step=2.050, val_loss=2.980, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 134:  99%|██████▉| 66/67 [00:20<00:00,  3.22it/s, loss=2.15, v_num=1, train_loss_step=2.050, val_loss=2.980, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 134: 100%|███████| 67/67 [00:21<00:00,  3.15it/s, loss=2.15, v_num=1, train_loss_step=2.050, val_loss=2.980, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 135:  45%|███▏   | 30/67 [00:06<00:07,  4.93it/s, loss=2.12, v_num=1, train_loss_step=0.568, val_loss=2.980, train_loss_epoch=1.990]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 135:  46%|███▏   | 31/67 [00:08<00:09,  3.68it/s, loss=2.12, v_num=1, train_loss_step=0.568, val_loss=2.980, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 135:  48%|███▎   | 32/67 [00:08<00:09,  3.65it/s, loss=2.12, v_num=1, train_loss_step=0.568, val_loss=2.980, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 135:  49%|███▍   | 33/67 [00:09<00:09,  3.62it/s, loss=2.12, v_num=1, train_loss_step=0.568, val_loss=2.980, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 135:  51%|███▌   | 34/67 [00:09<00:09,  3.59it/s, loss=2.12, v_num=1, train_loss_step=0.568, val_loss=2.980, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 135:  52%|███▋   | 35/67 [00:09<00:08,  3.57it/s, loss=2.12, v_num=1, train_loss_step=0.568, val_loss=2.980, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 135:  54%|███▊   | 36/67 [00:10<00:08,  3.55it/s, loss=2.12, v_num=1, train_loss_step=0.568, val_loss=2.980, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 135:  55%|███▊   | 37/67 [00:10<00:08,  3.52it/s, loss=2.12, v_num=1, train_loss_step=0.568, val_loss=2.980, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 135:  57%|███▉   | 38/67 [00:10<00:08,  3.50it/s, loss=2.12, v_num=1, train_loss_step=0.568, val_loss=2.980, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 135:  58%|████   | 39/67 [00:11<00:08,  3.48it/s, loss=2.12, v_num=1, train_loss_step=0.568, val_loss=2.980, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 135:  60%|████▏  | 40/67 [00:11<00:07,  3.46it/s, loss=2.12, v_num=1, train_loss_step=0.568, val_loss=2.980, train_loss_epoch=1.990]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 135:  61%|████▎  | 41/67 [00:12<00:07,  3.40it/s, loss=2.12, v_num=1, train_loss_step=0.568, val_loss=2.980, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 135:  63%|████▍  | 42/67 [00:12<00:07,  3.38it/s, loss=2.12, v_num=1, train_loss_step=0.568, val_loss=2.980, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 135:  64%|████▍  | 43/67 [00:12<00:07,  3.37it/s, loss=2.12, v_num=1, train_loss_step=0.568, val_loss=2.980, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 135:  66%|████▌  | 44/67 [00:13<00:06,  3.36it/s, loss=2.12, v_num=1, train_loss_step=0.568, val_loss=2.980, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 135:  67%|████▋  | 45/67 [00:13<00:06,  3.35it/s, loss=2.12, v_num=1, train_loss_step=0.568, val_loss=2.980, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 135:  69%|████▊  | 46/67 [00:13<00:06,  3.33it/s, loss=2.12, v_num=1, train_loss_step=0.568, val_loss=2.980, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 135:  70%|████▉  | 47/67 [00:14<00:06,  3.33it/s, loss=2.12, v_num=1, train_loss_step=0.568, val_loss=2.980, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 135:  72%|█████  | 48/67 [00:14<00:05,  3.32it/s, loss=2.12, v_num=1, train_loss_step=0.568, val_loss=2.980, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 135:  73%|█████  | 49/67 [00:14<00:05,  3.31it/s, loss=2.12, v_num=1, train_loss_step=0.568, val_loss=2.980, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 135:  75%|█████▏ | 50/67 [00:15<00:05,  3.30it/s, loss=2.12, v_num=1, train_loss_step=0.568, val_loss=2.980, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 135:  76%|█████▎ | 51/67 [00:15<00:04,  3.24it/s, loss=2.12, v_num=1, train_loss_step=0.568, val_loss=2.980, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 135:  78%|█████▍ | 52/67 [00:16<00:04,  3.23it/s, loss=2.12, v_num=1, train_loss_step=0.568, val_loss=2.980, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 135:  79%|█████▌ | 53/67 [00:16<00:04,  3.22it/s, loss=2.12, v_num=1, train_loss_step=0.568, val_loss=2.980, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 135:  81%|█████▋ | 54/67 [00:16<00:04,  3.21it/s, loss=2.12, v_num=1, train_loss_step=0.568, val_loss=2.980, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 135:  82%|█████▋ | 55/67 [00:17<00:03,  3.20it/s, loss=2.12, v_num=1, train_loss_step=0.568, val_loss=2.980, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 135:  84%|█████▊ | 56/67 [00:17<00:03,  3.19it/s, loss=2.12, v_num=1, train_loss_step=0.568, val_loss=2.980, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 135:  85%|█████▉ | 57/67 [00:17<00:03,  3.18it/s, loss=2.12, v_num=1, train_loss_step=0.568, val_loss=2.980, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 135:  87%|██████ | 58/67 [00:18<00:02,  3.17it/s, loss=2.12, v_num=1, train_loss_step=0.568, val_loss=2.980, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 135:  88%|██████▏| 59/67 [00:18<00:02,  3.16it/s, loss=2.12, v_num=1, train_loss_step=0.568, val_loss=2.980, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 135:  90%|██████▎| 60/67 [00:19<00:02,  3.15it/s, loss=2.12, v_num=1, train_loss_step=0.568, val_loss=2.980, train_loss_epoch=1.990]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 135:  91%|██████▎| 61/67 [00:19<00:01,  3.12it/s, loss=2.12, v_num=1, train_loss_step=0.568, val_loss=2.980, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 135:  93%|██████▍| 62/67 [00:19<00:01,  3.12it/s, loss=2.12, v_num=1, train_loss_step=0.568, val_loss=2.980, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 135:  94%|██████▌| 63/67 [00:20<00:01,  3.12it/s, loss=2.12, v_num=1, train_loss_step=0.568, val_loss=2.980, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 135:  96%|██████▋| 64/67 [00:20<00:00,  3.11it/s, loss=2.12, v_num=1, train_loss_step=0.568, val_loss=2.980, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 135:  97%|██████▊| 65/67 [00:20<00:00,  3.11it/s, loss=2.12, v_num=1, train_loss_step=0.568, val_loss=2.980, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 135:  99%|██████▉| 66/67 [00:21<00:00,  3.11it/s, loss=2.12, v_num=1, train_loss_step=0.568, val_loss=2.980, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 135: 100%|███████| 67/67 [00:21<00:00,  3.05it/s, loss=2.12, v_num=1, train_loss_step=0.568, val_loss=2.980, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 136:  45%|███▏   | 30/67 [00:05<00:07,  5.20it/s, loss=2.43, v_num=1, train_loss_step=6.570, val_loss=2.980, train_loss_epoch=2.060]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 136:  46%|███▏   | 31/67 [00:07<00:09,  3.91it/s, loss=2.43, v_num=1, train_loss_step=6.570, val_loss=2.980, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 136:  48%|███▎   | 32/67 [00:08<00:09,  3.87it/s, loss=2.43, v_num=1, train_loss_step=6.570, val_loss=2.980, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 136:  49%|███▍   | 33/67 [00:08<00:08,  3.83it/s, loss=2.43, v_num=1, train_loss_step=6.570, val_loss=2.980, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 136:  51%|███▌   | 34/67 [00:08<00:08,  3.78it/s, loss=2.43, v_num=1, train_loss_step=6.570, val_loss=2.980, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 136:  52%|███▋   | 35/67 [00:09<00:08,  3.76it/s, loss=2.43, v_num=1, train_loss_step=6.570, val_loss=2.980, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 136:  54%|███▊   | 36/67 [00:09<00:08,  3.73it/s, loss=2.43, v_num=1, train_loss_step=6.570, val_loss=2.980, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 136:  55%|███▊   | 37/67 [00:10<00:08,  3.70it/s, loss=2.43, v_num=1, train_loss_step=6.570, val_loss=2.980, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 136:  57%|███▉   | 38/67 [00:10<00:07,  3.68it/s, loss=2.43, v_num=1, train_loss_step=6.570, val_loss=2.980, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 136:  58%|████   | 39/67 [00:10<00:07,  3.65it/s, loss=2.43, v_num=1, train_loss_step=6.570, val_loss=2.980, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 136:  60%|████▏  | 40/67 [00:11<00:07,  3.62it/s, loss=2.43, v_num=1, train_loss_step=6.570, val_loss=2.980, train_loss_epoch=2.060]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 136:  61%|████▎  | 41/67 [00:11<00:07,  3.54it/s, loss=2.43, v_num=1, train_loss_step=6.570, val_loss=2.980, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 136:  63%|████▍  | 42/67 [00:11<00:07,  3.52it/s, loss=2.43, v_num=1, train_loss_step=6.570, val_loss=2.980, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 136:  64%|████▍  | 43/67 [00:12<00:06,  3.49it/s, loss=2.43, v_num=1, train_loss_step=6.570, val_loss=2.980, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 136:  66%|████▌  | 44/67 [00:12<00:06,  3.48it/s, loss=2.43, v_num=1, train_loss_step=6.570, val_loss=2.980, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 136:  67%|████▋  | 45/67 [00:12<00:06,  3.46it/s, loss=2.43, v_num=1, train_loss_step=6.570, val_loss=2.980, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 136:  69%|████▊  | 46/67 [00:13<00:06,  3.45it/s, loss=2.43, v_num=1, train_loss_step=6.570, val_loss=2.980, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 136:  70%|████▉  | 47/67 [00:13<00:05,  3.44it/s, loss=2.43, v_num=1, train_loss_step=6.570, val_loss=2.980, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 136:  72%|█████  | 48/67 [00:14<00:05,  3.42it/s, loss=2.43, v_num=1, train_loss_step=6.570, val_loss=2.980, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 136:  73%|█████  | 49/67 [00:14<00:05,  3.41it/s, loss=2.43, v_num=1, train_loss_step=6.570, val_loss=2.980, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 136:  75%|█████▏ | 50/67 [00:14<00:05,  3.40it/s, loss=2.43, v_num=1, train_loss_step=6.570, val_loss=2.980, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 136:  76%|█████▎ | 51/67 [00:15<00:04,  3.33it/s, loss=2.43, v_num=1, train_loss_step=6.570, val_loss=2.980, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 136:  78%|█████▍ | 52/67 [00:15<00:04,  3.32it/s, loss=2.43, v_num=1, train_loss_step=6.570, val_loss=2.980, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 136:  79%|█████▌ | 53/67 [00:16<00:04,  3.31it/s, loss=2.43, v_num=1, train_loss_step=6.570, val_loss=2.980, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 136:  81%|█████▋ | 54/67 [00:16<00:03,  3.30it/s, loss=2.43, v_num=1, train_loss_step=6.570, val_loss=2.980, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 136:  82%|█████▋ | 55/67 [00:16<00:03,  3.29it/s, loss=2.43, v_num=1, train_loss_step=6.570, val_loss=2.980, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 136:  84%|█████▊ | 56/67 [00:17<00:03,  3.29it/s, loss=2.43, v_num=1, train_loss_step=6.570, val_loss=2.980, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 136:  85%|█████▉ | 57/67 [00:17<00:03,  3.28it/s, loss=2.43, v_num=1, train_loss_step=6.570, val_loss=2.980, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 136:  87%|██████ | 58/67 [00:17<00:02,  3.28it/s, loss=2.43, v_num=1, train_loss_step=6.570, val_loss=2.980, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 136:  88%|██████▏| 59/67 [00:18<00:02,  3.27it/s, loss=2.43, v_num=1, train_loss_step=6.570, val_loss=2.980, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 136:  90%|██████▎| 60/67 [00:18<00:02,  3.26it/s, loss=2.43, v_num=1, train_loss_step=6.570, val_loss=2.980, train_loss_epoch=2.060]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 136:  91%|██████▎| 61/67 [00:18<00:01,  3.22it/s, loss=2.43, v_num=1, train_loss_step=6.570, val_loss=2.980, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 136:  93%|██████▍| 62/67 [00:19<00:01,  3.22it/s, loss=2.43, v_num=1, train_loss_step=6.570, val_loss=2.980, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 136:  94%|██████▌| 63/67 [00:19<00:01,  3.21it/s, loss=2.43, v_num=1, train_loss_step=6.570, val_loss=2.980, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 136:  96%|██████▋| 64/67 [00:19<00:00,  3.21it/s, loss=2.43, v_num=1, train_loss_step=6.570, val_loss=2.980, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 136:  97%|██████▊| 65/67 [00:20<00:00,  3.21it/s, loss=2.43, v_num=1, train_loss_step=6.570, val_loss=2.980, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 136:  99%|██████▉| 66/67 [00:20<00:00,  3.20it/s, loss=2.43, v_num=1, train_loss_step=6.570, val_loss=2.980, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 136: 100%|███████| 67/67 [00:21<00:00,  3.14it/s, loss=2.43, v_num=1, train_loss_step=6.570, val_loss=2.980, train_loss_epoch=2.060]\u001b[A\n",
      "Epoch 137:  45%|███▌    | 30/67 [00:05<00:07,  5.08it/s, loss=2.4, v_num=1, train_loss_step=1.880, val_loss=2.980, train_loss_epoch=2.030]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 137:  46%|███▋    | 31/67 [00:08<00:09,  3.85it/s, loss=2.4, v_num=1, train_loss_step=1.880, val_loss=2.980, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 137:  48%|███▊    | 32/67 [00:08<00:09,  3.81it/s, loss=2.4, v_num=1, train_loss_step=1.880, val_loss=2.980, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 137:  49%|███▉    | 33/67 [00:08<00:09,  3.78it/s, loss=2.4, v_num=1, train_loss_step=1.880, val_loss=2.980, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 137:  51%|████    | 34/67 [00:09<00:08,  3.74it/s, loss=2.4, v_num=1, train_loss_step=1.880, val_loss=2.980, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 137:  52%|████▏   | 35/67 [00:09<00:08,  3.70it/s, loss=2.4, v_num=1, train_loss_step=1.880, val_loss=2.980, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 137:  54%|████▎   | 36/67 [00:09<00:08,  3.67it/s, loss=2.4, v_num=1, train_loss_step=1.880, val_loss=2.980, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 137:  55%|████▍   | 37/67 [00:10<00:08,  3.64it/s, loss=2.4, v_num=1, train_loss_step=1.880, val_loss=2.980, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 137:  57%|████▌   | 38/67 [00:10<00:08,  3.61it/s, loss=2.4, v_num=1, train_loss_step=1.880, val_loss=2.980, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 137:  58%|████▋   | 39/67 [00:10<00:07,  3.59it/s, loss=2.4, v_num=1, train_loss_step=1.880, val_loss=2.980, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 137:  60%|████▊   | 40/67 [00:11<00:07,  3.57it/s, loss=2.4, v_num=1, train_loss_step=1.880, val_loss=2.980, train_loss_epoch=2.030]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 137:  61%|████▉   | 41/67 [00:11<00:07,  3.50it/s, loss=2.4, v_num=1, train_loss_step=1.880, val_loss=2.980, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 137:  63%|█████   | 42/67 [00:12<00:07,  3.48it/s, loss=2.4, v_num=1, train_loss_step=1.880, val_loss=2.980, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 137:  64%|█████▏  | 43/67 [00:12<00:06,  3.47it/s, loss=2.4, v_num=1, train_loss_step=1.880, val_loss=2.980, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 137:  66%|█████▎  | 44/67 [00:12<00:06,  3.46it/s, loss=2.4, v_num=1, train_loss_step=1.880, val_loss=2.980, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 137:  67%|█████▎  | 45/67 [00:13<00:06,  3.44it/s, loss=2.4, v_num=1, train_loss_step=1.880, val_loss=2.980, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 137:  69%|█████▍  | 46/67 [00:13<00:06,  3.43it/s, loss=2.4, v_num=1, train_loss_step=1.880, val_loss=2.980, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 137:  70%|█████▌  | 47/67 [00:13<00:05,  3.41it/s, loss=2.4, v_num=1, train_loss_step=1.880, val_loss=2.980, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 137:  72%|█████▋  | 48/67 [00:14<00:05,  3.40it/s, loss=2.4, v_num=1, train_loss_step=1.880, val_loss=2.980, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 137:  73%|█████▊  | 49/67 [00:14<00:05,  3.38it/s, loss=2.4, v_num=1, train_loss_step=1.880, val_loss=2.980, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 137:  75%|█████▉  | 50/67 [00:14<00:05,  3.38it/s, loss=2.4, v_num=1, train_loss_step=1.880, val_loss=2.980, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 137:  76%|██████  | 51/67 [00:15<00:04,  3.33it/s, loss=2.4, v_num=1, train_loss_step=1.880, val_loss=2.980, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 137:  78%|██████▏ | 52/67 [00:15<00:04,  3.31it/s, loss=2.4, v_num=1, train_loss_step=1.880, val_loss=2.980, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 137:  79%|██████▎ | 53/67 [00:16<00:04,  3.31it/s, loss=2.4, v_num=1, train_loss_step=1.880, val_loss=2.980, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 137:  81%|██████▍ | 54/67 [00:16<00:03,  3.30it/s, loss=2.4, v_num=1, train_loss_step=1.880, val_loss=2.980, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 137:  82%|██████▌ | 55/67 [00:16<00:03,  3.29it/s, loss=2.4, v_num=1, train_loss_step=1.880, val_loss=2.980, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 137:  84%|██████▋ | 56/67 [00:17<00:03,  3.29it/s, loss=2.4, v_num=1, train_loss_step=1.880, val_loss=2.980, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 137:  85%|██████▊ | 57/67 [00:17<00:03,  3.28it/s, loss=2.4, v_num=1, train_loss_step=1.880, val_loss=2.980, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 137:  87%|██████▉ | 58/67 [00:17<00:02,  3.25it/s, loss=2.4, v_num=1, train_loss_step=1.880, val_loss=2.980, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 137:  88%|███████ | 59/67 [00:18<00:02,  3.25it/s, loss=2.4, v_num=1, train_loss_step=1.880, val_loss=2.980, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 137:  90%|███████▏| 60/67 [00:18<00:02,  3.24it/s, loss=2.4, v_num=1, train_loss_step=1.880, val_loss=2.980, train_loss_epoch=2.030]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 137:  91%|███████▎| 61/67 [00:19<00:01,  3.20it/s, loss=2.4, v_num=1, train_loss_step=1.880, val_loss=2.980, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 137:  93%|███████▍| 62/67 [00:19<00:01,  3.20it/s, loss=2.4, v_num=1, train_loss_step=1.880, val_loss=2.980, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 137:  94%|███████▌| 63/67 [00:19<00:01,  3.20it/s, loss=2.4, v_num=1, train_loss_step=1.880, val_loss=2.980, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 137:  96%|███████▋| 64/67 [00:20<00:00,  3.20it/s, loss=2.4, v_num=1, train_loss_step=1.880, val_loss=2.980, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 137:  97%|███████▊| 65/67 [00:20<00:00,  3.19it/s, loss=2.4, v_num=1, train_loss_step=1.880, val_loss=2.980, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 137:  99%|███████▉| 66/67 [00:20<00:00,  3.19it/s, loss=2.4, v_num=1, train_loss_step=1.880, val_loss=2.980, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 137: 100%|████████| 67/67 [00:21<00:00,  3.13it/s, loss=2.4, v_num=1, train_loss_step=1.880, val_loss=2.980, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 138:  45%|███▏   | 30/67 [00:05<00:07,  5.27it/s, loss=2.35, v_num=1, train_loss_step=3.540, val_loss=2.980, train_loss_epoch=2.610]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 138:  46%|███▏   | 31/67 [00:07<00:09,  3.95it/s, loss=2.35, v_num=1, train_loss_step=3.540, val_loss=2.980, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 138:  48%|███▎   | 32/67 [00:08<00:08,  3.91it/s, loss=2.35, v_num=1, train_loss_step=3.540, val_loss=2.980, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 138:  49%|███▍   | 33/67 [00:08<00:08,  3.88it/s, loss=2.35, v_num=1, train_loss_step=3.540, val_loss=2.980, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 138:  51%|███▌   | 34/67 [00:08<00:08,  3.84it/s, loss=2.35, v_num=1, train_loss_step=3.540, val_loss=2.980, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 138:  52%|███▋   | 35/67 [00:09<00:08,  3.81it/s, loss=2.35, v_num=1, train_loss_step=3.540, val_loss=2.980, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 138:  54%|███▊   | 36/67 [00:09<00:08,  3.79it/s, loss=2.35, v_num=1, train_loss_step=3.540, val_loss=2.980, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 138:  55%|███▊   | 37/67 [00:09<00:07,  3.76it/s, loss=2.35, v_num=1, train_loss_step=3.540, val_loss=2.980, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 138:  57%|███▉   | 38/67 [00:10<00:07,  3.73it/s, loss=2.35, v_num=1, train_loss_step=3.540, val_loss=2.980, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 138:  58%|████   | 39/67 [00:10<00:07,  3.71it/s, loss=2.35, v_num=1, train_loss_step=3.540, val_loss=2.980, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 138:  60%|████▏  | 40/67 [00:10<00:07,  3.68it/s, loss=2.35, v_num=1, train_loss_step=3.540, val_loss=2.980, train_loss_epoch=2.610]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 138:  61%|████▎  | 41/67 [00:11<00:07,  3.61it/s, loss=2.35, v_num=1, train_loss_step=3.540, val_loss=2.980, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 138:  63%|████▍  | 42/67 [00:11<00:06,  3.59it/s, loss=2.35, v_num=1, train_loss_step=3.540, val_loss=2.980, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 138:  64%|████▍  | 43/67 [00:12<00:06,  3.57it/s, loss=2.35, v_num=1, train_loss_step=3.540, val_loss=2.980, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 138:  66%|████▌  | 44/67 [00:12<00:06,  3.55it/s, loss=2.35, v_num=1, train_loss_step=3.540, val_loss=2.980, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 138:  67%|████▋  | 45/67 [00:12<00:06,  3.53it/s, loss=2.35, v_num=1, train_loss_step=3.540, val_loss=2.980, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 138:  69%|████▊  | 46/67 [00:13<00:05,  3.51it/s, loss=2.35, v_num=1, train_loss_step=3.540, val_loss=2.980, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 138:  70%|████▉  | 47/67 [00:13<00:05,  3.50it/s, loss=2.35, v_num=1, train_loss_step=3.540, val_loss=2.980, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 138:  72%|█████  | 48/67 [00:13<00:05,  3.49it/s, loss=2.35, v_num=1, train_loss_step=3.540, val_loss=2.980, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 138:  73%|█████  | 49/67 [00:14<00:05,  3.47it/s, loss=2.35, v_num=1, train_loss_step=3.540, val_loss=2.980, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 138:  75%|█████▏ | 50/67 [00:14<00:04,  3.46it/s, loss=2.35, v_num=1, train_loss_step=3.540, val_loss=2.980, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 138:  76%|█████▎ | 51/67 [00:14<00:04,  3.42it/s, loss=2.35, v_num=1, train_loss_step=3.540, val_loss=2.980, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 138:  78%|█████▍ | 52/67 [00:15<00:04,  3.40it/s, loss=2.35, v_num=1, train_loss_step=3.540, val_loss=2.980, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 138:  79%|█████▌ | 53/67 [00:15<00:04,  3.40it/s, loss=2.35, v_num=1, train_loss_step=3.540, val_loss=2.980, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 138:  81%|█████▋ | 54/67 [00:15<00:03,  3.39it/s, loss=2.35, v_num=1, train_loss_step=3.540, val_loss=2.980, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 138:  82%|█████▋ | 55/67 [00:16<00:03,  3.38it/s, loss=2.35, v_num=1, train_loss_step=3.540, val_loss=2.980, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 138:  84%|█████▊ | 56/67 [00:16<00:03,  3.37it/s, loss=2.35, v_num=1, train_loss_step=3.540, val_loss=2.980, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 138:  85%|█████▉ | 57/67 [00:16<00:02,  3.37it/s, loss=2.35, v_num=1, train_loss_step=3.540, val_loss=2.980, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 138:  87%|██████ | 58/67 [00:17<00:02,  3.35it/s, loss=2.35, v_num=1, train_loss_step=3.540, val_loss=2.980, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 138:  88%|██████▏| 59/67 [00:17<00:02,  3.35it/s, loss=2.35, v_num=1, train_loss_step=3.540, val_loss=2.980, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 138:  90%|██████▎| 60/67 [00:17<00:02,  3.34it/s, loss=2.35, v_num=1, train_loss_step=3.540, val_loss=2.980, train_loss_epoch=2.610]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 138:  91%|██████▎| 61/67 [00:18<00:01,  3.30it/s, loss=2.35, v_num=1, train_loss_step=3.540, val_loss=2.980, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 138:  93%|██████▍| 62/67 [00:18<00:01,  3.29it/s, loss=2.35, v_num=1, train_loss_step=3.540, val_loss=2.980, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 138:  94%|██████▌| 63/67 [00:19<00:01,  3.29it/s, loss=2.35, v_num=1, train_loss_step=3.540, val_loss=2.980, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 138:  96%|██████▋| 64/67 [00:19<00:00,  3.28it/s, loss=2.35, v_num=1, train_loss_step=3.540, val_loss=2.980, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 138:  97%|██████▊| 65/67 [00:19<00:00,  3.28it/s, loss=2.35, v_num=1, train_loss_step=3.540, val_loss=2.980, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 138:  99%|██████▉| 66/67 [00:20<00:00,  3.27it/s, loss=2.35, v_num=1, train_loss_step=3.540, val_loss=2.980, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 138: 100%|███████| 67/67 [00:20<00:00,  3.20it/s, loss=2.35, v_num=1, train_loss_step=3.540, val_loss=2.980, train_loss_epoch=2.610]\u001b[A\n",
      "Epoch 139:  45%|███▏   | 30/67 [00:05<00:07,  5.06it/s, loss=2.05, v_num=1, train_loss_step=1.390, val_loss=2.980, train_loss_epoch=1.990]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 139:  46%|███▏   | 31/67 [00:07<00:09,  3.91it/s, loss=2.05, v_num=1, train_loss_step=1.390, val_loss=2.980, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 139:  48%|███▎   | 32/67 [00:08<00:09,  3.88it/s, loss=2.05, v_num=1, train_loss_step=1.390, val_loss=2.980, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 139:  49%|███▍   | 33/67 [00:08<00:08,  3.84it/s, loss=2.05, v_num=1, train_loss_step=1.390, val_loss=2.980, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 139:  51%|███▌   | 34/67 [00:08<00:08,  3.81it/s, loss=2.05, v_num=1, train_loss_step=1.390, val_loss=2.980, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 139:  52%|███▋   | 35/67 [00:09<00:08,  3.78it/s, loss=2.05, v_num=1, train_loss_step=1.390, val_loss=2.980, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 139:  54%|███▊   | 36/67 [00:09<00:08,  3.75it/s, loss=2.05, v_num=1, train_loss_step=1.390, val_loss=2.980, train_loss_epoch=1.990]\u001b[A\n",
      "Validation DataLoader 0:  19%|██████████████▌                                                              | 7/37 [00:02<00:10,  2.76it/s]\u001b[A\n",
      "Epoch 189:  60%|████▏  | 40/67 [00:11<00:07,  3.52it/s, loss=2.12, v_num=1, train_loss_step=1.480, val_loss=2.900, train_loss_epoch=2.070]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 189:  61%|████▎  | 41/67 [00:11<00:07,  3.46it/s, loss=2.12, v_num=1, train_loss_step=1.480, val_loss=2.900, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 189:  63%|████▍  | 42/67 [00:12<00:07,  3.44it/s, loss=2.12, v_num=1, train_loss_step=1.480, val_loss=2.900, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 189:  64%|████▍  | 43/67 [00:12<00:07,  3.42it/s, loss=2.12, v_num=1, train_loss_step=1.480, val_loss=2.900, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 189:  66%|████▌  | 44/67 [00:12<00:06,  3.41it/s, loss=2.12, v_num=1, train_loss_step=1.480, val_loss=2.900, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 189:  67%|████▋  | 45/67 [00:13<00:06,  3.40it/s, loss=2.12, v_num=1, train_loss_step=1.480, val_loss=2.900, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 189:  69%|████▊  | 46/67 [00:13<00:06,  3.38it/s, loss=2.12, v_num=1, train_loss_step=1.480, val_loss=2.900, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 189:  70%|████▉  | 47/67 [00:13<00:05,  3.37it/s, loss=2.12, v_num=1, train_loss_step=1.480, val_loss=2.900, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 189:  72%|█████  | 48/67 [00:14<00:05,  3.36it/s, loss=2.12, v_num=1, train_loss_step=1.480, val_loss=2.900, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 189:  73%|█████  | 49/67 [00:14<00:05,  3.34it/s, loss=2.12, v_num=1, train_loss_step=1.480, val_loss=2.900, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 189:  75%|█████▏ | 50/67 [00:15<00:05,  3.33it/s, loss=2.12, v_num=1, train_loss_step=1.480, val_loss=2.900, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 189:  76%|█████▎ | 51/67 [00:15<00:04,  3.29it/s, loss=2.12, v_num=1, train_loss_step=1.480, val_loss=2.900, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 189:  78%|█████▍ | 52/67 [00:15<00:04,  3.28it/s, loss=2.12, v_num=1, train_loss_step=1.480, val_loss=2.900, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 189:  79%|█████▌ | 53/67 [00:16<00:04,  3.27it/s, loss=2.12, v_num=1, train_loss_step=1.480, val_loss=2.900, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 189:  81%|█████▋ | 54/67 [00:16<00:03,  3.27it/s, loss=2.12, v_num=1, train_loss_step=1.480, val_loss=2.900, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 189:  82%|█████▋ | 55/67 [00:16<00:03,  3.26it/s, loss=2.12, v_num=1, train_loss_step=1.480, val_loss=2.900, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 189:  84%|█████▊ | 56/67 [00:17<00:03,  3.25it/s, loss=2.12, v_num=1, train_loss_step=1.480, val_loss=2.900, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 189:  85%|█████▉ | 57/67 [00:17<00:03,  3.25it/s, loss=2.12, v_num=1, train_loss_step=1.480, val_loss=2.900, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 189:  87%|██████ | 58/67 [00:17<00:02,  3.24it/s, loss=2.12, v_num=1, train_loss_step=1.480, val_loss=2.900, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 189:  88%|██████▏| 59/67 [00:18<00:02,  3.24it/s, loss=2.12, v_num=1, train_loss_step=1.480, val_loss=2.900, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 189:  90%|██████▎| 60/67 [00:18<00:02,  3.23it/s, loss=2.12, v_num=1, train_loss_step=1.480, val_loss=2.900, train_loss_epoch=2.070]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 189:  91%|██████▎| 61/67 [00:19<00:01,  3.19it/s, loss=2.12, v_num=1, train_loss_step=1.480, val_loss=2.900, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 189:  93%|██████▍| 62/67 [00:19<00:01,  3.18it/s, loss=2.12, v_num=1, train_loss_step=1.480, val_loss=2.900, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 189:  94%|██████▌| 63/67 [00:19<00:01,  3.18it/s, loss=2.12, v_num=1, train_loss_step=1.480, val_loss=2.900, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 189:  96%|██████▋| 64/67 [00:20<00:00,  3.17it/s, loss=2.12, v_num=1, train_loss_step=1.480, val_loss=2.900, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 189:  97%|██████▊| 65/67 [00:20<00:00,  3.17it/s, loss=2.12, v_num=1, train_loss_step=1.480, val_loss=2.900, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 189:  99%|██████▉| 66/67 [00:20<00:00,  3.17it/s, loss=2.12, v_num=1, train_loss_step=1.480, val_loss=2.900, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 189: 100%|███████| 67/67 [00:21<00:00,  3.11it/s, loss=2.12, v_num=1, train_loss_step=1.480, val_loss=2.900, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 190:  45%|███▏   | 30/67 [00:05<00:07,  5.08it/s, loss=2.23, v_num=1, train_loss_step=0.765, val_loss=2.900, train_loss_epoch=2.440]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 190:  46%|███▏   | 31/67 [00:08<00:09,  3.82it/s, loss=2.23, v_num=1, train_loss_step=0.765, val_loss=2.900, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 190:  48%|███▎   | 32/67 [00:08<00:09,  3.78it/s, loss=2.23, v_num=1, train_loss_step=0.765, val_loss=2.900, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 190:  49%|███▍   | 33/67 [00:08<00:09,  3.75it/s, loss=2.23, v_num=1, train_loss_step=0.765, val_loss=2.900, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 190:  51%|███▌   | 34/67 [00:09<00:08,  3.72it/s, loss=2.23, v_num=1, train_loss_step=0.765, val_loss=2.900, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 190:  52%|███▋   | 35/67 [00:09<00:08,  3.69it/s, loss=2.23, v_num=1, train_loss_step=0.765, val_loss=2.900, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 190:  54%|███▊   | 36/67 [00:09<00:08,  3.66it/s, loss=2.23, v_num=1, train_loss_step=0.765, val_loss=2.900, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 190:  55%|███▊   | 37/67 [00:10<00:08,  3.65it/s, loss=2.23, v_num=1, train_loss_step=0.765, val_loss=2.900, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 190:  57%|███▉   | 38/67 [00:10<00:08,  3.62it/s, loss=2.23, v_num=1, train_loss_step=0.765, val_loss=2.900, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 190:  58%|████   | 39/67 [00:10<00:07,  3.61it/s, loss=2.23, v_num=1, train_loss_step=0.765, val_loss=2.900, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 190:  60%|████▏  | 40/67 [00:11<00:07,  3.59it/s, loss=2.23, v_num=1, train_loss_step=0.765, val_loss=2.900, train_loss_epoch=2.440]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 190:  61%|████▎  | 41/67 [00:11<00:07,  3.52it/s, loss=2.23, v_num=1, train_loss_step=0.765, val_loss=2.900, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 190:  63%|████▍  | 42/67 [00:11<00:07,  3.51it/s, loss=2.23, v_num=1, train_loss_step=0.765, val_loss=2.900, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 190:  64%|████▍  | 43/67 [00:12<00:06,  3.50it/s, loss=2.23, v_num=1, train_loss_step=0.765, val_loss=2.900, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 190:  66%|████▌  | 44/67 [00:12<00:06,  3.48it/s, loss=2.23, v_num=1, train_loss_step=0.765, val_loss=2.900, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 190:  67%|████▋  | 45/67 [00:12<00:06,  3.47it/s, loss=2.23, v_num=1, train_loss_step=0.765, val_loss=2.900, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 190:  69%|████▊  | 46/67 [00:13<00:06,  3.46it/s, loss=2.23, v_num=1, train_loss_step=0.765, val_loss=2.900, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 190:  70%|████▉  | 47/67 [00:13<00:05,  3.44it/s, loss=2.23, v_num=1, train_loss_step=0.765, val_loss=2.900, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 190:  72%|█████  | 48/67 [00:14<00:05,  3.43it/s, loss=2.23, v_num=1, train_loss_step=0.765, val_loss=2.900, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 190:  73%|█████  | 49/67 [00:14<00:05,  3.41it/s, loss=2.23, v_num=1, train_loss_step=0.765, val_loss=2.900, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 190:  75%|█████▏ | 50/67 [00:14<00:05,  3.40it/s, loss=2.23, v_num=1, train_loss_step=0.765, val_loss=2.900, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 190:  76%|█████▎ | 51/67 [00:15<00:04,  3.35it/s, loss=2.23, v_num=1, train_loss_step=0.765, val_loss=2.900, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 190:  78%|█████▍ | 52/67 [00:15<00:04,  3.34it/s, loss=2.23, v_num=1, train_loss_step=0.765, val_loss=2.900, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 190:  79%|█████▌ | 53/67 [00:15<00:04,  3.32it/s, loss=2.23, v_num=1, train_loss_step=0.765, val_loss=2.900, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 190:  81%|█████▋ | 54/67 [00:16<00:03,  3.31it/s, loss=2.23, v_num=1, train_loss_step=0.765, val_loss=2.900, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 190:  82%|█████▋ | 55/67 [00:16<00:03,  3.30it/s, loss=2.23, v_num=1, train_loss_step=0.765, val_loss=2.900, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 190:  84%|█████▊ | 56/67 [00:17<00:03,  3.29it/s, loss=2.23, v_num=1, train_loss_step=0.765, val_loss=2.900, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 190:  85%|█████▉ | 57/67 [00:17<00:03,  3.28it/s, loss=2.23, v_num=1, train_loss_step=0.765, val_loss=2.900, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 190:  87%|██████ | 58/67 [00:17<00:02,  3.26it/s, loss=2.23, v_num=1, train_loss_step=0.765, val_loss=2.900, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 190:  88%|██████▏| 59/67 [00:18<00:02,  3.26it/s, loss=2.23, v_num=1, train_loss_step=0.765, val_loss=2.900, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 190:  90%|██████▎| 60/67 [00:18<00:02,  3.25it/s, loss=2.23, v_num=1, train_loss_step=0.765, val_loss=2.900, train_loss_epoch=2.440]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 190:  91%|██████▎| 61/67 [00:18<00:01,  3.21it/s, loss=2.23, v_num=1, train_loss_step=0.765, val_loss=2.900, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 190:  93%|██████▍| 62/67 [00:19<00:01,  3.21it/s, loss=2.23, v_num=1, train_loss_step=0.765, val_loss=2.900, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 190:  94%|██████▌| 63/67 [00:19<00:01,  3.21it/s, loss=2.23, v_num=1, train_loss_step=0.765, val_loss=2.900, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 190:  96%|██████▋| 64/67 [00:19<00:00,  3.20it/s, loss=2.23, v_num=1, train_loss_step=0.765, val_loss=2.900, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 190:  97%|██████▊| 65/67 [00:20<00:00,  3.20it/s, loss=2.23, v_num=1, train_loss_step=0.765, val_loss=2.900, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 190:  99%|██████▉| 66/67 [00:20<00:00,  3.20it/s, loss=2.23, v_num=1, train_loss_step=0.765, val_loss=2.900, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 190: 100%|███████| 67/67 [00:21<00:00,  3.13it/s, loss=2.23, v_num=1, train_loss_step=0.765, val_loss=2.900, train_loss_epoch=2.440]\u001b[A\n",
      "Epoch 191:  45%|███▏   | 30/67 [00:05<00:07,  5.04it/s, loss=2.98, v_num=1, train_loss_step=1.930, val_loss=2.900, train_loss_epoch=2.310]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 191:  46%|███▏   | 31/67 [00:08<00:09,  3.78it/s, loss=2.98, v_num=1, train_loss_step=1.930, val_loss=2.900, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 191:  48%|███▎   | 32/67 [00:08<00:09,  3.75it/s, loss=2.98, v_num=1, train_loss_step=1.930, val_loss=2.900, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 191:  49%|███▍   | 33/67 [00:08<00:09,  3.73it/s, loss=2.98, v_num=1, train_loss_step=1.930, val_loss=2.900, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 191:  51%|███▌   | 34/67 [00:09<00:08,  3.70it/s, loss=2.98, v_num=1, train_loss_step=1.930, val_loss=2.900, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 191:  52%|███▋   | 35/67 [00:09<00:08,  3.68it/s, loss=2.98, v_num=1, train_loss_step=1.930, val_loss=2.900, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 191:  54%|███▊   | 36/67 [00:09<00:08,  3.65it/s, loss=2.98, v_num=1, train_loss_step=1.930, val_loss=2.900, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 191:  55%|███▊   | 37/67 [00:10<00:08,  3.63it/s, loss=2.98, v_num=1, train_loss_step=1.930, val_loss=2.900, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 191:  57%|███▉   | 38/67 [00:10<00:08,  3.61it/s, loss=2.98, v_num=1, train_loss_step=1.930, val_loss=2.900, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 191:  58%|████   | 39/67 [00:10<00:07,  3.58it/s, loss=2.98, v_num=1, train_loss_step=1.930, val_loss=2.900, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 191:  60%|████▏  | 40/67 [00:11<00:07,  3.56it/s, loss=2.98, v_num=1, train_loss_step=1.930, val_loss=2.900, train_loss_epoch=2.310]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 191:  61%|████▎  | 41/67 [00:11<00:07,  3.49it/s, loss=2.98, v_num=1, train_loss_step=1.930, val_loss=2.900, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 191:  63%|████▍  | 42/67 [00:12<00:07,  3.48it/s, loss=2.98, v_num=1, train_loss_step=1.930, val_loss=2.900, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 191:  64%|████▍  | 43/67 [00:12<00:06,  3.46it/s, loss=2.98, v_num=1, train_loss_step=1.930, val_loss=2.900, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 191:  66%|████▌  | 44/67 [00:12<00:06,  3.45it/s, loss=2.98, v_num=1, train_loss_step=1.930, val_loss=2.900, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 191:  67%|████▋  | 45/67 [00:13<00:06,  3.44it/s, loss=2.98, v_num=1, train_loss_step=1.930, val_loss=2.900, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 191:  69%|████▊  | 46/67 [00:13<00:06,  3.42it/s, loss=2.98, v_num=1, train_loss_step=1.930, val_loss=2.900, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 191:  70%|████▉  | 47/67 [00:13<00:05,  3.42it/s, loss=2.98, v_num=1, train_loss_step=1.930, val_loss=2.900, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 191:  72%|█████  | 48/67 [00:14<00:05,  3.41it/s, loss=2.98, v_num=1, train_loss_step=1.930, val_loss=2.900, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 191:  73%|█████  | 49/67 [00:14<00:05,  3.39it/s, loss=2.98, v_num=1, train_loss_step=1.930, val_loss=2.900, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 191:  75%|█████▏ | 50/67 [00:14<00:05,  3.39it/s, loss=2.98, v_num=1, train_loss_step=1.930, val_loss=2.900, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 191:  76%|█████▎ | 51/67 [00:15<00:04,  3.34it/s, loss=2.98, v_num=1, train_loss_step=1.930, val_loss=2.900, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 191:  78%|█████▍ | 52/67 [00:15<00:04,  3.33it/s, loss=2.98, v_num=1, train_loss_step=1.930, val_loss=2.900, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 191:  79%|█████▌ | 53/67 [00:15<00:04,  3.32it/s, loss=2.98, v_num=1, train_loss_step=1.930, val_loss=2.900, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 191:  81%|█████▋ | 54/67 [00:16<00:03,  3.32it/s, loss=2.98, v_num=1, train_loss_step=1.930, val_loss=2.900, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 191:  82%|█████▋ | 55/67 [00:16<00:03,  3.31it/s, loss=2.98, v_num=1, train_loss_step=1.930, val_loss=2.900, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 191:  84%|█████▊ | 56/67 [00:16<00:03,  3.31it/s, loss=2.98, v_num=1, train_loss_step=1.930, val_loss=2.900, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 191:  85%|█████▉ | 57/67 [00:17<00:03,  3.30it/s, loss=2.98, v_num=1, train_loss_step=1.930, val_loss=2.900, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 191:  87%|██████ | 58/67 [00:17<00:02,  3.29it/s, loss=2.98, v_num=1, train_loss_step=1.930, val_loss=2.900, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 191:  88%|██████▏| 59/67 [00:17<00:02,  3.29it/s, loss=2.98, v_num=1, train_loss_step=1.930, val_loss=2.900, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 191:  90%|██████▎| 60/67 [00:18<00:02,  3.28it/s, loss=2.98, v_num=1, train_loss_step=1.930, val_loss=2.900, train_loss_epoch=2.310]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 191:  91%|██████▎| 61/67 [00:18<00:01,  3.24it/s, loss=2.98, v_num=1, train_loss_step=1.930, val_loss=2.900, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 191:  93%|██████▍| 62/67 [00:19<00:01,  3.23it/s, loss=2.98, v_num=1, train_loss_step=1.930, val_loss=2.900, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 191:  94%|██████▌| 63/67 [00:19<00:01,  3.23it/s, loss=2.98, v_num=1, train_loss_step=1.930, val_loss=2.900, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 191:  96%|██████▋| 64/67 [00:19<00:00,  3.22it/s, loss=2.98, v_num=1, train_loss_step=1.930, val_loss=2.900, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 191:  97%|██████▊| 65/67 [00:20<00:00,  3.22it/s, loss=2.98, v_num=1, train_loss_step=1.930, val_loss=2.900, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 191:  99%|██████▉| 66/67 [00:20<00:00,  3.21it/s, loss=2.98, v_num=1, train_loss_step=1.930, val_loss=2.900, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 191: 100%|███████| 67/67 [00:21<00:00,  3.14it/s, loss=2.98, v_num=1, train_loss_step=1.930, val_loss=2.900, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 192:  45%|███▏   | 30/67 [00:05<00:07,  5.10it/s, loss=2.02, v_num=1, train_loss_step=1.390, val_loss=2.900, train_loss_epoch=2.580]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 192:  46%|███▏   | 31/67 [00:08<00:09,  3.80it/s, loss=2.02, v_num=1, train_loss_step=1.390, val_loss=2.900, train_loss_epoch=2.580]\u001b[A\n",
      "Epoch 192:  48%|███▎   | 32/67 [00:08<00:09,  3.77it/s, loss=2.02, v_num=1, train_loss_step=1.390, val_loss=2.900, train_loss_epoch=2.580]\u001b[A\n",
      "Epoch 192:  49%|███▍   | 33/67 [00:08<00:09,  3.74it/s, loss=2.02, v_num=1, train_loss_step=1.390, val_loss=2.900, train_loss_epoch=2.580]\u001b[A\n",
      "Epoch 192:  51%|███▌   | 34/67 [00:09<00:08,  3.71it/s, loss=2.02, v_num=1, train_loss_step=1.390, val_loss=2.900, train_loss_epoch=2.580]\u001b[A\n",
      "Epoch 192:  52%|███▋   | 35/67 [00:09<00:08,  3.69it/s, loss=2.02, v_num=1, train_loss_step=1.390, val_loss=2.900, train_loss_epoch=2.580]\u001b[A\n",
      "Epoch 192:  54%|███▊   | 36/67 [00:09<00:08,  3.66it/s, loss=2.02, v_num=1, train_loss_step=1.390, val_loss=2.900, train_loss_epoch=2.580]\u001b[A\n",
      "Epoch 192:  55%|███▊   | 37/67 [00:10<00:08,  3.62it/s, loss=2.02, v_num=1, train_loss_step=1.390, val_loss=2.900, train_loss_epoch=2.580]\u001b[A\n",
      "Epoch 192:  57%|███▉   | 38/67 [00:10<00:08,  3.61it/s, loss=2.02, v_num=1, train_loss_step=1.390, val_loss=2.900, train_loss_epoch=2.580]\u001b[A\n",
      "Epoch 192:  58%|████   | 39/67 [00:10<00:07,  3.59it/s, loss=2.02, v_num=1, train_loss_step=1.390, val_loss=2.900, train_loss_epoch=2.580]\u001b[A\n",
      "Epoch 192:  60%|████▏  | 40/67 [00:11<00:07,  3.56it/s, loss=2.02, v_num=1, train_loss_step=1.390, val_loss=2.900, train_loss_epoch=2.580]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 192:  61%|████▎  | 41/67 [00:11<00:07,  3.50it/s, loss=2.02, v_num=1, train_loss_step=1.390, val_loss=2.900, train_loss_epoch=2.580]\u001b[A\n",
      "Epoch 192:  63%|████▍  | 42/67 [00:12<00:07,  3.48it/s, loss=2.02, v_num=1, train_loss_step=1.390, val_loss=2.900, train_loss_epoch=2.580]\u001b[A\n",
      "Epoch 192:  64%|████▍  | 43/67 [00:12<00:06,  3.46it/s, loss=2.02, v_num=1, train_loss_step=1.390, val_loss=2.900, train_loss_epoch=2.580]\u001b[A\n",
      "Epoch 192:  66%|████▌  | 44/67 [00:12<00:06,  3.44it/s, loss=2.02, v_num=1, train_loss_step=1.390, val_loss=2.900, train_loss_epoch=2.580]\u001b[A\n",
      "Epoch 192:  67%|████▋  | 45/67 [00:13<00:06,  3.43it/s, loss=2.02, v_num=1, train_loss_step=1.390, val_loss=2.900, train_loss_epoch=2.580]\u001b[A\n",
      "Epoch 192:  69%|████▊  | 46/67 [00:13<00:06,  3.41it/s, loss=2.02, v_num=1, train_loss_step=1.390, val_loss=2.900, train_loss_epoch=2.580]\u001b[A\n",
      "Epoch 192:  70%|████▉  | 47/67 [00:13<00:05,  3.40it/s, loss=2.02, v_num=1, train_loss_step=1.390, val_loss=2.900, train_loss_epoch=2.580]\u001b[A\n",
      "Epoch 192:  72%|█████  | 48/67 [00:14<00:05,  3.39it/s, loss=2.02, v_num=1, train_loss_step=1.390, val_loss=2.900, train_loss_epoch=2.580]\u001b[A\n",
      "Epoch 192:  73%|█████  | 49/67 [00:14<00:05,  3.36it/s, loss=2.02, v_num=1, train_loss_step=1.390, val_loss=2.900, train_loss_epoch=2.580]\u001b[A\n",
      "Epoch 192:  75%|█████▏ | 50/67 [00:14<00:05,  3.35it/s, loss=2.02, v_num=1, train_loss_step=1.390, val_loss=2.900, train_loss_epoch=2.580]\u001b[A\n",
      "Epoch 192:  76%|█████▎ | 51/67 [00:15<00:04,  3.31it/s, loss=2.02, v_num=1, train_loss_step=1.390, val_loss=2.900, train_loss_epoch=2.580]\u001b[A\n",
      "Epoch 192:  78%|█████▍ | 52/67 [00:15<00:04,  3.30it/s, loss=2.02, v_num=1, train_loss_step=1.390, val_loss=2.900, train_loss_epoch=2.580]\u001b[A\n",
      "Epoch 192:  79%|█████▌ | 53/67 [00:16<00:04,  3.29it/s, loss=2.02, v_num=1, train_loss_step=1.390, val_loss=2.900, train_loss_epoch=2.580]\u001b[A\n",
      "Epoch 192:  81%|█████▋ | 54/67 [00:16<00:03,  3.28it/s, loss=2.02, v_num=1, train_loss_step=1.390, val_loss=2.900, train_loss_epoch=2.580]\u001b[A\n",
      "Epoch 192:  82%|█████▋ | 55/67 [00:16<00:03,  3.27it/s, loss=2.02, v_num=1, train_loss_step=1.390, val_loss=2.900, train_loss_epoch=2.580]\u001b[A\n",
      "Epoch 192:  84%|█████▊ | 56/67 [00:17<00:03,  3.26it/s, loss=2.02, v_num=1, train_loss_step=1.390, val_loss=2.900, train_loss_epoch=2.580]\u001b[A\n",
      "Epoch 192:  85%|█████▉ | 57/67 [00:17<00:03,  3.25it/s, loss=2.02, v_num=1, train_loss_step=1.390, val_loss=2.900, train_loss_epoch=2.580]\u001b[A\n",
      "Epoch 192:  87%|██████ | 58/67 [00:18<00:02,  3.19it/s, loss=2.02, v_num=1, train_loss_step=1.390, val_loss=2.900, train_loss_epoch=2.580]\u001b[A\n",
      "Epoch 192:  88%|██████▏| 59/67 [00:18<00:02,  3.19it/s, loss=2.02, v_num=1, train_loss_step=1.390, val_loss=2.900, train_loss_epoch=2.580]\u001b[A\n",
      "Epoch 192:  90%|██████▎| 60/67 [00:18<00:02,  3.18it/s, loss=2.02, v_num=1, train_loss_step=1.390, val_loss=2.900, train_loss_epoch=2.580]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 192:  91%|██████▎| 61/67 [00:19<00:01,  3.11it/s, loss=2.02, v_num=1, train_loss_step=1.390, val_loss=2.900, train_loss_epoch=2.580]\u001b[A\n",
      "Epoch 192:  93%|██████▍| 62/67 [00:19<00:01,  3.11it/s, loss=2.02, v_num=1, train_loss_step=1.390, val_loss=2.900, train_loss_epoch=2.580]\u001b[A\n",
      "Epoch 192:  94%|██████▌| 63/67 [00:20<00:01,  3.11it/s, loss=2.02, v_num=1, train_loss_step=1.390, val_loss=2.900, train_loss_epoch=2.580]\u001b[A\n",
      "Epoch 192:  96%|██████▋| 64/67 [00:20<00:00,  3.10it/s, loss=2.02, v_num=1, train_loss_step=1.390, val_loss=2.900, train_loss_epoch=2.580]\u001b[A\n",
      "Epoch 192:  97%|██████▊| 65/67 [00:20<00:00,  3.10it/s, loss=2.02, v_num=1, train_loss_step=1.390, val_loss=2.900, train_loss_epoch=2.580]\u001b[A\n",
      "Epoch 192:  99%|██████▉| 66/67 [00:21<00:00,  3.10it/s, loss=2.02, v_num=1, train_loss_step=1.390, val_loss=2.900, train_loss_epoch=2.580]\u001b[A\n",
      "Epoch 192: 100%|███████| 67/67 [00:22<00:00,  3.03it/s, loss=2.02, v_num=1, train_loss_step=1.390, val_loss=2.890, train_loss_epoch=2.580]\u001b[A\n",
      "Epoch 193:  45%|███▏   | 30/67 [00:06<00:07,  4.82it/s, loss=2.18, v_num=1, train_loss_step=0.835, val_loss=2.890, train_loss_epoch=1.890]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 193:  46%|███▏   | 31/67 [00:08<00:09,  3.70it/s, loss=2.18, v_num=1, train_loss_step=0.835, val_loss=2.890, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 193:  48%|███▎   | 32/67 [00:08<00:09,  3.64it/s, loss=2.18, v_num=1, train_loss_step=0.835, val_loss=2.890, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 193:  49%|███▍   | 33/67 [00:09<00:09,  3.61it/s, loss=2.18, v_num=1, train_loss_step=0.835, val_loss=2.890, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 193:  51%|███▌   | 34/67 [00:09<00:09,  3.59it/s, loss=2.18, v_num=1, train_loss_step=0.835, val_loss=2.890, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 193:  52%|███▋   | 35/67 [00:09<00:08,  3.56it/s, loss=2.18, v_num=1, train_loss_step=0.835, val_loss=2.890, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 193:  54%|███▊   | 36/67 [00:10<00:08,  3.54it/s, loss=2.18, v_num=1, train_loss_step=0.835, val_loss=2.890, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 193:  55%|███▊   | 37/67 [00:10<00:08,  3.52it/s, loss=2.18, v_num=1, train_loss_step=0.835, val_loss=2.890, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 193:  57%|███▉   | 38/67 [00:10<00:08,  3.49it/s, loss=2.18, v_num=1, train_loss_step=0.835, val_loss=2.890, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 193:  58%|████   | 39/67 [00:11<00:08,  3.47it/s, loss=2.18, v_num=1, train_loss_step=0.835, val_loss=2.890, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 193:  60%|████▏  | 40/67 [00:11<00:07,  3.44it/s, loss=2.18, v_num=1, train_loss_step=0.835, val_loss=2.890, train_loss_epoch=1.890]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 193:  61%|████▎  | 41/67 [00:12<00:07,  3.38it/s, loss=2.18, v_num=1, train_loss_step=0.835, val_loss=2.890, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 193:  63%|████▍  | 42/67 [00:12<00:07,  3.36it/s, loss=2.18, v_num=1, train_loss_step=0.835, val_loss=2.890, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 193:  64%|████▍  | 43/67 [00:12<00:07,  3.34it/s, loss=2.18, v_num=1, train_loss_step=0.835, val_loss=2.890, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 193:  66%|████▌  | 44/67 [00:13<00:06,  3.32it/s, loss=2.18, v_num=1, train_loss_step=0.835, val_loss=2.890, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 193:  67%|████▋  | 45/67 [00:13<00:06,  3.31it/s, loss=2.18, v_num=1, train_loss_step=0.835, val_loss=2.890, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 193:  69%|████▊  | 46/67 [00:13<00:06,  3.30it/s, loss=2.18, v_num=1, train_loss_step=0.835, val_loss=2.890, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 193:  70%|████▉  | 47/67 [00:14<00:06,  3.28it/s, loss=2.18, v_num=1, train_loss_step=0.835, val_loss=2.890, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 193:  72%|█████  | 48/67 [00:14<00:05,  3.27it/s, loss=2.18, v_num=1, train_loss_step=0.835, val_loss=2.890, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 193:  73%|█████  | 49/67 [00:15<00:05,  3.26it/s, loss=2.18, v_num=1, train_loss_step=0.835, val_loss=2.890, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 193:  75%|█████▏ | 50/67 [00:15<00:05,  3.26it/s, loss=2.18, v_num=1, train_loss_step=0.835, val_loss=2.890, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 193:  76%|█████▎ | 51/67 [00:15<00:04,  3.21it/s, loss=2.18, v_num=1, train_loss_step=0.835, val_loss=2.890, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 193:  78%|█████▍ | 52/67 [00:16<00:04,  3.20it/s, loss=2.18, v_num=1, train_loss_step=0.835, val_loss=2.890, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 193:  79%|█████▌ | 53/67 [00:16<00:04,  3.18it/s, loss=2.18, v_num=1, train_loss_step=0.835, val_loss=2.890, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 193:  81%|█████▋ | 54/67 [00:17<00:04,  3.17it/s, loss=2.18, v_num=1, train_loss_step=0.835, val_loss=2.890, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 193:  82%|█████▋ | 55/67 [00:17<00:03,  3.17it/s, loss=2.18, v_num=1, train_loss_step=0.835, val_loss=2.890, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 193:  84%|█████▊ | 56/67 [00:17<00:03,  3.16it/s, loss=2.18, v_num=1, train_loss_step=0.835, val_loss=2.890, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 193:  85%|█████▉ | 57/67 [00:18<00:03,  3.15it/s, loss=2.18, v_num=1, train_loss_step=0.835, val_loss=2.890, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 193:  87%|██████ | 58/67 [00:18<00:02,  3.15it/s, loss=2.18, v_num=1, train_loss_step=0.835, val_loss=2.890, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 193:  88%|██████▏| 59/67 [00:18<00:02,  3.14it/s, loss=2.18, v_num=1, train_loss_step=0.835, val_loss=2.890, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 193:  90%|██████▎| 60/67 [00:19<00:02,  3.14it/s, loss=2.18, v_num=1, train_loss_step=0.835, val_loss=2.890, train_loss_epoch=1.890]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 193:  91%|██████▎| 61/67 [00:19<00:01,  3.11it/s, loss=2.18, v_num=1, train_loss_step=0.835, val_loss=2.890, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 193:  93%|██████▍| 62/67 [00:19<00:01,  3.11it/s, loss=2.18, v_num=1, train_loss_step=0.835, val_loss=2.890, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 193:  94%|██████▌| 63/67 [00:20<00:01,  3.11it/s, loss=2.18, v_num=1, train_loss_step=0.835, val_loss=2.890, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 193:  96%|██████▋| 64/67 [00:20<00:00,  3.11it/s, loss=2.18, v_num=1, train_loss_step=0.835, val_loss=2.890, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 193:  97%|██████▊| 65/67 [00:20<00:00,  3.10it/s, loss=2.18, v_num=1, train_loss_step=0.835, val_loss=2.890, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 193:  99%|██████▉| 66/67 [00:21<00:00,  3.10it/s, loss=2.18, v_num=1, train_loss_step=0.835, val_loss=2.890, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 193: 100%|███████| 67/67 [00:22<00:00,  3.04it/s, loss=2.18, v_num=1, train_loss_step=0.835, val_loss=2.890, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 194:  45%|███▏   | 30/67 [00:06<00:07,  4.92it/s, loss=1.75, v_num=1, train_loss_step=1.880, val_loss=2.890, train_loss_epoch=1.940]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 194:  46%|███▏   | 31/67 [00:08<00:09,  3.81it/s, loss=1.75, v_num=1, train_loss_step=1.880, val_loss=2.890, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 194:  48%|███▎   | 32/67 [00:08<00:09,  3.78it/s, loss=1.75, v_num=1, train_loss_step=1.880, val_loss=2.890, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 194:  49%|███▍   | 33/67 [00:08<00:09,  3.75it/s, loss=1.75, v_num=1, train_loss_step=1.880, val_loss=2.890, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 194:  51%|███▌   | 34/67 [00:09<00:08,  3.71it/s, loss=1.75, v_num=1, train_loss_step=1.880, val_loss=2.890, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 194:  52%|███▋   | 35/67 [00:09<00:08,  3.67it/s, loss=1.75, v_num=1, train_loss_step=1.880, val_loss=2.890, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 194:  54%|███▊   | 36/67 [00:09<00:08,  3.64it/s, loss=1.75, v_num=1, train_loss_step=1.880, val_loss=2.890, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 194:  55%|███▊   | 37/67 [00:10<00:08,  3.61it/s, loss=1.75, v_num=1, train_loss_step=1.880, val_loss=2.890, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 194:  57%|███▉   | 38/67 [00:10<00:08,  3.58it/s, loss=1.75, v_num=1, train_loss_step=1.880, val_loss=2.890, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 194:  58%|████   | 39/67 [00:10<00:07,  3.56it/s, loss=1.75, v_num=1, train_loss_step=1.880, val_loss=2.890, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 194:  60%|████▏  | 40/67 [00:11<00:07,  3.54it/s, loss=1.75, v_num=1, train_loss_step=1.880, val_loss=2.890, train_loss_epoch=1.940]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 194:  61%|████▎  | 41/67 [00:11<00:07,  3.47it/s, loss=1.75, v_num=1, train_loss_step=1.880, val_loss=2.890, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 194:  63%|████▍  | 42/67 [00:12<00:07,  3.45it/s, loss=1.75, v_num=1, train_loss_step=1.880, val_loss=2.890, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 194:  64%|████▍  | 43/67 [00:12<00:06,  3.44it/s, loss=1.75, v_num=1, train_loss_step=1.880, val_loss=2.890, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 194:  66%|████▌  | 44/67 [00:12<00:06,  3.42it/s, loss=1.75, v_num=1, train_loss_step=1.880, val_loss=2.890, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 194:  67%|████▋  | 45/67 [00:13<00:06,  3.41it/s, loss=1.75, v_num=1, train_loss_step=1.880, val_loss=2.890, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 194:  69%|████▊  | 46/67 [00:13<00:06,  3.40it/s, loss=1.75, v_num=1, train_loss_step=1.880, val_loss=2.890, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 194:  70%|████▉  | 47/67 [00:13<00:05,  3.38it/s, loss=1.75, v_num=1, train_loss_step=1.880, val_loss=2.890, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 194:  72%|█████  | 48/67 [00:14<00:05,  3.37it/s, loss=1.75, v_num=1, train_loss_step=1.880, val_loss=2.890, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 194:  73%|█████  | 49/67 [00:14<00:05,  3.36it/s, loss=1.75, v_num=1, train_loss_step=1.880, val_loss=2.890, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 194:  75%|█████▏ | 50/67 [00:14<00:05,  3.35it/s, loss=1.75, v_num=1, train_loss_step=1.880, val_loss=2.890, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 194:  76%|█████▎ | 51/67 [00:15<00:04,  3.31it/s, loss=1.75, v_num=1, train_loss_step=1.880, val_loss=2.890, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 194:  78%|█████▍ | 52/67 [00:15<00:04,  3.30it/s, loss=1.75, v_num=1, train_loss_step=1.880, val_loss=2.890, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 194:  79%|█████▌ | 53/67 [00:16<00:04,  3.29it/s, loss=1.75, v_num=1, train_loss_step=1.880, val_loss=2.890, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 194:  81%|█████▋ | 54/67 [00:16<00:03,  3.29it/s, loss=1.75, v_num=1, train_loss_step=1.880, val_loss=2.890, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 194:  82%|█████▋ | 55/67 [00:16<00:03,  3.28it/s, loss=1.75, v_num=1, train_loss_step=1.880, val_loss=2.890, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 194:  84%|█████▊ | 56/67 [00:17<00:03,  3.28it/s, loss=1.75, v_num=1, train_loss_step=1.880, val_loss=2.890, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 194:  85%|█████▉ | 57/67 [00:17<00:03,  3.27it/s, loss=1.75, v_num=1, train_loss_step=1.880, val_loss=2.890, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 194:  87%|██████ | 58/67 [00:17<00:02,  3.26it/s, loss=1.75, v_num=1, train_loss_step=1.880, val_loss=2.890, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 194:  88%|██████▏| 59/67 [00:18<00:02,  3.26it/s, loss=1.75, v_num=1, train_loss_step=1.880, val_loss=2.890, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 194:  90%|██████▎| 60/67 [00:18<00:02,  3.25it/s, loss=1.75, v_num=1, train_loss_step=1.880, val_loss=2.890, train_loss_epoch=1.940]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 194:  91%|██████▎| 61/67 [00:19<00:01,  3.21it/s, loss=1.75, v_num=1, train_loss_step=1.880, val_loss=2.890, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 194:  93%|██████▍| 62/67 [00:19<00:01,  3.20it/s, loss=1.75, v_num=1, train_loss_step=1.880, val_loss=2.890, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 194:  94%|██████▌| 63/67 [00:19<00:01,  3.19it/s, loss=1.75, v_num=1, train_loss_step=1.880, val_loss=2.890, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 194:  96%|██████▋| 64/67 [00:20<00:00,  3.19it/s, loss=1.75, v_num=1, train_loss_step=1.880, val_loss=2.890, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 194:  97%|██████▊| 65/67 [00:20<00:00,  3.18it/s, loss=1.75, v_num=1, train_loss_step=1.880, val_loss=2.890, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 194:  99%|██████▉| 66/67 [00:20<00:00,  3.18it/s, loss=1.75, v_num=1, train_loss_step=1.880, val_loss=2.890, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 194: 100%|███████| 67/67 [00:21<00:00,  3.11it/s, loss=1.75, v_num=1, train_loss_step=1.880, val_loss=2.890, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 195:  45%|███▏   | 30/67 [00:06<00:07,  4.91it/s, loss=1.73, v_num=1, train_loss_step=2.040, val_loss=2.890, train_loss_epoch=1.830]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 195:  46%|███▏   | 31/67 [00:08<00:09,  3.74it/s, loss=1.73, v_num=1, train_loss_step=2.040, val_loss=2.890, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 195:  48%|███▎   | 32/67 [00:08<00:09,  3.71it/s, loss=1.73, v_num=1, train_loss_step=2.040, val_loss=2.890, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 195:  49%|███▍   | 33/67 [00:08<00:09,  3.68it/s, loss=1.73, v_num=1, train_loss_step=2.040, val_loss=2.890, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 195:  51%|███▌   | 34/67 [00:09<00:09,  3.64it/s, loss=1.73, v_num=1, train_loss_step=2.040, val_loss=2.890, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 195:  52%|███▋   | 35/67 [00:09<00:08,  3.61it/s, loss=1.73, v_num=1, train_loss_step=2.040, val_loss=2.890, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 195:  54%|███▊   | 36/67 [00:10<00:08,  3.60it/s, loss=1.73, v_num=1, train_loss_step=2.040, val_loss=2.890, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 195:  55%|███▊   | 37/67 [00:10<00:08,  3.57it/s, loss=1.73, v_num=1, train_loss_step=2.040, val_loss=2.890, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 195:  57%|███▉   | 38/67 [00:10<00:08,  3.55it/s, loss=1.73, v_num=1, train_loss_step=2.040, val_loss=2.890, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 195:  58%|████   | 39/67 [00:11<00:07,  3.53it/s, loss=1.73, v_num=1, train_loss_step=2.040, val_loss=2.890, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 195:  60%|████▏  | 40/67 [00:11<00:07,  3.51it/s, loss=1.73, v_num=1, train_loss_step=2.040, val_loss=2.890, train_loss_epoch=1.830]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 195:  61%|████▎  | 41/67 [00:11<00:07,  3.44it/s, loss=1.73, v_num=1, train_loss_step=2.040, val_loss=2.890, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 195:  63%|████▍  | 42/67 [00:12<00:07,  3.43it/s, loss=1.73, v_num=1, train_loss_step=2.040, val_loss=2.890, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 195:  64%|████▍  | 43/67 [00:12<00:07,  3.41it/s, loss=1.73, v_num=1, train_loss_step=2.040, val_loss=2.890, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 195:  66%|████▌  | 44/67 [00:12<00:06,  3.40it/s, loss=1.73, v_num=1, train_loss_step=2.040, val_loss=2.890, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 195:  67%|████▋  | 45/67 [00:13<00:06,  3.39it/s, loss=1.73, v_num=1, train_loss_step=2.040, val_loss=2.890, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 195:  69%|████▊  | 46/67 [00:13<00:06,  3.38it/s, loss=1.73, v_num=1, train_loss_step=2.040, val_loss=2.890, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 195:  70%|████▉  | 47/67 [00:13<00:05,  3.36it/s, loss=1.73, v_num=1, train_loss_step=2.040, val_loss=2.890, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 195:  72%|█████  | 48/67 [00:14<00:05,  3.35it/s, loss=1.73, v_num=1, train_loss_step=2.040, val_loss=2.890, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 195:  73%|█████  | 49/67 [00:14<00:05,  3.34it/s, loss=1.73, v_num=1, train_loss_step=2.040, val_loss=2.890, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 195:  75%|█████▏ | 50/67 [00:15<00:05,  3.33it/s, loss=1.73, v_num=1, train_loss_step=2.040, val_loss=2.890, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 195:  76%|█████▎ | 51/67 [00:15<00:04,  3.27it/s, loss=1.73, v_num=1, train_loss_step=2.040, val_loss=2.890, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 195:  78%|█████▍ | 52/67 [00:15<00:04,  3.26it/s, loss=1.73, v_num=1, train_loss_step=2.040, val_loss=2.890, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 195:  79%|█████▌ | 53/67 [00:16<00:04,  3.25it/s, loss=1.73, v_num=1, train_loss_step=2.040, val_loss=2.890, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 195:  81%|█████▋ | 54/67 [00:16<00:04,  3.24it/s, loss=1.73, v_num=1, train_loss_step=2.040, val_loss=2.890, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 195:  82%|█████▋ | 55/67 [00:17<00:03,  3.23it/s, loss=1.73, v_num=1, train_loss_step=2.040, val_loss=2.890, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 195:  84%|█████▊ | 56/67 [00:17<00:03,  3.22it/s, loss=1.73, v_num=1, train_loss_step=2.040, val_loss=2.890, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 195:  85%|█████▉ | 57/67 [00:17<00:03,  3.21it/s, loss=1.73, v_num=1, train_loss_step=2.040, val_loss=2.890, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 195:  87%|██████ | 58/67 [00:18<00:02,  3.21it/s, loss=1.73, v_num=1, train_loss_step=2.040, val_loss=2.890, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 195:  88%|██████▏| 59/67 [00:18<00:02,  3.20it/s, loss=1.73, v_num=1, train_loss_step=2.040, val_loss=2.890, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 195:  90%|██████▎| 60/67 [00:18<00:02,  3.20it/s, loss=1.73, v_num=1, train_loss_step=2.040, val_loss=2.890, train_loss_epoch=1.830]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 195:  91%|██████▎| 61/67 [00:19<00:01,  3.16it/s, loss=1.73, v_num=1, train_loss_step=2.040, val_loss=2.890, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 195:  93%|██████▍| 62/67 [00:19<00:01,  3.15it/s, loss=1.73, v_num=1, train_loss_step=2.040, val_loss=2.890, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 195:  94%|██████▌| 63/67 [00:20<00:01,  3.15it/s, loss=1.73, v_num=1, train_loss_step=2.040, val_loss=2.890, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 195:  96%|██████▋| 64/67 [00:20<00:00,  3.14it/s, loss=1.73, v_num=1, train_loss_step=2.040, val_loss=2.890, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 195:  97%|██████▊| 65/67 [00:20<00:00,  3.14it/s, loss=1.73, v_num=1, train_loss_step=2.040, val_loss=2.890, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 195:  99%|██████▉| 66/67 [00:21<00:00,  3.14it/s, loss=1.73, v_num=1, train_loss_step=2.040, val_loss=2.890, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 195: 100%|███████| 67/67 [00:21<00:00,  3.07it/s, loss=1.73, v_num=1, train_loss_step=2.040, val_loss=2.890, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 196:  45%|███▏   | 30/67 [00:06<00:07,  4.87it/s, loss=2.22, v_num=1, train_loss_step=2.680, val_loss=2.890, train_loss_epoch=1.670]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 196:  46%|███▏   | 31/67 [00:08<00:09,  3.67it/s, loss=2.22, v_num=1, train_loss_step=2.680, val_loss=2.890, train_loss_epoch=1.670]\u001b[A\n",
      "Epoch 196:  48%|███▎   | 32/67 [00:08<00:09,  3.65it/s, loss=2.22, v_num=1, train_loss_step=2.680, val_loss=2.890, train_loss_epoch=1.670]\u001b[A\n",
      "Epoch 196:  49%|███▍   | 33/67 [00:09<00:09,  3.63it/s, loss=2.22, v_num=1, train_loss_step=2.680, val_loss=2.890, train_loss_epoch=1.670]\u001b[A\n",
      "Epoch 196:  51%|███▌   | 34/67 [00:09<00:09,  3.61it/s, loss=2.22, v_num=1, train_loss_step=2.680, val_loss=2.890, train_loss_epoch=1.670]\u001b[A\n",
      "Epoch 196:  52%|███▋   | 35/67 [00:09<00:08,  3.58it/s, loss=2.22, v_num=1, train_loss_step=2.680, val_loss=2.890, train_loss_epoch=1.670]\u001b[A\n",
      "Epoch 196:  54%|███▊   | 36/67 [00:10<00:08,  3.55it/s, loss=2.22, v_num=1, train_loss_step=2.680, val_loss=2.890, train_loss_epoch=1.670]\u001b[A\n",
      "Epoch 196:  55%|███▊   | 37/67 [00:10<00:08,  3.52it/s, loss=2.22, v_num=1, train_loss_step=2.680, val_loss=2.890, train_loss_epoch=1.670]\u001b[A\n",
      "Epoch 196:  57%|███▉   | 38/67 [00:10<00:08,  3.50it/s, loss=2.22, v_num=1, train_loss_step=2.680, val_loss=2.890, train_loss_epoch=1.670]\u001b[A\n",
      "Epoch 196:  58%|████   | 39/67 [00:11<00:08,  3.48it/s, loss=2.22, v_num=1, train_loss_step=2.680, val_loss=2.890, train_loss_epoch=1.670]\u001b[A\n",
      "Epoch 196:  60%|████▏  | 40/67 [00:11<00:07,  3.46it/s, loss=2.22, v_num=1, train_loss_step=2.680, val_loss=2.890, train_loss_epoch=1.670]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 196:  61%|████▎  | 41/67 [00:12<00:07,  3.40it/s, loss=2.22, v_num=1, train_loss_step=2.680, val_loss=2.890, train_loss_epoch=1.670]\u001b[A\n",
      "Epoch 196:  63%|████▍  | 42/67 [00:12<00:07,  3.39it/s, loss=2.22, v_num=1, train_loss_step=2.680, val_loss=2.890, train_loss_epoch=1.670]\u001b[A\n",
      "Epoch 196:  64%|████▍  | 43/67 [00:12<00:07,  3.37it/s, loss=2.22, v_num=1, train_loss_step=2.680, val_loss=2.890, train_loss_epoch=1.670]\u001b[A\n",
      "Epoch 196:  66%|████▌  | 44/67 [00:13<00:06,  3.36it/s, loss=2.22, v_num=1, train_loss_step=2.680, val_loss=2.890, train_loss_epoch=1.670]\u001b[A\n",
      "Epoch 196:  67%|████▋  | 45/67 [00:13<00:06,  3.35it/s, loss=2.22, v_num=1, train_loss_step=2.680, val_loss=2.890, train_loss_epoch=1.670]\u001b[A\n",
      "Epoch 196:  69%|████▊  | 46/67 [00:13<00:06,  3.33it/s, loss=2.22, v_num=1, train_loss_step=2.680, val_loss=2.890, train_loss_epoch=1.670]\u001b[A\n",
      "Epoch 196:  70%|████▉  | 47/67 [00:14<00:06,  3.32it/s, loss=2.22, v_num=1, train_loss_step=2.680, val_loss=2.890, train_loss_epoch=1.670]\u001b[A\n",
      "Epoch 196:  72%|█████  | 48/67 [00:14<00:05,  3.31it/s, loss=2.22, v_num=1, train_loss_step=2.680, val_loss=2.890, train_loss_epoch=1.670]\u001b[A\n",
      "Epoch 196:  73%|█████  | 49/67 [00:14<00:05,  3.30it/s, loss=2.22, v_num=1, train_loss_step=2.680, val_loss=2.890, train_loss_epoch=1.670]\u001b[A\n",
      "Epoch 196:  75%|█████▏ | 50/67 [00:15<00:05,  3.29it/s, loss=2.22, v_num=1, train_loss_step=2.680, val_loss=2.890, train_loss_epoch=1.670]\u001b[A\n",
      "Epoch 196:  76%|█████▎ | 51/67 [00:15<00:04,  3.25it/s, loss=2.22, v_num=1, train_loss_step=2.680, val_loss=2.890, train_loss_epoch=1.670]\u001b[A\n",
      "Epoch 196:  78%|█████▍ | 52/67 [00:16<00:04,  3.24it/s, loss=2.22, v_num=1, train_loss_step=2.680, val_loss=2.890, train_loss_epoch=1.670]\u001b[A\n",
      "Epoch 196:  79%|█████▌ | 53/67 [00:16<00:04,  3.22it/s, loss=2.22, v_num=1, train_loss_step=2.680, val_loss=2.890, train_loss_epoch=1.670]\u001b[A\n",
      "Epoch 196:  81%|█████▋ | 54/67 [00:16<00:04,  3.22it/s, loss=2.22, v_num=1, train_loss_step=2.680, val_loss=2.890, train_loss_epoch=1.670]\u001b[A\n",
      "Epoch 196:  82%|█████▋ | 55/67 [00:17<00:03,  3.21it/s, loss=2.22, v_num=1, train_loss_step=2.680, val_loss=2.890, train_loss_epoch=1.670]\u001b[A\n",
      "Epoch 196:  84%|█████▊ | 56/67 [00:17<00:03,  3.21it/s, loss=2.22, v_num=1, train_loss_step=2.680, val_loss=2.890, train_loss_epoch=1.670]\u001b[A\n",
      "Epoch 196:  85%|█████▉ | 57/67 [00:17<00:03,  3.20it/s, loss=2.22, v_num=1, train_loss_step=2.680, val_loss=2.890, train_loss_epoch=1.670]\u001b[A\n",
      "Epoch 196:  87%|██████ | 58/67 [00:18<00:02,  3.19it/s, loss=2.22, v_num=1, train_loss_step=2.680, val_loss=2.890, train_loss_epoch=1.670]\u001b[A\n",
      "Epoch 196:  88%|██████▏| 59/67 [00:18<00:02,  3.19it/s, loss=2.22, v_num=1, train_loss_step=2.680, val_loss=2.890, train_loss_epoch=1.670]\u001b[A\n",
      "Epoch 196:  90%|██████▎| 60/67 [00:18<00:02,  3.18it/s, loss=2.22, v_num=1, train_loss_step=2.680, val_loss=2.890, train_loss_epoch=1.670]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 196:  91%|██████▎| 61/67 [00:19<00:01,  3.14it/s, loss=2.22, v_num=1, train_loss_step=2.680, val_loss=2.890, train_loss_epoch=1.670]\u001b[A\n",
      "Epoch 196:  93%|██████▍| 62/67 [00:19<00:01,  3.14it/s, loss=2.22, v_num=1, train_loss_step=2.680, val_loss=2.890, train_loss_epoch=1.670]\u001b[A\n",
      "Epoch 196:  94%|██████▌| 63/67 [00:20<00:01,  3.14it/s, loss=2.22, v_num=1, train_loss_step=2.680, val_loss=2.890, train_loss_epoch=1.670]\u001b[A\n",
      "Epoch 196:  96%|██████▋| 64/67 [00:20<00:00,  3.13it/s, loss=2.22, v_num=1, train_loss_step=2.680, val_loss=2.890, train_loss_epoch=1.670]\u001b[A\n",
      "Epoch 196:  97%|██████▊| 65/67 [00:20<00:00,  3.13it/s, loss=2.22, v_num=1, train_loss_step=2.680, val_loss=2.890, train_loss_epoch=1.670]\u001b[A\n",
      "Epoch 196:  99%|██████▉| 66/67 [00:21<00:00,  3.13it/s, loss=2.22, v_num=1, train_loss_step=2.680, val_loss=2.890, train_loss_epoch=1.670]\u001b[A\n",
      "Epoch 196: 100%|███████| 67/67 [00:21<00:00,  3.05it/s, loss=2.22, v_num=1, train_loss_step=2.680, val_loss=2.890, train_loss_epoch=1.670]\u001b[A\n",
      "Epoch 197:  45%|███▏   | 30/67 [00:06<00:07,  4.78it/s, loss=1.74, v_num=1, train_loss_step=1.610, val_loss=2.890, train_loss_epoch=2.220]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 197:  46%|███▏   | 31/67 [00:08<00:09,  3.67it/s, loss=1.74, v_num=1, train_loss_step=1.610, val_loss=2.890, train_loss_epoch=2.220]\u001b[A\n",
      "Epoch 197:  48%|███▎   | 32/67 [00:08<00:09,  3.65it/s, loss=1.74, v_num=1, train_loss_step=1.610, val_loss=2.890, train_loss_epoch=2.220]\u001b[A\n",
      "Epoch 197:  49%|███▍   | 33/67 [00:09<00:09,  3.61it/s, loss=1.74, v_num=1, train_loss_step=1.610, val_loss=2.890, train_loss_epoch=2.220]\u001b[A\n",
      "Epoch 197:  51%|███▌   | 34/67 [00:09<00:09,  3.58it/s, loss=1.74, v_num=1, train_loss_step=1.610, val_loss=2.890, train_loss_epoch=2.220]\u001b[A\n",
      "Epoch 197:  52%|███▋   | 35/67 [00:09<00:08,  3.56it/s, loss=1.74, v_num=1, train_loss_step=1.610, val_loss=2.890, train_loss_epoch=2.220]\u001b[A\n",
      "Epoch 197:  54%|███▊   | 36/67 [00:10<00:08,  3.54it/s, loss=1.74, v_num=1, train_loss_step=1.610, val_loss=2.890, train_loss_epoch=2.220]\u001b[A\n",
      "Epoch 197:  55%|███▊   | 37/67 [00:10<00:08,  3.51it/s, loss=1.74, v_num=1, train_loss_step=1.610, val_loss=2.890, train_loss_epoch=2.220]\u001b[A\n",
      "Epoch 197:  57%|███▉   | 38/67 [00:10<00:08,  3.49it/s, loss=1.74, v_num=1, train_loss_step=1.610, val_loss=2.890, train_loss_epoch=2.220]\u001b[A\n",
      "Epoch 197:  58%|████   | 39/67 [00:11<00:08,  3.48it/s, loss=1.74, v_num=1, train_loss_step=1.610, val_loss=2.890, train_loss_epoch=2.220]\u001b[A\n",
      "Epoch 197:  60%|████▏  | 40/67 [00:11<00:07,  3.46it/s, loss=1.74, v_num=1, train_loss_step=1.610, val_loss=2.890, train_loss_epoch=2.220]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 197:  61%|████▎  | 41/67 [00:12<00:07,  3.40it/s, loss=1.74, v_num=1, train_loss_step=1.610, val_loss=2.890, train_loss_epoch=2.220]\u001b[A\n",
      "Epoch 197:  63%|████▍  | 42/67 [00:12<00:07,  3.38it/s, loss=1.74, v_num=1, train_loss_step=1.610, val_loss=2.890, train_loss_epoch=2.220]\u001b[A\n",
      "Epoch 197:  64%|████▍  | 43/67 [00:12<00:07,  3.36it/s, loss=1.74, v_num=1, train_loss_step=1.610, val_loss=2.890, train_loss_epoch=2.220]\u001b[A\n",
      "Epoch 197:  66%|████▌  | 44/67 [00:13<00:06,  3.34it/s, loss=1.74, v_num=1, train_loss_step=1.610, val_loss=2.890, train_loss_epoch=2.220]\u001b[A\n",
      "Epoch 197:  67%|████▋  | 45/67 [00:13<00:06,  3.33it/s, loss=1.74, v_num=1, train_loss_step=1.610, val_loss=2.890, train_loss_epoch=2.220]\u001b[A\n",
      "Epoch 197:  69%|████▊  | 46/67 [00:13<00:06,  3.32it/s, loss=1.74, v_num=1, train_loss_step=1.610, val_loss=2.890, train_loss_epoch=2.220]\u001b[A\n",
      "Epoch 197:  70%|████▉  | 47/67 [00:14<00:06,  3.30it/s, loss=1.74, v_num=1, train_loss_step=1.610, val_loss=2.890, train_loss_epoch=2.220]\u001b[A\n",
      "Epoch 197:  72%|█████  | 48/67 [00:14<00:05,  3.29it/s, loss=1.74, v_num=1, train_loss_step=1.610, val_loss=2.890, train_loss_epoch=2.220]\u001b[A\n",
      "Epoch 197:  73%|█████  | 49/67 [00:14<00:05,  3.27it/s, loss=1.74, v_num=1, train_loss_step=1.610, val_loss=2.890, train_loss_epoch=2.220]\u001b[A\n",
      "Epoch 197:  75%|█████▏ | 50/67 [00:15<00:05,  3.25it/s, loss=1.74, v_num=1, train_loss_step=1.610, val_loss=2.890, train_loss_epoch=2.220]\u001b[A\n",
      "Epoch 197:  76%|█████▎ | 51/67 [00:15<00:05,  3.20it/s, loss=1.74, v_num=1, train_loss_step=1.610, val_loss=2.890, train_loss_epoch=2.220]\u001b[A\n",
      "Epoch 197:  78%|█████▍ | 52/67 [00:16<00:04,  3.19it/s, loss=1.74, v_num=1, train_loss_step=1.610, val_loss=2.890, train_loss_epoch=2.220]\u001b[A\n",
      "Epoch 197:  79%|█████▌ | 53/67 [00:16<00:04,  3.18it/s, loss=1.74, v_num=1, train_loss_step=1.610, val_loss=2.890, train_loss_epoch=2.220]\u001b[A\n",
      "Epoch 197:  81%|█████▋ | 54/67 [00:17<00:04,  3.17it/s, loss=1.74, v_num=1, train_loss_step=1.610, val_loss=2.890, train_loss_epoch=2.220]\u001b[A\n",
      "Epoch 197:  82%|█████▋ | 55/67 [00:17<00:03,  3.16it/s, loss=1.74, v_num=1, train_loss_step=1.610, val_loss=2.890, train_loss_epoch=2.220]\u001b[A\n",
      "Epoch 197:  84%|█████▊ | 56/67 [00:17<00:03,  3.16it/s, loss=1.74, v_num=1, train_loss_step=1.610, val_loss=2.890, train_loss_epoch=2.220]\u001b[A\n",
      "Epoch 197:  85%|█████▉ | 57/67 [00:18<00:03,  3.16it/s, loss=1.74, v_num=1, train_loss_step=1.610, val_loss=2.890, train_loss_epoch=2.220]\u001b[A\n",
      "Epoch 197:  87%|██████ | 58/67 [00:18<00:02,  3.14it/s, loss=1.74, v_num=1, train_loss_step=1.610, val_loss=2.890, train_loss_epoch=2.220]\u001b[A\n",
      "Epoch 197:  88%|██████▏| 59/67 [00:18<00:02,  3.13it/s, loss=1.74, v_num=1, train_loss_step=1.610, val_loss=2.890, train_loss_epoch=2.220]\u001b[A\n",
      "Epoch 197:  90%|██████▎| 60/67 [00:19<00:02,  3.13it/s, loss=1.74, v_num=1, train_loss_step=1.610, val_loss=2.890, train_loss_epoch=2.220]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 197:  91%|██████▎| 61/67 [00:19<00:01,  3.09it/s, loss=1.74, v_num=1, train_loss_step=1.610, val_loss=2.890, train_loss_epoch=2.220]\u001b[A\n",
      "Epoch 197:  93%|██████▍| 62/67 [00:20<00:01,  3.08it/s, loss=1.74, v_num=1, train_loss_step=1.610, val_loss=2.890, train_loss_epoch=2.220]\u001b[A\n",
      "Epoch 197:  94%|██████▌| 63/67 [00:20<00:01,  3.08it/s, loss=1.74, v_num=1, train_loss_step=1.610, val_loss=2.890, train_loss_epoch=2.220]\u001b[A\n",
      "Epoch 197:  96%|██████▋| 64/67 [00:20<00:00,  3.08it/s, loss=1.74, v_num=1, train_loss_step=1.610, val_loss=2.890, train_loss_epoch=2.220]\u001b[A\n",
      "Epoch 197:  97%|██████▊| 65/67 [00:21<00:00,  3.08it/s, loss=1.74, v_num=1, train_loss_step=1.610, val_loss=2.890, train_loss_epoch=2.220]\u001b[A\n",
      "Epoch 197:  99%|██████▉| 66/67 [00:21<00:00,  3.07it/s, loss=1.74, v_num=1, train_loss_step=1.610, val_loss=2.890, train_loss_epoch=2.220]\u001b[A\n",
      "Epoch 197: 100%|███████| 67/67 [00:22<00:00,  3.01it/s, loss=1.74, v_num=1, train_loss_step=1.610, val_loss=2.880, train_loss_epoch=2.220]\u001b[A\n",
      "Epoch 198:  45%|███▌    | 30/67 [00:06<00:07,  4.98it/s, loss=2.1, v_num=1, train_loss_step=0.772, val_loss=2.880, train_loss_epoch=1.770]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 198:  46%|███▋    | 31/67 [00:08<00:09,  3.74it/s, loss=2.1, v_num=1, train_loss_step=0.772, val_loss=2.880, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 198:  48%|███▊    | 32/67 [00:08<00:09,  3.72it/s, loss=2.1, v_num=1, train_loss_step=0.772, val_loss=2.880, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 198:  49%|███▉    | 33/67 [00:08<00:09,  3.69it/s, loss=2.1, v_num=1, train_loss_step=0.772, val_loss=2.880, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 198:  51%|████    | 34/67 [00:09<00:09,  3.66it/s, loss=2.1, v_num=1, train_loss_step=0.772, val_loss=2.880, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 198:  52%|████▏   | 35/67 [00:09<00:08,  3.64it/s, loss=2.1, v_num=1, train_loss_step=0.772, val_loss=2.880, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 198:  54%|████▎   | 36/67 [00:09<00:08,  3.61it/s, loss=2.1, v_num=1, train_loss_step=0.772, val_loss=2.880, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 198:  55%|████▍   | 37/67 [00:10<00:08,  3.59it/s, loss=2.1, v_num=1, train_loss_step=0.772, val_loss=2.880, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 198:  57%|████▌   | 38/67 [00:10<00:08,  3.57it/s, loss=2.1, v_num=1, train_loss_step=0.772, val_loss=2.880, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 198:  58%|████▋   | 39/67 [00:10<00:07,  3.55it/s, loss=2.1, v_num=1, train_loss_step=0.772, val_loss=2.880, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 198:  60%|████▊   | 40/67 [00:11<00:07,  3.53it/s, loss=2.1, v_num=1, train_loss_step=0.772, val_loss=2.880, train_loss_epoch=1.770]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 198:  61%|████▉   | 41/67 [00:11<00:07,  3.46it/s, loss=2.1, v_num=1, train_loss_step=0.772, val_loss=2.880, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 198:  63%|█████   | 42/67 [00:12<00:07,  3.45it/s, loss=2.1, v_num=1, train_loss_step=0.772, val_loss=2.880, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 198:  64%|█████▏  | 43/67 [00:12<00:06,  3.43it/s, loss=2.1, v_num=1, train_loss_step=0.772, val_loss=2.880, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 198:  66%|█████▎  | 44/67 [00:12<00:06,  3.42it/s, loss=2.1, v_num=1, train_loss_step=0.772, val_loss=2.880, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 198:  67%|█████▎  | 45/67 [00:13<00:06,  3.41it/s, loss=2.1, v_num=1, train_loss_step=0.772, val_loss=2.880, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 198:  69%|█████▍  | 46/67 [00:13<00:06,  3.39it/s, loss=2.1, v_num=1, train_loss_step=0.772, val_loss=2.880, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 198:  70%|█████▌  | 47/67 [00:13<00:05,  3.37it/s, loss=2.1, v_num=1, train_loss_step=0.772, val_loss=2.880, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 198:  72%|█████▋  | 48/67 [00:14<00:05,  3.36it/s, loss=2.1, v_num=1, train_loss_step=0.772, val_loss=2.880, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 198:  73%|█████▊  | 49/67 [00:14<00:05,  3.33it/s, loss=2.1, v_num=1, train_loss_step=0.772, val_loss=2.880, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 198:  75%|█████▉  | 50/67 [00:15<00:05,  3.33it/s, loss=2.1, v_num=1, train_loss_step=0.772, val_loss=2.880, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 198:  76%|██████  | 51/67 [00:15<00:04,  3.28it/s, loss=2.1, v_num=1, train_loss_step=0.772, val_loss=2.880, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 198:  78%|██████▏ | 52/67 [00:15<00:04,  3.26it/s, loss=2.1, v_num=1, train_loss_step=0.772, val_loss=2.880, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 198:  79%|██████▎ | 53/67 [00:16<00:04,  3.26it/s, loss=2.1, v_num=1, train_loss_step=0.772, val_loss=2.880, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 198:  81%|██████▍ | 54/67 [00:16<00:03,  3.25it/s, loss=2.1, v_num=1, train_loss_step=0.772, val_loss=2.880, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 198:  82%|██████▌ | 55/67 [00:16<00:03,  3.24it/s, loss=2.1, v_num=1, train_loss_step=0.772, val_loss=2.880, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 198:  84%|██████▋ | 56/67 [00:17<00:03,  3.24it/s, loss=2.1, v_num=1, train_loss_step=0.772, val_loss=2.880, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 198:  85%|██████▊ | 57/67 [00:17<00:03,  3.24it/s, loss=2.1, v_num=1, train_loss_step=0.772, val_loss=2.880, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 198:  87%|██████▉ | 58/67 [00:18<00:02,  3.21it/s, loss=2.1, v_num=1, train_loss_step=0.772, val_loss=2.880, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 198:  88%|███████ | 59/67 [00:18<00:02,  3.21it/s, loss=2.1, v_num=1, train_loss_step=0.772, val_loss=2.880, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 198:  90%|███████▏| 60/67 [00:18<00:02,  3.20it/s, loss=2.1, v_num=1, train_loss_step=0.772, val_loss=2.880, train_loss_epoch=1.770]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 198:  91%|███████▎| 61/67 [00:19<00:01,  3.15it/s, loss=2.1, v_num=1, train_loss_step=0.772, val_loss=2.880, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 198:  93%|███████▍| 62/67 [00:19<00:01,  3.14it/s, loss=2.1, v_num=1, train_loss_step=0.772, val_loss=2.880, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 198:  94%|███████▌| 63/67 [00:20<00:01,  3.14it/s, loss=2.1, v_num=1, train_loss_step=0.772, val_loss=2.880, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 198:  96%|███████▋| 64/67 [00:20<00:00,  3.14it/s, loss=2.1, v_num=1, train_loss_step=0.772, val_loss=2.880, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 198:  97%|███████▊| 65/67 [00:20<00:00,  3.14it/s, loss=2.1, v_num=1, train_loss_step=0.772, val_loss=2.880, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 198:  99%|███████▉| 66/67 [00:21<00:00,  3.14it/s, loss=2.1, v_num=1, train_loss_step=0.772, val_loss=2.880, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 198: 100%|████████| 67/67 [00:21<00:00,  3.07it/s, loss=2.1, v_num=1, train_loss_step=0.772, val_loss=2.880, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 199:  45%|███▏   | 30/67 [00:05<00:07,  5.18it/s, loss=1.89, v_num=1, train_loss_step=2.040, val_loss=2.880, train_loss_epoch=2.110]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 199:  46%|███▏   | 31/67 [00:08<00:09,  3.82it/s, loss=1.89, v_num=1, train_loss_step=2.040, val_loss=2.880, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 199:  48%|███▎   | 32/67 [00:08<00:09,  3.78it/s, loss=1.89, v_num=1, train_loss_step=2.040, val_loss=2.880, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 199:  49%|███▍   | 33/67 [00:08<00:09,  3.76it/s, loss=1.89, v_num=1, train_loss_step=2.040, val_loss=2.880, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 199:  51%|███▌   | 34/67 [00:09<00:08,  3.72it/s, loss=1.89, v_num=1, train_loss_step=2.040, val_loss=2.880, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 199:  52%|███▋   | 35/67 [00:09<00:08,  3.69it/s, loss=1.89, v_num=1, train_loss_step=2.040, val_loss=2.880, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 199:  54%|███▊   | 36/67 [00:09<00:08,  3.67it/s, loss=1.89, v_num=1, train_loss_step=2.040, val_loss=2.880, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 199:  55%|███▊   | 37/67 [00:10<00:08,  3.64it/s, loss=1.89, v_num=1, train_loss_step=2.040, val_loss=2.880, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 199:  57%|███▉   | 38/67 [00:10<00:08,  3.62it/s, loss=1.89, v_num=1, train_loss_step=2.040, val_loss=2.880, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 199:  58%|████   | 39/67 [00:10<00:07,  3.60it/s, loss=1.89, v_num=1, train_loss_step=2.040, val_loss=2.880, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 199:  60%|████▏  | 40/67 [00:11<00:07,  3.58it/s, loss=1.89, v_num=1, train_loss_step=2.040, val_loss=2.880, train_loss_epoch=2.110]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 199:  61%|████▎  | 41/67 [00:11<00:07,  3.51it/s, loss=1.89, v_num=1, train_loss_step=2.040, val_loss=2.880, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 199:  63%|████▍  | 42/67 [00:11<00:07,  3.50it/s, loss=1.89, v_num=1, train_loss_step=2.040, val_loss=2.880, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 199:  64%|████▍  | 43/67 [00:12<00:06,  3.48it/s, loss=1.89, v_num=1, train_loss_step=2.040, val_loss=2.880, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 199:  66%|████▌  | 44/67 [00:12<00:06,  3.47it/s, loss=1.89, v_num=1, train_loss_step=2.040, val_loss=2.880, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 199:  67%|████▋  | 45/67 [00:13<00:06,  3.46it/s, loss=1.89, v_num=1, train_loss_step=2.040, val_loss=2.880, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 199:  69%|████▊  | 46/67 [00:13<00:06,  3.44it/s, loss=1.89, v_num=1, train_loss_step=2.040, val_loss=2.880, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 199:  70%|████▉  | 47/67 [00:13<00:05,  3.43it/s, loss=1.89, v_num=1, train_loss_step=2.040, val_loss=2.880, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 199:  72%|█████  | 48/67 [00:14<00:05,  3.42it/s, loss=1.89, v_num=1, train_loss_step=2.040, val_loss=2.880, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 199:  73%|█████  | 49/67 [00:14<00:05,  3.37it/s, loss=1.89, v_num=1, train_loss_step=2.040, val_loss=2.880, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 199:  75%|█████▏ | 50/67 [00:14<00:05,  3.36it/s, loss=1.89, v_num=1, train_loss_step=2.040, val_loss=2.880, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 199:  76%|█████▎ | 51/67 [00:15<00:04,  3.31it/s, loss=1.89, v_num=1, train_loss_step=2.040, val_loss=2.880, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 199:  78%|█████▍ | 52/67 [00:15<00:04,  3.30it/s, loss=1.89, v_num=1, train_loss_step=2.040, val_loss=2.880, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 199:  79%|█████▌ | 53/67 [00:16<00:04,  3.30it/s, loss=1.89, v_num=1, train_loss_step=2.040, val_loss=2.880, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 199:  81%|█████▋ | 54/67 [00:16<00:03,  3.29it/s, loss=1.89, v_num=1, train_loss_step=2.040, val_loss=2.880, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 199:  82%|█████▋ | 55/67 [00:16<00:03,  3.27it/s, loss=1.89, v_num=1, train_loss_step=2.040, val_loss=2.880, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 199:  84%|█████▊ | 56/67 [00:17<00:03,  3.27it/s, loss=1.89, v_num=1, train_loss_step=2.040, val_loss=2.880, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 199:  85%|█████▉ | 57/67 [00:17<00:03,  3.26it/s, loss=1.89, v_num=1, train_loss_step=2.040, val_loss=2.880, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 199:  87%|██████ | 58/67 [00:17<00:02,  3.24it/s, loss=1.89, v_num=1, train_loss_step=2.040, val_loss=2.880, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 199:  88%|██████▏| 59/67 [00:18<00:02,  3.23it/s, loss=1.89, v_num=1, train_loss_step=2.040, val_loss=2.880, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 199:  90%|██████▎| 60/67 [00:18<00:02,  3.23it/s, loss=1.89, v_num=1, train_loss_step=2.040, val_loss=2.880, train_loss_epoch=2.110]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 199:  91%|██████▎| 61/67 [00:19<00:01,  3.17it/s, loss=1.89, v_num=1, train_loss_step=2.040, val_loss=2.880, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 199:  93%|██████▍| 62/67 [00:19<00:01,  3.17it/s, loss=1.89, v_num=1, train_loss_step=2.040, val_loss=2.880, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 199:  94%|██████▌| 63/67 [00:19<00:01,  3.17it/s, loss=1.89, v_num=1, train_loss_step=2.040, val_loss=2.880, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 199:  96%|██████▋| 64/67 [00:20<00:00,  3.17it/s, loss=1.89, v_num=1, train_loss_step=2.040, val_loss=2.880, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 199:  97%|██████▊| 65/67 [00:20<00:00,  3.16it/s, loss=1.89, v_num=1, train_loss_step=2.040, val_loss=2.880, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 199:  99%|██████▉| 66/67 [00:20<00:00,  3.16it/s, loss=1.89, v_num=1, train_loss_step=2.040, val_loss=2.880, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 199: 100%|███████| 67/67 [00:21<00:00,  3.09it/s, loss=1.89, v_num=1, train_loss_step=2.040, val_loss=2.880, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 200:  45%|███▏   | 30/67 [00:05<00:07,  5.02it/s, loss=1.95, v_num=1, train_loss_step=2.440, val_loss=2.880, train_loss_epoch=2.170]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 200:  46%|███▏   | 31/67 [00:08<00:09,  3.75it/s, loss=1.95, v_num=1, train_loss_step=2.440, val_loss=2.880, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 200:  48%|███▎   | 32/67 [00:08<00:09,  3.71it/s, loss=1.95, v_num=1, train_loss_step=2.440, val_loss=2.880, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 200:  49%|███▍   | 33/67 [00:08<00:09,  3.69it/s, loss=1.95, v_num=1, train_loss_step=2.440, val_loss=2.880, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 200:  51%|███▌   | 34/67 [00:09<00:09,  3.66it/s, loss=1.95, v_num=1, train_loss_step=2.440, val_loss=2.880, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 200:  52%|███▋   | 35/67 [00:09<00:08,  3.64it/s, loss=1.95, v_num=1, train_loss_step=2.440, val_loss=2.880, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 200:  54%|███▊   | 36/67 [00:09<00:08,  3.62it/s, loss=1.95, v_num=1, train_loss_step=2.440, val_loss=2.880, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 200:  55%|███▊   | 37/67 [00:10<00:08,  3.59it/s, loss=1.95, v_num=1, train_loss_step=2.440, val_loss=2.880, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 200:  57%|███▉   | 38/67 [00:10<00:08,  3.57it/s, loss=1.95, v_num=1, train_loss_step=2.440, val_loss=2.880, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 200:  58%|████   | 39/67 [00:10<00:07,  3.55it/s, loss=1.95, v_num=1, train_loss_step=2.440, val_loss=2.880, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 200:  60%|████▏  | 40/67 [00:11<00:07,  3.52it/s, loss=1.95, v_num=1, train_loss_step=2.440, val_loss=2.880, train_loss_epoch=2.170]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 200:  61%|████▎  | 41/67 [00:11<00:07,  3.45it/s, loss=1.95, v_num=1, train_loss_step=2.440, val_loss=2.880, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 200:  63%|████▍  | 42/67 [00:12<00:07,  3.44it/s, loss=1.95, v_num=1, train_loss_step=2.440, val_loss=2.880, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 200:  64%|████▍  | 43/67 [00:12<00:07,  3.42it/s, loss=1.95, v_num=1, train_loss_step=2.440, val_loss=2.880, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 200:  66%|████▌  | 44/67 [00:12<00:06,  3.41it/s, loss=1.95, v_num=1, train_loss_step=2.440, val_loss=2.880, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 200:  67%|████▋  | 45/67 [00:13<00:06,  3.39it/s, loss=1.95, v_num=1, train_loss_step=2.440, val_loss=2.880, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 200:  69%|████▊  | 46/67 [00:13<00:06,  3.38it/s, loss=1.95, v_num=1, train_loss_step=2.440, val_loss=2.880, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 200:  70%|████▉  | 47/67 [00:13<00:05,  3.36it/s, loss=1.95, v_num=1, train_loss_step=2.440, val_loss=2.880, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 200:  72%|█████  | 48/67 [00:14<00:05,  3.35it/s, loss=1.95, v_num=1, train_loss_step=2.440, val_loss=2.880, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 200:  73%|█████  | 49/67 [00:14<00:05,  3.34it/s, loss=1.95, v_num=1, train_loss_step=2.440, val_loss=2.880, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 200:  75%|█████▏ | 50/67 [00:15<00:05,  3.33it/s, loss=1.95, v_num=1, train_loss_step=2.440, val_loss=2.880, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 200:  76%|█████▎ | 51/67 [00:15<00:04,  3.29it/s, loss=1.95, v_num=1, train_loss_step=2.440, val_loss=2.880, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 200:  78%|█████▍ | 52/67 [00:15<00:04,  3.28it/s, loss=1.95, v_num=1, train_loss_step=2.440, val_loss=2.880, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 200:  79%|█████▌ | 53/67 [00:16<00:04,  3.27it/s, loss=1.95, v_num=1, train_loss_step=2.440, val_loss=2.880, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 200:  81%|█████▋ | 54/67 [00:16<00:03,  3.26it/s, loss=1.95, v_num=1, train_loss_step=2.440, val_loss=2.880, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 200:  82%|█████▋ | 55/67 [00:16<00:03,  3.25it/s, loss=1.95, v_num=1, train_loss_step=2.440, val_loss=2.880, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 200:  84%|█████▊ | 56/67 [00:17<00:03,  3.25it/s, loss=1.95, v_num=1, train_loss_step=2.440, val_loss=2.880, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 200:  85%|█████▉ | 57/67 [00:17<00:03,  3.24it/s, loss=1.95, v_num=1, train_loss_step=2.440, val_loss=2.880, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 200:  87%|██████ | 58/67 [00:18<00:02,  3.22it/s, loss=1.95, v_num=1, train_loss_step=2.440, val_loss=2.880, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 200:  88%|██████▏| 59/67 [00:18<00:02,  3.21it/s, loss=1.95, v_num=1, train_loss_step=2.440, val_loss=2.880, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 200:  90%|██████▎| 60/67 [00:18<00:02,  3.20it/s, loss=1.95, v_num=1, train_loss_step=2.440, val_loss=2.880, train_loss_epoch=2.170]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 200:  91%|██████▎| 61/67 [00:19<00:01,  3.12it/s, loss=1.95, v_num=1, train_loss_step=2.440, val_loss=2.880, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 200:  93%|██████▍| 62/67 [00:19<00:01,  3.12it/s, loss=1.95, v_num=1, train_loss_step=2.440, val_loss=2.880, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 200:  94%|██████▌| 63/67 [00:20<00:01,  3.12it/s, loss=1.95, v_num=1, train_loss_step=2.440, val_loss=2.880, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 200:  96%|██████▋| 64/67 [00:20<00:00,  3.11it/s, loss=1.95, v_num=1, train_loss_step=2.440, val_loss=2.880, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 200:  97%|██████▊| 65/67 [00:20<00:00,  3.10it/s, loss=1.95, v_num=1, train_loss_step=2.440, val_loss=2.880, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 200:  99%|██████▉| 66/67 [00:21<00:00,  3.10it/s, loss=1.95, v_num=1, train_loss_step=2.440, val_loss=2.880, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 200: 100%|███████| 67/67 [00:22<00:00,  3.03it/s, loss=1.95, v_num=1, train_loss_step=2.440, val_loss=2.880, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 201:  45%|███▏   | 30/67 [00:06<00:07,  4.70it/s, loss=2.01, v_num=1, train_loss_step=1.720, val_loss=2.880, train_loss_epoch=1.790]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 201:  46%|███▏   | 31/67 [00:08<00:10,  3.56it/s, loss=2.01, v_num=1, train_loss_step=1.720, val_loss=2.880, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 201:  48%|███▎   | 32/67 [00:09<00:09,  3.54it/s, loss=2.01, v_num=1, train_loss_step=1.720, val_loss=2.880, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 201:  49%|███▍   | 33/67 [00:09<00:09,  3.52it/s, loss=2.01, v_num=1, train_loss_step=1.720, val_loss=2.880, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 201:  51%|███▌   | 34/67 [00:09<00:09,  3.50it/s, loss=2.01, v_num=1, train_loss_step=1.720, val_loss=2.880, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 201:  52%|███▋   | 35/67 [00:10<00:09,  3.48it/s, loss=2.01, v_num=1, train_loss_step=1.720, val_loss=2.880, train_loss_epoch=1.790]\u001b[A\n",
      "Validation DataLoader 0:  16%|████████████▍                                                                | 6/37 [00:02<00:12,  2.54it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 204:  61%|████▎  | 41/67 [00:11<00:07,  3.47it/s, loss=2.09, v_num=1, train_loss_step=1.230, val_loss=2.870, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 204:  63%|████▍  | 42/67 [00:12<00:07,  3.46it/s, loss=2.09, v_num=1, train_loss_step=1.230, val_loss=2.870, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 204:  64%|████▍  | 43/67 [00:12<00:06,  3.44it/s, loss=2.09, v_num=1, train_loss_step=1.230, val_loss=2.870, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 204:  66%|████▌  | 44/67 [00:12<00:06,  3.43it/s, loss=2.09, v_num=1, train_loss_step=1.230, val_loss=2.870, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 204:  67%|████▋  | 45/67 [00:13<00:06,  3.41it/s, loss=2.09, v_num=1, train_loss_step=1.230, val_loss=2.870, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 204:  69%|████▊  | 46/67 [00:13<00:06,  3.40it/s, loss=2.09, v_num=1, train_loss_step=1.230, val_loss=2.870, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 204:  70%|████▉  | 47/67 [00:13<00:05,  3.38it/s, loss=2.09, v_num=1, train_loss_step=1.230, val_loss=2.870, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 204:  72%|█████  | 48/67 [00:14<00:05,  3.37it/s, loss=2.09, v_num=1, train_loss_step=1.230, val_loss=2.870, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 204:  73%|█████  | 49/67 [00:14<00:05,  3.37it/s, loss=2.09, v_num=1, train_loss_step=1.230, val_loss=2.870, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 204:  75%|█████▏ | 50/67 [00:14<00:05,  3.36it/s, loss=2.09, v_num=1, train_loss_step=1.230, val_loss=2.870, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 204:  76%|█████▎ | 51/67 [00:15<00:04,  3.31it/s, loss=2.09, v_num=1, train_loss_step=1.230, val_loss=2.870, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 204:  78%|█████▍ | 52/67 [00:15<00:04,  3.30it/s, loss=2.09, v_num=1, train_loss_step=1.230, val_loss=2.870, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 204:  79%|█████▌ | 53/67 [00:16<00:04,  3.30it/s, loss=2.09, v_num=1, train_loss_step=1.230, val_loss=2.870, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 204:  81%|█████▋ | 54/67 [00:16<00:03,  3.29it/s, loss=2.09, v_num=1, train_loss_step=1.230, val_loss=2.870, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 204:  82%|█████▋ | 55/67 [00:16<00:03,  3.29it/s, loss=2.09, v_num=1, train_loss_step=1.230, val_loss=2.870, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 204:  84%|█████▊ | 56/67 [00:17<00:03,  3.28it/s, loss=2.09, v_num=1, train_loss_step=1.230, val_loss=2.870, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 204:  85%|█████▉ | 57/67 [00:17<00:03,  3.28it/s, loss=2.09, v_num=1, train_loss_step=1.230, val_loss=2.870, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 204:  87%|██████ | 58/67 [00:17<00:02,  3.27it/s, loss=2.09, v_num=1, train_loss_step=1.230, val_loss=2.870, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 204:  88%|██████▏| 59/67 [00:18<00:02,  3.27it/s, loss=2.09, v_num=1, train_loss_step=1.230, val_loss=2.870, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 204:  90%|██████▎| 60/67 [00:18<00:02,  3.26it/s, loss=2.09, v_num=1, train_loss_step=1.230, val_loss=2.870, train_loss_epoch=2.490]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 204:  91%|██████▎| 61/67 [00:18<00:01,  3.23it/s, loss=2.09, v_num=1, train_loss_step=1.230, val_loss=2.870, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 204:  93%|██████▍| 62/67 [00:19<00:01,  3.23it/s, loss=2.09, v_num=1, train_loss_step=1.230, val_loss=2.870, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 204:  94%|██████▌| 63/67 [00:19<00:01,  3.22it/s, loss=2.09, v_num=1, train_loss_step=1.230, val_loss=2.870, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 204:  96%|██████▋| 64/67 [00:19<00:00,  3.22it/s, loss=2.09, v_num=1, train_loss_step=1.230, val_loss=2.870, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 204:  97%|██████▊| 65/67 [00:20<00:00,  3.22it/s, loss=2.09, v_num=1, train_loss_step=1.230, val_loss=2.870, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 204:  99%|██████▉| 66/67 [00:20<00:00,  3.22it/s, loss=2.09, v_num=1, train_loss_step=1.230, val_loss=2.870, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 204: 100%|███████| 67/67 [00:21<00:00,  3.14it/s, loss=2.09, v_num=1, train_loss_step=1.230, val_loss=2.870, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 205:  45%|███▏   | 30/67 [00:05<00:07,  5.21it/s, loss=1.86, v_num=1, train_loss_step=2.470, val_loss=2.870, train_loss_epoch=1.890]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 205:  46%|███▏   | 31/67 [00:08<00:09,  3.87it/s, loss=1.86, v_num=1, train_loss_step=2.470, val_loss=2.870, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 205:  48%|███▎   | 32/67 [00:08<00:09,  3.83it/s, loss=1.86, v_num=1, train_loss_step=2.470, val_loss=2.870, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 205:  49%|███▍   | 33/67 [00:08<00:08,  3.80it/s, loss=1.86, v_num=1, train_loss_step=2.470, val_loss=2.870, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 205:  51%|███▌   | 34/67 [00:09<00:08,  3.76it/s, loss=1.86, v_num=1, train_loss_step=2.470, val_loss=2.870, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 205:  52%|███▋   | 35/67 [00:09<00:08,  3.75it/s, loss=1.86, v_num=1, train_loss_step=2.470, val_loss=2.870, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 205:  54%|███▊   | 36/67 [00:09<00:08,  3.72it/s, loss=1.86, v_num=1, train_loss_step=2.470, val_loss=2.870, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 205:  55%|███▊   | 37/67 [00:10<00:08,  3.69it/s, loss=1.86, v_num=1, train_loss_step=2.470, val_loss=2.870, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 205:  57%|███▉   | 38/67 [00:10<00:07,  3.67it/s, loss=1.86, v_num=1, train_loss_step=2.470, val_loss=2.870, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 205:  58%|████   | 39/67 [00:10<00:07,  3.66it/s, loss=1.86, v_num=1, train_loss_step=2.470, val_loss=2.870, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 205:  60%|████▏  | 40/67 [00:10<00:07,  3.64it/s, loss=1.86, v_num=1, train_loss_step=2.470, val_loss=2.870, train_loss_epoch=1.890]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 205:  61%|████▎  | 41/67 [00:11<00:07,  3.58it/s, loss=1.86, v_num=1, train_loss_step=2.470, val_loss=2.870, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 205:  63%|████▍  | 42/67 [00:11<00:07,  3.56it/s, loss=1.86, v_num=1, train_loss_step=2.470, val_loss=2.870, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 205:  64%|████▍  | 43/67 [00:12<00:06,  3.55it/s, loss=1.86, v_num=1, train_loss_step=2.470, val_loss=2.870, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 205:  66%|████▌  | 44/67 [00:12<00:06,  3.53it/s, loss=1.86, v_num=1, train_loss_step=2.470, val_loss=2.870, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 205:  67%|████▋  | 45/67 [00:12<00:06,  3.52it/s, loss=1.86, v_num=1, train_loss_step=2.470, val_loss=2.870, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 205:  69%|████▊  | 46/67 [00:13<00:05,  3.51it/s, loss=1.86, v_num=1, train_loss_step=2.470, val_loss=2.870, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 205:  70%|████▉  | 47/67 [00:13<00:05,  3.50it/s, loss=1.86, v_num=1, train_loss_step=2.470, val_loss=2.870, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 205:  72%|█████  | 48/67 [00:13<00:05,  3.49it/s, loss=1.86, v_num=1, train_loss_step=2.470, val_loss=2.870, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 205:  73%|█████  | 49/67 [00:14<00:05,  3.48it/s, loss=1.86, v_num=1, train_loss_step=2.470, val_loss=2.870, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 205:  75%|█████▏ | 50/67 [00:14<00:04,  3.47it/s, loss=1.86, v_num=1, train_loss_step=2.470, val_loss=2.870, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 205:  76%|█████▎ | 51/67 [00:14<00:04,  3.41it/s, loss=1.86, v_num=1, train_loss_step=2.470, val_loss=2.870, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 205:  78%|█████▍ | 52/67 [00:15<00:04,  3.40it/s, loss=1.86, v_num=1, train_loss_step=2.470, val_loss=2.870, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 205:  79%|█████▌ | 53/67 [00:15<00:04,  3.39it/s, loss=1.86, v_num=1, train_loss_step=2.470, val_loss=2.870, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 205:  81%|█████▋ | 54/67 [00:15<00:03,  3.39it/s, loss=1.86, v_num=1, train_loss_step=2.470, val_loss=2.870, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 205:  82%|█████▋ | 55/67 [00:16<00:03,  3.38it/s, loss=1.86, v_num=1, train_loss_step=2.470, val_loss=2.870, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 205:  84%|█████▊ | 56/67 [00:16<00:03,  3.38it/s, loss=1.86, v_num=1, train_loss_step=2.470, val_loss=2.870, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 205:  85%|█████▉ | 57/67 [00:16<00:02,  3.37it/s, loss=1.86, v_num=1, train_loss_step=2.470, val_loss=2.870, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 205:  87%|██████ | 58/67 [00:17<00:02,  3.37it/s, loss=1.86, v_num=1, train_loss_step=2.470, val_loss=2.870, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 205:  88%|██████▏| 59/67 [00:17<00:02,  3.36it/s, loss=1.86, v_num=1, train_loss_step=2.470, val_loss=2.870, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 205:  90%|██████▎| 60/67 [00:17<00:02,  3.35it/s, loss=1.86, v_num=1, train_loss_step=2.470, val_loss=2.870, train_loss_epoch=1.890]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 205:  91%|██████▎| 61/67 [00:18<00:01,  3.31it/s, loss=1.86, v_num=1, train_loss_step=2.470, val_loss=2.870, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 205:  93%|██████▍| 62/67 [00:18<00:01,  3.30it/s, loss=1.86, v_num=1, train_loss_step=2.470, val_loss=2.870, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 205:  94%|██████▌| 63/67 [00:19<00:01,  3.30it/s, loss=1.86, v_num=1, train_loss_step=2.470, val_loss=2.870, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 205:  96%|██████▋| 64/67 [00:19<00:00,  3.30it/s, loss=1.86, v_num=1, train_loss_step=2.470, val_loss=2.870, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 205:  97%|██████▊| 65/67 [00:19<00:00,  3.29it/s, loss=1.86, v_num=1, train_loss_step=2.470, val_loss=2.870, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 205:  99%|██████▉| 66/67 [00:20<00:00,  3.29it/s, loss=1.86, v_num=1, train_loss_step=2.470, val_loss=2.870, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 205: 100%|███████| 67/67 [00:20<00:00,  3.22it/s, loss=1.86, v_num=1, train_loss_step=2.470, val_loss=2.870, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 206:  45%|███▏   | 30/67 [00:05<00:07,  5.14it/s, loss=1.99, v_num=1, train_loss_step=0.810, val_loss=2.870, train_loss_epoch=1.840]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 206:  46%|███▏   | 31/67 [00:07<00:09,  3.91it/s, loss=1.99, v_num=1, train_loss_step=0.810, val_loss=2.870, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 206:  48%|███▎   | 32/67 [00:08<00:09,  3.88it/s, loss=1.99, v_num=1, train_loss_step=0.810, val_loss=2.870, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 206:  49%|███▍   | 33/67 [00:08<00:08,  3.84it/s, loss=1.99, v_num=1, train_loss_step=0.810, val_loss=2.870, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 206:  51%|███▌   | 34/67 [00:08<00:08,  3.81it/s, loss=1.99, v_num=1, train_loss_step=0.810, val_loss=2.870, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 206:  52%|███▋   | 35/67 [00:09<00:08,  3.78it/s, loss=1.99, v_num=1, train_loss_step=0.810, val_loss=2.870, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 206:  54%|███▊   | 36/67 [00:09<00:08,  3.75it/s, loss=1.99, v_num=1, train_loss_step=0.810, val_loss=2.870, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 206:  55%|███▊   | 37/67 [00:09<00:08,  3.73it/s, loss=1.99, v_num=1, train_loss_step=0.810, val_loss=2.870, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 206:  57%|███▉   | 38/67 [00:10<00:07,  3.70it/s, loss=1.99, v_num=1, train_loss_step=0.810, val_loss=2.870, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 206:  58%|████   | 39/67 [00:10<00:07,  3.68it/s, loss=1.99, v_num=1, train_loss_step=0.810, val_loss=2.870, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 206:  60%|████▏  | 40/67 [00:10<00:07,  3.66it/s, loss=1.99, v_num=1, train_loss_step=0.810, val_loss=2.870, train_loss_epoch=1.840]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 206:  61%|████▎  | 41/67 [00:11<00:07,  3.58it/s, loss=1.99, v_num=1, train_loss_step=0.810, val_loss=2.870, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 206:  63%|████▍  | 42/67 [00:11<00:07,  3.56it/s, loss=1.99, v_num=1, train_loss_step=0.810, val_loss=2.870, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 206:  64%|████▍  | 43/67 [00:12<00:06,  3.54it/s, loss=1.99, v_num=1, train_loss_step=0.810, val_loss=2.870, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 206:  66%|████▌  | 44/67 [00:12<00:06,  3.53it/s, loss=1.99, v_num=1, train_loss_step=0.810, val_loss=2.870, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 206:  67%|████▋  | 45/67 [00:12<00:06,  3.51it/s, loss=1.99, v_num=1, train_loss_step=0.810, val_loss=2.870, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 206:  69%|████▊  | 46/67 [00:13<00:06,  3.50it/s, loss=1.99, v_num=1, train_loss_step=0.810, val_loss=2.870, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 206:  70%|████▉  | 47/67 [00:13<00:05,  3.48it/s, loss=1.99, v_num=1, train_loss_step=0.810, val_loss=2.870, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 206:  72%|█████  | 48/67 [00:13<00:05,  3.47it/s, loss=1.99, v_num=1, train_loss_step=0.810, val_loss=2.870, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 206:  73%|█████  | 49/67 [00:14<00:05,  3.45it/s, loss=1.99, v_num=1, train_loss_step=0.810, val_loss=2.870, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 206:  75%|█████▏ | 50/67 [00:14<00:04,  3.44it/s, loss=1.99, v_num=1, train_loss_step=0.810, val_loss=2.870, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 206:  76%|█████▎ | 51/67 [00:15<00:04,  3.39it/s, loss=1.99, v_num=1, train_loss_step=0.810, val_loss=2.870, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 206:  78%|█████▍ | 52/67 [00:15<00:04,  3.38it/s, loss=1.99, v_num=1, train_loss_step=0.810, val_loss=2.870, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 206:  79%|█████▌ | 53/67 [00:15<00:04,  3.37it/s, loss=1.99, v_num=1, train_loss_step=0.810, val_loss=2.870, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 206:  81%|█████▋ | 54/67 [00:16<00:03,  3.36it/s, loss=1.99, v_num=1, train_loss_step=0.810, val_loss=2.870, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 206:  82%|█████▋ | 55/67 [00:16<00:03,  3.36it/s, loss=1.99, v_num=1, train_loss_step=0.810, val_loss=2.870, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 206:  84%|█████▊ | 56/67 [00:16<00:03,  3.35it/s, loss=1.99, v_num=1, train_loss_step=0.810, val_loss=2.870, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 206:  85%|█████▉ | 57/67 [00:17<00:02,  3.34it/s, loss=1.99, v_num=1, train_loss_step=0.810, val_loss=2.870, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 206:  87%|██████ | 58/67 [00:17<00:02,  3.34it/s, loss=1.99, v_num=1, train_loss_step=0.810, val_loss=2.870, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 206:  88%|██████▏| 59/67 [00:17<00:02,  3.33it/s, loss=1.99, v_num=1, train_loss_step=0.810, val_loss=2.870, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 206:  90%|██████▎| 60/67 [00:18<00:02,  3.33it/s, loss=1.99, v_num=1, train_loss_step=0.810, val_loss=2.870, train_loss_epoch=1.840]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 206:  91%|██████▎| 61/67 [00:18<00:01,  3.29it/s, loss=1.99, v_num=1, train_loss_step=0.810, val_loss=2.870, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 206:  93%|██████▍| 62/67 [00:18<00:01,  3.29it/s, loss=1.99, v_num=1, train_loss_step=0.810, val_loss=2.870, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 206:  94%|██████▌| 63/67 [00:19<00:01,  3.28it/s, loss=1.99, v_num=1, train_loss_step=0.810, val_loss=2.870, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 206:  96%|██████▋| 64/67 [00:19<00:00,  3.28it/s, loss=1.99, v_num=1, train_loss_step=0.810, val_loss=2.870, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 206:  97%|██████▊| 65/67 [00:19<00:00,  3.27it/s, loss=1.99, v_num=1, train_loss_step=0.810, val_loss=2.870, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 206:  99%|██████▉| 66/67 [00:20<00:00,  3.27it/s, loss=1.99, v_num=1, train_loss_step=0.810, val_loss=2.870, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 206: 100%|███████| 67/67 [00:20<00:00,  3.20it/s, loss=1.99, v_num=1, train_loss_step=0.810, val_loss=2.870, train_loss_epoch=1.840]\u001b[A\n",
      "Epoch 207:  45%|███▏   | 30/67 [00:05<00:07,  5.01it/s, loss=2.19, v_num=1, train_loss_step=1.670, val_loss=2.870, train_loss_epoch=1.800]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 207:  46%|███▏   | 31/67 [00:08<00:09,  3.79it/s, loss=2.19, v_num=1, train_loss_step=1.670, val_loss=2.870, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 207:  48%|███▎   | 32/67 [00:08<00:09,  3.76it/s, loss=2.19, v_num=1, train_loss_step=1.670, val_loss=2.870, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 207:  49%|███▍   | 33/67 [00:08<00:09,  3.73it/s, loss=2.19, v_num=1, train_loss_step=1.670, val_loss=2.870, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 207:  51%|███▌   | 34/67 [00:09<00:08,  3.70it/s, loss=2.19, v_num=1, train_loss_step=1.670, val_loss=2.870, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 207:  52%|███▋   | 35/67 [00:09<00:08,  3.67it/s, loss=2.19, v_num=1, train_loss_step=1.670, val_loss=2.870, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 207:  54%|███▊   | 36/67 [00:09<00:08,  3.65it/s, loss=2.19, v_num=1, train_loss_step=1.670, val_loss=2.870, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 207:  55%|███▊   | 37/67 [00:10<00:08,  3.63it/s, loss=2.19, v_num=1, train_loss_step=1.670, val_loss=2.870, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 207:  57%|███▉   | 38/67 [00:10<00:08,  3.60it/s, loss=2.19, v_num=1, train_loss_step=1.670, val_loss=2.870, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 207:  58%|████   | 39/67 [00:10<00:07,  3.58it/s, loss=2.19, v_num=1, train_loss_step=1.670, val_loss=2.870, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 207:  60%|████▏  | 40/67 [00:11<00:07,  3.56it/s, loss=2.19, v_num=1, train_loss_step=1.670, val_loss=2.870, train_loss_epoch=1.800]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 207:  61%|████▎  | 41/67 [00:11<00:07,  3.51it/s, loss=2.19, v_num=1, train_loss_step=1.670, val_loss=2.870, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 207:  63%|████▍  | 42/67 [00:12<00:07,  3.49it/s, loss=2.19, v_num=1, train_loss_step=1.670, val_loss=2.870, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 207:  64%|████▍  | 43/67 [00:12<00:06,  3.47it/s, loss=2.19, v_num=1, train_loss_step=1.670, val_loss=2.870, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 207:  66%|████▌  | 44/67 [00:12<00:06,  3.46it/s, loss=2.19, v_num=1, train_loss_step=1.670, val_loss=2.870, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 207:  67%|████▋  | 45/67 [00:13<00:06,  3.45it/s, loss=2.19, v_num=1, train_loss_step=1.670, val_loss=2.870, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 207:  69%|████▊  | 46/67 [00:13<00:06,  3.44it/s, loss=2.19, v_num=1, train_loss_step=1.670, val_loss=2.870, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 207:  70%|████▉  | 47/67 [00:13<00:05,  3.42it/s, loss=2.19, v_num=1, train_loss_step=1.670, val_loss=2.870, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 207:  72%|█████  | 48/67 [00:14<00:05,  3.41it/s, loss=2.19, v_num=1, train_loss_step=1.670, val_loss=2.870, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 207:  73%|█████  | 49/67 [00:14<00:05,  3.40it/s, loss=2.19, v_num=1, train_loss_step=1.670, val_loss=2.870, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 207:  75%|█████▏ | 50/67 [00:14<00:05,  3.39it/s, loss=2.19, v_num=1, train_loss_step=1.670, val_loss=2.870, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 207:  76%|█████▎ | 51/67 [00:15<00:04,  3.34it/s, loss=2.19, v_num=1, train_loss_step=1.670, val_loss=2.870, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 207:  78%|█████▍ | 52/67 [00:15<00:04,  3.34it/s, loss=2.19, v_num=1, train_loss_step=1.670, val_loss=2.870, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 207:  79%|█████▌ | 53/67 [00:15<00:04,  3.33it/s, loss=2.19, v_num=1, train_loss_step=1.670, val_loss=2.870, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 207:  81%|█████▋ | 54/67 [00:16<00:03,  3.32it/s, loss=2.19, v_num=1, train_loss_step=1.670, val_loss=2.870, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 207:  82%|█████▋ | 55/67 [00:16<00:03,  3.31it/s, loss=2.19, v_num=1, train_loss_step=1.670, val_loss=2.870, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 207:  84%|█████▊ | 56/67 [00:16<00:03,  3.31it/s, loss=2.19, v_num=1, train_loss_step=1.670, val_loss=2.870, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 207:  85%|█████▉ | 57/67 [00:17<00:03,  3.30it/s, loss=2.19, v_num=1, train_loss_step=1.670, val_loss=2.870, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 207:  87%|██████ | 58/67 [00:17<00:02,  3.30it/s, loss=2.19, v_num=1, train_loss_step=1.670, val_loss=2.870, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 207:  88%|██████▏| 59/67 [00:17<00:02,  3.29it/s, loss=2.19, v_num=1, train_loss_step=1.670, val_loss=2.870, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 207:  90%|██████▎| 60/67 [00:18<00:02,  3.29it/s, loss=2.19, v_num=1, train_loss_step=1.670, val_loss=2.870, train_loss_epoch=1.800]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 207:  91%|██████▎| 61/67 [00:18<00:01,  3.25it/s, loss=2.19, v_num=1, train_loss_step=1.670, val_loss=2.870, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 207:  93%|██████▍| 62/67 [00:19<00:01,  3.25it/s, loss=2.19, v_num=1, train_loss_step=1.670, val_loss=2.870, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 207:  94%|██████▌| 63/67 [00:19<00:01,  3.24it/s, loss=2.19, v_num=1, train_loss_step=1.670, val_loss=2.870, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 207:  96%|██████▋| 64/67 [00:19<00:00,  3.24it/s, loss=2.19, v_num=1, train_loss_step=1.670, val_loss=2.870, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 207:  97%|██████▊| 65/67 [00:20<00:00,  3.24it/s, loss=2.19, v_num=1, train_loss_step=1.670, val_loss=2.870, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 207:  99%|██████▉| 66/67 [00:20<00:00,  3.24it/s, loss=2.19, v_num=1, train_loss_step=1.670, val_loss=2.870, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 207: 100%|███████| 67/67 [00:21<00:00,  3.17it/s, loss=2.19, v_num=1, train_loss_step=1.670, val_loss=2.870, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 208:  45%|███▏   | 30/67 [00:05<00:07,  5.03it/s, loss=2.27, v_num=1, train_loss_step=3.130, val_loss=2.870, train_loss_epoch=1.970]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 208:  46%|███▏   | 31/67 [00:08<00:09,  3.84it/s, loss=2.27, v_num=1, train_loss_step=3.130, val_loss=2.870, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 208:  48%|███▎   | 32/67 [00:08<00:09,  3.80it/s, loss=2.27, v_num=1, train_loss_step=3.130, val_loss=2.870, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 208:  49%|███▍   | 33/67 [00:08<00:09,  3.76it/s, loss=2.27, v_num=1, train_loss_step=3.130, val_loss=2.870, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 208:  51%|███▌   | 34/67 [00:09<00:08,  3.73it/s, loss=2.27, v_num=1, train_loss_step=3.130, val_loss=2.870, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 208:  52%|███▋   | 35/67 [00:09<00:08,  3.70it/s, loss=2.27, v_num=1, train_loss_step=3.130, val_loss=2.870, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 208:  54%|███▊   | 36/67 [00:09<00:08,  3.67it/s, loss=2.27, v_num=1, train_loss_step=3.130, val_loss=2.870, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 208:  55%|███▊   | 37/67 [00:10<00:08,  3.65it/s, loss=2.27, v_num=1, train_loss_step=3.130, val_loss=2.870, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 208:  57%|███▉   | 38/67 [00:10<00:07,  3.63it/s, loss=2.27, v_num=1, train_loss_step=3.130, val_loss=2.870, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 208:  58%|████   | 39/67 [00:10<00:07,  3.60it/s, loss=2.27, v_num=1, train_loss_step=3.130, val_loss=2.870, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 208:  60%|████▏  | 40/67 [00:11<00:07,  3.58it/s, loss=2.27, v_num=1, train_loss_step=3.130, val_loss=2.870, train_loss_epoch=1.970]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 208:  61%|████▎  | 41/67 [00:11<00:07,  3.51it/s, loss=2.27, v_num=1, train_loss_step=3.130, val_loss=2.870, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 208:  63%|████▍  | 42/67 [00:12<00:07,  3.49it/s, loss=2.27, v_num=1, train_loss_step=3.130, val_loss=2.870, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 208:  64%|████▍  | 43/67 [00:12<00:06,  3.48it/s, loss=2.27, v_num=1, train_loss_step=3.130, val_loss=2.870, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 208:  66%|████▌  | 44/67 [00:12<00:06,  3.46it/s, loss=2.27, v_num=1, train_loss_step=3.130, val_loss=2.870, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 208:  67%|████▋  | 45/67 [00:13<00:06,  3.44it/s, loss=2.27, v_num=1, train_loss_step=3.130, val_loss=2.870, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 208:  69%|████▊  | 46/67 [00:13<00:06,  3.43it/s, loss=2.27, v_num=1, train_loss_step=3.130, val_loss=2.870, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 208:  70%|████▉  | 47/67 [00:13<00:05,  3.42it/s, loss=2.27, v_num=1, train_loss_step=3.130, val_loss=2.870, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 208:  72%|█████  | 48/67 [00:14<00:05,  3.41it/s, loss=2.27, v_num=1, train_loss_step=3.130, val_loss=2.870, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 208:  73%|█████  | 49/67 [00:14<00:05,  3.40it/s, loss=2.27, v_num=1, train_loss_step=3.130, val_loss=2.870, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 208:  75%|█████▏ | 50/67 [00:14<00:05,  3.39it/s, loss=2.27, v_num=1, train_loss_step=3.130, val_loss=2.870, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 208:  76%|█████▎ | 51/67 [00:15<00:04,  3.34it/s, loss=2.27, v_num=1, train_loss_step=3.130, val_loss=2.870, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 208:  78%|█████▍ | 52/67 [00:15<00:04,  3.33it/s, loss=2.27, v_num=1, train_loss_step=3.130, val_loss=2.870, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 208:  79%|█████▌ | 53/67 [00:15<00:04,  3.32it/s, loss=2.27, v_num=1, train_loss_step=3.130, val_loss=2.870, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 208:  81%|█████▋ | 54/67 [00:16<00:03,  3.31it/s, loss=2.27, v_num=1, train_loss_step=3.130, val_loss=2.870, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 208:  82%|█████▋ | 55/67 [00:16<00:03,  3.30it/s, loss=2.27, v_num=1, train_loss_step=3.130, val_loss=2.870, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 208:  84%|█████▊ | 56/67 [00:17<00:03,  3.29it/s, loss=2.27, v_num=1, train_loss_step=3.130, val_loss=2.870, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 208:  85%|█████▉ | 57/67 [00:17<00:03,  3.28it/s, loss=2.27, v_num=1, train_loss_step=3.130, val_loss=2.870, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 208:  87%|██████ | 58/67 [00:17<00:02,  3.28it/s, loss=2.27, v_num=1, train_loss_step=3.130, val_loss=2.870, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 208:  88%|██████▏| 59/67 [00:18<00:02,  3.27it/s, loss=2.27, v_num=1, train_loss_step=3.130, val_loss=2.870, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 208:  90%|██████▎| 60/67 [00:18<00:02,  3.26it/s, loss=2.27, v_num=1, train_loss_step=3.130, val_loss=2.870, train_loss_epoch=1.970]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 208:  91%|██████▎| 61/67 [00:18<00:01,  3.23it/s, loss=2.27, v_num=1, train_loss_step=3.130, val_loss=2.870, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 208:  93%|██████▍| 62/67 [00:19<00:01,  3.23it/s, loss=2.27, v_num=1, train_loss_step=3.130, val_loss=2.870, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 208:  94%|██████▌| 63/67 [00:19<00:01,  3.22it/s, loss=2.27, v_num=1, train_loss_step=3.130, val_loss=2.870, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 208:  96%|██████▋| 64/67 [00:19<00:00,  3.22it/s, loss=2.27, v_num=1, train_loss_step=3.130, val_loss=2.870, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 208:  97%|██████▊| 65/67 [00:20<00:00,  3.22it/s, loss=2.27, v_num=1, train_loss_step=3.130, val_loss=2.870, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 208:  99%|██████▉| 66/67 [00:20<00:00,  3.22it/s, loss=2.27, v_num=1, train_loss_step=3.130, val_loss=2.870, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 208: 100%|███████| 67/67 [00:21<00:00,  3.14it/s, loss=2.27, v_num=1, train_loss_step=3.130, val_loss=2.870, train_loss_epoch=1.970]\u001b[A\n",
      "Epoch 209:  45%|███▏   | 30/67 [00:05<00:07,  5.11it/s, loss=2.12, v_num=1, train_loss_step=1.240, val_loss=2.870, train_loss_epoch=2.260]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 209:  46%|███▏   | 31/67 [00:08<00:09,  3.81it/s, loss=2.12, v_num=1, train_loss_step=1.240, val_loss=2.870, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 209:  48%|███▎   | 32/67 [00:08<00:09,  3.78it/s, loss=2.12, v_num=1, train_loss_step=1.240, val_loss=2.870, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 209:  49%|███▍   | 33/67 [00:08<00:09,  3.74it/s, loss=2.12, v_num=1, train_loss_step=1.240, val_loss=2.870, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 209:  51%|███▌   | 34/67 [00:09<00:08,  3.71it/s, loss=2.12, v_num=1, train_loss_step=1.240, val_loss=2.870, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 209:  52%|███▋   | 35/67 [00:09<00:08,  3.69it/s, loss=2.12, v_num=1, train_loss_step=1.240, val_loss=2.870, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 209:  54%|███▊   | 36/67 [00:09<00:08,  3.67it/s, loss=2.12, v_num=1, train_loss_step=1.240, val_loss=2.870, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 209:  55%|███▊   | 37/67 [00:10<00:08,  3.64it/s, loss=2.12, v_num=1, train_loss_step=1.240, val_loss=2.870, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 209:  57%|███▉   | 38/67 [00:10<00:08,  3.62it/s, loss=2.12, v_num=1, train_loss_step=1.240, val_loss=2.870, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 209:  58%|████   | 39/67 [00:10<00:07,  3.60it/s, loss=2.12, v_num=1, train_loss_step=1.240, val_loss=2.870, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 209:  60%|████▏  | 40/67 [00:11<00:07,  3.58it/s, loss=2.12, v_num=1, train_loss_step=1.240, val_loss=2.870, train_loss_epoch=2.260]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 209:  61%|████▎  | 41/67 [00:11<00:07,  3.50it/s, loss=2.12, v_num=1, train_loss_step=1.240, val_loss=2.870, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 209:  63%|████▍  | 42/67 [00:12<00:07,  3.49it/s, loss=2.12, v_num=1, train_loss_step=1.240, val_loss=2.870, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 209:  64%|████▍  | 43/67 [00:12<00:06,  3.47it/s, loss=2.12, v_num=1, train_loss_step=1.240, val_loss=2.870, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 209:  66%|████▌  | 44/67 [00:12<00:06,  3.46it/s, loss=2.12, v_num=1, train_loss_step=1.240, val_loss=2.870, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 209:  67%|████▋  | 45/67 [00:13<00:06,  3.44it/s, loss=2.12, v_num=1, train_loss_step=1.240, val_loss=2.870, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 209:  69%|████▊  | 46/67 [00:13<00:06,  3.43it/s, loss=2.12, v_num=1, train_loss_step=1.240, val_loss=2.870, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 209:  70%|████▉  | 47/67 [00:13<00:05,  3.42it/s, loss=2.12, v_num=1, train_loss_step=1.240, val_loss=2.870, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 209:  72%|█████  | 48/67 [00:14<00:05,  3.41it/s, loss=2.12, v_num=1, train_loss_step=1.240, val_loss=2.870, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 209:  73%|█████  | 49/67 [00:14<00:05,  3.40it/s, loss=2.12, v_num=1, train_loss_step=1.240, val_loss=2.870, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 209:  75%|█████▏ | 50/67 [00:14<00:05,  3.39it/s, loss=2.12, v_num=1, train_loss_step=1.240, val_loss=2.870, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 209:  76%|█████▎ | 51/67 [00:15<00:04,  3.35it/s, loss=2.12, v_num=1, train_loss_step=1.240, val_loss=2.870, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 209:  78%|█████▍ | 52/67 [00:15<00:04,  3.34it/s, loss=2.12, v_num=1, train_loss_step=1.240, val_loss=2.870, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 209:  79%|█████▌ | 53/67 [00:15<00:04,  3.33it/s, loss=2.12, v_num=1, train_loss_step=1.240, val_loss=2.870, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 209:  81%|█████▋ | 54/67 [00:16<00:03,  3.33it/s, loss=2.12, v_num=1, train_loss_step=1.240, val_loss=2.870, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 209:  82%|█████▋ | 55/67 [00:16<00:03,  3.32it/s, loss=2.12, v_num=1, train_loss_step=1.240, val_loss=2.870, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 209:  84%|█████▊ | 56/67 [00:16<00:03,  3.31it/s, loss=2.12, v_num=1, train_loss_step=1.240, val_loss=2.870, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 209:  85%|█████▉ | 57/67 [00:17<00:03,  3.31it/s, loss=2.12, v_num=1, train_loss_step=1.240, val_loss=2.870, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 209:  87%|██████ | 58/67 [00:17<00:02,  3.30it/s, loss=2.12, v_num=1, train_loss_step=1.240, val_loss=2.870, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 209:  88%|██████▏| 59/67 [00:17<00:02,  3.29it/s, loss=2.12, v_num=1, train_loss_step=1.240, val_loss=2.870, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 209:  90%|██████▎| 60/67 [00:18<00:02,  3.28it/s, loss=2.12, v_num=1, train_loss_step=1.240, val_loss=2.870, train_loss_epoch=2.260]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 209:  91%|██████▎| 61/67 [00:18<00:01,  3.24it/s, loss=2.12, v_num=1, train_loss_step=1.240, val_loss=2.870, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 209:  93%|██████▍| 62/67 [00:19<00:01,  3.24it/s, loss=2.12, v_num=1, train_loss_step=1.240, val_loss=2.870, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 209:  94%|██████▌| 63/67 [00:19<00:01,  3.24it/s, loss=2.12, v_num=1, train_loss_step=1.240, val_loss=2.870, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 209:  96%|██████▋| 64/67 [00:19<00:00,  3.23it/s, loss=2.12, v_num=1, train_loss_step=1.240, val_loss=2.870, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 209:  97%|██████▊| 65/67 [00:20<00:00,  3.23it/s, loss=2.12, v_num=1, train_loss_step=1.240, val_loss=2.870, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 209:  99%|██████▉| 66/67 [00:20<00:00,  3.23it/s, loss=2.12, v_num=1, train_loss_step=1.240, val_loss=2.870, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 209: 100%|███████| 67/67 [00:21<00:00,  3.16it/s, loss=2.12, v_num=1, train_loss_step=1.240, val_loss=2.860, train_loss_epoch=2.260]\u001b[A\n",
      "Epoch 210:  45%|███▌    | 30/67 [00:06<00:07,  4.96it/s, loss=2.3, v_num=1, train_loss_step=1.370, val_loss=2.860, train_loss_epoch=1.890]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 210:  46%|███▋    | 31/67 [00:08<00:09,  3.73it/s, loss=2.3, v_num=1, train_loss_step=1.370, val_loss=2.860, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 210:  48%|███▊    | 32/67 [00:08<00:09,  3.68it/s, loss=2.3, v_num=1, train_loss_step=1.370, val_loss=2.860, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 210:  49%|███▉    | 33/67 [00:09<00:09,  3.65it/s, loss=2.3, v_num=1, train_loss_step=1.370, val_loss=2.860, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 210:  51%|████    | 34/67 [00:09<00:09,  3.62it/s, loss=2.3, v_num=1, train_loss_step=1.370, val_loss=2.860, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 210:  52%|████▏   | 35/67 [00:09<00:08,  3.59it/s, loss=2.3, v_num=1, train_loss_step=1.370, val_loss=2.860, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 210:  54%|████▎   | 36/67 [00:10<00:08,  3.55it/s, loss=2.3, v_num=1, train_loss_step=1.370, val_loss=2.860, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 210:  55%|████▍   | 37/67 [00:10<00:08,  3.53it/s, loss=2.3, v_num=1, train_loss_step=1.370, val_loss=2.860, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 210:  57%|████▌   | 38/67 [00:10<00:08,  3.51it/s, loss=2.3, v_num=1, train_loss_step=1.370, val_loss=2.860, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 210:  58%|████▋   | 39/67 [00:11<00:08,  3.49it/s, loss=2.3, v_num=1, train_loss_step=1.370, val_loss=2.860, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 210:  60%|████▊   | 40/67 [00:11<00:07,  3.48it/s, loss=2.3, v_num=1, train_loss_step=1.370, val_loss=2.860, train_loss_epoch=1.890]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 210:  61%|████▉   | 41/67 [00:12<00:07,  3.41it/s, loss=2.3, v_num=1, train_loss_step=1.370, val_loss=2.860, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 210:  63%|█████   | 42/67 [00:12<00:07,  3.40it/s, loss=2.3, v_num=1, train_loss_step=1.370, val_loss=2.860, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 210:  64%|█████▏  | 43/67 [00:12<00:07,  3.39it/s, loss=2.3, v_num=1, train_loss_step=1.370, val_loss=2.860, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 210:  66%|█████▎  | 44/67 [00:13<00:06,  3.38it/s, loss=2.3, v_num=1, train_loss_step=1.370, val_loss=2.860, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 210:  67%|█████▎  | 45/67 [00:13<00:06,  3.37it/s, loss=2.3, v_num=1, train_loss_step=1.370, val_loss=2.860, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 210:  69%|█████▍  | 46/67 [00:13<00:06,  3.35it/s, loss=2.3, v_num=1, train_loss_step=1.370, val_loss=2.860, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 210:  70%|█████▌  | 47/67 [00:14<00:05,  3.34it/s, loss=2.3, v_num=1, train_loss_step=1.370, val_loss=2.860, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 210:  72%|█████▋  | 48/67 [00:14<00:05,  3.32it/s, loss=2.3, v_num=1, train_loss_step=1.370, val_loss=2.860, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 210:  73%|█████▊  | 49/67 [00:14<00:05,  3.30it/s, loss=2.3, v_num=1, train_loss_step=1.370, val_loss=2.860, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 210:  75%|█████▉  | 50/67 [00:15<00:05,  3.30it/s, loss=2.3, v_num=1, train_loss_step=1.370, val_loss=2.860, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 210:  76%|██████  | 51/67 [00:15<00:04,  3.24it/s, loss=2.3, v_num=1, train_loss_step=1.370, val_loss=2.860, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 210:  78%|██████▏ | 52/67 [00:16<00:04,  3.23it/s, loss=2.3, v_num=1, train_loss_step=1.370, val_loss=2.860, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 210:  79%|██████▎ | 53/67 [00:16<00:04,  3.21it/s, loss=2.3, v_num=1, train_loss_step=1.370, val_loss=2.860, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 210:  81%|██████▍ | 54/67 [00:16<00:04,  3.20it/s, loss=2.3, v_num=1, train_loss_step=1.370, val_loss=2.860, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 210:  82%|██████▌ | 55/67 [00:17<00:03,  3.18it/s, loss=2.3, v_num=1, train_loss_step=1.370, val_loss=2.860, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 210:  84%|██████▋ | 56/67 [00:17<00:03,  3.17it/s, loss=2.3, v_num=1, train_loss_step=1.370, val_loss=2.860, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 210:  85%|██████▊ | 57/67 [00:18<00:03,  3.17it/s, loss=2.3, v_num=1, train_loss_step=1.370, val_loss=2.860, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 210:  87%|██████▉ | 58/67 [00:18<00:02,  3.11it/s, loss=2.3, v_num=1, train_loss_step=1.370, val_loss=2.860, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 210:  88%|███████ | 59/67 [00:18<00:02,  3.11it/s, loss=2.3, v_num=1, train_loss_step=1.370, val_loss=2.860, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 210:  90%|███████▏| 60/67 [00:19<00:02,  3.10it/s, loss=2.3, v_num=1, train_loss_step=1.370, val_loss=2.860, train_loss_epoch=1.890]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 210:  91%|███████▎| 61/67 [00:20<00:01,  3.03it/s, loss=2.3, v_num=1, train_loss_step=1.370, val_loss=2.860, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 210:  93%|███████▍| 62/67 [00:20<00:01,  3.02it/s, loss=2.3, v_num=1, train_loss_step=1.370, val_loss=2.860, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 210:  94%|███████▌| 63/67 [00:20<00:01,  3.02it/s, loss=2.3, v_num=1, train_loss_step=1.370, val_loss=2.860, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 210:  96%|███████▋| 64/67 [00:21<00:00,  3.02it/s, loss=2.3, v_num=1, train_loss_step=1.370, val_loss=2.860, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 210:  97%|███████▊| 65/67 [00:21<00:00,  3.01it/s, loss=2.3, v_num=1, train_loss_step=1.370, val_loss=2.860, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 210:  99%|███████▉| 66/67 [00:21<00:00,  3.01it/s, loss=2.3, v_num=1, train_loss_step=1.370, val_loss=2.860, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 210: 100%|████████| 67/67 [00:22<00:00,  2.94it/s, loss=2.3, v_num=1, train_loss_step=1.370, val_loss=2.860, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 211:  45%|███▏   | 30/67 [00:06<00:08,  4.56it/s, loss=1.94, v_num=1, train_loss_step=1.330, val_loss=2.860, train_loss_epoch=2.330]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 211:  46%|███▏   | 31/67 [00:08<00:10,  3.49it/s, loss=1.94, v_num=1, train_loss_step=1.330, val_loss=2.860, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 211:  48%|███▎   | 32/67 [00:09<00:10,  3.47it/s, loss=1.94, v_num=1, train_loss_step=1.330, val_loss=2.860, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 211:  49%|███▍   | 33/67 [00:09<00:09,  3.45it/s, loss=1.94, v_num=1, train_loss_step=1.330, val_loss=2.860, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 211:  51%|███▌   | 34/67 [00:09<00:09,  3.42it/s, loss=1.94, v_num=1, train_loss_step=1.330, val_loss=2.860, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 211:  52%|███▋   | 35/67 [00:10<00:09,  3.40it/s, loss=1.94, v_num=1, train_loss_step=1.330, val_loss=2.860, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 211:  54%|███▊   | 36/67 [00:10<00:09,  3.37it/s, loss=1.94, v_num=1, train_loss_step=1.330, val_loss=2.860, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 211:  55%|███▊   | 37/67 [00:11<00:08,  3.35it/s, loss=1.94, v_num=1, train_loss_step=1.330, val_loss=2.860, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 211:  57%|███▉   | 38/67 [00:11<00:08,  3.33it/s, loss=1.94, v_num=1, train_loss_step=1.330, val_loss=2.860, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 211:  58%|████   | 39/67 [00:11<00:08,  3.31it/s, loss=1.94, v_num=1, train_loss_step=1.330, val_loss=2.860, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 211:  60%|████▏  | 40/67 [00:12<00:08,  3.30it/s, loss=1.94, v_num=1, train_loss_step=1.330, val_loss=2.860, train_loss_epoch=2.330]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 211:  61%|████▎  | 41/67 [00:12<00:07,  3.25it/s, loss=1.94, v_num=1, train_loss_step=1.330, val_loss=2.860, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 211:  63%|████▍  | 42/67 [00:12<00:07,  3.24it/s, loss=1.94, v_num=1, train_loss_step=1.330, val_loss=2.860, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 211:  64%|████▍  | 43/67 [00:13<00:07,  3.23it/s, loss=1.94, v_num=1, train_loss_step=1.330, val_loss=2.860, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 211:  66%|████▌  | 44/67 [00:13<00:07,  3.23it/s, loss=1.94, v_num=1, train_loss_step=1.330, val_loss=2.860, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 211:  67%|████▋  | 45/67 [00:13<00:06,  3.22it/s, loss=1.94, v_num=1, train_loss_step=1.330, val_loss=2.860, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 211:  69%|████▊  | 46/67 [00:14<00:06,  3.22it/s, loss=1.94, v_num=1, train_loss_step=1.330, val_loss=2.860, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 211:  70%|████▉  | 47/67 [00:14<00:06,  3.21it/s, loss=1.94, v_num=1, train_loss_step=1.330, val_loss=2.860, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 211:  72%|█████  | 48/67 [00:14<00:05,  3.21it/s, loss=1.94, v_num=1, train_loss_step=1.330, val_loss=2.860, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 211:  73%|█████  | 49/67 [00:15<00:05,  3.18it/s, loss=1.94, v_num=1, train_loss_step=1.330, val_loss=2.860, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 211:  75%|█████▏ | 50/67 [00:15<00:05,  3.18it/s, loss=1.94, v_num=1, train_loss_step=1.330, val_loss=2.860, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 211:  76%|█████▎ | 51/67 [00:16<00:05,  3.14it/s, loss=1.94, v_num=1, train_loss_step=1.330, val_loss=2.860, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 211:  78%|█████▍ | 52/67 [00:16<00:04,  3.14it/s, loss=1.94, v_num=1, train_loss_step=1.330, val_loss=2.860, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 211:  79%|█████▌ | 53/67 [00:16<00:04,  3.13it/s, loss=1.94, v_num=1, train_loss_step=1.330, val_loss=2.860, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 211:  81%|█████▋ | 54/67 [00:17<00:04,  3.13it/s, loss=1.94, v_num=1, train_loss_step=1.330, val_loss=2.860, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 211:  82%|█████▋ | 55/67 [00:17<00:03,  3.12it/s, loss=1.94, v_num=1, train_loss_step=1.330, val_loss=2.860, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 211:  84%|█████▊ | 56/67 [00:17<00:03,  3.12it/s, loss=1.94, v_num=1, train_loss_step=1.330, val_loss=2.860, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 211:  85%|█████▉ | 57/67 [00:18<00:03,  3.12it/s, loss=1.94, v_num=1, train_loss_step=1.330, val_loss=2.860, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 211:  87%|██████ | 58/67 [00:18<00:02,  3.10it/s, loss=1.94, v_num=1, train_loss_step=1.330, val_loss=2.860, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 211:  88%|██████▏| 59/67 [00:19<00:02,  3.10it/s, loss=1.94, v_num=1, train_loss_step=1.330, val_loss=2.860, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 211:  90%|██████▎| 60/67 [00:19<00:02,  3.09it/s, loss=1.94, v_num=1, train_loss_step=1.330, val_loss=2.860, train_loss_epoch=2.330]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 211:  91%|██████▎| 61/67 [00:19<00:01,  3.05it/s, loss=1.94, v_num=1, train_loss_step=1.330, val_loss=2.860, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 211:  93%|██████▍| 62/67 [00:20<00:01,  3.05it/s, loss=1.94, v_num=1, train_loss_step=1.330, val_loss=2.860, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 211:  94%|██████▌| 63/67 [00:20<00:01,  3.05it/s, loss=1.94, v_num=1, train_loss_step=1.330, val_loss=2.860, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 211:  96%|██████▋| 64/67 [00:20<00:00,  3.05it/s, loss=1.94, v_num=1, train_loss_step=1.330, val_loss=2.860, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 211:  97%|██████▊| 65/67 [00:21<00:00,  3.05it/s, loss=1.94, v_num=1, train_loss_step=1.330, val_loss=2.860, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 211:  99%|██████▉| 66/67 [00:21<00:00,  3.05it/s, loss=1.94, v_num=1, train_loss_step=1.330, val_loss=2.860, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 211: 100%|███████| 67/67 [00:22<00:00,  2.99it/s, loss=1.94, v_num=1, train_loss_step=1.330, val_loss=2.860, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 212:  45%|███▏   | 30/67 [00:06<00:07,  4.86it/s, loss=2.09, v_num=1, train_loss_step=1.380, val_loss=2.860, train_loss_epoch=2.030]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 212:  46%|███▏   | 31/67 [00:08<00:09,  3.64it/s, loss=2.09, v_num=1, train_loss_step=1.380, val_loss=2.860, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 212:  48%|███▎   | 32/67 [00:08<00:09,  3.62it/s, loss=2.09, v_num=1, train_loss_step=1.380, val_loss=2.860, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 212:  49%|███▍   | 33/67 [00:09<00:09,  3.60it/s, loss=2.09, v_num=1, train_loss_step=1.380, val_loss=2.860, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 212:  51%|███▌   | 34/67 [00:09<00:09,  3.57it/s, loss=2.09, v_num=1, train_loss_step=1.380, val_loss=2.860, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 212:  52%|███▋   | 35/67 [00:09<00:08,  3.56it/s, loss=2.09, v_num=1, train_loss_step=1.380, val_loss=2.860, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 212:  54%|███▊   | 36/67 [00:10<00:08,  3.54it/s, loss=2.09, v_num=1, train_loss_step=1.380, val_loss=2.860, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 212:  55%|███▊   | 37/67 [00:10<00:08,  3.51it/s, loss=2.09, v_num=1, train_loss_step=1.380, val_loss=2.860, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 212:  57%|███▉   | 38/67 [00:10<00:08,  3.50it/s, loss=2.09, v_num=1, train_loss_step=1.380, val_loss=2.860, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 212:  58%|████   | 39/67 [00:11<00:08,  3.48it/s, loss=2.09, v_num=1, train_loss_step=1.380, val_loss=2.860, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 212:  60%|████▏  | 40/67 [00:11<00:07,  3.45it/s, loss=2.09, v_num=1, train_loss_step=1.380, val_loss=2.860, train_loss_epoch=2.030]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 212:  61%|████▎  | 41/67 [00:12<00:07,  3.38it/s, loss=2.09, v_num=1, train_loss_step=1.380, val_loss=2.860, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 212:  63%|████▍  | 42/67 [00:12<00:07,  3.37it/s, loss=2.09, v_num=1, train_loss_step=1.380, val_loss=2.860, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 212:  64%|████▍  | 43/67 [00:12<00:07,  3.36it/s, loss=2.09, v_num=1, train_loss_step=1.380, val_loss=2.860, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 212:  66%|████▌  | 44/67 [00:13<00:06,  3.35it/s, loss=2.09, v_num=1, train_loss_step=1.380, val_loss=2.860, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 212:  67%|████▋  | 45/67 [00:13<00:06,  3.34it/s, loss=2.09, v_num=1, train_loss_step=1.380, val_loss=2.860, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 212:  69%|████▊  | 46/67 [00:13<00:06,  3.31it/s, loss=2.09, v_num=1, train_loss_step=1.380, val_loss=2.860, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 212:  70%|████▉  | 47/67 [00:14<00:06,  3.31it/s, loss=2.09, v_num=1, train_loss_step=1.380, val_loss=2.860, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 212:  72%|█████  | 48/67 [00:14<00:05,  3.30it/s, loss=2.09, v_num=1, train_loss_step=1.380, val_loss=2.860, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 212:  73%|█████  | 49/67 [00:14<00:05,  3.27it/s, loss=2.09, v_num=1, train_loss_step=1.380, val_loss=2.860, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 212:  75%|█████▏ | 50/67 [00:15<00:05,  3.27it/s, loss=2.09, v_num=1, train_loss_step=1.380, val_loss=2.860, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 212:  76%|█████▎ | 51/67 [00:15<00:04,  3.22it/s, loss=2.09, v_num=1, train_loss_step=1.380, val_loss=2.860, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 212:  78%|█████▍ | 52/67 [00:16<00:04,  3.21it/s, loss=2.09, v_num=1, train_loss_step=1.380, val_loss=2.860, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 212:  79%|█████▌ | 53/67 [00:16<00:04,  3.21it/s, loss=2.09, v_num=1, train_loss_step=1.380, val_loss=2.860, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 212:  81%|█████▋ | 54/67 [00:16<00:04,  3.20it/s, loss=2.09, v_num=1, train_loss_step=1.380, val_loss=2.860, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 212:  82%|█████▋ | 55/67 [00:17<00:03,  3.20it/s, loss=2.09, v_num=1, train_loss_step=1.380, val_loss=2.860, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 212:  84%|█████▊ | 56/67 [00:17<00:03,  3.19it/s, loss=2.09, v_num=1, train_loss_step=1.380, val_loss=2.860, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 212:  85%|█████▉ | 57/67 [00:17<00:03,  3.19it/s, loss=2.09, v_num=1, train_loss_step=1.380, val_loss=2.860, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 212:  87%|██████ | 58/67 [00:18<00:02,  3.18it/s, loss=2.09, v_num=1, train_loss_step=1.380, val_loss=2.860, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 212:  88%|██████▏| 59/67 [00:18<00:02,  3.18it/s, loss=2.09, v_num=1, train_loss_step=1.380, val_loss=2.860, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 212:  90%|██████▎| 60/67 [00:18<00:02,  3.17it/s, loss=2.09, v_num=1, train_loss_step=1.380, val_loss=2.860, train_loss_epoch=2.030]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 212:  91%|██████▎| 61/67 [00:19<00:01,  3.13it/s, loss=2.09, v_num=1, train_loss_step=1.380, val_loss=2.860, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 212:  93%|██████▍| 62/67 [00:19<00:01,  3.13it/s, loss=2.09, v_num=1, train_loss_step=1.380, val_loss=2.860, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 212:  94%|██████▌| 63/67 [00:20<00:01,  3.13it/s, loss=2.09, v_num=1, train_loss_step=1.380, val_loss=2.860, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 212:  96%|██████▋| 64/67 [00:20<00:00,  3.13it/s, loss=2.09, v_num=1, train_loss_step=1.380, val_loss=2.860, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 212:  97%|██████▊| 65/67 [00:20<00:00,  3.13it/s, loss=2.09, v_num=1, train_loss_step=1.380, val_loss=2.860, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 212:  99%|██████▉| 66/67 [00:21<00:00,  3.13it/s, loss=2.09, v_num=1, train_loss_step=1.380, val_loss=2.860, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 212: 100%|███████| 67/67 [00:21<00:00,  3.07it/s, loss=2.09, v_num=1, train_loss_step=1.380, val_loss=2.860, train_loss_epoch=2.030]\u001b[A\n",
      "Epoch 213:  45%|███▏   | 30/67 [00:05<00:07,  5.08it/s, loss=2.18, v_num=1, train_loss_step=1.470, val_loss=2.860, train_loss_epoch=2.000]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 213:  46%|███▏   | 31/67 [00:08<00:09,  3.80it/s, loss=2.18, v_num=1, train_loss_step=1.470, val_loss=2.860, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 213:  48%|███▎   | 32/67 [00:08<00:09,  3.77it/s, loss=2.18, v_num=1, train_loss_step=1.470, val_loss=2.860, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 213:  49%|███▍   | 33/67 [00:08<00:09,  3.73it/s, loss=2.18, v_num=1, train_loss_step=1.470, val_loss=2.860, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 213:  51%|███▌   | 34/67 [00:09<00:08,  3.70it/s, loss=2.18, v_num=1, train_loss_step=1.470, val_loss=2.860, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 213:  52%|███▋   | 35/67 [00:09<00:08,  3.67it/s, loss=2.18, v_num=1, train_loss_step=1.470, val_loss=2.860, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 213:  54%|███▊   | 36/67 [00:09<00:08,  3.64it/s, loss=2.18, v_num=1, train_loss_step=1.470, val_loss=2.860, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 213:  55%|███▊   | 37/67 [00:10<00:08,  3.61it/s, loss=2.18, v_num=1, train_loss_step=1.470, val_loss=2.860, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 213:  57%|███▉   | 38/67 [00:10<00:08,  3.59it/s, loss=2.18, v_num=1, train_loss_step=1.470, val_loss=2.860, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 213:  58%|████   | 39/67 [00:10<00:07,  3.57it/s, loss=2.18, v_num=1, train_loss_step=1.470, val_loss=2.860, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 213:  60%|████▏  | 40/67 [00:11<00:07,  3.55it/s, loss=2.18, v_num=1, train_loss_step=1.470, val_loss=2.860, train_loss_epoch=2.000]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 213:  61%|████▎  | 41/67 [00:11<00:07,  3.49it/s, loss=2.18, v_num=1, train_loss_step=1.470, val_loss=2.860, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 213:  63%|████▍  | 42/67 [00:12<00:07,  3.48it/s, loss=2.18, v_num=1, train_loss_step=1.470, val_loss=2.860, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 213:  64%|████▍  | 43/67 [00:12<00:06,  3.46it/s, loss=2.18, v_num=1, train_loss_step=1.470, val_loss=2.860, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 213:  66%|████▌  | 44/67 [00:12<00:06,  3.45it/s, loss=2.18, v_num=1, train_loss_step=1.470, val_loss=2.860, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 213:  67%|████▋  | 45/67 [00:13<00:06,  3.43it/s, loss=2.18, v_num=1, train_loss_step=1.470, val_loss=2.860, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 213:  69%|████▊  | 46/67 [00:13<00:06,  3.42it/s, loss=2.18, v_num=1, train_loss_step=1.470, val_loss=2.860, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 213:  70%|████▉  | 47/67 [00:13<00:05,  3.41it/s, loss=2.18, v_num=1, train_loss_step=1.470, val_loss=2.860, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 213:  72%|█████  | 48/67 [00:14<00:05,  3.40it/s, loss=2.18, v_num=1, train_loss_step=1.470, val_loss=2.860, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 213:  73%|█████  | 49/67 [00:14<00:05,  3.39it/s, loss=2.18, v_num=1, train_loss_step=1.470, val_loss=2.860, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 213:  75%|█████▏ | 50/67 [00:14<00:05,  3.38it/s, loss=2.18, v_num=1, train_loss_step=1.470, val_loss=2.860, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 213:  76%|█████▎ | 51/67 [00:15<00:04,  3.33it/s, loss=2.18, v_num=1, train_loss_step=1.470, val_loss=2.860, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 213:  78%|█████▍ | 52/67 [00:15<00:04,  3.32it/s, loss=2.18, v_num=1, train_loss_step=1.470, val_loss=2.860, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 213:  79%|█████▌ | 53/67 [00:15<00:04,  3.32it/s, loss=2.18, v_num=1, train_loss_step=1.470, val_loss=2.860, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 213:  81%|█████▋ | 54/67 [00:16<00:03,  3.31it/s, loss=2.18, v_num=1, train_loss_step=1.470, val_loss=2.860, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 213:  82%|█████▋ | 55/67 [00:16<00:03,  3.30it/s, loss=2.18, v_num=1, train_loss_step=1.470, val_loss=2.860, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 213:  84%|█████▊ | 56/67 [00:17<00:03,  3.29it/s, loss=2.18, v_num=1, train_loss_step=1.470, val_loss=2.860, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 213:  85%|█████▉ | 57/67 [00:17<00:03,  3.28it/s, loss=2.18, v_num=1, train_loss_step=1.470, val_loss=2.860, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 213:  87%|██████ | 58/67 [00:17<00:02,  3.27it/s, loss=2.18, v_num=1, train_loss_step=1.470, val_loss=2.860, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 213:  88%|██████▏| 59/67 [00:18<00:02,  3.26it/s, loss=2.18, v_num=1, train_loss_step=1.470, val_loss=2.860, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 213:  90%|██████▎| 60/67 [00:18<00:02,  3.25it/s, loss=2.18, v_num=1, train_loss_step=1.470, val_loss=2.860, train_loss_epoch=2.000]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 213:  91%|██████▎| 61/67 [00:19<00:01,  3.21it/s, loss=2.18, v_num=1, train_loss_step=1.470, val_loss=2.860, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 213:  93%|██████▍| 62/67 [00:19<00:01,  3.20it/s, loss=2.18, v_num=1, train_loss_step=1.470, val_loss=2.860, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 213:  94%|██████▌| 63/67 [00:19<00:01,  3.20it/s, loss=2.18, v_num=1, train_loss_step=1.470, val_loss=2.860, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 213:  96%|██████▋| 64/67 [00:20<00:00,  3.20it/s, loss=2.18, v_num=1, train_loss_step=1.470, val_loss=2.860, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 213:  97%|██████▊| 65/67 [00:20<00:00,  3.19it/s, loss=2.18, v_num=1, train_loss_step=1.470, val_loss=2.860, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 213:  99%|██████▉| 66/67 [00:20<00:00,  3.19it/s, loss=2.18, v_num=1, train_loss_step=1.470, val_loss=2.860, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 213: 100%|███████| 67/67 [00:21<00:00,  3.12it/s, loss=2.18, v_num=1, train_loss_step=1.470, val_loss=2.860, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 214:  45%|███▏   | 30/67 [00:05<00:07,  5.15it/s, loss=1.94, v_num=1, train_loss_step=0.818, val_loss=2.860, train_loss_epoch=2.130]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 214:  46%|███▏   | 31/67 [00:08<00:09,  3.85it/s, loss=1.94, v_num=1, train_loss_step=0.818, val_loss=2.860, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 214:  48%|███▎   | 32/67 [00:08<00:09,  3.81it/s, loss=1.94, v_num=1, train_loss_step=0.818, val_loss=2.860, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 214:  49%|███▍   | 33/67 [00:08<00:09,  3.77it/s, loss=1.94, v_num=1, train_loss_step=0.818, val_loss=2.860, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 214:  51%|███▌   | 34/67 [00:09<00:08,  3.74it/s, loss=1.94, v_num=1, train_loss_step=0.818, val_loss=2.860, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 214:  52%|███▋   | 35/67 [00:09<00:08,  3.71it/s, loss=1.94, v_num=1, train_loss_step=0.818, val_loss=2.860, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 214:  54%|███▊   | 36/67 [00:09<00:08,  3.69it/s, loss=1.94, v_num=1, train_loss_step=0.818, val_loss=2.860, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 214:  55%|███▊   | 37/67 [00:10<00:08,  3.65it/s, loss=1.94, v_num=1, train_loss_step=0.818, val_loss=2.860, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 214:  57%|███▉   | 38/67 [00:10<00:07,  3.63it/s, loss=1.94, v_num=1, train_loss_step=0.818, val_loss=2.860, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 214:  58%|████   | 39/67 [00:10<00:07,  3.60it/s, loss=1.94, v_num=1, train_loss_step=0.818, val_loss=2.860, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 214:  60%|████▏  | 40/67 [00:11<00:07,  3.58it/s, loss=1.94, v_num=1, train_loss_step=0.818, val_loss=2.860, train_loss_epoch=2.130]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 214:  61%|████▎  | 41/67 [00:11<00:07,  3.51it/s, loss=1.94, v_num=1, train_loss_step=0.818, val_loss=2.860, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 214:  63%|████▍  | 42/67 [00:12<00:07,  3.50it/s, loss=1.94, v_num=1, train_loss_step=0.818, val_loss=2.860, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 214:  64%|████▍  | 43/67 [00:12<00:06,  3.48it/s, loss=1.94, v_num=1, train_loss_step=0.818, val_loss=2.860, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 214:  66%|████▌  | 44/67 [00:12<00:06,  3.46it/s, loss=1.94, v_num=1, train_loss_step=0.818, val_loss=2.860, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 214:  67%|████▋  | 45/67 [00:13<00:06,  3.45it/s, loss=1.94, v_num=1, train_loss_step=0.818, val_loss=2.860, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 214:  69%|████▊  | 46/67 [00:13<00:06,  3.43it/s, loss=1.94, v_num=1, train_loss_step=0.818, val_loss=2.860, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 214:  70%|████▉  | 47/67 [00:13<00:05,  3.43it/s, loss=1.94, v_num=1, train_loss_step=0.818, val_loss=2.860, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 214:  72%|█████  | 48/67 [00:14<00:05,  3.42it/s, loss=1.94, v_num=1, train_loss_step=0.818, val_loss=2.860, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 214:  73%|█████  | 49/67 [00:14<00:05,  3.41it/s, loss=1.94, v_num=1, train_loss_step=0.818, val_loss=2.860, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 214:  75%|█████▏ | 50/67 [00:14<00:05,  3.40it/s, loss=1.94, v_num=1, train_loss_step=0.818, val_loss=2.860, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 214:  76%|█████▎ | 51/67 [00:15<00:04,  3.35it/s, loss=1.94, v_num=1, train_loss_step=0.818, val_loss=2.860, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 214:  78%|█████▍ | 52/67 [00:15<00:04,  3.34it/s, loss=1.94, v_num=1, train_loss_step=0.818, val_loss=2.860, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 214:  79%|█████▌ | 53/67 [00:15<00:04,  3.33it/s, loss=1.94, v_num=1, train_loss_step=0.818, val_loss=2.860, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 214:  81%|█████▋ | 54/67 [00:16<00:03,  3.32it/s, loss=1.94, v_num=1, train_loss_step=0.818, val_loss=2.860, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 214:  82%|█████▋ | 55/67 [00:16<00:03,  3.31it/s, loss=1.94, v_num=1, train_loss_step=0.818, val_loss=2.860, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 214:  84%|█████▊ | 56/67 [00:16<00:03,  3.30it/s, loss=1.94, v_num=1, train_loss_step=0.818, val_loss=2.860, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 214:  85%|█████▉ | 57/67 [00:17<00:03,  3.29it/s, loss=1.94, v_num=1, train_loss_step=0.818, val_loss=2.860, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 214:  87%|██████ | 58/67 [00:17<00:02,  3.28it/s, loss=1.94, v_num=1, train_loss_step=0.818, val_loss=2.860, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 214:  88%|██████▏| 59/67 [00:18<00:02,  3.27it/s, loss=1.94, v_num=1, train_loss_step=0.818, val_loss=2.860, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 214:  90%|██████▎| 60/67 [00:18<00:02,  3.26it/s, loss=1.94, v_num=1, train_loss_step=0.818, val_loss=2.860, train_loss_epoch=2.130]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 214:  91%|██████▎| 61/67 [00:19<00:01,  3.21it/s, loss=1.94, v_num=1, train_loss_step=0.818, val_loss=2.860, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 214:  93%|██████▍| 62/67 [00:19<00:01,  3.20it/s, loss=1.94, v_num=1, train_loss_step=0.818, val_loss=2.860, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 214:  94%|██████▌| 63/67 [00:19<00:01,  3.20it/s, loss=1.94, v_num=1, train_loss_step=0.818, val_loss=2.860, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 214:  96%|██████▋| 64/67 [00:20<00:00,  3.20it/s, loss=1.94, v_num=1, train_loss_step=0.818, val_loss=2.860, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 214:  97%|██████▊| 65/67 [00:20<00:00,  3.20it/s, loss=1.94, v_num=1, train_loss_step=0.818, val_loss=2.860, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 214:  99%|██████▉| 66/67 [00:20<00:00,  3.19it/s, loss=1.94, v_num=1, train_loss_step=0.818, val_loss=2.860, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 214: 100%|███████| 67/67 [00:21<00:00,  3.13it/s, loss=1.94, v_num=1, train_loss_step=0.818, val_loss=2.860, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 215:  45%|███▏   | 30/67 [00:05<00:07,  5.09it/s, loss=2.18, v_num=1, train_loss_step=0.632, val_loss=2.860, train_loss_epoch=2.350]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 215:  46%|███▏   | 31/67 [00:08<00:09,  3.85it/s, loss=2.18, v_num=1, train_loss_step=0.632, val_loss=2.860, train_loss_epoch=2.350]\u001b[A\n",
      "Epoch 215:  48%|███▎   | 32/67 [00:08<00:09,  3.81it/s, loss=2.18, v_num=1, train_loss_step=0.632, val_loss=2.860, train_loss_epoch=2.350]\u001b[A\n",
      "Epoch 215:  49%|███▍   | 33/67 [00:08<00:09,  3.78it/s, loss=2.18, v_num=1, train_loss_step=0.632, val_loss=2.860, train_loss_epoch=2.350]\u001b[A\n",
      "Epoch 215:  51%|███▌   | 34/67 [00:09<00:08,  3.74it/s, loss=2.18, v_num=1, train_loss_step=0.632, val_loss=2.860, train_loss_epoch=2.350]\u001b[A\n",
      "Epoch 215:  52%|███▋   | 35/67 [00:09<00:08,  3.71it/s, loss=2.18, v_num=1, train_loss_step=0.632, val_loss=2.860, train_loss_epoch=2.350]\u001b[A\n",
      "Epoch 215:  54%|███▊   | 36/67 [00:09<00:08,  3.68it/s, loss=2.18, v_num=1, train_loss_step=0.632, val_loss=2.860, train_loss_epoch=2.350]\u001b[A\n",
      "Epoch 215:  55%|███▊   | 37/67 [00:10<00:08,  3.65it/s, loss=2.18, v_num=1, train_loss_step=0.632, val_loss=2.860, train_loss_epoch=2.350]\u001b[A\n",
      "Epoch 215:  57%|███▉   | 38/67 [00:10<00:07,  3.63it/s, loss=2.18, v_num=1, train_loss_step=0.632, val_loss=2.860, train_loss_epoch=2.350]\u001b[A\n",
      "Epoch 215:  58%|████   | 39/67 [00:10<00:07,  3.61it/s, loss=2.18, v_num=1, train_loss_step=0.632, val_loss=2.860, train_loss_epoch=2.350]\u001b[A\n",
      "Epoch 215:  60%|████▏  | 40/67 [00:11<00:07,  3.58it/s, loss=2.18, v_num=1, train_loss_step=0.632, val_loss=2.860, train_loss_epoch=2.350]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 215:  61%|████▎  | 41/67 [00:11<00:07,  3.50it/s, loss=2.18, v_num=1, train_loss_step=0.632, val_loss=2.860, train_loss_epoch=2.350]\u001b[A\n",
      "Epoch 215:  63%|████▍  | 42/67 [00:12<00:07,  3.48it/s, loss=2.18, v_num=1, train_loss_step=0.632, val_loss=2.860, train_loss_epoch=2.350]\u001b[A\n",
      "Epoch 215:  64%|████▍  | 43/67 [00:12<00:06,  3.46it/s, loss=2.18, v_num=1, train_loss_step=0.632, val_loss=2.860, train_loss_epoch=2.350]\u001b[A\n",
      "Epoch 215:  66%|████▌  | 44/67 [00:12<00:06,  3.45it/s, loss=2.18, v_num=1, train_loss_step=0.632, val_loss=2.860, train_loss_epoch=2.350]\u001b[A\n",
      "Epoch 215:  67%|████▋  | 45/67 [00:13<00:06,  3.43it/s, loss=2.18, v_num=1, train_loss_step=0.632, val_loss=2.860, train_loss_epoch=2.350]\u001b[A\n",
      "Epoch 215:  69%|████▊  | 46/67 [00:13<00:06,  3.42it/s, loss=2.18, v_num=1, train_loss_step=0.632, val_loss=2.860, train_loss_epoch=2.350]\u001b[A\n",
      "Epoch 215:  70%|████▉  | 47/67 [00:13<00:05,  3.41it/s, loss=2.18, v_num=1, train_loss_step=0.632, val_loss=2.860, train_loss_epoch=2.350]\u001b[A\n",
      "Epoch 215:  72%|█████  | 48/67 [00:14<00:05,  3.40it/s, loss=2.18, v_num=1, train_loss_step=0.632, val_loss=2.860, train_loss_epoch=2.350]\u001b[A\n",
      "Epoch 215:  73%|█████  | 49/67 [00:14<00:05,  3.39it/s, loss=2.18, v_num=1, train_loss_step=0.632, val_loss=2.860, train_loss_epoch=2.350]\u001b[A\n",
      "Epoch 215:  75%|█████▏ | 50/67 [00:14<00:05,  3.37it/s, loss=2.18, v_num=1, train_loss_step=0.632, val_loss=2.860, train_loss_epoch=2.350]\u001b[A\n",
      "Epoch 215:  76%|█████▎ | 51/67 [00:15<00:04,  3.32it/s, loss=2.18, v_num=1, train_loss_step=0.632, val_loss=2.860, train_loss_epoch=2.350]\u001b[A\n",
      "Epoch 215:  78%|█████▍ | 52/67 [00:15<00:04,  3.31it/s, loss=2.18, v_num=1, train_loss_step=0.632, val_loss=2.860, train_loss_epoch=2.350]\u001b[A\n",
      "Epoch 215:  79%|█████▌ | 53/67 [00:16<00:04,  3.31it/s, loss=2.18, v_num=1, train_loss_step=0.632, val_loss=2.860, train_loss_epoch=2.350]\u001b[A\n",
      "Epoch 215:  81%|█████▋ | 54/67 [00:16<00:03,  3.30it/s, loss=2.18, v_num=1, train_loss_step=0.632, val_loss=2.860, train_loss_epoch=2.350]\u001b[A\n",
      "Epoch 215:  82%|█████▋ | 55/67 [00:16<00:03,  3.29it/s, loss=2.18, v_num=1, train_loss_step=0.632, val_loss=2.860, train_loss_epoch=2.350]\u001b[A\n",
      "Epoch 215:  84%|█████▊ | 56/67 [00:17<00:03,  3.29it/s, loss=2.18, v_num=1, train_loss_step=0.632, val_loss=2.860, train_loss_epoch=2.350]\u001b[A\n",
      "Epoch 215:  85%|█████▉ | 57/67 [00:17<00:03,  3.28it/s, loss=2.18, v_num=1, train_loss_step=0.632, val_loss=2.860, train_loss_epoch=2.350]\u001b[A\n",
      "Epoch 215:  87%|██████ | 58/67 [00:17<00:02,  3.27it/s, loss=2.18, v_num=1, train_loss_step=0.632, val_loss=2.860, train_loss_epoch=2.350]\u001b[A\n",
      "Epoch 215:  88%|██████▏| 59/67 [00:18<00:02,  3.26it/s, loss=2.18, v_num=1, train_loss_step=0.632, val_loss=2.860, train_loss_epoch=2.350]\u001b[A\n",
      "Epoch 215:  90%|██████▎| 60/67 [00:18<00:02,  3.26it/s, loss=2.18, v_num=1, train_loss_step=0.632, val_loss=2.860, train_loss_epoch=2.350]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 215:  91%|██████▎| 61/67 [00:18<00:01,  3.22it/s, loss=2.18, v_num=1, train_loss_step=0.632, val_loss=2.860, train_loss_epoch=2.350]\u001b[A\n",
      "Epoch 215:  93%|██████▍| 62/67 [00:19<00:01,  3.21it/s, loss=2.18, v_num=1, train_loss_step=0.632, val_loss=2.860, train_loss_epoch=2.350]\u001b[A\n",
      "Epoch 215:  94%|██████▌| 63/67 [00:19<00:01,  3.21it/s, loss=2.18, v_num=1, train_loss_step=0.632, val_loss=2.860, train_loss_epoch=2.350]\u001b[A\n",
      "Epoch 215:  96%|██████▋| 64/67 [00:19<00:00,  3.20it/s, loss=2.18, v_num=1, train_loss_step=0.632, val_loss=2.860, train_loss_epoch=2.350]\u001b[A\n",
      "Epoch 215:  97%|██████▊| 65/67 [00:20<00:00,  3.20it/s, loss=2.18, v_num=1, train_loss_step=0.632, val_loss=2.860, train_loss_epoch=2.350]\u001b[A\n",
      "Epoch 215:  99%|██████▉| 66/67 [00:20<00:00,  3.20it/s, loss=2.18, v_num=1, train_loss_step=0.632, val_loss=2.860, train_loss_epoch=2.350]\u001b[A\n",
      "Epoch 215: 100%|███████| 67/67 [00:21<00:00,  3.12it/s, loss=2.18, v_num=1, train_loss_step=0.632, val_loss=2.860, train_loss_epoch=2.350]\u001b[A\n",
      "Epoch 216:  45%|███▏   | 30/67 [00:06<00:07,  4.86it/s, loss=1.88, v_num=1, train_loss_step=2.100, val_loss=2.860, train_loss_epoch=2.140]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 216:  46%|███▏   | 31/67 [00:08<00:09,  3.69it/s, loss=1.88, v_num=1, train_loss_step=2.100, val_loss=2.860, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 216:  48%|███▎   | 32/67 [00:08<00:09,  3.66it/s, loss=1.88, v_num=1, train_loss_step=2.100, val_loss=2.860, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 216:  49%|███▍   | 33/67 [00:09<00:09,  3.64it/s, loss=1.88, v_num=1, train_loss_step=2.100, val_loss=2.860, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 216:  51%|███▌   | 34/67 [00:09<00:09,  3.62it/s, loss=1.88, v_num=1, train_loss_step=2.100, val_loss=2.860, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 216:  52%|███▋   | 35/67 [00:09<00:08,  3.60it/s, loss=1.88, v_num=1, train_loss_step=2.100, val_loss=2.860, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 216:  54%|███▊   | 36/67 [00:10<00:08,  3.58it/s, loss=1.88, v_num=1, train_loss_step=2.100, val_loss=2.860, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 216:  55%|███▊   | 37/67 [00:10<00:08,  3.55it/s, loss=1.88, v_num=1, train_loss_step=2.100, val_loss=2.860, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 216:  57%|███▉   | 38/67 [00:10<00:08,  3.53it/s, loss=1.88, v_num=1, train_loss_step=2.100, val_loss=2.860, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 216:  58%|████   | 39/67 [00:11<00:07,  3.52it/s, loss=1.88, v_num=1, train_loss_step=2.100, val_loss=2.860, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 216:  60%|████▏  | 40/67 [00:11<00:07,  3.50it/s, loss=1.88, v_num=1, train_loss_step=2.100, val_loss=2.860, train_loss_epoch=2.140]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 216:  61%|████▎  | 41/67 [00:11<00:07,  3.43it/s, loss=1.88, v_num=1, train_loss_step=2.100, val_loss=2.860, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 216:  63%|████▍  | 42/67 [00:12<00:07,  3.41it/s, loss=1.88, v_num=1, train_loss_step=2.100, val_loss=2.860, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 216:  64%|████▍  | 43/67 [00:12<00:07,  3.40it/s, loss=1.88, v_num=1, train_loss_step=2.100, val_loss=2.860, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 216:  66%|████▌  | 44/67 [00:12<00:06,  3.39it/s, loss=1.88, v_num=1, train_loss_step=2.100, val_loss=2.860, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 216:  67%|████▋  | 45/67 [00:13<00:06,  3.37it/s, loss=1.88, v_num=1, train_loss_step=2.100, val_loss=2.860, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 216:  69%|████▊  | 46/67 [00:13<00:06,  3.36it/s, loss=1.88, v_num=1, train_loss_step=2.100, val_loss=2.860, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 216:  70%|████▉  | 47/67 [00:14<00:05,  3.36it/s, loss=1.88, v_num=1, train_loss_step=2.100, val_loss=2.860, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 216:  72%|█████  | 48/67 [00:14<00:05,  3.35it/s, loss=1.88, v_num=1, train_loss_step=2.100, val_loss=2.860, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 216:  73%|█████  | 49/67 [00:14<00:05,  3.34it/s, loss=1.88, v_num=1, train_loss_step=2.100, val_loss=2.860, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 216:  75%|█████▏ | 50/67 [00:14<00:05,  3.34it/s, loss=1.88, v_num=1, train_loss_step=2.100, val_loss=2.860, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 216:  76%|█████▎ | 51/67 [00:15<00:04,  3.29it/s, loss=1.88, v_num=1, train_loss_step=2.100, val_loss=2.860, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 216:  78%|█████▍ | 52/67 [00:15<00:04,  3.28it/s, loss=1.88, v_num=1, train_loss_step=2.100, val_loss=2.860, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 216:  79%|█████▌ | 53/67 [00:16<00:04,  3.28it/s, loss=1.88, v_num=1, train_loss_step=2.100, val_loss=2.860, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 216:  81%|█████▋ | 54/67 [00:16<00:03,  3.27it/s, loss=1.88, v_num=1, train_loss_step=2.100, val_loss=2.860, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 216:  82%|█████▋ | 55/67 [00:16<00:03,  3.27it/s, loss=1.88, v_num=1, train_loss_step=2.100, val_loss=2.860, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 216:  84%|█████▊ | 56/67 [00:17<00:03,  3.26it/s, loss=1.88, v_num=1, train_loss_step=2.100, val_loss=2.860, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 216:  85%|█████▉ | 57/67 [00:17<00:03,  3.25it/s, loss=1.88, v_num=1, train_loss_step=2.100, val_loss=2.860, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 216:  87%|██████ | 58/67 [00:17<00:02,  3.25it/s, loss=1.88, v_num=1, train_loss_step=2.100, val_loss=2.860, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 216:  88%|██████▏| 59/67 [00:18<00:02,  3.24it/s, loss=1.88, v_num=1, train_loss_step=2.100, val_loss=2.860, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 216:  90%|██████▎| 60/67 [00:18<00:02,  3.24it/s, loss=1.88, v_num=1, train_loss_step=2.100, val_loss=2.860, train_loss_epoch=2.140]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 216:  91%|██████▎| 61/67 [00:19<00:01,  3.20it/s, loss=1.88, v_num=1, train_loss_step=2.100, val_loss=2.860, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 216:  93%|██████▍| 62/67 [00:19<00:01,  3.19it/s, loss=1.88, v_num=1, train_loss_step=2.100, val_loss=2.860, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 216:  94%|██████▌| 63/67 [00:19<00:01,  3.18it/s, loss=1.88, v_num=1, train_loss_step=2.100, val_loss=2.860, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 216:  96%|██████▋| 64/67 [00:24<00:01,  2.58it/s, loss=1.88, v_num=1, train_loss_step=2.100, val_loss=2.860, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 216:  97%|██████▊| 65/67 [00:25<00:00,  2.58it/s, loss=1.88, v_num=1, train_loss_step=2.100, val_loss=2.860, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 216:  99%|██████▉| 66/67 [00:25<00:00,  2.59it/s, loss=1.88, v_num=1, train_loss_step=2.100, val_loss=2.860, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 216: 100%|███████| 67/67 [00:26<00:00,  2.55it/s, loss=1.88, v_num=1, train_loss_step=2.100, val_loss=2.850, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 217:  45%|███▏   | 30/67 [00:05<00:07,  5.06it/s, loss=1.82, v_num=1, train_loss_step=3.260, val_loss=2.850, train_loss_epoch=1.730]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 217:  46%|███▏   | 31/67 [00:08<00:09,  3.84it/s, loss=1.82, v_num=1, train_loss_step=3.260, val_loss=2.850, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 217:  48%|███▎   | 32/67 [00:08<00:09,  3.80it/s, loss=1.82, v_num=1, train_loss_step=3.260, val_loss=2.850, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 217:  49%|███▍   | 33/67 [00:08<00:09,  3.77it/s, loss=1.82, v_num=1, train_loss_step=3.260, val_loss=2.850, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 217:  51%|███▌   | 34/67 [00:09<00:08,  3.74it/s, loss=1.82, v_num=1, train_loss_step=3.260, val_loss=2.850, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 217:  52%|███▋   | 35/67 [00:09<00:08,  3.72it/s, loss=1.82, v_num=1, train_loss_step=3.260, val_loss=2.850, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 217:  54%|███▊   | 36/67 [00:09<00:08,  3.70it/s, loss=1.82, v_num=1, train_loss_step=3.260, val_loss=2.850, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 217:  55%|███▊   | 37/67 [00:10<00:08,  3.67it/s, loss=1.82, v_num=1, train_loss_step=3.260, val_loss=2.850, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 217:  57%|███▉   | 38/67 [00:10<00:07,  3.65it/s, loss=1.82, v_num=1, train_loss_step=3.260, val_loss=2.850, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 217:  58%|████   | 39/67 [00:10<00:07,  3.63it/s, loss=1.82, v_num=1, train_loss_step=3.260, val_loss=2.850, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 217:  60%|████▏  | 40/67 [00:11<00:07,  3.60it/s, loss=1.82, v_num=1, train_loss_step=3.260, val_loss=2.850, train_loss_epoch=1.730]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 217:  61%|████▎  | 41/67 [00:11<00:07,  3.54it/s, loss=1.82, v_num=1, train_loss_step=3.260, val_loss=2.850, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 217:  63%|████▍  | 42/67 [00:11<00:07,  3.52it/s, loss=1.82, v_num=1, train_loss_step=3.260, val_loss=2.850, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 217:  64%|████▍  | 43/67 [00:12<00:06,  3.50it/s, loss=1.82, v_num=1, train_loss_step=3.260, val_loss=2.850, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 217:  66%|████▌  | 44/67 [00:12<00:06,  3.49it/s, loss=1.82, v_num=1, train_loss_step=3.260, val_loss=2.850, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 217:  67%|████▋  | 45/67 [00:12<00:06,  3.48it/s, loss=1.82, v_num=1, train_loss_step=3.260, val_loss=2.850, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 217:  69%|████▊  | 46/67 [00:13<00:06,  3.47it/s, loss=1.82, v_num=1, train_loss_step=3.260, val_loss=2.850, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 217:  70%|████▉  | 47/67 [00:13<00:05,  3.46it/s, loss=1.82, v_num=1, train_loss_step=3.260, val_loss=2.850, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 217:  72%|█████  | 48/67 [00:13<00:05,  3.45it/s, loss=1.82, v_num=1, train_loss_step=3.260, val_loss=2.850, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 217:  73%|█████  | 49/67 [00:14<00:05,  3.43it/s, loss=1.82, v_num=1, train_loss_step=3.260, val_loss=2.850, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 217:  75%|█████▏ | 50/67 [00:14<00:04,  3.42it/s, loss=1.82, v_num=1, train_loss_step=3.260, val_loss=2.850, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 217:  76%|█████▎ | 51/67 [00:15<00:04,  3.36it/s, loss=1.82, v_num=1, train_loss_step=3.260, val_loss=2.850, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 217:  78%|█████▍ | 52/67 [00:19<00:05,  2.65it/s, loss=1.82, v_num=1, train_loss_step=3.260, val_loss=2.850, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 217:  79%|█████▌ | 53/67 [00:19<00:05,  2.66it/s, loss=1.82, v_num=1, train_loss_step=3.260, val_loss=2.850, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 217:  81%|█████▋ | 54/67 [00:20<00:04,  2.66it/s, loss=1.82, v_num=1, train_loss_step=3.260, val_loss=2.850, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 217:  82%|█████▋ | 55/67 [00:20<00:04,  2.67it/s, loss=1.82, v_num=1, train_loss_step=3.260, val_loss=2.850, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 217:  84%|█████▊ | 56/67 [00:20<00:04,  2.67it/s, loss=1.82, v_num=1, train_loss_step=3.260, val_loss=2.850, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 217:  85%|█████▉ | 57/67 [00:21<00:03,  2.68it/s, loss=1.82, v_num=1, train_loss_step=3.260, val_loss=2.850, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 217:  87%|██████ | 58/67 [00:21<00:03,  2.68it/s, loss=1.82, v_num=1, train_loss_step=3.260, val_loss=2.850, train_loss_epoch=1.730]\u001b[A\n",
      "Validation DataLoader 0:  78%|███████████████████████████████████████████████████████████▌                | 29/37 [00:14<00:03,  2.00it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 229:  46%|███▏   | 31/67 [00:08<00:09,  3.80it/s, loss=1.55, v_num=1, train_loss_step=2.540, val_loss=2.840, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 229:  48%|███▎   | 32/67 [00:08<00:09,  3.76it/s, loss=1.55, v_num=1, train_loss_step=2.540, val_loss=2.840, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 229:  49%|███▍   | 33/67 [00:08<00:09,  3.74it/s, loss=1.55, v_num=1, train_loss_step=2.540, val_loss=2.840, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 229:  51%|███▌   | 34/67 [00:09<00:08,  3.71it/s, loss=1.55, v_num=1, train_loss_step=2.540, val_loss=2.840, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 229:  52%|███▋   | 35/67 [00:09<00:08,  3.69it/s, loss=1.55, v_num=1, train_loss_step=2.540, val_loss=2.840, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 229:  54%|███▊   | 36/67 [00:09<00:08,  3.67it/s, loss=1.55, v_num=1, train_loss_step=2.540, val_loss=2.840, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 229:  55%|███▊   | 37/67 [00:10<00:08,  3.64it/s, loss=1.55, v_num=1, train_loss_step=2.540, val_loss=2.840, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 229:  57%|███▉   | 38/67 [00:10<00:08,  3.62it/s, loss=1.55, v_num=1, train_loss_step=2.540, val_loss=2.840, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 229:  58%|████   | 39/67 [00:10<00:07,  3.60it/s, loss=1.55, v_num=1, train_loss_step=2.540, val_loss=2.840, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 229:  60%|████▏  | 40/67 [00:11<00:07,  3.58it/s, loss=1.55, v_num=1, train_loss_step=2.540, val_loss=2.840, train_loss_epoch=1.930]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 229:  61%|████▎  | 41/67 [00:11<00:07,  3.53it/s, loss=1.55, v_num=1, train_loss_step=2.540, val_loss=2.840, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 229:  63%|████▍  | 42/67 [00:11<00:07,  3.52it/s, loss=1.55, v_num=1, train_loss_step=2.540, val_loss=2.840, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 229:  64%|████▍  | 43/67 [00:12<00:06,  3.50it/s, loss=1.55, v_num=1, train_loss_step=2.540, val_loss=2.840, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 229:  66%|████▌  | 44/67 [00:12<00:06,  3.49it/s, loss=1.55, v_num=1, train_loss_step=2.540, val_loss=2.840, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 229:  67%|████▋  | 45/67 [00:12<00:06,  3.48it/s, loss=1.55, v_num=1, train_loss_step=2.540, val_loss=2.840, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 229:  69%|████▊  | 46/67 [00:13<00:06,  3.46it/s, loss=1.55, v_num=1, train_loss_step=2.540, val_loss=2.840, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 229:  70%|████▉  | 47/67 [00:13<00:05,  3.45it/s, loss=1.55, v_num=1, train_loss_step=2.540, val_loss=2.840, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 229:  72%|█████  | 48/67 [00:13<00:05,  3.44it/s, loss=1.55, v_num=1, train_loss_step=2.540, val_loss=2.840, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 229:  73%|█████  | 49/67 [00:14<00:05,  3.42it/s, loss=1.55, v_num=1, train_loss_step=2.540, val_loss=2.840, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 229:  75%|█████▏ | 50/67 [00:14<00:04,  3.42it/s, loss=1.55, v_num=1, train_loss_step=2.540, val_loss=2.840, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 229:  76%|█████▎ | 51/67 [00:15<00:04,  3.36it/s, loss=1.55, v_num=1, train_loss_step=2.540, val_loss=2.840, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 229:  78%|█████▍ | 52/67 [00:15<00:04,  3.35it/s, loss=1.55, v_num=1, train_loss_step=2.540, val_loss=2.840, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 229:  79%|█████▌ | 53/67 [00:15<00:04,  3.34it/s, loss=1.55, v_num=1, train_loss_step=2.540, val_loss=2.840, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 229:  81%|█████▋ | 54/67 [00:16<00:03,  3.34it/s, loss=1.55, v_num=1, train_loss_step=2.540, val_loss=2.840, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 229:  82%|█████▋ | 55/67 [00:16<00:03,  3.33it/s, loss=1.55, v_num=1, train_loss_step=2.540, val_loss=2.840, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 229:  84%|█████▊ | 56/67 [00:16<00:03,  3.32it/s, loss=1.55, v_num=1, train_loss_step=2.540, val_loss=2.840, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 229:  85%|█████▉ | 57/67 [00:17<00:03,  3.32it/s, loss=1.55, v_num=1, train_loss_step=2.540, val_loss=2.840, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 229:  87%|██████ | 58/67 [00:17<00:02,  3.31it/s, loss=1.55, v_num=1, train_loss_step=2.540, val_loss=2.840, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 229:  88%|██████▏| 59/67 [00:17<00:02,  3.30it/s, loss=1.55, v_num=1, train_loss_step=2.540, val_loss=2.840, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 229:  90%|██████▎| 60/67 [00:18<00:02,  3.30it/s, loss=1.55, v_num=1, train_loss_step=2.540, val_loss=2.840, train_loss_epoch=1.930]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 229:  91%|██████▎| 61/67 [00:18<00:01,  3.26it/s, loss=1.55, v_num=1, train_loss_step=2.540, val_loss=2.840, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 229:  93%|██████▍| 62/67 [00:19<00:01,  3.26it/s, loss=1.55, v_num=1, train_loss_step=2.540, val_loss=2.840, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 229:  94%|██████▌| 63/67 [00:19<00:01,  3.26it/s, loss=1.55, v_num=1, train_loss_step=2.540, val_loss=2.840, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 229:  96%|██████▋| 64/67 [00:19<00:00,  3.25it/s, loss=1.55, v_num=1, train_loss_step=2.540, val_loss=2.840, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 229:  97%|██████▊| 65/67 [00:19<00:00,  3.25it/s, loss=1.55, v_num=1, train_loss_step=2.540, val_loss=2.840, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 229:  99%|██████▉| 66/67 [00:20<00:00,  3.25it/s, loss=1.55, v_num=1, train_loss_step=2.540, val_loss=2.840, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 229: 100%|███████| 67/67 [00:21<00:00,  3.18it/s, loss=1.55, v_num=1, train_loss_step=2.540, val_loss=2.840, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 230:  37%|██▌    | 25/67 [00:04<00:08,  5.03it/s, loss=1.73, v_num=1, train_loss_step=1.340, val_loss=2.840, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 233:  81%|██████▍ | 54/67 [00:15<00:03,  3.41it/s, loss=2.1, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 233:  82%|██████▌ | 55/67 [00:16<00:03,  3.40it/s, loss=2.1, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 233:  84%|██████▋ | 56/67 [00:16<00:03,  3.39it/s, loss=2.1, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 233:  85%|██████▊ | 57/67 [00:16<00:02,  3.38it/s, loss=2.1, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 233:  87%|██████▉ | 58/67 [00:17<00:02,  3.37it/s, loss=2.1, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 233:  88%|███████ | 59/67 [00:17<00:02,  3.37it/s, loss=2.1, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 233:  90%|███████▏| 60/67 [00:17<00:02,  3.36it/s, loss=2.1, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=2.310]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 233:  91%|███████▎| 61/67 [00:18<00:01,  3.33it/s, loss=2.1, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 233:  93%|███████▍| 62/67 [00:18<00:01,  3.32it/s, loss=2.1, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 233:  94%|███████▌| 63/67 [00:18<00:01,  3.32it/s, loss=2.1, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 233:  96%|███████▋| 64/67 [00:19<00:00,  3.32it/s, loss=2.1, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 233:  97%|███████▊| 65/67 [00:19<00:00,  3.32it/s, loss=2.1, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 233:  99%|███████▉| 66/67 [00:19<00:00,  3.31it/s, loss=2.1, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 233: 100%|████████| 67/67 [00:20<00:00,  3.24it/s, loss=2.1, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=2.310]\u001b[A\n",
      "Epoch 234:  45%|███▏   | 30/67 [00:05<00:07,  5.04it/s, loss=1.83, v_num=1, train_loss_step=2.870, val_loss=2.830, train_loss_epoch=1.980]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 234:  46%|███▏   | 31/67 [00:08<00:09,  3.84it/s, loss=1.83, v_num=1, train_loss_step=2.870, val_loss=2.830, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 234:  48%|███▎   | 32/67 [00:08<00:09,  3.81it/s, loss=1.83, v_num=1, train_loss_step=2.870, val_loss=2.830, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 234:  49%|███▍   | 33/67 [00:08<00:08,  3.78it/s, loss=1.83, v_num=1, train_loss_step=2.870, val_loss=2.830, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 234:  51%|███▌   | 34/67 [00:09<00:08,  3.75it/s, loss=1.83, v_num=1, train_loss_step=2.870, val_loss=2.830, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 234:  52%|███▋   | 35/67 [00:09<00:08,  3.72it/s, loss=1.83, v_num=1, train_loss_step=2.870, val_loss=2.830, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 234:  54%|███▊   | 36/67 [00:09<00:08,  3.70it/s, loss=1.83, v_num=1, train_loss_step=2.870, val_loss=2.830, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 234:  55%|███▊   | 37/67 [00:10<00:08,  3.68it/s, loss=1.83, v_num=1, train_loss_step=2.870, val_loss=2.830, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 234:  57%|███▉   | 38/67 [00:10<00:07,  3.66it/s, loss=1.83, v_num=1, train_loss_step=2.870, val_loss=2.830, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 234:  58%|████   | 39/67 [00:10<00:07,  3.64it/s, loss=1.83, v_num=1, train_loss_step=2.870, val_loss=2.830, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 234:  60%|████▏  | 40/67 [00:11<00:07,  3.63it/s, loss=1.83, v_num=1, train_loss_step=2.870, val_loss=2.830, train_loss_epoch=1.980]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 234:  61%|████▎  | 41/67 [00:11<00:07,  3.56it/s, loss=1.83, v_num=1, train_loss_step=2.870, val_loss=2.830, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 234:  63%|████▍  | 42/67 [00:11<00:07,  3.55it/s, loss=1.83, v_num=1, train_loss_step=2.870, val_loss=2.830, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 234:  64%|████▍  | 43/67 [00:12<00:06,  3.53it/s, loss=1.83, v_num=1, train_loss_step=2.870, val_loss=2.830, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 234:  66%|████▌  | 44/67 [00:12<00:06,  3.52it/s, loss=1.83, v_num=1, train_loss_step=2.870, val_loss=2.830, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 234:  67%|████▋  | 45/67 [00:12<00:06,  3.51it/s, loss=1.83, v_num=1, train_loss_step=2.870, val_loss=2.830, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 234:  69%|████▊  | 46/67 [00:13<00:06,  3.50it/s, loss=1.83, v_num=1, train_loss_step=2.870, val_loss=2.830, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 234:  70%|████▉  | 47/67 [00:13<00:05,  3.49it/s, loss=1.83, v_num=1, train_loss_step=2.870, val_loss=2.830, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 234:  72%|█████  | 48/67 [00:13<00:05,  3.48it/s, loss=1.83, v_num=1, train_loss_step=2.870, val_loss=2.830, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 234:  73%|█████  | 49/67 [00:14<00:05,  3.47it/s, loss=1.83, v_num=1, train_loss_step=2.870, val_loss=2.830, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 234:  75%|█████▏ | 50/67 [00:14<00:04,  3.46it/s, loss=1.83, v_num=1, train_loss_step=2.870, val_loss=2.830, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 234:  76%|█████▎ | 51/67 [00:14<00:04,  3.41it/s, loss=1.83, v_num=1, train_loss_step=2.870, val_loss=2.830, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 234:  78%|█████▍ | 52/67 [00:15<00:04,  3.40it/s, loss=1.83, v_num=1, train_loss_step=2.870, val_loss=2.830, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 234:  79%|█████▌ | 53/67 [00:15<00:04,  3.40it/s, loss=1.83, v_num=1, train_loss_step=2.870, val_loss=2.830, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 234:  81%|█████▋ | 54/67 [00:15<00:03,  3.39it/s, loss=1.83, v_num=1, train_loss_step=2.870, val_loss=2.830, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 234:  82%|█████▋ | 55/67 [00:16<00:03,  3.39it/s, loss=1.83, v_num=1, train_loss_step=2.870, val_loss=2.830, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 234:  84%|█████▊ | 56/67 [00:16<00:03,  3.38it/s, loss=1.83, v_num=1, train_loss_step=2.870, val_loss=2.830, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 234:  85%|█████▉ | 57/67 [00:16<00:02,  3.37it/s, loss=1.83, v_num=1, train_loss_step=2.870, val_loss=2.830, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 234:  87%|██████ | 58/67 [00:17<00:02,  3.36it/s, loss=1.83, v_num=1, train_loss_step=2.870, val_loss=2.830, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 234:  88%|██████▏| 59/67 [00:17<00:02,  3.36it/s, loss=1.83, v_num=1, train_loss_step=2.870, val_loss=2.830, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 234:  90%|██████▎| 60/67 [00:17<00:02,  3.35it/s, loss=1.83, v_num=1, train_loss_step=2.870, val_loss=2.830, train_loss_epoch=1.980]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 234:  91%|██████▎| 61/67 [00:18<00:01,  3.32it/s, loss=1.83, v_num=1, train_loss_step=2.870, val_loss=2.830, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 234:  93%|██████▍| 62/67 [00:18<00:01,  3.32it/s, loss=1.83, v_num=1, train_loss_step=2.870, val_loss=2.830, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 234:  94%|██████▌| 63/67 [00:19<00:01,  3.31it/s, loss=1.83, v_num=1, train_loss_step=2.870, val_loss=2.830, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 234:  96%|██████▋| 64/67 [00:19<00:00,  3.31it/s, loss=1.83, v_num=1, train_loss_step=2.870, val_loss=2.830, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 234:  97%|██████▊| 65/67 [00:19<00:00,  3.30it/s, loss=1.83, v_num=1, train_loss_step=2.870, val_loss=2.830, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 234:  99%|██████▉| 66/67 [00:19<00:00,  3.30it/s, loss=1.83, v_num=1, train_loss_step=2.870, val_loss=2.830, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 234: 100%|███████| 67/67 [00:20<00:00,  3.23it/s, loss=1.83, v_num=1, train_loss_step=2.870, val_loss=2.830, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 235:  45%|███▏   | 30/67 [00:05<00:07,  5.18it/s, loss=1.97, v_num=1, train_loss_step=5.390, val_loss=2.830, train_loss_epoch=1.820]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 235:  46%|███▏   | 31/67 [00:08<00:09,  3.85it/s, loss=1.97, v_num=1, train_loss_step=5.390, val_loss=2.830, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 235:  48%|███▎   | 32/67 [00:08<00:09,  3.82it/s, loss=1.97, v_num=1, train_loss_step=5.390, val_loss=2.830, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 235:  49%|███▍   | 33/67 [00:08<00:08,  3.79it/s, loss=1.97, v_num=1, train_loss_step=5.390, val_loss=2.830, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 235:  51%|███▌   | 34/67 [00:09<00:08,  3.76it/s, loss=1.97, v_num=1, train_loss_step=5.390, val_loss=2.830, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 235:  52%|███▋   | 35/67 [00:09<00:08,  3.73it/s, loss=1.97, v_num=1, train_loss_step=5.390, val_loss=2.830, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 235:  54%|███▊   | 36/67 [00:09<00:08,  3.70it/s, loss=1.97, v_num=1, train_loss_step=5.390, val_loss=2.830, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 235:  55%|███▊   | 37/67 [00:10<00:08,  3.68it/s, loss=1.97, v_num=1, train_loss_step=5.390, val_loss=2.830, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 235:  57%|███▉   | 38/67 [00:10<00:07,  3.66it/s, loss=1.97, v_num=1, train_loss_step=5.390, val_loss=2.830, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 235:  58%|████   | 39/67 [00:10<00:07,  3.65it/s, loss=1.97, v_num=1, train_loss_step=5.390, val_loss=2.830, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 235:  60%|████▏  | 40/67 [00:11<00:07,  3.62it/s, loss=1.97, v_num=1, train_loss_step=5.390, val_loss=2.830, train_loss_epoch=1.820]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 235:  61%|████▎  | 41/67 [00:11<00:07,  3.57it/s, loss=1.97, v_num=1, train_loss_step=5.390, val_loss=2.830, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 235:  63%|████▍  | 42/67 [00:11<00:07,  3.55it/s, loss=1.97, v_num=1, train_loss_step=5.390, val_loss=2.830, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 235:  64%|████▍  | 43/67 [00:12<00:06,  3.53it/s, loss=1.97, v_num=1, train_loss_step=5.390, val_loss=2.830, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 235:  66%|████▌  | 44/67 [00:12<00:06,  3.51it/s, loss=1.97, v_num=1, train_loss_step=5.390, val_loss=2.830, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 235:  67%|████▋  | 45/67 [00:12<00:06,  3.50it/s, loss=1.97, v_num=1, train_loss_step=5.390, val_loss=2.830, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 235:  69%|████▊  | 46/67 [00:13<00:06,  3.48it/s, loss=1.97, v_num=1, train_loss_step=5.390, val_loss=2.830, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 235:  70%|████▉  | 47/67 [00:13<00:05,  3.47it/s, loss=1.97, v_num=1, train_loss_step=5.390, val_loss=2.830, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 235:  72%|█████  | 48/67 [00:13<00:05,  3.46it/s, loss=1.97, v_num=1, train_loss_step=5.390, val_loss=2.830, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 235:  73%|█████  | 49/67 [00:14<00:05,  3.45it/s, loss=1.97, v_num=1, train_loss_step=5.390, val_loss=2.830, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 235:  75%|█████▏ | 50/67 [00:14<00:04,  3.44it/s, loss=1.97, v_num=1, train_loss_step=5.390, val_loss=2.830, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 235:  76%|█████▎ | 51/67 [00:14<00:04,  3.40it/s, loss=1.97, v_num=1, train_loss_step=5.390, val_loss=2.830, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 235:  78%|█████▍ | 52/67 [00:15<00:04,  3.39it/s, loss=1.97, v_num=1, train_loss_step=5.390, val_loss=2.830, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 235:  79%|█████▌ | 53/67 [00:15<00:04,  3.39it/s, loss=1.97, v_num=1, train_loss_step=5.390, val_loss=2.830, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 235:  81%|█████▋ | 54/67 [00:15<00:03,  3.38it/s, loss=1.97, v_num=1, train_loss_step=5.390, val_loss=2.830, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 235:  82%|█████▋ | 55/67 [00:16<00:03,  3.37it/s, loss=1.97, v_num=1, train_loss_step=5.390, val_loss=2.830, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 235:  84%|█████▊ | 56/67 [00:16<00:03,  3.36it/s, loss=1.97, v_num=1, train_loss_step=5.390, val_loss=2.830, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 235:  85%|█████▉ | 57/67 [00:16<00:02,  3.36it/s, loss=1.97, v_num=1, train_loss_step=5.390, val_loss=2.830, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 235:  87%|██████ | 58/67 [00:17<00:02,  3.35it/s, loss=1.97, v_num=1, train_loss_step=5.390, val_loss=2.830, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 235:  88%|██████▏| 59/67 [00:17<00:02,  3.35it/s, loss=1.97, v_num=1, train_loss_step=5.390, val_loss=2.830, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 235:  90%|██████▎| 60/67 [00:17<00:02,  3.34it/s, loss=1.97, v_num=1, train_loss_step=5.390, val_loss=2.830, train_loss_epoch=1.820]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 235:  91%|██████▎| 61/67 [00:18<00:01,  3.31it/s, loss=1.97, v_num=1, train_loss_step=5.390, val_loss=2.830, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 235:  93%|██████▍| 62/67 [00:18<00:01,  3.31it/s, loss=1.97, v_num=1, train_loss_step=5.390, val_loss=2.830, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 235:  94%|██████▌| 63/67 [00:19<00:01,  3.30it/s, loss=1.97, v_num=1, train_loss_step=5.390, val_loss=2.830, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 235:  96%|██████▋| 64/67 [00:19<00:00,  3.30it/s, loss=1.97, v_num=1, train_loss_step=5.390, val_loss=2.830, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 235:  97%|██████▊| 65/67 [00:19<00:00,  3.30it/s, loss=1.97, v_num=1, train_loss_step=5.390, val_loss=2.830, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 235:  99%|██████▉| 66/67 [00:20<00:00,  3.29it/s, loss=1.97, v_num=1, train_loss_step=5.390, val_loss=2.830, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 235: 100%|███████| 67/67 [00:20<00:00,  3.23it/s, loss=1.97, v_num=1, train_loss_step=5.390, val_loss=2.830, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 236:  45%|███▌    | 30/67 [00:05<00:07,  5.03it/s, loss=1.9, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=1.780]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 236:  46%|███▋    | 31/67 [00:08<00:09,  3.75it/s, loss=1.9, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=1.780]\u001b[A\n",
      "Epoch 236:  48%|███▊    | 32/67 [00:08<00:09,  3.72it/s, loss=1.9, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=1.780]\u001b[A\n",
      "Epoch 236:  49%|███▉    | 33/67 [00:08<00:09,  3.70it/s, loss=1.9, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=1.780]\u001b[A\n",
      "Epoch 236:  51%|████    | 34/67 [00:09<00:09,  3.66it/s, loss=1.9, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=1.780]\u001b[A\n",
      "Epoch 236:  52%|████▏   | 35/67 [00:09<00:08,  3.65it/s, loss=1.9, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=1.780]\u001b[A\n",
      "Epoch 236:  54%|████▎   | 36/67 [00:09<00:08,  3.63it/s, loss=1.9, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=1.780]\u001b[A\n",
      "Epoch 236:  55%|████▍   | 37/67 [00:10<00:08,  3.60it/s, loss=1.9, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=1.780]\u001b[A\n",
      "Epoch 236:  57%|████▌   | 38/67 [00:10<00:08,  3.58it/s, loss=1.9, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=1.780]\u001b[A\n",
      "Epoch 236:  58%|████▋   | 39/67 [00:10<00:07,  3.57it/s, loss=1.9, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=1.780]\u001b[A\n",
      "Epoch 236:  60%|████▊   | 40/67 [00:11<00:07,  3.54it/s, loss=1.9, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=1.780]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 236:  61%|████▉   | 41/67 [00:11<00:07,  3.47it/s, loss=1.9, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=1.780]\u001b[A\n",
      "Epoch 236:  63%|█████   | 42/67 [00:12<00:07,  3.45it/s, loss=1.9, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=1.780]\u001b[A\n",
      "Epoch 236:  64%|█████▏  | 43/67 [00:12<00:06,  3.44it/s, loss=1.9, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=1.780]\u001b[A\n",
      "Epoch 236:  66%|█████▎  | 44/67 [00:12<00:06,  3.43it/s, loss=1.9, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=1.780]\u001b[A\n",
      "Epoch 236:  67%|█████▎  | 45/67 [00:13<00:06,  3.42it/s, loss=1.9, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=1.780]\u001b[A\n",
      "Epoch 236:  69%|█████▍  | 46/67 [00:13<00:06,  3.40it/s, loss=1.9, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=1.780]\u001b[A\n",
      "Epoch 236:  70%|█████▌  | 47/67 [00:13<00:05,  3.39it/s, loss=1.9, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=1.780]\u001b[A\n",
      "Epoch 236:  72%|█████▋  | 48/67 [00:14<00:05,  3.38it/s, loss=1.9, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=1.780]\u001b[A\n",
      "Epoch 236:  73%|█████▊  | 49/67 [00:14<00:05,  3.37it/s, loss=1.9, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=1.780]\u001b[A\n",
      "Epoch 236:  75%|█████▉  | 50/67 [00:14<00:05,  3.36it/s, loss=1.9, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=1.780]\u001b[A\n",
      "Epoch 236:  76%|██████  | 51/67 [00:15<00:04,  3.32it/s, loss=1.9, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=1.780]\u001b[A\n",
      "Epoch 236:  78%|██████▏ | 52/67 [00:15<00:04,  3.31it/s, loss=1.9, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=1.780]\u001b[A\n",
      "Epoch 236:  79%|██████▎ | 53/67 [00:16<00:04,  3.30it/s, loss=1.9, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=1.780]\u001b[A\n",
      "Epoch 236:  81%|██████▍ | 54/67 [00:16<00:03,  3.30it/s, loss=1.9, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=1.780]\u001b[A\n",
      "Epoch 236:  82%|██████▌ | 55/67 [00:16<00:03,  3.29it/s, loss=1.9, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=1.780]\u001b[A\n",
      "Epoch 236:  84%|██████▋ | 56/67 [00:17<00:03,  3.28it/s, loss=1.9, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=1.780]\u001b[A\n",
      "Epoch 236:  85%|██████▊ | 57/67 [00:17<00:03,  3.28it/s, loss=1.9, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=1.780]\u001b[A\n",
      "Epoch 236:  87%|██████▉ | 58/67 [00:17<00:02,  3.27it/s, loss=1.9, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=1.780]\u001b[A\n",
      "Epoch 236:  88%|███████ | 59/67 [00:18<00:02,  3.26it/s, loss=1.9, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=1.780]\u001b[A\n",
      "Epoch 236:  90%|███████▏| 60/67 [00:18<00:02,  3.26it/s, loss=1.9, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=1.780]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 236:  91%|███████▎| 61/67 [00:18<00:01,  3.23it/s, loss=1.9, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=1.780]\u001b[A\n",
      "Epoch 236:  93%|███████▍| 62/67 [00:19<00:01,  3.22it/s, loss=1.9, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=1.780]\u001b[A\n",
      "Epoch 236:  94%|███████▌| 63/67 [00:19<00:01,  3.22it/s, loss=1.9, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=1.780]\u001b[A\n",
      "Epoch 236:  96%|███████▋| 64/67 [00:19<00:00,  3.21it/s, loss=1.9, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=1.780]\u001b[A\n",
      "Epoch 236:  97%|███████▊| 65/67 [00:20<00:00,  3.21it/s, loss=1.9, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=1.780]\u001b[A\n",
      "Epoch 236:  99%|███████▉| 66/67 [00:20<00:00,  3.21it/s, loss=1.9, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=1.780]\u001b[A\n",
      "Epoch 236: 100%|████████| 67/67 [00:21<00:00,  3.13it/s, loss=1.9, v_num=1, train_loss_step=1.980, val_loss=2.830, train_loss_epoch=1.780]\u001b[A\n",
      "Epoch 237:  45%|███▏   | 30/67 [00:05<00:07,  5.07it/s, loss=2.47, v_num=1, train_loss_step=1.700, val_loss=2.830, train_loss_epoch=1.860]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 237:  46%|███▏   | 31/67 [00:08<00:09,  3.81it/s, loss=2.47, v_num=1, train_loss_step=1.700, val_loss=2.830, train_loss_epoch=1.860]\u001b[A\n",
      "Epoch 237:  48%|███▎   | 32/67 [00:08<00:09,  3.77it/s, loss=2.47, v_num=1, train_loss_step=1.700, val_loss=2.830, train_loss_epoch=1.860]\u001b[A\n",
      "Epoch 237:  49%|███▍   | 33/67 [00:08<00:09,  3.74it/s, loss=2.47, v_num=1, train_loss_step=1.700, val_loss=2.830, train_loss_epoch=1.860]\u001b[A\n",
      "Epoch 237:  51%|███▌   | 34/67 [00:09<00:08,  3.70it/s, loss=2.47, v_num=1, train_loss_step=1.700, val_loss=2.830, train_loss_epoch=1.860]\u001b[A\n",
      "Epoch 237:  52%|███▋   | 35/67 [00:09<00:08,  3.68it/s, loss=2.47, v_num=1, train_loss_step=1.700, val_loss=2.830, train_loss_epoch=1.860]\u001b[A\n",
      "Epoch 237:  54%|███▊   | 36/67 [00:09<00:08,  3.66it/s, loss=2.47, v_num=1, train_loss_step=1.700, val_loss=2.830, train_loss_epoch=1.860]\u001b[A\n",
      "Epoch 237:  55%|███▊   | 37/67 [00:10<00:08,  3.64it/s, loss=2.47, v_num=1, train_loss_step=1.700, val_loss=2.830, train_loss_epoch=1.860]\u001b[A\n",
      "Epoch 237:  57%|███▉   | 38/67 [00:10<00:08,  3.61it/s, loss=2.47, v_num=1, train_loss_step=1.700, val_loss=2.830, train_loss_epoch=1.860]\u001b[A\n",
      "Epoch 237:  58%|████   | 39/67 [00:10<00:07,  3.59it/s, loss=2.47, v_num=1, train_loss_step=1.700, val_loss=2.830, train_loss_epoch=1.860]\u001b[A\n",
      "Epoch 237:  60%|████▏  | 40/67 [00:11<00:07,  3.56it/s, loss=2.47, v_num=1, train_loss_step=1.700, val_loss=2.830, train_loss_epoch=1.860]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 237:  61%|████▎  | 41/67 [00:11<00:07,  3.50it/s, loss=2.47, v_num=1, train_loss_step=1.700, val_loss=2.830, train_loss_epoch=1.860]\u001b[A\n",
      "Epoch 237:  63%|████▍  | 42/67 [00:12<00:07,  3.48it/s, loss=2.47, v_num=1, train_loss_step=1.700, val_loss=2.830, train_loss_epoch=1.860]\u001b[A\n",
      "Epoch 237:  64%|████▍  | 43/67 [00:12<00:06,  3.47it/s, loss=2.47, v_num=1, train_loss_step=1.700, val_loss=2.830, train_loss_epoch=1.860]\u001b[A\n",
      "Epoch 237:  66%|████▌  | 44/67 [00:12<00:06,  3.46it/s, loss=2.47, v_num=1, train_loss_step=1.700, val_loss=2.830, train_loss_epoch=1.860]\u001b[A\n",
      "Epoch 237:  67%|████▋  | 45/67 [00:13<00:06,  3.45it/s, loss=2.47, v_num=1, train_loss_step=1.700, val_loss=2.830, train_loss_epoch=1.860]\u001b[A\n",
      "Epoch 237:  69%|████▊  | 46/67 [00:13<00:06,  3.44it/s, loss=2.47, v_num=1, train_loss_step=1.700, val_loss=2.830, train_loss_epoch=1.860]\u001b[A\n",
      "Epoch 237:  70%|████▉  | 47/67 [00:13<00:05,  3.42it/s, loss=2.47, v_num=1, train_loss_step=1.700, val_loss=2.830, train_loss_epoch=1.860]\u001b[A\n",
      "Epoch 237:  72%|█████  | 48/67 [00:14<00:05,  3.41it/s, loss=2.47, v_num=1, train_loss_step=1.700, val_loss=2.830, train_loss_epoch=1.860]\u001b[A\n",
      "Epoch 237:  73%|█████  | 49/67 [00:14<00:05,  3.39it/s, loss=2.47, v_num=1, train_loss_step=1.700, val_loss=2.830, train_loss_epoch=1.860]\u001b[A\n",
      "Epoch 237:  75%|█████▏ | 50/67 [00:14<00:05,  3.38it/s, loss=2.47, v_num=1, train_loss_step=1.700, val_loss=2.830, train_loss_epoch=1.860]\u001b[A\n",
      "Epoch 237:  76%|█████▎ | 51/67 [00:15<00:04,  3.34it/s, loss=2.47, v_num=1, train_loss_step=1.700, val_loss=2.830, train_loss_epoch=1.860]\u001b[A\n",
      "Epoch 237:  78%|█████▍ | 52/67 [00:15<00:04,  3.33it/s, loss=2.47, v_num=1, train_loss_step=1.700, val_loss=2.830, train_loss_epoch=1.860]\u001b[A\n",
      "Epoch 237:  79%|█████▌ | 53/67 [00:15<00:04,  3.32it/s, loss=2.47, v_num=1, train_loss_step=1.700, val_loss=2.830, train_loss_epoch=1.860]\u001b[A\n",
      "Epoch 237:  81%|█████▋ | 54/67 [00:16<00:03,  3.31it/s, loss=2.47, v_num=1, train_loss_step=1.700, val_loss=2.830, train_loss_epoch=1.860]\u001b[A\n",
      "Epoch 237:  82%|█████▋ | 55/67 [00:16<00:03,  3.30it/s, loss=2.47, v_num=1, train_loss_step=1.700, val_loss=2.830, train_loss_epoch=1.860]\u001b[A\n",
      "Epoch 237:  84%|█████▊ | 56/67 [00:17<00:03,  3.29it/s, loss=2.47, v_num=1, train_loss_step=1.700, val_loss=2.830, train_loss_epoch=1.860]\u001b[A\n",
      "Epoch 237:  85%|█████▉ | 57/67 [00:17<00:03,  3.29it/s, loss=2.47, v_num=1, train_loss_step=1.700, val_loss=2.830, train_loss_epoch=1.860]\u001b[A\n",
      "Epoch 237:  87%|██████ | 58/67 [00:17<00:02,  3.28it/s, loss=2.47, v_num=1, train_loss_step=1.700, val_loss=2.830, train_loss_epoch=1.860]\u001b[A\n",
      "Epoch 237:  88%|██████▏| 59/67 [00:18<00:02,  3.27it/s, loss=2.47, v_num=1, train_loss_step=1.700, val_loss=2.830, train_loss_epoch=1.860]\u001b[A\n",
      "Epoch 237:  90%|██████▎| 60/67 [00:18<00:02,  3.27it/s, loss=2.47, v_num=1, train_loss_step=1.700, val_loss=2.830, train_loss_epoch=1.860]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 237:  91%|██████▎| 61/67 [00:18<00:01,  3.23it/s, loss=2.47, v_num=1, train_loss_step=1.700, val_loss=2.830, train_loss_epoch=1.860]\u001b[A\n",
      "Epoch 237:  93%|██████▍| 62/67 [00:19<00:01,  3.22it/s, loss=2.47, v_num=1, train_loss_step=1.700, val_loss=2.830, train_loss_epoch=1.860]\u001b[A\n",
      "Epoch 237:  94%|██████▌| 63/67 [00:19<00:01,  3.22it/s, loss=2.47, v_num=1, train_loss_step=1.700, val_loss=2.830, train_loss_epoch=1.860]\u001b[A\n",
      "Epoch 237:  96%|██████▋| 64/67 [00:19<00:00,  3.22it/s, loss=2.47, v_num=1, train_loss_step=1.700, val_loss=2.830, train_loss_epoch=1.860]\u001b[A\n",
      "Epoch 237:  97%|██████▊| 65/67 [00:20<00:00,  3.21it/s, loss=2.47, v_num=1, train_loss_step=1.700, val_loss=2.830, train_loss_epoch=1.860]\u001b[A\n",
      "Epoch 237:  99%|██████▉| 66/67 [00:20<00:00,  3.21it/s, loss=2.47, v_num=1, train_loss_step=1.700, val_loss=2.830, train_loss_epoch=1.860]\u001b[A\n",
      "Epoch 237: 100%|███████| 67/67 [00:21<00:00,  3.14it/s, loss=2.47, v_num=1, train_loss_step=1.700, val_loss=2.820, train_loss_epoch=1.860]\u001b[A\n",
      "Epoch 238:  45%|███▏   | 30/67 [00:05<00:07,  5.15it/s, loss=2.35, v_num=1, train_loss_step=3.940, val_loss=2.820, train_loss_epoch=2.300]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 238:  46%|███▏   | 31/67 [00:07<00:09,  3.89it/s, loss=2.35, v_num=1, train_loss_step=3.940, val_loss=2.820, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 238:  48%|███▎   | 32/67 [00:08<00:09,  3.85it/s, loss=2.35, v_num=1, train_loss_step=3.940, val_loss=2.820, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 238:  49%|███▍   | 33/67 [00:08<00:08,  3.82it/s, loss=2.35, v_num=1, train_loss_step=3.940, val_loss=2.820, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 238:  51%|███▌   | 34/67 [00:08<00:08,  3.78it/s, loss=2.35, v_num=1, train_loss_step=3.940, val_loss=2.820, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 238:  52%|███▋   | 35/67 [00:09<00:08,  3.75it/s, loss=2.35, v_num=1, train_loss_step=3.940, val_loss=2.820, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 238:  54%|███▊   | 36/67 [00:09<00:08,  3.72it/s, loss=2.35, v_num=1, train_loss_step=3.940, val_loss=2.820, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 238:  55%|███▊   | 37/67 [00:10<00:08,  3.69it/s, loss=2.35, v_num=1, train_loss_step=3.940, val_loss=2.820, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 238:  57%|███▉   | 38/67 [00:10<00:07,  3.67it/s, loss=2.35, v_num=1, train_loss_step=3.940, val_loss=2.820, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 238:  58%|████   | 39/67 [00:10<00:07,  3.65it/s, loss=2.35, v_num=1, train_loss_step=3.940, val_loss=2.820, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 238:  60%|████▏  | 40/67 [00:11<00:07,  3.62it/s, loss=2.35, v_num=1, train_loss_step=3.940, val_loss=2.820, train_loss_epoch=2.300]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 238:  61%|████▎  | 41/67 [00:11<00:07,  3.55it/s, loss=2.35, v_num=1, train_loss_step=3.940, val_loss=2.820, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 238:  63%|████▍  | 42/67 [00:11<00:07,  3.54it/s, loss=2.35, v_num=1, train_loss_step=3.940, val_loss=2.820, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 238:  64%|████▍  | 43/67 [00:12<00:06,  3.53it/s, loss=2.35, v_num=1, train_loss_step=3.940, val_loss=2.820, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 238:  66%|████▌  | 44/67 [00:12<00:06,  3.52it/s, loss=2.35, v_num=1, train_loss_step=3.940, val_loss=2.820, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 238:  67%|████▋  | 45/67 [00:12<00:06,  3.50it/s, loss=2.35, v_num=1, train_loss_step=3.940, val_loss=2.820, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 238:  69%|████▊  | 46/67 [00:13<00:06,  3.49it/s, loss=2.35, v_num=1, train_loss_step=3.940, val_loss=2.820, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 238:  70%|████▉  | 47/67 [00:13<00:05,  3.48it/s, loss=2.35, v_num=1, train_loss_step=3.940, val_loss=2.820, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 238:  72%|█████  | 48/67 [00:13<00:05,  3.47it/s, loss=2.35, v_num=1, train_loss_step=3.940, val_loss=2.820, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 238:  73%|█████  | 49/67 [00:14<00:05,  3.46it/s, loss=2.35, v_num=1, train_loss_step=3.940, val_loss=2.820, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 238:  75%|█████▏ | 50/67 [00:14<00:04,  3.45it/s, loss=2.35, v_num=1, train_loss_step=3.940, val_loss=2.820, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 238:  76%|█████▎ | 51/67 [00:14<00:04,  3.40it/s, loss=2.35, v_num=1, train_loss_step=3.940, val_loss=2.820, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 238:  78%|█████▍ | 52/67 [00:15<00:04,  3.39it/s, loss=2.35, v_num=1, train_loss_step=3.940, val_loss=2.820, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 238:  79%|█████▌ | 53/67 [00:15<00:04,  3.39it/s, loss=2.35, v_num=1, train_loss_step=3.940, val_loss=2.820, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 238:  81%|█████▋ | 54/67 [00:15<00:03,  3.38it/s, loss=2.35, v_num=1, train_loss_step=3.940, val_loss=2.820, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 238:  82%|█████▋ | 55/67 [00:16<00:03,  3.37it/s, loss=2.35, v_num=1, train_loss_step=3.940, val_loss=2.820, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 238:  84%|█████▊ | 56/67 [00:16<00:03,  3.36it/s, loss=2.35, v_num=1, train_loss_step=3.940, val_loss=2.820, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 238:  85%|█████▉ | 57/67 [00:17<00:02,  3.35it/s, loss=2.35, v_num=1, train_loss_step=3.940, val_loss=2.820, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 238:  87%|██████ | 58/67 [00:17<00:02,  3.34it/s, loss=2.35, v_num=1, train_loss_step=3.940, val_loss=2.820, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 238:  88%|██████▏| 59/67 [00:17<00:02,  3.34it/s, loss=2.35, v_num=1, train_loss_step=3.940, val_loss=2.820, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 238:  90%|██████▎| 60/67 [00:18<00:02,  3.33it/s, loss=2.35, v_num=1, train_loss_step=3.940, val_loss=2.820, train_loss_epoch=2.300]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 238:  91%|██████▎| 61/67 [00:18<00:01,  3.28it/s, loss=2.35, v_num=1, train_loss_step=3.940, val_loss=2.820, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 238:  93%|██████▍| 62/67 [00:18<00:01,  3.27it/s, loss=2.35, v_num=1, train_loss_step=3.940, val_loss=2.820, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 238:  94%|██████▌| 63/67 [00:19<00:01,  3.26it/s, loss=2.35, v_num=1, train_loss_step=3.940, val_loss=2.820, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 238:  96%|██████▋| 64/67 [00:19<00:00,  3.26it/s, loss=2.35, v_num=1, train_loss_step=3.940, val_loss=2.820, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 238:  97%|██████▊| 65/67 [00:19<00:00,  3.25it/s, loss=2.35, v_num=1, train_loss_step=3.940, val_loss=2.820, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 238:  99%|██████▉| 66/67 [00:20<00:00,  3.25it/s, loss=2.35, v_num=1, train_loss_step=3.940, val_loss=2.820, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 238: 100%|███████| 67/67 [00:21<00:00,  3.18it/s, loss=2.35, v_num=1, train_loss_step=3.940, val_loss=2.820, train_loss_epoch=2.300]\u001b[A\n",
      "Epoch 239:  45%|███▏   | 30/67 [00:05<00:07,  5.10it/s, loss=1.97, v_num=1, train_loss_step=0.591, val_loss=2.820, train_loss_epoch=2.170]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 239:  46%|███▏   | 31/67 [00:07<00:09,  3.89it/s, loss=1.97, v_num=1, train_loss_step=0.591, val_loss=2.820, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 239:  48%|███▎   | 32/67 [00:08<00:09,  3.85it/s, loss=1.97, v_num=1, train_loss_step=0.591, val_loss=2.820, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 239:  49%|███▍   | 33/67 [00:08<00:08,  3.82it/s, loss=1.97, v_num=1, train_loss_step=0.591, val_loss=2.820, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 239:  51%|███▌   | 34/67 [00:08<00:08,  3.78it/s, loss=1.97, v_num=1, train_loss_step=0.591, val_loss=2.820, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 239:  52%|███▋   | 35/67 [00:09<00:08,  3.75it/s, loss=1.97, v_num=1, train_loss_step=0.591, val_loss=2.820, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 239:  54%|███▊   | 36/67 [00:09<00:08,  3.73it/s, loss=1.97, v_num=1, train_loss_step=0.591, val_loss=2.820, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 239:  55%|███▊   | 37/67 [00:09<00:08,  3.70it/s, loss=1.97, v_num=1, train_loss_step=0.591, val_loss=2.820, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 239:  57%|███▉   | 38/67 [00:10<00:07,  3.68it/s, loss=1.97, v_num=1, train_loss_step=0.591, val_loss=2.820, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 239:  58%|████   | 39/67 [00:10<00:07,  3.66it/s, loss=1.97, v_num=1, train_loss_step=0.591, val_loss=2.820, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 239:  60%|████▏  | 40/67 [00:10<00:07,  3.64it/s, loss=1.97, v_num=1, train_loss_step=0.591, val_loss=2.820, train_loss_epoch=2.170]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 239:  61%|████▎  | 41/67 [00:11<00:07,  3.56it/s, loss=1.97, v_num=1, train_loss_step=0.591, val_loss=2.820, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 239:  63%|████▍  | 42/67 [00:11<00:07,  3.55it/s, loss=1.97, v_num=1, train_loss_step=0.591, val_loss=2.820, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 239:  64%|████▍  | 43/67 [00:12<00:06,  3.54it/s, loss=1.97, v_num=1, train_loss_step=0.591, val_loss=2.820, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 239:  66%|████▌  | 44/67 [00:12<00:06,  3.52it/s, loss=1.97, v_num=1, train_loss_step=0.591, val_loss=2.820, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 239:  67%|████▋  | 45/67 [00:12<00:06,  3.51it/s, loss=1.97, v_num=1, train_loss_step=0.591, val_loss=2.820, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 239:  69%|████▊  | 46/67 [00:13<00:06,  3.50it/s, loss=1.97, v_num=1, train_loss_step=0.591, val_loss=2.820, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 239:  70%|████▉  | 47/67 [00:13<00:05,  3.49it/s, loss=1.97, v_num=1, train_loss_step=0.591, val_loss=2.820, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 239:  72%|█████  | 48/67 [00:13<00:05,  3.47it/s, loss=1.97, v_num=1, train_loss_step=0.591, val_loss=2.820, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 239:  73%|█████  | 49/67 [00:14<00:05,  3.46it/s, loss=1.97, v_num=1, train_loss_step=0.591, val_loss=2.820, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 239:  75%|█████▏ | 50/67 [00:14<00:04,  3.45it/s, loss=1.97, v_num=1, train_loss_step=0.591, val_loss=2.820, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 239:  76%|█████▎ | 51/67 [00:14<00:04,  3.40it/s, loss=1.97, v_num=1, train_loss_step=0.591, val_loss=2.820, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 239:  78%|█████▍ | 52/67 [00:15<00:04,  3.39it/s, loss=1.97, v_num=1, train_loss_step=0.591, val_loss=2.820, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 239:  79%|█████▌ | 53/67 [00:15<00:04,  3.38it/s, loss=1.97, v_num=1, train_loss_step=0.591, val_loss=2.820, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 239:  81%|█████▋ | 54/67 [00:16<00:03,  3.37it/s, loss=1.97, v_num=1, train_loss_step=0.591, val_loss=2.820, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 239:  82%|█████▋ | 55/67 [00:16<00:03,  3.37it/s, loss=1.97, v_num=1, train_loss_step=0.591, val_loss=2.820, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 239:  84%|█████▊ | 56/67 [00:16<00:03,  3.36it/s, loss=1.97, v_num=1, train_loss_step=0.591, val_loss=2.820, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 239:  85%|█████▉ | 57/67 [00:17<00:02,  3.35it/s, loss=1.97, v_num=1, train_loss_step=0.591, val_loss=2.820, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 239:  87%|██████ | 58/67 [00:17<00:02,  3.34it/s, loss=1.97, v_num=1, train_loss_step=0.591, val_loss=2.820, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 239:  88%|██████▏| 59/67 [00:17<00:02,  3.34it/s, loss=1.97, v_num=1, train_loss_step=0.591, val_loss=2.820, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 239:  90%|██████▎| 60/67 [00:18<00:02,  3.33it/s, loss=1.97, v_num=1, train_loss_step=0.591, val_loss=2.820, train_loss_epoch=2.170]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 239:  91%|██████▎| 61/67 [00:18<00:01,  3.29it/s, loss=1.97, v_num=1, train_loss_step=0.591, val_loss=2.820, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 239:  93%|██████▍| 62/67 [00:18<00:01,  3.28it/s, loss=1.97, v_num=1, train_loss_step=0.591, val_loss=2.820, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 239:  94%|██████▌| 63/67 [00:19<00:01,  3.27it/s, loss=1.97, v_num=1, train_loss_step=0.591, val_loss=2.820, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 239:  96%|██████▋| 64/67 [00:19<00:00,  3.27it/s, loss=1.97, v_num=1, train_loss_step=0.591, val_loss=2.820, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 239:  97%|██████▊| 65/67 [00:19<00:00,  3.27it/s, loss=1.97, v_num=1, train_loss_step=0.591, val_loss=2.820, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 239:  99%|██████▉| 66/67 [00:20<00:00,  3.26it/s, loss=1.97, v_num=1, train_loss_step=0.591, val_loss=2.820, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 239: 100%|███████| 67/67 [00:21<00:00,  3.19it/s, loss=1.97, v_num=1, train_loss_step=0.591, val_loss=2.820, train_loss_epoch=2.170]\u001b[A\n",
      "Epoch 240:  45%|███▏   | 30/67 [00:06<00:07,  4.95it/s, loss=1.96, v_num=1, train_loss_step=0.928, val_loss=2.820, train_loss_epoch=1.820]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 240:  46%|███▏   | 31/67 [00:08<00:09,  3.74it/s, loss=1.96, v_num=1, train_loss_step=0.928, val_loss=2.820, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 240:  48%|███▎   | 32/67 [00:08<00:09,  3.71it/s, loss=1.96, v_num=1, train_loss_step=0.928, val_loss=2.820, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 240:  49%|███▍   | 33/67 [00:08<00:09,  3.69it/s, loss=1.96, v_num=1, train_loss_step=0.928, val_loss=2.820, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 240:  51%|███▌   | 34/67 [00:09<00:09,  3.66it/s, loss=1.96, v_num=1, train_loss_step=0.928, val_loss=2.820, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 240:  52%|███▋   | 35/67 [00:09<00:08,  3.64it/s, loss=1.96, v_num=1, train_loss_step=0.928, val_loss=2.820, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 240:  54%|███▊   | 36/67 [00:09<00:08,  3.62it/s, loss=1.96, v_num=1, train_loss_step=0.928, val_loss=2.820, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 240:  55%|███▊   | 37/67 [00:10<00:08,  3.60it/s, loss=1.96, v_num=1, train_loss_step=0.928, val_loss=2.820, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 240:  57%|███▉   | 38/67 [00:10<00:08,  3.58it/s, loss=1.96, v_num=1, train_loss_step=0.928, val_loss=2.820, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 240:  58%|████   | 39/67 [00:10<00:07,  3.56it/s, loss=1.96, v_num=1, train_loss_step=0.928, val_loss=2.820, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 240:  60%|████▏  | 40/67 [00:11<00:07,  3.54it/s, loss=1.96, v_num=1, train_loss_step=0.928, val_loss=2.820, train_loss_epoch=1.820]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 240:  61%|████▎  | 41/67 [00:11<00:07,  3.47it/s, loss=1.96, v_num=1, train_loss_step=0.928, val_loss=2.820, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 240:  63%|████▍  | 42/67 [00:12<00:07,  3.46it/s, loss=1.96, v_num=1, train_loss_step=0.928, val_loss=2.820, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 240:  64%|████▍  | 43/67 [00:12<00:06,  3.45it/s, loss=1.96, v_num=1, train_loss_step=0.928, val_loss=2.820, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 240:  66%|████▌  | 44/67 [00:12<00:06,  3.44it/s, loss=1.96, v_num=1, train_loss_step=0.928, val_loss=2.820, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 240:  67%|████▋  | 45/67 [00:13<00:06,  3.43it/s, loss=1.96, v_num=1, train_loss_step=0.928, val_loss=2.820, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 240:  69%|████▊  | 46/67 [00:13<00:06,  3.42it/s, loss=1.96, v_num=1, train_loss_step=0.928, val_loss=2.820, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 240:  70%|████▉  | 47/67 [00:13<00:05,  3.41it/s, loss=1.96, v_num=1, train_loss_step=0.928, val_loss=2.820, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 240:  72%|█████  | 48/67 [00:14<00:05,  3.40it/s, loss=1.96, v_num=1, train_loss_step=0.928, val_loss=2.820, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 240:  73%|█████  | 49/67 [00:14<00:05,  3.38it/s, loss=1.96, v_num=1, train_loss_step=0.928, val_loss=2.820, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 240:  75%|█████▏ | 50/67 [00:14<00:05,  3.37it/s, loss=1.96, v_num=1, train_loss_step=0.928, val_loss=2.820, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 240:  76%|█████▎ | 51/67 [00:15<00:04,  3.32it/s, loss=1.96, v_num=1, train_loss_step=0.928, val_loss=2.820, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 240:  78%|█████▍ | 52/67 [00:15<00:04,  3.31it/s, loss=1.96, v_num=1, train_loss_step=0.928, val_loss=2.820, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 240:  79%|█████▌ | 53/67 [00:16<00:04,  3.30it/s, loss=1.96, v_num=1, train_loss_step=0.928, val_loss=2.820, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 240:  81%|█████▋ | 54/67 [00:16<00:03,  3.29it/s, loss=1.96, v_num=1, train_loss_step=0.928, val_loss=2.820, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 240:  82%|█████▋ | 55/67 [00:16<00:03,  3.28it/s, loss=1.96, v_num=1, train_loss_step=0.928, val_loss=2.820, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 240:  84%|█████▊ | 56/67 [00:17<00:03,  3.27it/s, loss=1.96, v_num=1, train_loss_step=0.928, val_loss=2.820, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 240:  85%|█████▉ | 57/67 [00:17<00:03,  3.26it/s, loss=1.96, v_num=1, train_loss_step=0.928, val_loss=2.820, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 240:  87%|██████ | 58/67 [00:17<00:02,  3.24it/s, loss=1.96, v_num=1, train_loss_step=0.928, val_loss=2.820, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 240:  88%|██████▏| 59/67 [00:18<00:02,  3.23it/s, loss=1.96, v_num=1, train_loss_step=0.928, val_loss=2.820, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 240:  90%|██████▎| 60/67 [00:18<00:02,  3.23it/s, loss=1.96, v_num=1, train_loss_step=0.928, val_loss=2.820, train_loss_epoch=1.820]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 240:  91%|██████▎| 61/67 [00:19<00:01,  3.17it/s, loss=1.96, v_num=1, train_loss_step=0.928, val_loss=2.820, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 240:  93%|██████▍| 62/67 [00:19<00:01,  3.17it/s, loss=1.96, v_num=1, train_loss_step=0.928, val_loss=2.820, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 240:  94%|██████▌| 63/67 [00:19<00:01,  3.16it/s, loss=1.96, v_num=1, train_loss_step=0.928, val_loss=2.820, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 240:  96%|██████▋| 64/67 [00:20<00:00,  3.16it/s, loss=1.96, v_num=1, train_loss_step=0.928, val_loss=2.820, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 240:  97%|██████▊| 65/67 [00:20<00:00,  3.16it/s, loss=1.96, v_num=1, train_loss_step=0.928, val_loss=2.820, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 240:  99%|██████▉| 66/67 [00:20<00:00,  3.15it/s, loss=1.96, v_num=1, train_loss_step=0.928, val_loss=2.820, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 240: 100%|███████| 67/67 [00:21<00:00,  3.08it/s, loss=1.96, v_num=1, train_loss_step=0.928, val_loss=2.820, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 241:  45%|███▌    | 30/67 [00:06<00:07,  5.00it/s, loss=1.9, v_num=1, train_loss_step=1.890, val_loss=2.820, train_loss_epoch=1.740]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 241:  46%|███▋    | 31/67 [00:08<00:09,  3.69it/s, loss=1.9, v_num=1, train_loss_step=1.890, val_loss=2.820, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 241:  48%|███▊    | 32/67 [00:08<00:09,  3.66it/s, loss=1.9, v_num=1, train_loss_step=1.890, val_loss=2.820, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 241:  49%|███▉    | 33/67 [00:09<00:09,  3.64it/s, loss=1.9, v_num=1, train_loss_step=1.890, val_loss=2.820, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 241:  51%|████    | 34/67 [00:09<00:09,  3.60it/s, loss=1.9, v_num=1, train_loss_step=1.890, val_loss=2.820, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 241:  52%|████▏   | 35/67 [00:09<00:08,  3.58it/s, loss=1.9, v_num=1, train_loss_step=1.890, val_loss=2.820, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 241:  54%|████▎   | 36/67 [00:10<00:08,  3.56it/s, loss=1.9, v_num=1, train_loss_step=1.890, val_loss=2.820, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 241:  55%|████▍   | 37/67 [00:10<00:08,  3.53it/s, loss=1.9, v_num=1, train_loss_step=1.890, val_loss=2.820, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 241:  57%|████▌   | 38/67 [00:10<00:08,  3.51it/s, loss=1.9, v_num=1, train_loss_step=1.890, val_loss=2.820, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 241:  58%|████▋   | 39/67 [00:11<00:08,  3.48it/s, loss=1.9, v_num=1, train_loss_step=1.890, val_loss=2.820, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 241:  60%|████▊   | 40/67 [00:11<00:07,  3.46it/s, loss=1.9, v_num=1, train_loss_step=1.890, val_loss=2.820, train_loss_epoch=1.740]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 241:  61%|████▉   | 41/67 [00:12<00:07,  3.38it/s, loss=1.9, v_num=1, train_loss_step=1.890, val_loss=2.820, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 241:  63%|█████   | 42/67 [00:12<00:07,  3.37it/s, loss=1.9, v_num=1, train_loss_step=1.890, val_loss=2.820, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 241:  64%|█████▏  | 43/67 [00:12<00:07,  3.36it/s, loss=1.9, v_num=1, train_loss_step=1.890, val_loss=2.820, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 241:  66%|█████▎  | 44/67 [00:13<00:06,  3.34it/s, loss=1.9, v_num=1, train_loss_step=1.890, val_loss=2.820, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 241:  67%|█████▎  | 45/67 [00:13<00:06,  3.33it/s, loss=1.9, v_num=1, train_loss_step=1.890, val_loss=2.820, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 241:  69%|█████▍  | 46/67 [00:13<00:06,  3.32it/s, loss=1.9, v_num=1, train_loss_step=1.890, val_loss=2.820, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 241:  70%|█████▌  | 47/67 [00:14<00:06,  3.31it/s, loss=1.9, v_num=1, train_loss_step=1.890, val_loss=2.820, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 241:  72%|█████▋  | 48/67 [00:14<00:05,  3.31it/s, loss=1.9, v_num=1, train_loss_step=1.890, val_loss=2.820, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 241:  73%|█████▊  | 49/67 [00:14<00:05,  3.30it/s, loss=1.9, v_num=1, train_loss_step=1.890, val_loss=2.820, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 241:  75%|█████▉  | 50/67 [00:15<00:05,  3.29it/s, loss=1.9, v_num=1, train_loss_step=1.890, val_loss=2.820, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 241:  76%|██████  | 51/67 [00:15<00:04,  3.25it/s, loss=1.9, v_num=1, train_loss_step=1.890, val_loss=2.820, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 241:  78%|██████▏ | 52/67 [00:16<00:04,  3.24it/s, loss=1.9, v_num=1, train_loss_step=1.890, val_loss=2.820, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 241:  79%|██████▎ | 53/67 [00:16<00:04,  3.24it/s, loss=1.9, v_num=1, train_loss_step=1.890, val_loss=2.820, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 241:  81%|██████▍ | 54/67 [00:16<00:04,  3.23it/s, loss=1.9, v_num=1, train_loss_step=1.890, val_loss=2.820, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 241:  82%|██████▌ | 55/67 [00:17<00:03,  3.23it/s, loss=1.9, v_num=1, train_loss_step=1.890, val_loss=2.820, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 241:  84%|██████▋ | 56/67 [00:17<00:03,  3.22it/s, loss=1.9, v_num=1, train_loss_step=1.890, val_loss=2.820, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 241:  85%|██████▊ | 57/67 [00:17<00:03,  3.22it/s, loss=1.9, v_num=1, train_loss_step=1.890, val_loss=2.820, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 241:  87%|██████▉ | 58/67 [00:18<00:02,  3.22it/s, loss=1.9, v_num=1, train_loss_step=1.890, val_loss=2.820, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 241:  88%|███████ | 59/67 [00:18<00:02,  3.21it/s, loss=1.9, v_num=1, train_loss_step=1.890, val_loss=2.820, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 241:  90%|███████▏| 60/67 [00:18<00:02,  3.21it/s, loss=1.9, v_num=1, train_loss_step=1.890, val_loss=2.820, train_loss_epoch=1.740]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 241:  91%|███████▎| 61/67 [00:19<00:01,  3.16it/s, loss=1.9, v_num=1, train_loss_step=1.890, val_loss=2.820, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 241:  93%|███████▍| 62/67 [00:19<00:01,  3.16it/s, loss=1.9, v_num=1, train_loss_step=1.890, val_loss=2.820, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 241:  94%|███████▌| 63/67 [00:19<00:01,  3.15it/s, loss=1.9, v_num=1, train_loss_step=1.890, val_loss=2.820, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 241:  96%|███████▋| 64/67 [00:20<00:00,  3.15it/s, loss=1.9, v_num=1, train_loss_step=1.890, val_loss=2.820, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 241:  97%|███████▊| 65/67 [00:20<00:00,  3.15it/s, loss=1.9, v_num=1, train_loss_step=1.890, val_loss=2.820, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 241:  99%|███████▉| 66/67 [00:20<00:00,  3.15it/s, loss=1.9, v_num=1, train_loss_step=1.890, val_loss=2.820, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 241: 100%|████████| 67/67 [00:21<00:00,  3.09it/s, loss=1.9, v_num=1, train_loss_step=1.890, val_loss=2.820, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 242:  45%|███▏   | 30/67 [00:05<00:07,  5.07it/s, loss=1.57, v_num=1, train_loss_step=3.910, val_loss=2.820, train_loss_epoch=1.870]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 242:  46%|███▏   | 31/67 [00:08<00:09,  3.77it/s, loss=1.57, v_num=1, train_loss_step=3.910, val_loss=2.820, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 242:  48%|███▎   | 32/67 [00:08<00:09,  3.74it/s, loss=1.57, v_num=1, train_loss_step=3.910, val_loss=2.820, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 242:  49%|███▍   | 33/67 [00:08<00:09,  3.71it/s, loss=1.57, v_num=1, train_loss_step=3.910, val_loss=2.820, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 242:  51%|███▌   | 34/67 [00:09<00:08,  3.68it/s, loss=1.57, v_num=1, train_loss_step=3.910, val_loss=2.820, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 242:  52%|███▋   | 35/67 [00:09<00:08,  3.65it/s, loss=1.57, v_num=1, train_loss_step=3.910, val_loss=2.820, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 242:  54%|███▊   | 36/67 [00:09<00:08,  3.62it/s, loss=1.57, v_num=1, train_loss_step=3.910, val_loss=2.820, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 242:  55%|███▊   | 37/67 [00:10<00:08,  3.59it/s, loss=1.57, v_num=1, train_loss_step=3.910, val_loss=2.820, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 242:  57%|███▉   | 38/67 [00:10<00:08,  3.56it/s, loss=1.57, v_num=1, train_loss_step=3.910, val_loss=2.820, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 242:  58%|████   | 39/67 [00:11<00:07,  3.55it/s, loss=1.57, v_num=1, train_loss_step=3.910, val_loss=2.820, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 242:  60%|████▏  | 40/67 [00:11<00:07,  3.52it/s, loss=1.57, v_num=1, train_loss_step=3.910, val_loss=2.820, train_loss_epoch=1.870]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 242:  61%|████▎  | 41/67 [00:11<00:07,  3.45it/s, loss=1.57, v_num=1, train_loss_step=3.910, val_loss=2.820, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 242:  63%|████▍  | 42/67 [00:12<00:07,  3.44it/s, loss=1.57, v_num=1, train_loss_step=3.910, val_loss=2.820, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 242:  64%|████▍  | 43/67 [00:12<00:07,  3.42it/s, loss=1.57, v_num=1, train_loss_step=3.910, val_loss=2.820, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 242:  66%|████▌  | 44/67 [00:12<00:06,  3.41it/s, loss=1.57, v_num=1, train_loss_step=3.910, val_loss=2.820, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 242:  67%|████▋  | 45/67 [00:13<00:06,  3.40it/s, loss=1.57, v_num=1, train_loss_step=3.910, val_loss=2.820, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 242:  69%|████▊  | 46/67 [00:13<00:06,  3.38it/s, loss=1.57, v_num=1, train_loss_step=3.910, val_loss=2.820, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 242:  70%|████▉  | 47/67 [00:13<00:05,  3.37it/s, loss=1.57, v_num=1, train_loss_step=3.910, val_loss=2.820, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 242:  72%|█████  | 48/67 [00:14<00:05,  3.36it/s, loss=1.57, v_num=1, train_loss_step=3.910, val_loss=2.820, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 242:  73%|█████  | 49/67 [00:14<00:05,  3.35it/s, loss=1.57, v_num=1, train_loss_step=3.910, val_loss=2.820, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 242:  75%|█████▏ | 50/67 [00:14<00:05,  3.33it/s, loss=1.57, v_num=1, train_loss_step=3.910, val_loss=2.820, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 242:  76%|█████▎ | 51/67 [00:15<00:04,  3.28it/s, loss=1.57, v_num=1, train_loss_step=3.910, val_loss=2.820, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 242:  78%|█████▍ | 52/67 [00:15<00:04,  3.27it/s, loss=1.57, v_num=1, train_loss_step=3.910, val_loss=2.820, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 242:  79%|█████▌ | 53/67 [00:16<00:04,  3.27it/s, loss=1.57, v_num=1, train_loss_step=3.910, val_loss=2.820, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 242:  81%|█████▋ | 54/67 [00:16<00:03,  3.26it/s, loss=1.57, v_num=1, train_loss_step=3.910, val_loss=2.820, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 242:  82%|█████▋ | 55/67 [00:16<00:03,  3.25it/s, loss=1.57, v_num=1, train_loss_step=3.910, val_loss=2.820, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 242:  84%|█████▊ | 56/67 [00:17<00:03,  3.24it/s, loss=1.57, v_num=1, train_loss_step=3.910, val_loss=2.820, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 242:  85%|█████▉ | 57/67 [00:17<00:03,  3.24it/s, loss=1.57, v_num=1, train_loss_step=3.910, val_loss=2.820, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 242:  87%|██████ | 58/67 [00:17<00:02,  3.23it/s, loss=1.57, v_num=1, train_loss_step=3.910, val_loss=2.820, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 242:  88%|██████▏| 59/67 [00:18<00:02,  3.22it/s, loss=1.57, v_num=1, train_loss_step=3.910, val_loss=2.820, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 242:  90%|██████▎| 60/67 [00:18<00:02,  3.22it/s, loss=1.57, v_num=1, train_loss_step=3.910, val_loss=2.820, train_loss_epoch=1.870]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 242:  91%|██████▎| 61/67 [00:19<00:01,  3.18it/s, loss=1.57, v_num=1, train_loss_step=3.910, val_loss=2.820, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 242:  93%|██████▍| 62/67 [00:19<00:01,  3.17it/s, loss=1.57, v_num=1, train_loss_step=3.910, val_loss=2.820, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 242:  94%|██████▌| 63/67 [00:19<00:01,  3.17it/s, loss=1.57, v_num=1, train_loss_step=3.910, val_loss=2.820, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 242:  96%|██████▋| 64/67 [00:20<00:00,  3.17it/s, loss=1.57, v_num=1, train_loss_step=3.910, val_loss=2.820, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 242:  97%|██████▊| 65/67 [00:20<00:00,  3.16it/s, loss=1.57, v_num=1, train_loss_step=3.910, val_loss=2.820, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 242:  99%|██████▉| 66/67 [00:20<00:00,  3.16it/s, loss=1.57, v_num=1, train_loss_step=3.910, val_loss=2.820, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 242: 100%|███████| 67/67 [00:21<00:00,  3.09it/s, loss=1.57, v_num=1, train_loss_step=3.910, val_loss=2.820, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 243:  45%|███▏   | 30/67 [00:06<00:07,  4.87it/s, loss=1.78, v_num=1, train_loss_step=1.480, val_loss=2.820, train_loss_epoch=1.770]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 243:  46%|███▏   | 31/67 [00:08<00:09,  3.69it/s, loss=1.78, v_num=1, train_loss_step=1.480, val_loss=2.820, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 243:  48%|███▎   | 32/67 [00:08<00:09,  3.65it/s, loss=1.78, v_num=1, train_loss_step=1.480, val_loss=2.820, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 243:  49%|███▍   | 33/67 [00:09<00:09,  3.62it/s, loss=1.78, v_num=1, train_loss_step=1.480, val_loss=2.820, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 243:  51%|███▌   | 34/67 [00:09<00:09,  3.60it/s, loss=1.78, v_num=1, train_loss_step=1.480, val_loss=2.820, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 243:  52%|███▋   | 35/67 [00:09<00:08,  3.56it/s, loss=1.78, v_num=1, train_loss_step=1.480, val_loss=2.820, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 243:  54%|███▊   | 36/67 [00:10<00:08,  3.54it/s, loss=1.78, v_num=1, train_loss_step=1.480, val_loss=2.820, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 243:  55%|███▊   | 37/67 [00:10<00:08,  3.52it/s, loss=1.78, v_num=1, train_loss_step=1.480, val_loss=2.820, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 243:  57%|███▉   | 38/67 [00:10<00:08,  3.51it/s, loss=1.78, v_num=1, train_loss_step=1.480, val_loss=2.820, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 243:  58%|████   | 39/67 [00:11<00:08,  3.50it/s, loss=1.78, v_num=1, train_loss_step=1.480, val_loss=2.820, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 243:  60%|████▏  | 40/67 [00:11<00:07,  3.48it/s, loss=1.78, v_num=1, train_loss_step=1.480, val_loss=2.820, train_loss_epoch=1.770]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 243:  61%|████▎  | 41/67 [00:11<00:07,  3.42it/s, loss=1.78, v_num=1, train_loss_step=1.480, val_loss=2.820, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 243:  63%|████▍  | 42/67 [00:12<00:07,  3.41it/s, loss=1.78, v_num=1, train_loss_step=1.480, val_loss=2.820, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 243:  64%|████▍  | 43/67 [00:12<00:07,  3.39it/s, loss=1.78, v_num=1, train_loss_step=1.480, val_loss=2.820, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 243:  66%|████▌  | 44/67 [00:13<00:06,  3.38it/s, loss=1.78, v_num=1, train_loss_step=1.480, val_loss=2.820, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 243:  67%|████▋  | 45/67 [00:13<00:06,  3.36it/s, loss=1.78, v_num=1, train_loss_step=1.480, val_loss=2.820, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 243:  69%|████▊  | 46/67 [00:13<00:06,  3.35it/s, loss=1.78, v_num=1, train_loss_step=1.480, val_loss=2.820, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 243:  70%|████▉  | 47/67 [00:14<00:05,  3.34it/s, loss=1.78, v_num=1, train_loss_step=1.480, val_loss=2.820, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 243:  72%|█████  | 48/67 [00:14<00:05,  3.33it/s, loss=1.78, v_num=1, train_loss_step=1.480, val_loss=2.820, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 243:  73%|█████  | 49/67 [00:14<00:05,  3.32it/s, loss=1.78, v_num=1, train_loss_step=1.480, val_loss=2.820, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 243:  75%|█████▏ | 50/67 [00:15<00:05,  3.31it/s, loss=1.78, v_num=1, train_loss_step=1.480, val_loss=2.820, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 243:  76%|█████▎ | 51/67 [00:15<00:04,  3.26it/s, loss=1.78, v_num=1, train_loss_step=1.480, val_loss=2.820, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 243:  78%|█████▍ | 52/67 [00:16<00:04,  3.25it/s, loss=1.78, v_num=1, train_loss_step=1.480, val_loss=2.820, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 243:  79%|█████▌ | 53/67 [00:16<00:04,  3.24it/s, loss=1.78, v_num=1, train_loss_step=1.480, val_loss=2.820, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 243:  81%|█████▋ | 54/67 [00:16<00:04,  3.23it/s, loss=1.78, v_num=1, train_loss_step=1.480, val_loss=2.820, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 243:  82%|█████▋ | 55/67 [00:17<00:03,  3.22it/s, loss=1.78, v_num=1, train_loss_step=1.480, val_loss=2.820, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 243:  84%|█████▊ | 56/67 [00:17<00:03,  3.22it/s, loss=1.78, v_num=1, train_loss_step=1.480, val_loss=2.820, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 243:  85%|█████▉ | 57/67 [00:17<00:03,  3.22it/s, loss=1.78, v_num=1, train_loss_step=1.480, val_loss=2.820, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 243:  87%|██████ | 58/67 [00:18<00:02,  3.21it/s, loss=1.78, v_num=1, train_loss_step=1.480, val_loss=2.820, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 243:  88%|██████▏| 59/67 [00:18<00:02,  3.21it/s, loss=1.78, v_num=1, train_loss_step=1.480, val_loss=2.820, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 243:  90%|██████▎| 60/67 [00:18<00:02,  3.20it/s, loss=1.78, v_num=1, train_loss_step=1.480, val_loss=2.820, train_loss_epoch=1.770]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 243:  91%|██████▎| 61/67 [00:19<00:01,  3.17it/s, loss=1.78, v_num=1, train_loss_step=1.480, val_loss=2.820, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 243:  93%|██████▍| 62/67 [00:19<00:01,  3.17it/s, loss=1.78, v_num=1, train_loss_step=1.480, val_loss=2.820, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 243:  94%|██████▌| 63/67 [00:19<00:01,  3.16it/s, loss=1.78, v_num=1, train_loss_step=1.480, val_loss=2.820, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 243:  96%|██████▋| 64/67 [00:20<00:00,  3.16it/s, loss=1.78, v_num=1, train_loss_step=1.480, val_loss=2.820, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 243:  97%|██████▊| 65/67 [00:20<00:00,  3.16it/s, loss=1.78, v_num=1, train_loss_step=1.480, val_loss=2.820, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 243:  99%|██████▉| 66/67 [00:20<00:00,  3.15it/s, loss=1.78, v_num=1, train_loss_step=1.480, val_loss=2.820, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 243: 100%|███████| 67/67 [00:21<00:00,  3.08it/s, loss=1.78, v_num=1, train_loss_step=1.480, val_loss=2.810, train_loss_epoch=1.770]\u001b[A\n",
      "Epoch 244:  45%|███▏   | 30/67 [00:06<00:07,  4.87it/s, loss=1.93, v_num=1, train_loss_step=0.682, val_loss=2.810, train_loss_epoch=1.740]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 244:  46%|███▏   | 31/67 [00:08<00:09,  3.64it/s, loss=1.93, v_num=1, train_loss_step=0.682, val_loss=2.810, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 244:  48%|███▎   | 32/67 [00:08<00:09,  3.62it/s, loss=1.93, v_num=1, train_loss_step=0.682, val_loss=2.810, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 244:  49%|███▍   | 33/67 [00:09<00:09,  3.60it/s, loss=1.93, v_num=1, train_loss_step=0.682, val_loss=2.810, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 244:  51%|███▌   | 34/67 [00:09<00:09,  3.57it/s, loss=1.93, v_num=1, train_loss_step=0.682, val_loss=2.810, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 244:  52%|███▋   | 35/67 [00:09<00:09,  3.55it/s, loss=1.93, v_num=1, train_loss_step=0.682, val_loss=2.810, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 244:  54%|███▊   | 36/67 [00:10<00:08,  3.53it/s, loss=1.93, v_num=1, train_loss_step=0.682, val_loss=2.810, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 244:  55%|███▊   | 37/67 [00:10<00:08,  3.51it/s, loss=1.93, v_num=1, train_loss_step=0.682, val_loss=2.810, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 244:  57%|███▉   | 38/67 [00:10<00:08,  3.49it/s, loss=1.93, v_num=1, train_loss_step=0.682, val_loss=2.810, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 244:  58%|████   | 39/67 [00:11<00:08,  3.48it/s, loss=1.93, v_num=1, train_loss_step=0.682, val_loss=2.810, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 244:  60%|████▏  | 40/67 [00:11<00:07,  3.46it/s, loss=1.93, v_num=1, train_loss_step=0.682, val_loss=2.810, train_loss_epoch=1.740]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 244:  61%|████▎  | 41/67 [00:12<00:07,  3.40it/s, loss=1.93, v_num=1, train_loss_step=0.682, val_loss=2.810, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 244:  63%|████▍  | 42/67 [00:12<00:07,  3.39it/s, loss=1.93, v_num=1, train_loss_step=0.682, val_loss=2.810, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 244:  64%|████▍  | 43/67 [00:12<00:07,  3.37it/s, loss=1.93, v_num=1, train_loss_step=0.682, val_loss=2.810, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 244:  66%|████▌  | 44/67 [00:13<00:06,  3.37it/s, loss=1.93, v_num=1, train_loss_step=0.682, val_loss=2.810, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 244:  67%|████▋  | 45/67 [00:13<00:06,  3.36it/s, loss=1.93, v_num=1, train_loss_step=0.682, val_loss=2.810, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 244:  69%|████▊  | 46/67 [00:13<00:06,  3.35it/s, loss=1.93, v_num=1, train_loss_step=0.682, val_loss=2.810, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 244:  70%|████▉  | 47/67 [00:14<00:05,  3.34it/s, loss=1.93, v_num=1, train_loss_step=0.682, val_loss=2.810, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 244:  72%|█████  | 48/67 [00:14<00:05,  3.34it/s, loss=1.93, v_num=1, train_loss_step=0.682, val_loss=2.810, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 244:  73%|█████  | 49/67 [00:14<00:05,  3.33it/s, loss=1.93, v_num=1, train_loss_step=0.682, val_loss=2.810, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 244:  75%|█████▏ | 50/67 [00:15<00:05,  3.32it/s, loss=1.93, v_num=1, train_loss_step=0.682, val_loss=2.810, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 244:  76%|█████▎ | 51/67 [00:15<00:04,  3.29it/s, loss=1.93, v_num=1, train_loss_step=0.682, val_loss=2.810, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 244:  78%|█████▍ | 52/67 [00:15<00:04,  3.28it/s, loss=1.93, v_num=1, train_loss_step=0.682, val_loss=2.810, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 244:  79%|█████▌ | 53/67 [00:16<00:04,  3.27it/s, loss=1.93, v_num=1, train_loss_step=0.682, val_loss=2.810, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 244:  81%|█████▋ | 54/67 [00:16<00:03,  3.27it/s, loss=1.93, v_num=1, train_loss_step=0.682, val_loss=2.810, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 244:  82%|█████▋ | 55/67 [00:16<00:03,  3.26it/s, loss=1.93, v_num=1, train_loss_step=0.682, val_loss=2.810, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 244:  84%|█████▊ | 56/67 [00:17<00:03,  3.25it/s, loss=1.93, v_num=1, train_loss_step=0.682, val_loss=2.810, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 244:  85%|█████▉ | 57/67 [00:17<00:03,  3.25it/s, loss=1.93, v_num=1, train_loss_step=0.682, val_loss=2.810, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 244:  87%|██████ | 58/67 [00:17<00:02,  3.24it/s, loss=1.93, v_num=1, train_loss_step=0.682, val_loss=2.810, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 244:  88%|██████▏| 59/67 [00:18<00:02,  3.24it/s, loss=1.93, v_num=1, train_loss_step=0.682, val_loss=2.810, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 244:  90%|██████▎| 60/67 [00:18<00:02,  3.23it/s, loss=1.93, v_num=1, train_loss_step=0.682, val_loss=2.810, train_loss_epoch=1.740]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 244:  91%|██████▎| 61/67 [00:19<00:01,  3.19it/s, loss=1.93, v_num=1, train_loss_step=0.682, val_loss=2.810, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 244:  93%|██████▍| 62/67 [00:19<00:01,  3.19it/s, loss=1.93, v_num=1, train_loss_step=0.682, val_loss=2.810, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 244:  94%|██████▌| 63/67 [00:19<00:01,  3.19it/s, loss=1.93, v_num=1, train_loss_step=0.682, val_loss=2.810, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 244:  96%|██████▋| 64/67 [00:20<00:00,  3.19it/s, loss=1.93, v_num=1, train_loss_step=0.682, val_loss=2.810, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 244:  97%|██████▊| 65/67 [00:20<00:00,  3.19it/s, loss=1.93, v_num=1, train_loss_step=0.682, val_loss=2.810, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 244:  99%|██████▉| 66/67 [00:20<00:00,  3.18it/s, loss=1.93, v_num=1, train_loss_step=0.682, val_loss=2.810, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 244: 100%|███████| 67/67 [00:21<00:00,  3.11it/s, loss=1.93, v_num=1, train_loss_step=0.682, val_loss=2.810, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 245:  45%|███▏   | 30/67 [00:05<00:07,  5.08it/s, loss=2.06, v_num=1, train_loss_step=2.060, val_loss=2.810, train_loss_epoch=1.950]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 245:  46%|███▏   | 31/67 [00:08<00:09,  3.80it/s, loss=2.06, v_num=1, train_loss_step=2.060, val_loss=2.810, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 245:  48%|███▎   | 32/67 [00:08<00:09,  3.77it/s, loss=2.06, v_num=1, train_loss_step=2.060, val_loss=2.810, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 245:  49%|███▍   | 33/67 [00:08<00:09,  3.75it/s, loss=2.06, v_num=1, train_loss_step=2.060, val_loss=2.810, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 245:  51%|███▌   | 34/67 [00:09<00:08,  3.72it/s, loss=2.06, v_num=1, train_loss_step=2.060, val_loss=2.810, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 245:  52%|███▋   | 35/67 [00:09<00:08,  3.69it/s, loss=2.06, v_num=1, train_loss_step=2.060, val_loss=2.810, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 245:  54%|███▊   | 36/67 [00:09<00:08,  3.67it/s, loss=2.06, v_num=1, train_loss_step=2.060, val_loss=2.810, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 245:  55%|███▊   | 37/67 [00:10<00:08,  3.65it/s, loss=2.06, v_num=1, train_loss_step=2.060, val_loss=2.810, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 245:  57%|███▉   | 38/67 [00:10<00:07,  3.63it/s, loss=2.06, v_num=1, train_loss_step=2.060, val_loss=2.810, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 245:  58%|████   | 39/67 [00:10<00:07,  3.61it/s, loss=2.06, v_num=1, train_loss_step=2.060, val_loss=2.810, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 245:  60%|████▏  | 40/67 [00:11<00:07,  3.59it/s, loss=2.06, v_num=1, train_loss_step=2.060, val_loss=2.810, train_loss_epoch=1.950]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 245:  61%|████▎  | 41/67 [00:11<00:07,  3.51it/s, loss=2.06, v_num=1, train_loss_step=2.060, val_loss=2.810, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 245:  63%|████▍  | 42/67 [00:12<00:07,  3.50it/s, loss=2.06, v_num=1, train_loss_step=2.060, val_loss=2.810, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 245:  64%|████▍  | 43/67 [00:12<00:06,  3.48it/s, loss=2.06, v_num=1, train_loss_step=2.060, val_loss=2.810, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 245:  66%|████▌  | 44/67 [00:12<00:06,  3.47it/s, loss=2.06, v_num=1, train_loss_step=2.060, val_loss=2.810, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 245:  67%|████▋  | 45/67 [00:13<00:06,  3.46it/s, loss=2.06, v_num=1, train_loss_step=2.060, val_loss=2.810, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 245:  69%|████▊  | 46/67 [00:13<00:06,  3.44it/s, loss=2.06, v_num=1, train_loss_step=2.060, val_loss=2.810, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 245:  70%|████▉  | 47/67 [00:13<00:05,  3.43it/s, loss=2.06, v_num=1, train_loss_step=2.060, val_loss=2.810, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 245:  72%|█████  | 48/67 [00:14<00:05,  3.42it/s, loss=2.06, v_num=1, train_loss_step=2.060, val_loss=2.810, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 245:  73%|█████  | 49/67 [00:14<00:05,  3.40it/s, loss=2.06, v_num=1, train_loss_step=2.060, val_loss=2.810, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 245:  75%|█████▏ | 50/67 [00:14<00:05,  3.39it/s, loss=2.06, v_num=1, train_loss_step=2.060, val_loss=2.810, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 245:  76%|█████▎ | 51/67 [00:15<00:04,  3.35it/s, loss=2.06, v_num=1, train_loss_step=2.060, val_loss=2.810, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 245:  78%|█████▍ | 52/67 [00:15<00:04,  3.34it/s, loss=2.06, v_num=1, train_loss_step=2.060, val_loss=2.810, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 245:  79%|█████▌ | 53/67 [00:15<00:04,  3.33it/s, loss=2.06, v_num=1, train_loss_step=2.060, val_loss=2.810, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 245:  81%|█████▋ | 54/67 [00:16<00:03,  3.33it/s, loss=2.06, v_num=1, train_loss_step=2.060, val_loss=2.810, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 245:  82%|█████▋ | 55/67 [00:16<00:03,  3.29it/s, loss=2.06, v_num=1, train_loss_step=2.060, val_loss=2.810, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 245:  84%|█████▊ | 56/67 [00:17<00:03,  3.29it/s, loss=2.06, v_num=1, train_loss_step=2.060, val_loss=2.810, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 245:  85%|█████▉ | 57/67 [00:17<00:03,  3.29it/s, loss=2.06, v_num=1, train_loss_step=2.060, val_loss=2.810, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 245:  87%|██████ | 58/67 [00:17<00:02,  3.27it/s, loss=2.06, v_num=1, train_loss_step=2.060, val_loss=2.810, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 245:  88%|██████▏| 59/67 [00:18<00:02,  3.26it/s, loss=2.06, v_num=1, train_loss_step=2.060, val_loss=2.810, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 245:  90%|██████▎| 60/67 [00:18<00:02,  3.26it/s, loss=2.06, v_num=1, train_loss_step=2.060, val_loss=2.810, train_loss_epoch=1.950]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 245:  91%|██████▎| 61/67 [00:18<00:01,  3.22it/s, loss=2.06, v_num=1, train_loss_step=2.060, val_loss=2.810, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 245:  93%|██████▍| 62/67 [00:19<00:01,  3.22it/s, loss=2.06, v_num=1, train_loss_step=2.060, val_loss=2.810, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 245:  94%|██████▌| 63/67 [00:19<00:01,  3.21it/s, loss=2.06, v_num=1, train_loss_step=2.060, val_loss=2.810, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 245:  96%|██████▋| 64/67 [00:19<00:00,  3.21it/s, loss=2.06, v_num=1, train_loss_step=2.060, val_loss=2.810, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 245:  97%|██████▊| 65/67 [00:20<00:00,  3.21it/s, loss=2.06, v_num=1, train_loss_step=2.060, val_loss=2.810, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 245:  99%|██████▉| 66/67 [00:20<00:00,  3.21it/s, loss=2.06, v_num=1, train_loss_step=2.060, val_loss=2.810, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 245: 100%|███████| 67/67 [00:21<00:00,  3.13it/s, loss=2.06, v_num=1, train_loss_step=2.060, val_loss=2.810, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 246:  45%|███▏   | 30/67 [00:06<00:07,  4.99it/s, loss=1.85, v_num=1, train_loss_step=1.170, val_loss=2.810, train_loss_epoch=2.550]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 246:  46%|███▏   | 31/67 [00:08<00:09,  3.75it/s, loss=1.85, v_num=1, train_loss_step=1.170, val_loss=2.810, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 246:  48%|███▎   | 32/67 [00:08<00:09,  3.72it/s, loss=1.85, v_num=1, train_loss_step=1.170, val_loss=2.810, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 246:  49%|███▍   | 33/67 [00:08<00:09,  3.69it/s, loss=1.85, v_num=1, train_loss_step=1.170, val_loss=2.810, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 246:  51%|███▌   | 34/67 [00:09<00:09,  3.66it/s, loss=1.85, v_num=1, train_loss_step=1.170, val_loss=2.810, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 246:  52%|███▋   | 35/67 [00:09<00:08,  3.63it/s, loss=1.85, v_num=1, train_loss_step=1.170, val_loss=2.810, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 246:  54%|███▊   | 36/67 [00:09<00:08,  3.61it/s, loss=1.85, v_num=1, train_loss_step=1.170, val_loss=2.810, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 246:  55%|███▊   | 37/67 [00:10<00:08,  3.59it/s, loss=1.85, v_num=1, train_loss_step=1.170, val_loss=2.810, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 246:  57%|███▉   | 38/67 [00:10<00:08,  3.58it/s, loss=1.85, v_num=1, train_loss_step=1.170, val_loss=2.810, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 246:  58%|████   | 39/67 [00:10<00:07,  3.56it/s, loss=1.85, v_num=1, train_loss_step=1.170, val_loss=2.810, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 246:  60%|████▏  | 40/67 [00:11<00:07,  3.54it/s, loss=1.85, v_num=1, train_loss_step=1.170, val_loss=2.810, train_loss_epoch=2.550]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 246:  61%|████▎  | 41/67 [00:11<00:07,  3.47it/s, loss=1.85, v_num=1, train_loss_step=1.170, val_loss=2.810, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 246:  63%|████▍  | 42/67 [00:12<00:07,  3.46it/s, loss=1.85, v_num=1, train_loss_step=1.170, val_loss=2.810, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 246:  64%|████▍  | 43/67 [00:12<00:06,  3.45it/s, loss=1.85, v_num=1, train_loss_step=1.170, val_loss=2.810, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 246:  66%|████▌  | 44/67 [00:12<00:06,  3.43it/s, loss=1.85, v_num=1, train_loss_step=1.170, val_loss=2.810, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 246:  67%|████▋  | 45/67 [00:13<00:06,  3.42it/s, loss=1.85, v_num=1, train_loss_step=1.170, val_loss=2.810, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 246:  69%|████▊  | 46/67 [00:13<00:06,  3.41it/s, loss=1.85, v_num=1, train_loss_step=1.170, val_loss=2.810, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 246:  70%|████▉  | 47/67 [00:13<00:05,  3.39it/s, loss=1.85, v_num=1, train_loss_step=1.170, val_loss=2.810, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 246:  72%|█████  | 48/67 [00:14<00:05,  3.38it/s, loss=1.85, v_num=1, train_loss_step=1.170, val_loss=2.810, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 246:  73%|█████  | 49/67 [00:14<00:05,  3.37it/s, loss=1.85, v_num=1, train_loss_step=1.170, val_loss=2.810, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 246:  75%|█████▏ | 50/67 [00:14<00:05,  3.35it/s, loss=1.85, v_num=1, train_loss_step=1.170, val_loss=2.810, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 246:  76%|█████▎ | 51/67 [00:15<00:04,  3.29it/s, loss=1.85, v_num=1, train_loss_step=1.170, val_loss=2.810, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 246:  78%|█████▍ | 52/67 [00:15<00:04,  3.29it/s, loss=1.85, v_num=1, train_loss_step=1.170, val_loss=2.810, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 246:  79%|█████▌ | 53/67 [00:16<00:04,  3.28it/s, loss=1.85, v_num=1, train_loss_step=1.170, val_loss=2.810, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 246:  81%|█████▋ | 54/67 [00:16<00:03,  3.27it/s, loss=1.85, v_num=1, train_loss_step=1.170, val_loss=2.810, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 246:  82%|█████▋ | 55/67 [00:16<00:03,  3.26it/s, loss=1.85, v_num=1, train_loss_step=1.170, val_loss=2.810, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 246:  84%|█████▊ | 56/67 [00:17<00:03,  3.25it/s, loss=1.85, v_num=1, train_loss_step=1.170, val_loss=2.810, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 246:  85%|█████▉ | 57/67 [00:17<00:03,  3.25it/s, loss=1.85, v_num=1, train_loss_step=1.170, val_loss=2.810, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 246:  87%|██████ | 58/67 [00:17<00:02,  3.24it/s, loss=1.85, v_num=1, train_loss_step=1.170, val_loss=2.810, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 246:  88%|██████▏| 59/67 [00:18<00:02,  3.24it/s, loss=1.85, v_num=1, train_loss_step=1.170, val_loss=2.810, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 246:  90%|██████▎| 60/67 [00:18<00:02,  3.23it/s, loss=1.85, v_num=1, train_loss_step=1.170, val_loss=2.810, train_loss_epoch=2.550]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 246:  91%|██████▎| 61/67 [00:19<00:01,  3.19it/s, loss=1.85, v_num=1, train_loss_step=1.170, val_loss=2.810, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 246:  93%|██████▍| 62/67 [00:19<00:01,  3.18it/s, loss=1.85, v_num=1, train_loss_step=1.170, val_loss=2.810, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 246:  94%|██████▌| 63/67 [00:19<00:01,  3.17it/s, loss=1.85, v_num=1, train_loss_step=1.170, val_loss=2.810, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 246:  96%|██████▋| 64/67 [00:20<00:00,  3.17it/s, loss=1.85, v_num=1, train_loss_step=1.170, val_loss=2.810, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 246:  97%|██████▊| 65/67 [00:20<00:00,  3.16it/s, loss=1.85, v_num=1, train_loss_step=1.170, val_loss=2.810, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 246:  99%|██████▉| 66/67 [00:20<00:00,  3.16it/s, loss=1.85, v_num=1, train_loss_step=1.170, val_loss=2.810, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 246: 100%|███████| 67/67 [00:21<00:00,  3.09it/s, loss=1.85, v_num=1, train_loss_step=1.170, val_loss=2.810, train_loss_epoch=2.550]\u001b[A\n",
      "Epoch 247:  10%|▊       | 7/67 [00:02<00:18,  3.32it/s, loss=1.93, v_num=1, train_loss_step=2.480, val_loss=2.810, train_loss_epoch=1.910]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 269:  70%|████▉  | 47/67 [00:13<00:05,  3.45it/s, loss=1.93, v_num=1, train_loss_step=1.230, val_loss=2.780, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 269:  72%|█████  | 48/67 [00:13<00:05,  3.44it/s, loss=1.93, v_num=1, train_loss_step=1.230, val_loss=2.780, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 269:  73%|█████  | 49/67 [00:14<00:05,  3.43it/s, loss=1.93, v_num=1, train_loss_step=1.230, val_loss=2.780, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 269:  75%|█████▏ | 50/67 [00:14<00:04,  3.42it/s, loss=1.93, v_num=1, train_loss_step=1.230, val_loss=2.780, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 269:  76%|█████▎ | 51/67 [00:15<00:04,  3.38it/s, loss=1.93, v_num=1, train_loss_step=1.230, val_loss=2.780, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 269:  78%|█████▍ | 52/67 [00:15<00:04,  3.37it/s, loss=1.93, v_num=1, train_loss_step=1.230, val_loss=2.780, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 269:  79%|█████▌ | 53/67 [00:15<00:04,  3.36it/s, loss=1.93, v_num=1, train_loss_step=1.230, val_loss=2.780, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 269:  81%|█████▋ | 54/67 [00:16<00:03,  3.36it/s, loss=1.93, v_num=1, train_loss_step=1.230, val_loss=2.780, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 269:  82%|█████▋ | 55/67 [00:16<00:03,  3.35it/s, loss=1.93, v_num=1, train_loss_step=1.230, val_loss=2.780, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 269:  84%|█████▊ | 56/67 [00:16<00:03,  3.34it/s, loss=1.93, v_num=1, train_loss_step=1.230, val_loss=2.780, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 269:  85%|█████▉ | 57/67 [00:17<00:02,  3.34it/s, loss=1.93, v_num=1, train_loss_step=1.230, val_loss=2.780, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 269:  87%|██████ | 58/67 [00:17<00:02,  3.33it/s, loss=1.93, v_num=1, train_loss_step=1.230, val_loss=2.780, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 269:  88%|██████▏| 59/67 [00:17<00:02,  3.33it/s, loss=1.93, v_num=1, train_loss_step=1.230, val_loss=2.780, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 269:  90%|██████▎| 60/67 [00:18<00:02,  3.32it/s, loss=1.93, v_num=1, train_loss_step=1.230, val_loss=2.780, train_loss_epoch=1.640]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 269:  91%|██████▎| 61/67 [00:18<00:01,  3.29it/s, loss=1.93, v_num=1, train_loss_step=1.230, val_loss=2.780, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 269:  93%|██████▍| 62/67 [00:18<00:01,  3.28it/s, loss=1.93, v_num=1, train_loss_step=1.230, val_loss=2.780, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 269:  94%|██████▌| 63/67 [00:19<00:01,  3.28it/s, loss=1.93, v_num=1, train_loss_step=1.230, val_loss=2.780, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 269:  96%|██████▋| 64/67 [00:19<00:00,  3.28it/s, loss=1.93, v_num=1, train_loss_step=1.230, val_loss=2.780, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 269:  97%|██████▊| 65/67 [00:19<00:00,  3.27it/s, loss=1.93, v_num=1, train_loss_step=1.230, val_loss=2.780, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 269:  99%|██████▉| 66/67 [00:20<00:00,  3.27it/s, loss=1.93, v_num=1, train_loss_step=1.230, val_loss=2.780, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 269: 100%|███████| 67/67 [00:20<00:00,  3.20it/s, loss=1.93, v_num=1, train_loss_step=1.230, val_loss=2.790, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 270:  45%|███▏   | 30/67 [00:06<00:07,  4.77it/s, loss=1.96, v_num=1, train_loss_step=2.590, val_loss=2.790, train_loss_epoch=1.790]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 270:  46%|███▏   | 31/67 [00:08<00:09,  3.66it/s, loss=1.96, v_num=1, train_loss_step=2.590, val_loss=2.790, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 270:  48%|███▎   | 32/67 [00:08<00:09,  3.63it/s, loss=1.96, v_num=1, train_loss_step=2.590, val_loss=2.790, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 270:  49%|███▍   | 33/67 [00:09<00:09,  3.62it/s, loss=1.96, v_num=1, train_loss_step=2.590, val_loss=2.790, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 270:  51%|███▌   | 34/67 [00:09<00:09,  3.59it/s, loss=1.96, v_num=1, train_loss_step=2.590, val_loss=2.790, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 270:  52%|███▋   | 35/67 [00:09<00:08,  3.58it/s, loss=1.96, v_num=1, train_loss_step=2.590, val_loss=2.790, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 270:  54%|███▊   | 36/67 [00:10<00:08,  3.56it/s, loss=1.96, v_num=1, train_loss_step=2.590, val_loss=2.790, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 270:  55%|███▊   | 37/67 [00:10<00:08,  3.54it/s, loss=1.96, v_num=1, train_loss_step=2.590, val_loss=2.790, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 270:  57%|███▉   | 38/67 [00:10<00:08,  3.52it/s, loss=1.96, v_num=1, train_loss_step=2.590, val_loss=2.790, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 270:  58%|████   | 39/67 [00:11<00:07,  3.51it/s, loss=1.96, v_num=1, train_loss_step=2.590, val_loss=2.790, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 270:  60%|████▏  | 40/67 [00:11<00:07,  3.49it/s, loss=1.96, v_num=1, train_loss_step=2.590, val_loss=2.790, train_loss_epoch=1.790]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 270:  61%|████▎  | 41/67 [00:11<00:07,  3.44it/s, loss=1.96, v_num=1, train_loss_step=2.590, val_loss=2.790, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 270:  63%|████▍  | 42/67 [00:12<00:07,  3.43it/s, loss=1.96, v_num=1, train_loss_step=2.590, val_loss=2.790, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 270:  64%|████▍  | 43/67 [00:12<00:07,  3.41it/s, loss=1.96, v_num=1, train_loss_step=2.590, val_loss=2.790, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 270:  66%|████▌  | 44/67 [00:12<00:06,  3.40it/s, loss=1.96, v_num=1, train_loss_step=2.590, val_loss=2.790, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 270:  67%|████▋  | 45/67 [00:13<00:06,  3.39it/s, loss=1.96, v_num=1, train_loss_step=2.590, val_loss=2.790, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 270:  69%|████▊  | 46/67 [00:13<00:06,  3.38it/s, loss=1.96, v_num=1, train_loss_step=2.590, val_loss=2.790, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 270:  70%|████▉  | 47/67 [00:13<00:05,  3.36it/s, loss=1.96, v_num=1, train_loss_step=2.590, val_loss=2.790, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 270:  72%|█████  | 48/67 [00:14<00:05,  3.36it/s, loss=1.96, v_num=1, train_loss_step=2.590, val_loss=2.790, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 270:  73%|█████  | 49/67 [00:14<00:05,  3.35it/s, loss=1.96, v_num=1, train_loss_step=2.590, val_loss=2.790, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 270:  75%|█████▏ | 50/67 [00:14<00:05,  3.34it/s, loss=1.96, v_num=1, train_loss_step=2.590, val_loss=2.790, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 270:  76%|█████▎ | 51/67 [00:15<00:04,  3.30it/s, loss=1.96, v_num=1, train_loss_step=2.590, val_loss=2.790, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 270:  78%|█████▍ | 52/67 [00:15<00:04,  3.29it/s, loss=1.96, v_num=1, train_loss_step=2.590, val_loss=2.790, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 270:  79%|█████▌ | 53/67 [00:16<00:04,  3.29it/s, loss=1.96, v_num=1, train_loss_step=2.590, val_loss=2.790, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 270:  81%|█████▋ | 54/67 [00:16<00:03,  3.28it/s, loss=1.96, v_num=1, train_loss_step=2.590, val_loss=2.790, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 270:  82%|█████▋ | 55/67 [00:16<00:03,  3.28it/s, loss=1.96, v_num=1, train_loss_step=2.590, val_loss=2.790, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 270:  84%|█████▊ | 56/67 [00:17<00:03,  3.27it/s, loss=1.96, v_num=1, train_loss_step=2.590, val_loss=2.790, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 270:  85%|█████▉ | 57/67 [00:17<00:03,  3.27it/s, loss=1.96, v_num=1, train_loss_step=2.590, val_loss=2.790, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 270:  87%|██████ | 58/67 [00:17<00:02,  3.27it/s, loss=1.96, v_num=1, train_loss_step=2.590, val_loss=2.790, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 270:  88%|██████▏| 59/67 [00:18<00:02,  3.26it/s, loss=1.96, v_num=1, train_loss_step=2.590, val_loss=2.790, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 270:  90%|██████▎| 60/67 [00:18<00:02,  3.26it/s, loss=1.96, v_num=1, train_loss_step=2.590, val_loss=2.790, train_loss_epoch=1.790]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 270:  91%|██████▎| 61/67 [00:18<00:01,  3.22it/s, loss=1.96, v_num=1, train_loss_step=2.590, val_loss=2.790, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 270:  93%|██████▍| 62/67 [00:19<00:01,  3.22it/s, loss=1.96, v_num=1, train_loss_step=2.590, val_loss=2.790, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 270:  94%|██████▌| 63/67 [00:19<00:01,  3.22it/s, loss=1.96, v_num=1, train_loss_step=2.590, val_loss=2.790, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 270:  96%|██████▋| 64/67 [00:19<00:00,  3.22it/s, loss=1.96, v_num=1, train_loss_step=2.590, val_loss=2.790, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 270:  97%|██████▊| 65/67 [00:20<00:00,  3.22it/s, loss=1.96, v_num=1, train_loss_step=2.590, val_loss=2.790, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 270:  99%|██████▉| 66/67 [00:20<00:00,  3.21it/s, loss=1.96, v_num=1, train_loss_step=2.590, val_loss=2.790, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 270: 100%|███████| 67/67 [00:21<00:00,  3.14it/s, loss=1.96, v_num=1, train_loss_step=2.590, val_loss=2.780, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 271:  45%|███▏   | 30/67 [00:06<00:07,  4.94it/s, loss=2.06, v_num=1, train_loss_step=1.750, val_loss=2.780, train_loss_epoch=2.270]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 271:  46%|███▏   | 31/67 [00:08<00:09,  3.70it/s, loss=2.06, v_num=1, train_loss_step=1.750, val_loss=2.780, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 271:  48%|███▎   | 32/67 [00:08<00:09,  3.67it/s, loss=2.06, v_num=1, train_loss_step=1.750, val_loss=2.780, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 271:  49%|███▍   | 33/67 [00:09<00:09,  3.63it/s, loss=2.06, v_num=1, train_loss_step=1.750, val_loss=2.780, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 271:  51%|███▌   | 34/67 [00:09<00:09,  3.59it/s, loss=2.06, v_num=1, train_loss_step=1.750, val_loss=2.780, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 271:  52%|███▋   | 35/67 [00:09<00:08,  3.57it/s, loss=2.06, v_num=1, train_loss_step=1.750, val_loss=2.780, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 271:  54%|███▊   | 36/67 [00:10<00:08,  3.55it/s, loss=2.06, v_num=1, train_loss_step=1.750, val_loss=2.780, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 271:  55%|███▊   | 37/67 [00:10<00:08,  3.54it/s, loss=2.06, v_num=1, train_loss_step=1.750, val_loss=2.780, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 271:  57%|███▉   | 38/67 [00:10<00:08,  3.53it/s, loss=2.06, v_num=1, train_loss_step=1.750, val_loss=2.780, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 271:  58%|████   | 39/67 [00:11<00:07,  3.51it/s, loss=2.06, v_num=1, train_loss_step=1.750, val_loss=2.780, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 271:  60%|████▏  | 40/67 [00:11<00:07,  3.50it/s, loss=2.06, v_num=1, train_loss_step=1.750, val_loss=2.780, train_loss_epoch=2.270]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 271:  61%|████▎  | 41/67 [00:11<00:07,  3.43it/s, loss=2.06, v_num=1, train_loss_step=1.750, val_loss=2.780, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 271:  63%|████▍  | 42/67 [00:12<00:07,  3.42it/s, loss=2.06, v_num=1, train_loss_step=1.750, val_loss=2.780, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 271:  64%|████▍  | 43/67 [00:12<00:07,  3.41it/s, loss=2.06, v_num=1, train_loss_step=1.750, val_loss=2.780, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 271:  66%|████▌  | 44/67 [00:12<00:06,  3.40it/s, loss=2.06, v_num=1, train_loss_step=1.750, val_loss=2.780, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 271:  67%|████▋  | 45/67 [00:13<00:06,  3.39it/s, loss=2.06, v_num=1, train_loss_step=1.750, val_loss=2.780, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 271:  69%|████▊  | 46/67 [00:13<00:06,  3.38it/s, loss=2.06, v_num=1, train_loss_step=1.750, val_loss=2.780, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 271:  70%|████▉  | 47/67 [00:13<00:05,  3.37it/s, loss=2.06, v_num=1, train_loss_step=1.750, val_loss=2.780, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 271:  72%|█████  | 48/67 [00:14<00:05,  3.36it/s, loss=2.06, v_num=1, train_loss_step=1.750, val_loss=2.780, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 271:  73%|█████  | 49/67 [00:14<00:05,  3.35it/s, loss=2.06, v_num=1, train_loss_step=1.750, val_loss=2.780, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 271:  75%|█████▏ | 50/67 [00:14<00:05,  3.35it/s, loss=2.06, v_num=1, train_loss_step=1.750, val_loss=2.780, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 271:  76%|█████▎ | 51/67 [00:15<00:04,  3.30it/s, loss=2.06, v_num=1, train_loss_step=1.750, val_loss=2.780, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 271:  78%|█████▍ | 52/67 [00:15<00:04,  3.29it/s, loss=2.06, v_num=1, train_loss_step=1.750, val_loss=2.780, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 271:  79%|█████▌ | 53/67 [00:16<00:04,  3.29it/s, loss=2.06, v_num=1, train_loss_step=1.750, val_loss=2.780, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 271:  81%|█████▋ | 54/67 [00:16<00:03,  3.28it/s, loss=2.06, v_num=1, train_loss_step=1.750, val_loss=2.780, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 271:  82%|█████▋ | 55/67 [00:16<00:03,  3.28it/s, loss=2.06, v_num=1, train_loss_step=1.750, val_loss=2.780, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 271:  84%|█████▊ | 56/67 [00:17<00:03,  3.27it/s, loss=2.06, v_num=1, train_loss_step=1.750, val_loss=2.780, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 271:  85%|█████▉ | 57/67 [00:17<00:03,  3.26it/s, loss=2.06, v_num=1, train_loss_step=1.750, val_loss=2.780, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 271:  87%|██████ | 58/67 [00:17<00:02,  3.26it/s, loss=2.06, v_num=1, train_loss_step=1.750, val_loss=2.780, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 271:  88%|██████▏| 59/67 [00:18<00:02,  3.25it/s, loss=2.06, v_num=1, train_loss_step=1.750, val_loss=2.780, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 271:  90%|██████▎| 60/67 [00:18<00:02,  3.25it/s, loss=2.06, v_num=1, train_loss_step=1.750, val_loss=2.780, train_loss_epoch=2.270]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 271:  91%|██████▎| 61/67 [00:18<00:01,  3.22it/s, loss=2.06, v_num=1, train_loss_step=1.750, val_loss=2.780, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 271:  93%|██████▍| 62/67 [00:19<00:01,  3.21it/s, loss=2.06, v_num=1, train_loss_step=1.750, val_loss=2.780, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 271:  94%|██████▌| 63/67 [00:19<00:01,  3.21it/s, loss=2.06, v_num=1, train_loss_step=1.750, val_loss=2.780, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 271:  96%|██████▋| 64/67 [00:19<00:00,  3.21it/s, loss=2.06, v_num=1, train_loss_step=1.750, val_loss=2.780, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 271:  97%|██████▊| 65/67 [00:20<00:00,  3.20it/s, loss=2.06, v_num=1, train_loss_step=1.750, val_loss=2.780, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 271:  99%|██████▉| 66/67 [00:20<00:00,  3.20it/s, loss=2.06, v_num=1, train_loss_step=1.750, val_loss=2.780, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 271: 100%|███████| 67/67 [00:21<00:00,  3.13it/s, loss=2.06, v_num=1, train_loss_step=1.750, val_loss=2.780, train_loss_epoch=2.270]\u001b[A\n",
      "Epoch 272:  45%|███▌    | 30/67 [00:06<00:07,  4.97it/s, loss=2.1, v_num=1, train_loss_step=2.720, val_loss=2.780, train_loss_epoch=1.940]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 272:  46%|███▋    | 31/67 [00:08<00:09,  3.82it/s, loss=2.1, v_num=1, train_loss_step=2.720, val_loss=2.780, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 272:  48%|███▊    | 32/67 [00:08<00:09,  3.80it/s, loss=2.1, v_num=1, train_loss_step=2.720, val_loss=2.780, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 272:  49%|███▉    | 33/67 [00:08<00:09,  3.77it/s, loss=2.1, v_num=1, train_loss_step=2.720, val_loss=2.780, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 272:  51%|████    | 34/67 [00:09<00:08,  3.75it/s, loss=2.1, v_num=1, train_loss_step=2.720, val_loss=2.780, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 272:  52%|████▏   | 35/67 [00:09<00:08,  3.73it/s, loss=2.1, v_num=1, train_loss_step=2.720, val_loss=2.780, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 272:  54%|████▎   | 36/67 [00:09<00:08,  3.72it/s, loss=2.1, v_num=1, train_loss_step=2.720, val_loss=2.780, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 272:  55%|████▍   | 37/67 [00:10<00:08,  3.70it/s, loss=2.1, v_num=1, train_loss_step=2.720, val_loss=2.780, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 272:  57%|████▌   | 38/67 [00:10<00:07,  3.69it/s, loss=2.1, v_num=1, train_loss_step=2.720, val_loss=2.780, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 272:  58%|████▋   | 39/67 [00:10<00:07,  3.68it/s, loss=2.1, v_num=1, train_loss_step=2.720, val_loss=2.780, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 272:  60%|████▊   | 40/67 [00:10<00:07,  3.66it/s, loss=2.1, v_num=1, train_loss_step=2.720, val_loss=2.780, train_loss_epoch=1.940]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 272:  61%|████▉   | 41/67 [00:11<00:07,  3.61it/s, loss=2.1, v_num=1, train_loss_step=2.720, val_loss=2.780, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 272:  63%|█████   | 42/67 [00:11<00:06,  3.60it/s, loss=2.1, v_num=1, train_loss_step=2.720, val_loss=2.780, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 272:  64%|█████▏  | 43/67 [00:11<00:06,  3.59it/s, loss=2.1, v_num=1, train_loss_step=2.720, val_loss=2.780, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 272:  66%|█████▎  | 44/67 [00:12<00:06,  3.57it/s, loss=2.1, v_num=1, train_loss_step=2.720, val_loss=2.780, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 272:  67%|█████▎  | 45/67 [00:12<00:06,  3.56it/s, loss=2.1, v_num=1, train_loss_step=2.720, val_loss=2.780, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 272:  69%|█████▍  | 46/67 [00:12<00:05,  3.56it/s, loss=2.1, v_num=1, train_loss_step=2.720, val_loss=2.780, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 272:  70%|█████▌  | 47/67 [00:13<00:05,  3.55it/s, loss=2.1, v_num=1, train_loss_step=2.720, val_loss=2.780, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 272:  72%|█████▋  | 48/67 [00:13<00:05,  3.54it/s, loss=2.1, v_num=1, train_loss_step=2.720, val_loss=2.780, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 272:  73%|█████▊  | 49/67 [00:13<00:05,  3.53it/s, loss=2.1, v_num=1, train_loss_step=2.720, val_loss=2.780, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 272:  75%|█████▉  | 50/67 [00:14<00:04,  3.52it/s, loss=2.1, v_num=1, train_loss_step=2.720, val_loss=2.780, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 272:  76%|██████  | 51/67 [00:14<00:04,  3.47it/s, loss=2.1, v_num=1, train_loss_step=2.720, val_loss=2.780, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 272:  78%|██████▏ | 52/67 [00:15<00:04,  3.46it/s, loss=2.1, v_num=1, train_loss_step=2.720, val_loss=2.780, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 272:  79%|██████▎ | 53/67 [00:15<00:04,  3.46it/s, loss=2.1, v_num=1, train_loss_step=2.720, val_loss=2.780, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 272:  81%|██████▍ | 54/67 [00:15<00:03,  3.45it/s, loss=2.1, v_num=1, train_loss_step=2.720, val_loss=2.780, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 272:  82%|██████▌ | 55/67 [00:15<00:03,  3.44it/s, loss=2.1, v_num=1, train_loss_step=2.720, val_loss=2.780, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 272:  84%|██████▋ | 56/67 [00:16<00:03,  3.44it/s, loss=2.1, v_num=1, train_loss_step=2.720, val_loss=2.780, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 272:  85%|██████▊ | 57/67 [00:16<00:02,  3.43it/s, loss=2.1, v_num=1, train_loss_step=2.720, val_loss=2.780, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 272:  87%|██████▉ | 58/67 [00:16<00:02,  3.42it/s, loss=2.1, v_num=1, train_loss_step=2.720, val_loss=2.780, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 272:  88%|███████ | 59/67 [00:17<00:02,  3.42it/s, loss=2.1, v_num=1, train_loss_step=2.720, val_loss=2.780, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 272:  90%|███████▏| 60/67 [00:17<00:02,  3.41it/s, loss=2.1, v_num=1, train_loss_step=2.720, val_loss=2.780, train_loss_epoch=1.940]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 272:  91%|███████▎| 61/67 [00:18<00:01,  3.38it/s, loss=2.1, v_num=1, train_loss_step=2.720, val_loss=2.780, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 272:  93%|███████▍| 62/67 [00:18<00:01,  3.37it/s, loss=2.1, v_num=1, train_loss_step=2.720, val_loss=2.780, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 272:  94%|███████▌| 63/67 [00:18<00:01,  3.37it/s, loss=2.1, v_num=1, train_loss_step=2.720, val_loss=2.780, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 272:  96%|███████▋| 64/67 [00:19<00:00,  3.37it/s, loss=2.1, v_num=1, train_loss_step=2.720, val_loss=2.780, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 272:  97%|███████▊| 65/67 [00:19<00:00,  3.36it/s, loss=2.1, v_num=1, train_loss_step=2.720, val_loss=2.780, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 272:  99%|███████▉| 66/67 [00:19<00:00,  3.36it/s, loss=2.1, v_num=1, train_loss_step=2.720, val_loss=2.780, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 272: 100%|████████| 67/67 [00:20<00:00,  3.29it/s, loss=2.1, v_num=1, train_loss_step=2.720, val_loss=2.780, train_loss_epoch=1.940]\u001b[A\n",
      "Epoch 273:  45%|███▏   | 30/67 [00:05<00:07,  5.02it/s, loss=2.24, v_num=1, train_loss_step=4.990, val_loss=2.780, train_loss_epoch=1.960]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 273:  46%|███▏   | 31/67 [00:08<00:09,  3.82it/s, loss=2.24, v_num=1, train_loss_step=4.990, val_loss=2.780, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 273:  48%|███▎   | 32/67 [00:08<00:09,  3.78it/s, loss=2.24, v_num=1, train_loss_step=4.990, val_loss=2.780, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 273:  49%|███▍   | 33/67 [00:08<00:09,  3.75it/s, loss=2.24, v_num=1, train_loss_step=4.990, val_loss=2.780, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 273:  51%|███▌   | 34/67 [00:09<00:08,  3.72it/s, loss=2.24, v_num=1, train_loss_step=4.990, val_loss=2.780, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 273:  52%|███▋   | 35/67 [00:09<00:08,  3.69it/s, loss=2.24, v_num=1, train_loss_step=4.990, val_loss=2.780, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 273:  54%|███▊   | 36/67 [00:09<00:08,  3.66it/s, loss=2.24, v_num=1, train_loss_step=4.990, val_loss=2.780, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 273:  55%|███▊   | 37/67 [00:10<00:08,  3.63it/s, loss=2.24, v_num=1, train_loss_step=4.990, val_loss=2.780, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 273:  57%|███▉   | 38/67 [00:10<00:08,  3.61it/s, loss=2.24, v_num=1, train_loss_step=4.990, val_loss=2.780, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 273:  58%|████   | 39/67 [00:10<00:07,  3.58it/s, loss=2.24, v_num=1, train_loss_step=4.990, val_loss=2.780, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 273:  60%|████▏  | 40/67 [00:11<00:07,  3.56it/s, loss=2.24, v_num=1, train_loss_step=4.990, val_loss=2.780, train_loss_epoch=1.960]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 273:  61%|████▎  | 41/67 [00:11<00:07,  3.50it/s, loss=2.24, v_num=1, train_loss_step=4.990, val_loss=2.780, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 273:  63%|████▍  | 42/67 [00:12<00:07,  3.50it/s, loss=2.24, v_num=1, train_loss_step=4.990, val_loss=2.780, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 273:  64%|████▍  | 43/67 [00:12<00:06,  3.49it/s, loss=2.24, v_num=1, train_loss_step=4.990, val_loss=2.780, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 273:  66%|████▌  | 44/67 [00:12<00:06,  3.49it/s, loss=2.24, v_num=1, train_loss_step=4.990, val_loss=2.780, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 273:  67%|████▋  | 45/67 [00:12<00:06,  3.48it/s, loss=2.24, v_num=1, train_loss_step=4.990, val_loss=2.780, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 273:  69%|████▊  | 46/67 [00:13<00:06,  3.48it/s, loss=2.24, v_num=1, train_loss_step=4.990, val_loss=2.780, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 273:  70%|████▉  | 47/67 [00:13<00:05,  3.48it/s, loss=2.24, v_num=1, train_loss_step=4.990, val_loss=2.780, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 273:  72%|█████  | 48/67 [00:13<00:05,  3.48it/s, loss=2.24, v_num=1, train_loss_step=4.990, val_loss=2.780, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 273:  73%|█████  | 49/67 [00:14<00:05,  3.47it/s, loss=2.24, v_num=1, train_loss_step=4.990, val_loss=2.780, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 273:  75%|█████▏ | 50/67 [00:14<00:04,  3.47it/s, loss=2.24, v_num=1, train_loss_step=4.990, val_loss=2.780, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 273:  76%|█████▎ | 51/67 [00:14<00:04,  3.44it/s, loss=2.24, v_num=1, train_loss_step=4.990, val_loss=2.780, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 273:  78%|█████▍ | 52/67 [00:15<00:04,  3.43it/s, loss=2.24, v_num=1, train_loss_step=4.990, val_loss=2.780, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 273:  79%|█████▌ | 53/67 [00:15<00:04,  3.43it/s, loss=2.24, v_num=1, train_loss_step=4.990, val_loss=2.780, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 273:  81%|█████▋ | 54/67 [00:15<00:03,  3.43it/s, loss=2.24, v_num=1, train_loss_step=4.990, val_loss=2.780, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 273:  82%|█████▋ | 55/67 [00:16<00:03,  3.43it/s, loss=2.24, v_num=1, train_loss_step=4.990, val_loss=2.780, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 273:  84%|█████▊ | 56/67 [00:16<00:03,  3.43it/s, loss=2.24, v_num=1, train_loss_step=4.990, val_loss=2.780, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 273:  85%|█████▉ | 57/67 [00:16<00:02,  3.43it/s, loss=2.24, v_num=1, train_loss_step=4.990, val_loss=2.780, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 273:  87%|██████ | 58/67 [00:16<00:02,  3.43it/s, loss=2.24, v_num=1, train_loss_step=4.990, val_loss=2.780, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 273:  88%|██████▏| 59/67 [00:17<00:02,  3.43it/s, loss=2.24, v_num=1, train_loss_step=4.990, val_loss=2.780, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 273:  90%|██████▎| 60/67 [00:17<00:02,  3.42it/s, loss=2.24, v_num=1, train_loss_step=4.990, val_loss=2.780, train_loss_epoch=1.960]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 273:  91%|██████▎| 61/67 [00:18<00:01,  3.39it/s, loss=2.24, v_num=1, train_loss_step=4.990, val_loss=2.780, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 273:  93%|██████▍| 62/67 [00:18<00:01,  3.38it/s, loss=2.24, v_num=1, train_loss_step=4.990, val_loss=2.780, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 273:  94%|██████▌| 63/67 [00:18<00:01,  3.38it/s, loss=2.24, v_num=1, train_loss_step=4.990, val_loss=2.780, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 273:  96%|██████▋| 64/67 [00:19<00:00,  3.37it/s, loss=2.24, v_num=1, train_loss_step=4.990, val_loss=2.780, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 273:  97%|██████▊| 65/67 [00:19<00:00,  3.36it/s, loss=2.24, v_num=1, train_loss_step=4.990, val_loss=2.780, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 273:  99%|██████▉| 66/67 [00:19<00:00,  3.36it/s, loss=2.24, v_num=1, train_loss_step=4.990, val_loss=2.780, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 273: 100%|███████| 67/67 [00:20<00:00,  3.31it/s, loss=2.24, v_num=1, train_loss_step=4.990, val_loss=2.780, train_loss_epoch=1.960]\u001b[A\n",
      "Epoch 274:  45%|███▏   | 30/67 [00:05<00:06,  5.91it/s, loss=1.73, v_num=1, train_loss_step=1.170, val_loss=2.780, train_loss_epoch=2.070]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 274:  46%|███▏   | 31/67 [00:06<00:08,  4.49it/s, loss=1.73, v_num=1, train_loss_step=1.170, val_loss=2.780, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 274:  48%|███▎   | 32/67 [00:07<00:07,  4.45it/s, loss=1.73, v_num=1, train_loss_step=1.170, val_loss=2.780, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 274:  49%|███▍   | 33/67 [00:07<00:07,  4.40it/s, loss=1.73, v_num=1, train_loss_step=1.170, val_loss=2.780, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 274:  51%|███▌   | 34/67 [00:07<00:07,  4.36it/s, loss=1.73, v_num=1, train_loss_step=1.170, val_loss=2.780, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 274:  52%|███▋   | 35/67 [00:08<00:07,  4.33it/s, loss=1.73, v_num=1, train_loss_step=1.170, val_loss=2.780, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 274:  54%|███▊   | 36/67 [00:08<00:07,  4.28it/s, loss=1.73, v_num=1, train_loss_step=1.170, val_loss=2.780, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 274:  55%|███▊   | 37/67 [00:08<00:07,  4.22it/s, loss=1.73, v_num=1, train_loss_step=1.170, val_loss=2.780, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 274:  57%|███▉   | 38/67 [00:09<00:06,  4.19it/s, loss=1.73, v_num=1, train_loss_step=1.170, val_loss=2.780, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 274:  58%|████   | 39/67 [00:09<00:06,  4.16it/s, loss=1.73, v_num=1, train_loss_step=1.170, val_loss=2.780, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 274:  60%|████▏  | 40/67 [00:09<00:06,  4.13it/s, loss=1.73, v_num=1, train_loss_step=1.170, val_loss=2.780, train_loss_epoch=2.070]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 274:  61%|████▎  | 41/67 [00:10<00:06,  4.05it/s, loss=1.73, v_num=1, train_loss_step=1.170, val_loss=2.780, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 274:  63%|████▍  | 42/67 [00:10<00:06,  4.04it/s, loss=1.73, v_num=1, train_loss_step=1.170, val_loss=2.780, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 274:  64%|████▍  | 43/67 [00:10<00:05,  4.02it/s, loss=1.73, v_num=1, train_loss_step=1.170, val_loss=2.780, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 274:  66%|████▌  | 44/67 [00:11<00:05,  4.00it/s, loss=1.73, v_num=1, train_loss_step=1.170, val_loss=2.780, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 274:  67%|████▋  | 45/67 [00:11<00:05,  3.98it/s, loss=1.73, v_num=1, train_loss_step=1.170, val_loss=2.780, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 274:  69%|████▊  | 46/67 [00:11<00:05,  3.97it/s, loss=1.73, v_num=1, train_loss_step=1.170, val_loss=2.780, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 274:  70%|████▉  | 47/67 [00:11<00:05,  3.95it/s, loss=1.73, v_num=1, train_loss_step=1.170, val_loss=2.780, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 274:  72%|█████  | 48/67 [00:12<00:04,  3.94it/s, loss=1.73, v_num=1, train_loss_step=1.170, val_loss=2.780, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 274:  73%|█████  | 49/67 [00:12<00:04,  3.93it/s, loss=1.73, v_num=1, train_loss_step=1.170, val_loss=2.780, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 274:  75%|█████▏ | 50/67 [00:12<00:04,  3.91it/s, loss=1.73, v_num=1, train_loss_step=1.170, val_loss=2.780, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 274:  76%|█████▎ | 51/67 [00:13<00:04,  3.86it/s, loss=1.73, v_num=1, train_loss_step=1.170, val_loss=2.780, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 274:  78%|█████▍ | 52/67 [00:13<00:03,  3.85it/s, loss=1.73, v_num=1, train_loss_step=1.170, val_loss=2.780, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 274:  79%|█████▌ | 53/67 [00:13<00:03,  3.84it/s, loss=1.73, v_num=1, train_loss_step=1.170, val_loss=2.780, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 274:  81%|█████▋ | 54/67 [00:14<00:03,  3.83it/s, loss=1.73, v_num=1, train_loss_step=1.170, val_loss=2.780, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 274:  82%|█████▋ | 55/67 [00:14<00:03,  3.82it/s, loss=1.73, v_num=1, train_loss_step=1.170, val_loss=2.780, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 274:  84%|█████▊ | 56/67 [00:14<00:02,  3.81it/s, loss=1.73, v_num=1, train_loss_step=1.170, val_loss=2.780, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 274:  85%|█████▉ | 57/67 [00:15<00:02,  3.80it/s, loss=1.73, v_num=1, train_loss_step=1.170, val_loss=2.780, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 274:  87%|██████ | 58/67 [00:15<00:02,  3.79it/s, loss=1.73, v_num=1, train_loss_step=1.170, val_loss=2.780, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 274:  88%|██████▏| 59/67 [00:15<00:02,  3.78it/s, loss=1.73, v_num=1, train_loss_step=1.170, val_loss=2.780, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 274:  90%|██████▎| 60/67 [00:15<00:01,  3.78it/s, loss=1.73, v_num=1, train_loss_step=1.170, val_loss=2.780, train_loss_epoch=2.070]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 274:  91%|██████▎| 61/67 [00:16<00:01,  3.74it/s, loss=1.73, v_num=1, train_loss_step=1.170, val_loss=2.780, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 274:  93%|██████▍| 62/67 [00:16<00:01,  3.73it/s, loss=1.73, v_num=1, train_loss_step=1.170, val_loss=2.780, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 274:  94%|██████▌| 63/67 [00:16<00:01,  3.73it/s, loss=1.73, v_num=1, train_loss_step=1.170, val_loss=2.780, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 274:  96%|██████▋| 64/67 [00:17<00:00,  3.72it/s, loss=1.73, v_num=1, train_loss_step=1.170, val_loss=2.780, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 274:  97%|██████▊| 65/67 [00:17<00:00,  3.71it/s, loss=1.73, v_num=1, train_loss_step=1.170, val_loss=2.780, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 274:  99%|██████▉| 66/67 [00:17<00:00,  3.71it/s, loss=1.73, v_num=1, train_loss_step=1.170, val_loss=2.780, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 274: 100%|███████| 67/67 [00:18<00:00,  3.62it/s, loss=1.73, v_num=1, train_loss_step=1.170, val_loss=2.770, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 275:  45%|███▏   | 30/67 [00:05<00:06,  5.55it/s, loss=2.16, v_num=1, train_loss_step=1.160, val_loss=2.770, train_loss_epoch=1.730]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 275:  46%|███▏   | 31/67 [00:07<00:08,  4.24it/s, loss=2.16, v_num=1, train_loss_step=1.160, val_loss=2.770, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 275:  48%|███▎   | 32/67 [00:07<00:08,  4.18it/s, loss=2.16, v_num=1, train_loss_step=1.160, val_loss=2.770, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 275:  49%|███▍   | 33/67 [00:07<00:08,  4.14it/s, loss=2.16, v_num=1, train_loss_step=1.160, val_loss=2.770, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 275:  51%|███▌   | 34/67 [00:08<00:08,  4.10it/s, loss=2.16, v_num=1, train_loss_step=1.160, val_loss=2.770, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 275:  52%|███▋   | 35/67 [00:08<00:07,  4.06it/s, loss=2.16, v_num=1, train_loss_step=1.160, val_loss=2.770, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 275:  54%|███▊   | 36/67 [00:08<00:07,  4.03it/s, loss=2.16, v_num=1, train_loss_step=1.160, val_loss=2.770, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 275:  55%|███▊   | 37/67 [00:09<00:07,  4.00it/s, loss=2.16, v_num=1, train_loss_step=1.160, val_loss=2.770, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 275:  57%|███▉   | 38/67 [00:09<00:07,  3.97it/s, loss=2.16, v_num=1, train_loss_step=1.160, val_loss=2.770, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 275:  58%|████   | 39/67 [00:09<00:07,  3.94it/s, loss=2.16, v_num=1, train_loss_step=1.160, val_loss=2.770, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 275:  60%|████▏  | 40/67 [00:10<00:06,  3.91it/s, loss=2.16, v_num=1, train_loss_step=1.160, val_loss=2.770, train_loss_epoch=1.730]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 275:  61%|████▎  | 41/67 [00:10<00:06,  3.80it/s, loss=2.16, v_num=1, train_loss_step=1.160, val_loss=2.770, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 275:  63%|████▍  | 42/67 [00:11<00:06,  3.78it/s, loss=2.16, v_num=1, train_loss_step=1.160, val_loss=2.770, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 275:  64%|████▍  | 43/67 [00:11<00:06,  3.75it/s, loss=2.16, v_num=1, train_loss_step=1.160, val_loss=2.770, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 275:  66%|████▌  | 44/67 [00:11<00:06,  3.73it/s, loss=2.16, v_num=1, train_loss_step=1.160, val_loss=2.770, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 275:  67%|████▋  | 45/67 [00:12<00:05,  3.72it/s, loss=2.16, v_num=1, train_loss_step=1.160, val_loss=2.770, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 275:  69%|████▊  | 46/67 [00:12<00:05,  3.69it/s, loss=2.16, v_num=1, train_loss_step=1.160, val_loss=2.770, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 275:  70%|████▉  | 47/67 [00:12<00:05,  3.67it/s, loss=2.16, v_num=1, train_loss_step=1.160, val_loss=2.770, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 275:  72%|█████  | 48/67 [00:13<00:05,  3.65it/s, loss=2.16, v_num=1, train_loss_step=1.160, val_loss=2.770, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 275:  73%|█████  | 49/67 [00:13<00:04,  3.62it/s, loss=2.16, v_num=1, train_loss_step=1.160, val_loss=2.770, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 275:  75%|█████▏ | 50/67 [00:13<00:04,  3.61it/s, loss=2.16, v_num=1, train_loss_step=1.160, val_loss=2.770, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 275:  76%|█████▎ | 51/67 [00:14<00:04,  3.56it/s, loss=2.16, v_num=1, train_loss_step=1.160, val_loss=2.770, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 275:  78%|█████▍ | 52/67 [00:14<00:04,  3.54it/s, loss=2.16, v_num=1, train_loss_step=1.160, val_loss=2.770, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 275:  79%|█████▌ | 53/67 [00:14<00:03,  3.54it/s, loss=2.16, v_num=1, train_loss_step=1.160, val_loss=2.770, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 275:  81%|█████▋ | 54/67 [00:15<00:03,  3.53it/s, loss=2.16, v_num=1, train_loss_step=1.160, val_loss=2.770, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 275:  82%|█████▋ | 55/67 [00:15<00:03,  3.52it/s, loss=2.16, v_num=1, train_loss_step=1.160, val_loss=2.770, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 275:  84%|█████▊ | 56/67 [00:15<00:03,  3.51it/s, loss=2.16, v_num=1, train_loss_step=1.160, val_loss=2.770, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 275:  85%|█████▉ | 57/67 [00:16<00:02,  3.50it/s, loss=2.16, v_num=1, train_loss_step=1.160, val_loss=2.770, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 275:  87%|██████ | 58/67 [00:16<00:02,  3.49it/s, loss=2.16, v_num=1, train_loss_step=1.160, val_loss=2.770, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 275:  88%|██████▏| 59/67 [00:16<00:02,  3.48it/s, loss=2.16, v_num=1, train_loss_step=1.160, val_loss=2.770, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 275:  90%|██████▎| 60/67 [00:17<00:02,  3.48it/s, loss=2.16, v_num=1, train_loss_step=1.160, val_loss=2.770, train_loss_epoch=1.730]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 275:  91%|██████▎| 61/67 [00:17<00:01,  3.45it/s, loss=2.16, v_num=1, train_loss_step=1.160, val_loss=2.770, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 275:  93%|██████▍| 62/67 [00:17<00:01,  3.45it/s, loss=2.16, v_num=1, train_loss_step=1.160, val_loss=2.770, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 275:  94%|██████▌| 63/67 [00:18<00:01,  3.45it/s, loss=2.16, v_num=1, train_loss_step=1.160, val_loss=2.770, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 275:  96%|██████▋| 64/67 [00:18<00:00,  3.45it/s, loss=2.16, v_num=1, train_loss_step=1.160, val_loss=2.770, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 275:  97%|██████▊| 65/67 [00:18<00:00,  3.45it/s, loss=2.16, v_num=1, train_loss_step=1.160, val_loss=2.770, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 275:  99%|██████▉| 66/67 [00:19<00:00,  3.45it/s, loss=2.16, v_num=1, train_loss_step=1.160, val_loss=2.770, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 275: 100%|███████| 67/67 [00:19<00:00,  3.38it/s, loss=2.16, v_num=1, train_loss_step=1.160, val_loss=2.770, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 276:  45%|███▏   | 30/67 [00:05<00:06,  5.81it/s, loss=1.54, v_num=1, train_loss_step=0.367, val_loss=2.770, train_loss_epoch=2.040]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 276:  46%|███▏   | 31/67 [00:07<00:08,  4.39it/s, loss=1.54, v_num=1, train_loss_step=0.367, val_loss=2.770, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 276:  48%|███▎   | 32/67 [00:07<00:08,  4.35it/s, loss=1.54, v_num=1, train_loss_step=0.367, val_loss=2.770, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 276:  49%|███▍   | 33/67 [00:07<00:07,  4.30it/s, loss=1.54, v_num=1, train_loss_step=0.367, val_loss=2.770, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 276:  51%|███▌   | 34/67 [00:07<00:07,  4.26it/s, loss=1.54, v_num=1, train_loss_step=0.367, val_loss=2.770, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 276:  52%|███▋   | 35/67 [00:08<00:07,  4.23it/s, loss=1.54, v_num=1, train_loss_step=0.367, val_loss=2.770, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 276:  54%|███▊   | 36/67 [00:08<00:07,  4.20it/s, loss=1.54, v_num=1, train_loss_step=0.367, val_loss=2.770, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 276:  55%|███▊   | 37/67 [00:08<00:07,  4.17it/s, loss=1.54, v_num=1, train_loss_step=0.367, val_loss=2.770, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 276:  57%|███▉   | 38/67 [00:09<00:06,  4.14it/s, loss=1.54, v_num=1, train_loss_step=0.367, val_loss=2.770, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 276:  58%|████   | 39/67 [00:09<00:06,  4.12it/s, loss=1.54, v_num=1, train_loss_step=0.367, val_loss=2.770, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 276:  60%|████▏  | 40/67 [00:09<00:06,  4.09it/s, loss=1.54, v_num=1, train_loss_step=0.367, val_loss=2.770, train_loss_epoch=2.040]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 276:  61%|████▎  | 41/67 [00:10<00:06,  4.02it/s, loss=1.54, v_num=1, train_loss_step=0.367, val_loss=2.770, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 276:  63%|████▍  | 42/67 [00:10<00:06,  4.00it/s, loss=1.54, v_num=1, train_loss_step=0.367, val_loss=2.770, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 276:  64%|████▍  | 43/67 [00:10<00:06,  3.98it/s, loss=1.54, v_num=1, train_loss_step=0.367, val_loss=2.770, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 276:  66%|████▌  | 44/67 [00:11<00:05,  3.97it/s, loss=1.54, v_num=1, train_loss_step=0.367, val_loss=2.770, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 276:  67%|████▋  | 45/67 [00:11<00:05,  3.95it/s, loss=1.54, v_num=1, train_loss_step=0.367, val_loss=2.770, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 276:  69%|████▊  | 46/67 [00:11<00:05,  3.93it/s, loss=1.54, v_num=1, train_loss_step=0.367, val_loss=2.770, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 276:  70%|████▉  | 47/67 [00:12<00:05,  3.92it/s, loss=1.54, v_num=1, train_loss_step=0.367, val_loss=2.770, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 276:  72%|█████  | 48/67 [00:12<00:04,  3.89it/s, loss=1.54, v_num=1, train_loss_step=0.367, val_loss=2.770, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 276:  73%|█████  | 49/67 [00:12<00:04,  3.88it/s, loss=1.54, v_num=1, train_loss_step=0.367, val_loss=2.770, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 276:  75%|█████▏ | 50/67 [00:12<00:04,  3.87it/s, loss=1.54, v_num=1, train_loss_step=0.367, val_loss=2.770, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 276:  76%|█████▎ | 51/67 [00:13<00:04,  3.81it/s, loss=1.54, v_num=1, train_loss_step=0.367, val_loss=2.770, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 276:  78%|█████▍ | 52/67 [00:13<00:03,  3.80it/s, loss=1.54, v_num=1, train_loss_step=0.367, val_loss=2.770, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 276:  79%|█████▌ | 53/67 [00:14<00:03,  3.78it/s, loss=1.54, v_num=1, train_loss_step=0.367, val_loss=2.770, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 276:  81%|█████▋ | 54/67 [00:14<00:03,  3.77it/s, loss=1.54, v_num=1, train_loss_step=0.367, val_loss=2.770, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 276:  82%|█████▋ | 55/67 [00:14<00:03,  3.75it/s, loss=1.54, v_num=1, train_loss_step=0.367, val_loss=2.770, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 276:  84%|█████▊ | 56/67 [00:15<00:02,  3.73it/s, loss=1.54, v_num=1, train_loss_step=0.367, val_loss=2.770, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 276:  85%|█████▉ | 57/67 [00:15<00:02,  3.71it/s, loss=1.54, v_num=1, train_loss_step=0.367, val_loss=2.770, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 276:  87%|██████ | 58/67 [00:15<00:02,  3.70it/s, loss=1.54, v_num=1, train_loss_step=0.367, val_loss=2.770, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 276:  88%|██████▏| 59/67 [00:16<00:02,  3.69it/s, loss=1.54, v_num=1, train_loss_step=0.367, val_loss=2.770, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 276:  90%|██████▎| 60/67 [00:16<00:01,  3.68it/s, loss=1.54, v_num=1, train_loss_step=0.367, val_loss=2.770, train_loss_epoch=2.040]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 276:  91%|██████▎| 61/67 [00:16<00:01,  3.63it/s, loss=1.54, v_num=1, train_loss_step=0.367, val_loss=2.770, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 276:  93%|██████▍| 62/67 [00:17<00:01,  3.62it/s, loss=1.54, v_num=1, train_loss_step=0.367, val_loss=2.770, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 276:  94%|██████▌| 63/67 [00:17<00:01,  3.61it/s, loss=1.54, v_num=1, train_loss_step=0.367, val_loss=2.770, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 276:  96%|██████▋| 64/67 [00:17<00:00,  3.60it/s, loss=1.54, v_num=1, train_loss_step=0.367, val_loss=2.770, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 276:  97%|██████▊| 65/67 [00:18<00:00,  3.59it/s, loss=1.54, v_num=1, train_loss_step=0.367, val_loss=2.770, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 276:  99%|██████▉| 66/67 [00:18<00:00,  3.58it/s, loss=1.54, v_num=1, train_loss_step=0.367, val_loss=2.770, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 276: 100%|███████| 67/67 [00:19<00:00,  3.50it/s, loss=1.54, v_num=1, train_loss_step=0.367, val_loss=2.770, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 277:  45%|███▏   | 30/67 [00:05<00:06,  5.42it/s, loss=1.76, v_num=1, train_loss_step=0.463, val_loss=2.770, train_loss_epoch=1.620]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 277:  46%|███▏   | 31/67 [00:07<00:08,  4.02it/s, loss=1.76, v_num=1, train_loss_step=0.463, val_loss=2.770, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 277:  48%|███▎   | 32/67 [00:08<00:08,  4.00it/s, loss=1.76, v_num=1, train_loss_step=0.463, val_loss=2.770, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 277:  49%|███▍   | 33/67 [00:08<00:08,  3.97it/s, loss=1.76, v_num=1, train_loss_step=0.463, val_loss=2.770, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 277:  51%|███▌   | 34/67 [00:08<00:08,  3.94it/s, loss=1.76, v_num=1, train_loss_step=0.463, val_loss=2.770, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 277:  52%|███▋   | 35/67 [00:08<00:08,  3.92it/s, loss=1.76, v_num=1, train_loss_step=0.463, val_loss=2.770, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 277:  54%|███▊   | 36/67 [00:09<00:07,  3.89it/s, loss=1.76, v_num=1, train_loss_step=0.463, val_loss=2.770, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 277:  55%|███▊   | 37/67 [00:09<00:07,  3.85it/s, loss=1.76, v_num=1, train_loss_step=0.463, val_loss=2.770, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 277:  57%|███▉   | 38/67 [00:09<00:07,  3.83it/s, loss=1.76, v_num=1, train_loss_step=0.463, val_loss=2.770, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 277:  58%|████   | 39/67 [00:10<00:07,  3.81it/s, loss=1.76, v_num=1, train_loss_step=0.463, val_loss=2.770, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 277:  60%|████▏  | 40/67 [00:10<00:07,  3.78it/s, loss=1.76, v_num=1, train_loss_step=0.463, val_loss=2.770, train_loss_epoch=1.620]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 277:  61%|████▎  | 41/67 [00:11<00:06,  3.72it/s, loss=1.76, v_num=1, train_loss_step=0.463, val_loss=2.770, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 277:  63%|████▍  | 42/67 [00:11<00:06,  3.71it/s, loss=1.76, v_num=1, train_loss_step=0.463, val_loss=2.770, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 277:  64%|████▍  | 43/67 [00:11<00:06,  3.70it/s, loss=1.76, v_num=1, train_loss_step=0.463, val_loss=2.770, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 277:  66%|████▌  | 44/67 [00:11<00:06,  3.69it/s, loss=1.76, v_num=1, train_loss_step=0.463, val_loss=2.770, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 277:  67%|████▋  | 45/67 [00:12<00:05,  3.68it/s, loss=1.76, v_num=1, train_loss_step=0.463, val_loss=2.770, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 277:  69%|████▊  | 46/67 [00:12<00:05,  3.67it/s, loss=1.76, v_num=1, train_loss_step=0.463, val_loss=2.770, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 277:  70%|████▉  | 47/67 [00:12<00:05,  3.66it/s, loss=1.76, v_num=1, train_loss_step=0.463, val_loss=2.770, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 277:  72%|█████  | 48/67 [00:13<00:05,  3.65it/s, loss=1.76, v_num=1, train_loss_step=0.463, val_loss=2.770, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 277:  73%|█████  | 49/67 [00:13<00:04,  3.64it/s, loss=1.76, v_num=1, train_loss_step=0.463, val_loss=2.770, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 277:  75%|█████▏ | 50/67 [00:13<00:04,  3.62it/s, loss=1.76, v_num=1, train_loss_step=0.463, val_loss=2.770, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 277:  76%|█████▎ | 51/67 [00:14<00:04,  3.57it/s, loss=1.76, v_num=1, train_loss_step=0.463, val_loss=2.770, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 277:  78%|█████▍ | 52/67 [00:14<00:04,  3.56it/s, loss=1.76, v_num=1, train_loss_step=0.463, val_loss=2.770, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 277:  79%|█████▌ | 53/67 [00:14<00:03,  3.55it/s, loss=1.76, v_num=1, train_loss_step=0.463, val_loss=2.770, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 277:  81%|█████▋ | 54/67 [00:15<00:03,  3.54it/s, loss=1.76, v_num=1, train_loss_step=0.463, val_loss=2.770, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 277:  82%|█████▋ | 55/67 [00:15<00:03,  3.53it/s, loss=1.76, v_num=1, train_loss_step=0.463, val_loss=2.770, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 277:  84%|█████▊ | 56/67 [00:15<00:03,  3.52it/s, loss=1.76, v_num=1, train_loss_step=0.463, val_loss=2.770, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 277:  85%|█████▉ | 57/67 [00:16<00:02,  3.51it/s, loss=1.76, v_num=1, train_loss_step=0.463, val_loss=2.770, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 277:  87%|██████ | 58/67 [00:16<00:02,  3.50it/s, loss=1.76, v_num=1, train_loss_step=0.463, val_loss=2.770, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 277:  88%|██████▏| 59/67 [00:16<00:02,  3.49it/s, loss=1.76, v_num=1, train_loss_step=0.463, val_loss=2.770, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 277:  90%|██████▎| 60/67 [00:17<00:02,  3.49it/s, loss=1.76, v_num=1, train_loss_step=0.463, val_loss=2.770, train_loss_epoch=1.620]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 277:  91%|██████▎| 61/67 [00:17<00:01,  3.46it/s, loss=1.76, v_num=1, train_loss_step=0.463, val_loss=2.770, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 277:  93%|██████▍| 62/67 [00:17<00:01,  3.46it/s, loss=1.76, v_num=1, train_loss_step=0.463, val_loss=2.770, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 277:  94%|██████▌| 63/67 [00:18<00:01,  3.46it/s, loss=1.76, v_num=1, train_loss_step=0.463, val_loss=2.770, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 277:  96%|██████▋| 64/67 [00:18<00:00,  3.45it/s, loss=1.76, v_num=1, train_loss_step=0.463, val_loss=2.770, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 277:  97%|██████▊| 65/67 [00:18<00:00,  3.44it/s, loss=1.76, v_num=1, train_loss_step=0.463, val_loss=2.770, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 277:  99%|██████▉| 66/67 [00:19<00:00,  3.44it/s, loss=1.76, v_num=1, train_loss_step=0.463, val_loss=2.770, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 277: 100%|███████| 67/67 [00:19<00:00,  3.37it/s, loss=1.76, v_num=1, train_loss_step=0.463, val_loss=2.770, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 278:  45%|███▏   | 30/67 [00:05<00:06,  5.65it/s, loss=1.69, v_num=1, train_loss_step=0.879, val_loss=2.770, train_loss_epoch=1.740]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 278:  46%|███▏   | 31/67 [00:07<00:08,  4.29it/s, loss=1.69, v_num=1, train_loss_step=0.879, val_loss=2.770, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 278:  48%|███▎   | 32/67 [00:07<00:08,  4.25it/s, loss=1.69, v_num=1, train_loss_step=0.879, val_loss=2.770, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 278:  49%|███▍   | 33/67 [00:07<00:08,  4.20it/s, loss=1.69, v_num=1, train_loss_step=0.879, val_loss=2.770, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 278:  51%|███▌   | 34/67 [00:08<00:07,  4.16it/s, loss=1.69, v_num=1, train_loss_step=0.879, val_loss=2.770, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 278:  52%|███▋   | 35/67 [00:08<00:07,  4.11it/s, loss=1.69, v_num=1, train_loss_step=0.879, val_loss=2.770, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 278:  54%|███▊   | 36/67 [00:08<00:07,  4.07it/s, loss=1.69, v_num=1, train_loss_step=0.879, val_loss=2.770, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 278:  55%|███▊   | 37/67 [00:09<00:07,  4.03it/s, loss=1.69, v_num=1, train_loss_step=0.879, val_loss=2.770, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 278:  57%|███▉   | 38/67 [00:09<00:07,  4.01it/s, loss=1.69, v_num=1, train_loss_step=0.879, val_loss=2.770, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 278:  58%|████   | 39/67 [00:09<00:07,  3.99it/s, loss=1.69, v_num=1, train_loss_step=0.879, val_loss=2.770, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 278:  60%|████▏  | 40/67 [00:10<00:06,  3.96it/s, loss=1.69, v_num=1, train_loss_step=0.879, val_loss=2.770, train_loss_epoch=1.740]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 278:  61%|████▎  | 41/67 [00:10<00:06,  3.90it/s, loss=1.69, v_num=1, train_loss_step=0.879, val_loss=2.770, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 278:  63%|████▍  | 42/67 [00:10<00:06,  3.89it/s, loss=1.69, v_num=1, train_loss_step=0.879, val_loss=2.770, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 278:  64%|████▍  | 43/67 [00:11<00:06,  3.87it/s, loss=1.69, v_num=1, train_loss_step=0.879, val_loss=2.770, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 278:  66%|████▌  | 44/67 [00:11<00:05,  3.85it/s, loss=1.69, v_num=1, train_loss_step=0.879, val_loss=2.770, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 278:  67%|████▋  | 45/67 [00:11<00:05,  3.84it/s, loss=1.69, v_num=1, train_loss_step=0.879, val_loss=2.770, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 278:  69%|████▊  | 46/67 [00:12<00:05,  3.83it/s, loss=1.69, v_num=1, train_loss_step=0.879, val_loss=2.770, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 278:  70%|████▉  | 47/67 [00:12<00:05,  3.82it/s, loss=1.69, v_num=1, train_loss_step=0.879, val_loss=2.770, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 278:  72%|█████  | 48/67 [00:12<00:05,  3.80it/s, loss=1.69, v_num=1, train_loss_step=0.879, val_loss=2.770, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 278:  73%|█████  | 49/67 [00:12<00:04,  3.79it/s, loss=1.69, v_num=1, train_loss_step=0.879, val_loss=2.770, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 278:  75%|█████▏ | 50/67 [00:13<00:04,  3.78it/s, loss=1.69, v_num=1, train_loss_step=0.879, val_loss=2.770, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 278:  76%|█████▎ | 51/67 [00:13<00:04,  3.73it/s, loss=1.69, v_num=1, train_loss_step=0.879, val_loss=2.770, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 278:  78%|█████▍ | 52/67 [00:14<00:04,  3.71it/s, loss=1.69, v_num=1, train_loss_step=0.879, val_loss=2.770, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 278:  79%|█████▌ | 53/67 [00:14<00:03,  3.71it/s, loss=1.69, v_num=1, train_loss_step=0.879, val_loss=2.770, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 278:  81%|█████▋ | 54/67 [00:14<00:03,  3.70it/s, loss=1.69, v_num=1, train_loss_step=0.879, val_loss=2.770, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 278:  82%|█████▋ | 55/67 [00:14<00:03,  3.68it/s, loss=1.69, v_num=1, train_loss_step=0.879, val_loss=2.770, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 278:  84%|█████▊ | 56/67 [00:15<00:02,  3.67it/s, loss=1.69, v_num=1, train_loss_step=0.879, val_loss=2.770, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 278:  85%|█████▉ | 57/67 [00:15<00:02,  3.67it/s, loss=1.69, v_num=1, train_loss_step=0.879, val_loss=2.770, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 278:  87%|██████ | 58/67 [00:15<00:02,  3.66it/s, loss=1.69, v_num=1, train_loss_step=0.879, val_loss=2.770, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 278:  88%|██████▏| 59/67 [00:16<00:02,  3.64it/s, loss=1.69, v_num=1, train_loss_step=0.879, val_loss=2.770, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 278:  90%|██████▎| 60/67 [00:16<00:01,  3.64it/s, loss=1.69, v_num=1, train_loss_step=0.879, val_loss=2.770, train_loss_epoch=1.740]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 278:  91%|██████▎| 61/67 [00:16<00:01,  3.61it/s, loss=1.69, v_num=1, train_loss_step=0.879, val_loss=2.770, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 278:  93%|██████▍| 62/67 [00:17<00:01,  3.60it/s, loss=1.69, v_num=1, train_loss_step=0.879, val_loss=2.770, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 278:  94%|██████▌| 63/67 [00:17<00:01,  3.59it/s, loss=1.69, v_num=1, train_loss_step=0.879, val_loss=2.770, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 278:  96%|██████▋| 64/67 [00:17<00:00,  3.59it/s, loss=1.69, v_num=1, train_loss_step=0.879, val_loss=2.770, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 278:  97%|██████▊| 65/67 [00:18<00:00,  3.59it/s, loss=1.69, v_num=1, train_loss_step=0.879, val_loss=2.770, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 278:  99%|██████▉| 66/67 [00:18<00:00,  3.59it/s, loss=1.69, v_num=1, train_loss_step=0.879, val_loss=2.770, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 278: 100%|███████| 67/67 [00:19<00:00,  3.51it/s, loss=1.69, v_num=1, train_loss_step=0.879, val_loss=2.770, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 279:  45%|███▏   | 30/67 [00:05<00:06,  5.67it/s, loss=1.91, v_num=1, train_loss_step=2.500, val_loss=2.770, train_loss_epoch=1.640]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 279:  46%|███▏   | 31/67 [00:07<00:08,  4.33it/s, loss=1.91, v_num=1, train_loss_step=2.500, val_loss=2.770, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 279:  48%|███▎   | 32/67 [00:07<00:08,  4.29it/s, loss=1.91, v_num=1, train_loss_step=2.500, val_loss=2.770, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 279:  49%|███▍   | 33/67 [00:07<00:08,  4.25it/s, loss=1.91, v_num=1, train_loss_step=2.500, val_loss=2.770, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 279:  51%|███▌   | 34/67 [00:08<00:07,  4.21it/s, loss=1.91, v_num=1, train_loss_step=2.500, val_loss=2.770, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 279:  52%|███▋   | 35/67 [00:08<00:07,  4.18it/s, loss=1.91, v_num=1, train_loss_step=2.500, val_loss=2.770, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 279:  54%|███▊   | 36/67 [00:08<00:07,  4.15it/s, loss=1.91, v_num=1, train_loss_step=2.500, val_loss=2.770, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 279:  55%|███▊   | 37/67 [00:08<00:07,  4.12it/s, loss=1.91, v_num=1, train_loss_step=2.500, val_loss=2.770, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 279:  57%|███▉   | 38/67 [00:09<00:07,  4.09it/s, loss=1.91, v_num=1, train_loss_step=2.500, val_loss=2.770, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 279:  58%|████   | 39/67 [00:09<00:06,  4.07it/s, loss=1.91, v_num=1, train_loss_step=2.500, val_loss=2.770, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 279:  60%|████▏  | 40/67 [00:09<00:06,  4.05it/s, loss=1.91, v_num=1, train_loss_step=2.500, val_loss=2.770, train_loss_epoch=1.640]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 279:  61%|████▎  | 41/67 [00:10<00:06,  3.97it/s, loss=1.91, v_num=1, train_loss_step=2.500, val_loss=2.770, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 279:  63%|████▍  | 42/67 [00:10<00:06,  3.95it/s, loss=1.91, v_num=1, train_loss_step=2.500, val_loss=2.770, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 279:  64%|████▍  | 43/67 [00:10<00:06,  3.93it/s, loss=1.91, v_num=1, train_loss_step=2.500, val_loss=2.770, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 279:  66%|████▌  | 44/67 [00:11<00:05,  3.91it/s, loss=1.91, v_num=1, train_loss_step=2.500, val_loss=2.770, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 279:  67%|████▋  | 45/67 [00:11<00:05,  3.89it/s, loss=1.91, v_num=1, train_loss_step=2.500, val_loss=2.770, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 279:  69%|████▊  | 46/67 [00:11<00:05,  3.88it/s, loss=1.91, v_num=1, train_loss_step=2.500, val_loss=2.770, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 279:  70%|████▉  | 47/67 [00:12<00:05,  3.87it/s, loss=1.91, v_num=1, train_loss_step=2.500, val_loss=2.770, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 279:  72%|█████  | 48/67 [00:12<00:04,  3.86it/s, loss=1.91, v_num=1, train_loss_step=2.500, val_loss=2.770, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 279:  73%|█████  | 49/67 [00:12<00:04,  3.84it/s, loss=1.91, v_num=1, train_loss_step=2.500, val_loss=2.770, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 279:  75%|█████▏ | 50/67 [00:13<00:04,  3.83it/s, loss=1.91, v_num=1, train_loss_step=2.500, val_loss=2.770, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 279:  76%|█████▎ | 51/67 [00:13<00:04,  3.78it/s, loss=1.91, v_num=1, train_loss_step=2.500, val_loss=2.770, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 279:  78%|█████▍ | 52/67 [00:13<00:03,  3.77it/s, loss=1.91, v_num=1, train_loss_step=2.500, val_loss=2.770, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 279:  79%|█████▌ | 53/67 [00:14<00:03,  3.75it/s, loss=1.91, v_num=1, train_loss_step=2.500, val_loss=2.770, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 279:  81%|█████▋ | 54/67 [00:14<00:03,  3.75it/s, loss=1.91, v_num=1, train_loss_step=2.500, val_loss=2.770, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 279:  82%|█████▋ | 55/67 [00:14<00:03,  3.74it/s, loss=1.91, v_num=1, train_loss_step=2.500, val_loss=2.770, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 279:  84%|█████▊ | 56/67 [00:15<00:02,  3.73it/s, loss=1.91, v_num=1, train_loss_step=2.500, val_loss=2.770, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 279:  85%|█████▉ | 57/67 [00:15<00:02,  3.72it/s, loss=1.91, v_num=1, train_loss_step=2.500, val_loss=2.770, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 279:  87%|██████ | 58/67 [00:15<00:02,  3.72it/s, loss=1.91, v_num=1, train_loss_step=2.500, val_loss=2.770, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 279:  88%|██████▏| 59/67 [00:15<00:02,  3.71it/s, loss=1.91, v_num=1, train_loss_step=2.500, val_loss=2.770, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 279:  90%|██████▎| 60/67 [00:16<00:01,  3.70it/s, loss=1.91, v_num=1, train_loss_step=2.500, val_loss=2.770, train_loss_epoch=1.640]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 279:  91%|██████▎| 61/67 [00:16<00:01,  3.67it/s, loss=1.91, v_num=1, train_loss_step=2.500, val_loss=2.770, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 279:  93%|██████▍| 62/67 [00:16<00:01,  3.66it/s, loss=1.91, v_num=1, train_loss_step=2.500, val_loss=2.770, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 279:  94%|██████▌| 63/67 [00:17<00:01,  3.66it/s, loss=1.91, v_num=1, train_loss_step=2.500, val_loss=2.770, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 279:  96%|██████▋| 64/67 [00:17<00:00,  3.65it/s, loss=1.91, v_num=1, train_loss_step=2.500, val_loss=2.770, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 279:  97%|██████▊| 65/67 [00:17<00:00,  3.65it/s, loss=1.91, v_num=1, train_loss_step=2.500, val_loss=2.770, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 279:  99%|██████▉| 66/67 [00:18<00:00,  3.65it/s, loss=1.91, v_num=1, train_loss_step=2.500, val_loss=2.770, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 279: 100%|███████| 67/67 [00:18<00:00,  3.57it/s, loss=1.91, v_num=1, train_loss_step=2.500, val_loss=2.770, train_loss_epoch=1.640]\u001b[A\n",
      "Epoch 280:  45%|███▏   | 30/67 [00:05<00:06,  5.83it/s, loss=1.77, v_num=1, train_loss_step=2.290, val_loss=2.770, train_loss_epoch=1.830]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 280:  46%|███▏   | 31/67 [00:07<00:08,  4.38it/s, loss=1.77, v_num=1, train_loss_step=2.290, val_loss=2.770, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 280:  48%|███▎   | 32/67 [00:07<00:08,  4.33it/s, loss=1.77, v_num=1, train_loss_step=2.290, val_loss=2.770, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 280:  49%|███▍   | 33/67 [00:07<00:07,  4.29it/s, loss=1.77, v_num=1, train_loss_step=2.290, val_loss=2.770, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 280:  51%|███▌   | 34/67 [00:08<00:07,  4.25it/s, loss=1.77, v_num=1, train_loss_step=2.290, val_loss=2.770, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 280:  52%|███▋   | 35/67 [00:08<00:07,  4.21it/s, loss=1.77, v_num=1, train_loss_step=2.290, val_loss=2.770, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 280:  54%|███▊   | 36/67 [00:08<00:07,  4.18it/s, loss=1.77, v_num=1, train_loss_step=2.290, val_loss=2.770, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 280:  55%|███▊   | 37/67 [00:08<00:07,  4.14it/s, loss=1.77, v_num=1, train_loss_step=2.290, val_loss=2.770, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 280:  57%|███▉   | 38/67 [00:09<00:07,  4.11it/s, loss=1.77, v_num=1, train_loss_step=2.290, val_loss=2.770, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 280:  58%|████   | 39/67 [00:09<00:06,  4.09it/s, loss=1.77, v_num=1, train_loss_step=2.290, val_loss=2.770, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 280:  60%|████▏  | 40/67 [00:09<00:06,  4.06it/s, loss=1.77, v_num=1, train_loss_step=2.290, val_loss=2.770, train_loss_epoch=1.830]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 280:  61%|████▎  | 41/67 [00:10<00:06,  3.99it/s, loss=1.77, v_num=1, train_loss_step=2.290, val_loss=2.770, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 280:  63%|████▍  | 42/67 [00:10<00:06,  3.97it/s, loss=1.77, v_num=1, train_loss_step=2.290, val_loss=2.770, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 280:  64%|████▍  | 43/67 [00:10<00:06,  3.95it/s, loss=1.77, v_num=1, train_loss_step=2.290, val_loss=2.770, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 280:  66%|████▌  | 44/67 [00:11<00:05,  3.93it/s, loss=1.77, v_num=1, train_loss_step=2.290, val_loss=2.770, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 280:  67%|████▋  | 45/67 [00:11<00:05,  3.92it/s, loss=1.77, v_num=1, train_loss_step=2.290, val_loss=2.770, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 280:  69%|████▊  | 46/67 [00:11<00:05,  3.90it/s, loss=1.77, v_num=1, train_loss_step=2.290, val_loss=2.770, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 280:  70%|████▉  | 47/67 [00:12<00:05,  3.89it/s, loss=1.77, v_num=1, train_loss_step=2.290, val_loss=2.770, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 280:  72%|█████  | 48/67 [00:12<00:04,  3.88it/s, loss=1.77, v_num=1, train_loss_step=2.290, val_loss=2.770, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 280:  73%|█████  | 49/67 [00:12<00:04,  3.86it/s, loss=1.77, v_num=1, train_loss_step=2.290, val_loss=2.770, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 280:  75%|█████▏ | 50/67 [00:12<00:04,  3.85it/s, loss=1.77, v_num=1, train_loss_step=2.290, val_loss=2.770, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 280:  76%|█████▎ | 51/67 [00:13<00:04,  3.80it/s, loss=1.77, v_num=1, train_loss_step=2.290, val_loss=2.770, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 280:  78%|█████▍ | 52/67 [00:13<00:03,  3.79it/s, loss=1.77, v_num=1, train_loss_step=2.290, val_loss=2.770, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 280:  79%|█████▌ | 53/67 [00:14<00:03,  3.78it/s, loss=1.77, v_num=1, train_loss_step=2.290, val_loss=2.770, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 280:  81%|█████▋ | 54/67 [00:14<00:03,  3.77it/s, loss=1.77, v_num=1, train_loss_step=2.290, val_loss=2.770, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 280:  82%|█████▋ | 55/67 [00:14<00:03,  3.75it/s, loss=1.77, v_num=1, train_loss_step=2.290, val_loss=2.770, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 280:  84%|█████▊ | 56/67 [00:14<00:02,  3.74it/s, loss=1.77, v_num=1, train_loss_step=2.290, val_loss=2.770, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 280:  85%|█████▉ | 57/67 [00:15<00:02,  3.73it/s, loss=1.77, v_num=1, train_loss_step=2.290, val_loss=2.770, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 280:  87%|██████ | 58/67 [00:15<00:02,  3.72it/s, loss=1.77, v_num=1, train_loss_step=2.290, val_loss=2.770, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 280:  88%|██████▏| 59/67 [00:15<00:02,  3.72it/s, loss=1.77, v_num=1, train_loss_step=2.290, val_loss=2.770, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 280:  90%|██████▎| 60/67 [00:16<00:01,  3.71it/s, loss=1.77, v_num=1, train_loss_step=2.290, val_loss=2.770, train_loss_epoch=1.830]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 280:  91%|██████▎| 61/67 [00:16<00:01,  3.67it/s, loss=1.77, v_num=1, train_loss_step=2.290, val_loss=2.770, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 280:  93%|██████▍| 62/67 [00:16<00:01,  3.66it/s, loss=1.77, v_num=1, train_loss_step=2.290, val_loss=2.770, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 280:  94%|██████▌| 63/67 [00:17<00:01,  3.65it/s, loss=1.77, v_num=1, train_loss_step=2.290, val_loss=2.770, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 280:  96%|██████▋| 64/67 [00:17<00:00,  3.64it/s, loss=1.77, v_num=1, train_loss_step=2.290, val_loss=2.770, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 280:  97%|██████▊| 65/67 [00:17<00:00,  3.63it/s, loss=1.77, v_num=1, train_loss_step=2.290, val_loss=2.770, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 280:  99%|██████▉| 66/67 [00:18<00:00,  3.63it/s, loss=1.77, v_num=1, train_loss_step=2.290, val_loss=2.770, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 280: 100%|███████| 67/67 [00:18<00:00,  3.54it/s, loss=1.77, v_num=1, train_loss_step=2.290, val_loss=2.770, train_loss_epoch=1.830]\u001b[A\n",
      "Epoch 281:  45%|███▌    | 30/67 [00:05<00:06,  5.80it/s, loss=1.5, v_num=1, train_loss_step=0.953, val_loss=2.770, train_loss_epoch=1.710]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 281:  46%|███▋    | 31/67 [00:07<00:08,  4.36it/s, loss=1.5, v_num=1, train_loss_step=0.953, val_loss=2.770, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 281:  48%|███▊    | 32/67 [00:07<00:08,  4.32it/s, loss=1.5, v_num=1, train_loss_step=0.953, val_loss=2.770, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 281:  49%|███▉    | 33/67 [00:07<00:07,  4.28it/s, loss=1.5, v_num=1, train_loss_step=0.953, val_loss=2.770, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 281:  51%|████    | 34/67 [00:08<00:07,  4.25it/s, loss=1.5, v_num=1, train_loss_step=0.953, val_loss=2.770, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 281:  52%|████▏   | 35/67 [00:08<00:07,  4.21it/s, loss=1.5, v_num=1, train_loss_step=0.953, val_loss=2.770, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 281:  54%|████▎   | 36/67 [00:08<00:07,  4.18it/s, loss=1.5, v_num=1, train_loss_step=0.953, val_loss=2.770, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 281:  55%|████▍   | 37/67 [00:08<00:07,  4.14it/s, loss=1.5, v_num=1, train_loss_step=0.953, val_loss=2.770, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 281:  57%|████▌   | 38/67 [00:09<00:07,  4.10it/s, loss=1.5, v_num=1, train_loss_step=0.953, val_loss=2.770, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 281:  58%|████▋   | 39/67 [00:09<00:06,  4.07it/s, loss=1.5, v_num=1, train_loss_step=0.953, val_loss=2.770, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 281:  60%|████▊   | 40/67 [00:09<00:06,  4.04it/s, loss=1.5, v_num=1, train_loss_step=0.953, val_loss=2.770, train_loss_epoch=1.710]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 281:  61%|████▉   | 41/67 [00:10<00:06,  3.97it/s, loss=1.5, v_num=1, train_loss_step=0.953, val_loss=2.770, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 281:  63%|█████   | 42/67 [00:10<00:06,  3.94it/s, loss=1.5, v_num=1, train_loss_step=0.953, val_loss=2.770, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 281:  64%|█████▏  | 43/67 [00:10<00:06,  3.91it/s, loss=1.5, v_num=1, train_loss_step=0.953, val_loss=2.770, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 281:  66%|█████▎  | 44/67 [00:11<00:05,  3.89it/s, loss=1.5, v_num=1, train_loss_step=0.953, val_loss=2.770, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 281:  67%|█████▎  | 45/67 [00:11<00:05,  3.87it/s, loss=1.5, v_num=1, train_loss_step=0.953, val_loss=2.770, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 281:  69%|█████▍  | 46/67 [00:11<00:05,  3.85it/s, loss=1.5, v_num=1, train_loss_step=0.953, val_loss=2.770, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 281:  70%|█████▌  | 47/67 [00:12<00:05,  3.84it/s, loss=1.5, v_num=1, train_loss_step=0.953, val_loss=2.770, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 281:  72%|█████▋  | 48/67 [00:12<00:04,  3.82it/s, loss=1.5, v_num=1, train_loss_step=0.953, val_loss=2.770, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 281:  73%|█████▊  | 49/67 [00:12<00:04,  3.80it/s, loss=1.5, v_num=1, train_loss_step=0.953, val_loss=2.770, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 281:  75%|█████▉  | 50/67 [00:13<00:04,  3.78it/s, loss=1.5, v_num=1, train_loss_step=0.953, val_loss=2.770, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 281:  76%|██████  | 51/67 [00:13<00:04,  3.73it/s, loss=1.5, v_num=1, train_loss_step=0.953, val_loss=2.770, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 281:  78%|██████▏ | 52/67 [00:13<00:04,  3.73it/s, loss=1.5, v_num=1, train_loss_step=0.953, val_loss=2.770, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 281:  79%|██████▎ | 53/67 [00:14<00:03,  3.72it/s, loss=1.5, v_num=1, train_loss_step=0.953, val_loss=2.770, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 281:  81%|██████▍ | 54/67 [00:14<00:03,  3.71it/s, loss=1.5, v_num=1, train_loss_step=0.953, val_loss=2.770, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 281:  82%|██████▌ | 55/67 [00:14<00:03,  3.71it/s, loss=1.5, v_num=1, train_loss_step=0.953, val_loss=2.770, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 281:  84%|██████▋ | 56/67 [00:15<00:02,  3.70it/s, loss=1.5, v_num=1, train_loss_step=0.953, val_loss=2.770, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 281:  85%|██████▊ | 57/67 [00:15<00:02,  3.69it/s, loss=1.5, v_num=1, train_loss_step=0.953, val_loss=2.770, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 281:  87%|██████▉ | 58/67 [00:15<00:02,  3.68it/s, loss=1.5, v_num=1, train_loss_step=0.953, val_loss=2.770, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 281:  88%|███████ | 59/67 [00:16<00:02,  3.67it/s, loss=1.5, v_num=1, train_loss_step=0.953, val_loss=2.770, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 281:  90%|███████▏| 60/67 [00:16<00:01,  3.66it/s, loss=1.5, v_num=1, train_loss_step=0.953, val_loss=2.770, train_loss_epoch=1.710]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 281:  91%|███████▎| 61/67 [00:16<00:01,  3.62it/s, loss=1.5, v_num=1, train_loss_step=0.953, val_loss=2.770, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 281:  93%|███████▍| 62/67 [00:17<00:01,  3.62it/s, loss=1.5, v_num=1, train_loss_step=0.953, val_loss=2.770, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 281:  94%|███████▌| 63/67 [00:17<00:01,  3.61it/s, loss=1.5, v_num=1, train_loss_step=0.953, val_loss=2.770, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 281:  96%|███████▋| 64/67 [00:17<00:00,  3.61it/s, loss=1.5, v_num=1, train_loss_step=0.953, val_loss=2.770, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 281:  97%|███████▊| 65/67 [00:18<00:00,  3.60it/s, loss=1.5, v_num=1, train_loss_step=0.953, val_loss=2.770, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 281:  99%|███████▉| 66/67 [00:18<00:00,  3.59it/s, loss=1.5, v_num=1, train_loss_step=0.953, val_loss=2.770, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 281: 100%|████████| 67/67 [00:19<00:00,  3.50it/s, loss=1.5, v_num=1, train_loss_step=0.953, val_loss=2.770, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 282:  45%|███▏   | 30/67 [00:05<00:06,  5.70it/s, loss=1.24, v_num=1, train_loss_step=1.210, val_loss=2.770, train_loss_epoch=1.580]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 282:  46%|███▏   | 31/67 [00:07<00:08,  4.30it/s, loss=1.24, v_num=1, train_loss_step=1.210, val_loss=2.770, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 282:  48%|███▎   | 32/67 [00:07<00:08,  4.26it/s, loss=1.24, v_num=1, train_loss_step=1.210, val_loss=2.770, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 282:  49%|███▍   | 33/67 [00:07<00:08,  4.22it/s, loss=1.24, v_num=1, train_loss_step=1.210, val_loss=2.770, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 282:  51%|███▌   | 34/67 [00:08<00:07,  4.19it/s, loss=1.24, v_num=1, train_loss_step=1.210, val_loss=2.770, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 282:  52%|███▋   | 35/67 [00:08<00:07,  4.16it/s, loss=1.24, v_num=1, train_loss_step=1.210, val_loss=2.770, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 282:  54%|███▊   | 36/67 [00:08<00:07,  4.13it/s, loss=1.24, v_num=1, train_loss_step=1.210, val_loss=2.770, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 282:  55%|███▊   | 37/67 [00:09<00:07,  4.10it/s, loss=1.24, v_num=1, train_loss_step=1.210, val_loss=2.770, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 282:  57%|███▉   | 38/67 [00:09<00:07,  4.07it/s, loss=1.24, v_num=1, train_loss_step=1.210, val_loss=2.770, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 282:  58%|████   | 39/67 [00:09<00:06,  4.05it/s, loss=1.24, v_num=1, train_loss_step=1.210, val_loss=2.770, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 282:  60%|████▏  | 40/67 [00:09<00:06,  4.02it/s, loss=1.24, v_num=1, train_loss_step=1.210, val_loss=2.770, train_loss_epoch=1.580]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 282:  61%|████▎  | 41/67 [00:10<00:06,  3.94it/s, loss=1.24, v_num=1, train_loss_step=1.210, val_loss=2.770, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 282:  63%|████▍  | 42/67 [00:10<00:06,  3.92it/s, loss=1.24, v_num=1, train_loss_step=1.210, val_loss=2.770, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 282:  64%|████▍  | 43/67 [00:11<00:06,  3.89it/s, loss=1.24, v_num=1, train_loss_step=1.210, val_loss=2.770, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 282:  66%|████▌  | 44/67 [00:11<00:05,  3.87it/s, loss=1.24, v_num=1, train_loss_step=1.210, val_loss=2.770, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 282:  67%|████▋  | 45/67 [00:11<00:05,  3.86it/s, loss=1.24, v_num=1, train_loss_step=1.210, val_loss=2.770, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 282:  69%|████▊  | 46/67 [00:11<00:05,  3.84it/s, loss=1.24, v_num=1, train_loss_step=1.210, val_loss=2.770, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 282:  70%|████▉  | 47/67 [00:12<00:05,  3.83it/s, loss=1.24, v_num=1, train_loss_step=1.210, val_loss=2.770, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 282:  72%|█████  | 48/67 [00:12<00:04,  3.82it/s, loss=1.24, v_num=1, train_loss_step=1.210, val_loss=2.770, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 282:  73%|█████  | 49/67 [00:12<00:04,  3.81it/s, loss=1.24, v_num=1, train_loss_step=1.210, val_loss=2.770, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 282:  75%|█████▏ | 50/67 [00:13<00:04,  3.79it/s, loss=1.24, v_num=1, train_loss_step=1.210, val_loss=2.770, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 282:  76%|█████▎ | 51/67 [00:13<00:04,  3.74it/s, loss=1.24, v_num=1, train_loss_step=1.210, val_loss=2.770, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 282:  78%|█████▍ | 52/67 [00:13<00:04,  3.73it/s, loss=1.24, v_num=1, train_loss_step=1.210, val_loss=2.770, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 282:  79%|█████▌ | 53/67 [00:14<00:03,  3.72it/s, loss=1.24, v_num=1, train_loss_step=1.210, val_loss=2.770, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 282:  81%|█████▋ | 54/67 [00:14<00:03,  3.71it/s, loss=1.24, v_num=1, train_loss_step=1.210, val_loss=2.770, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 282:  82%|█████▋ | 55/67 [00:14<00:03,  3.70it/s, loss=1.24, v_num=1, train_loss_step=1.210, val_loss=2.770, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 282:  84%|█████▊ | 56/67 [00:15<00:02,  3.68it/s, loss=1.24, v_num=1, train_loss_step=1.210, val_loss=2.770, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 282:  85%|█████▉ | 57/67 [00:15<00:02,  3.67it/s, loss=1.24, v_num=1, train_loss_step=1.210, val_loss=2.770, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 282:  87%|██████ | 58/67 [00:15<00:02,  3.65it/s, loss=1.24, v_num=1, train_loss_step=1.210, val_loss=2.770, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 282:  88%|██████▏| 59/67 [00:16<00:02,  3.63it/s, loss=1.24, v_num=1, train_loss_step=1.210, val_loss=2.770, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 282:  90%|██████▎| 60/67 [00:16<00:01,  3.62it/s, loss=1.24, v_num=1, train_loss_step=1.210, val_loss=2.770, train_loss_epoch=1.580]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 282:  91%|██████▎| 61/67 [00:17<00:01,  3.58it/s, loss=1.24, v_num=1, train_loss_step=1.210, val_loss=2.770, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 282:  93%|██████▍| 62/67 [00:17<00:01,  3.58it/s, loss=1.24, v_num=1, train_loss_step=1.210, val_loss=2.770, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 282:  94%|██████▌| 63/67 [00:17<00:01,  3.58it/s, loss=1.24, v_num=1, train_loss_step=1.210, val_loss=2.770, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 282:  96%|██████▋| 64/67 [00:17<00:00,  3.57it/s, loss=1.24, v_num=1, train_loss_step=1.210, val_loss=2.770, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 282:  97%|██████▊| 65/67 [00:18<00:00,  3.57it/s, loss=1.24, v_num=1, train_loss_step=1.210, val_loss=2.770, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 282:  99%|██████▉| 66/67 [00:18<00:00,  3.56it/s, loss=1.24, v_num=1, train_loss_step=1.210, val_loss=2.770, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 282: 100%|███████| 67/67 [00:19<00:00,  3.49it/s, loss=1.24, v_num=1, train_loss_step=1.210, val_loss=2.760, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 283:  45%|███▏   | 30/67 [00:05<00:06,  5.42it/s, loss=2.03, v_num=1, train_loss_step=2.090, val_loss=2.760, train_loss_epoch=1.250]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 283:  46%|███▏   | 31/67 [00:07<00:08,  4.12it/s, loss=2.03, v_num=1, train_loss_step=2.090, val_loss=2.760, train_loss_epoch=1.250]\u001b[A\n",
      "Epoch 283:  48%|███▎   | 32/67 [00:07<00:08,  4.08it/s, loss=2.03, v_num=1, train_loss_step=2.090, val_loss=2.760, train_loss_epoch=1.250]\u001b[A\n",
      "Epoch 283:  49%|███▍   | 33/67 [00:08<00:08,  4.03it/s, loss=2.03, v_num=1, train_loss_step=2.090, val_loss=2.760, train_loss_epoch=1.250]\u001b[A\n",
      "Epoch 283:  51%|███▌   | 34/67 [00:08<00:08,  3.99it/s, loss=2.03, v_num=1, train_loss_step=2.090, val_loss=2.760, train_loss_epoch=1.250]\u001b[A\n",
      "Epoch 283:  52%|███▋   | 35/67 [00:08<00:08,  3.96it/s, loss=2.03, v_num=1, train_loss_step=2.090, val_loss=2.760, train_loss_epoch=1.250]\u001b[A\n",
      "Epoch 283:  54%|███▊   | 36/67 [00:09<00:07,  3.94it/s, loss=2.03, v_num=1, train_loss_step=2.090, val_loss=2.760, train_loss_epoch=1.250]\u001b[A\n",
      "Epoch 283:  55%|███▊   | 37/67 [00:09<00:07,  3.90it/s, loss=2.03, v_num=1, train_loss_step=2.090, val_loss=2.760, train_loss_epoch=1.250]\u001b[A\n",
      "Epoch 283:  57%|███▉   | 38/67 [00:09<00:07,  3.88it/s, loss=2.03, v_num=1, train_loss_step=2.090, val_loss=2.760, train_loss_epoch=1.250]\u001b[A\n",
      "Epoch 283:  58%|████   | 39/67 [00:10<00:07,  3.86it/s, loss=2.03, v_num=1, train_loss_step=2.090, val_loss=2.760, train_loss_epoch=1.250]\u001b[A\n",
      "Epoch 283:  60%|████▏  | 40/67 [00:10<00:07,  3.84it/s, loss=2.03, v_num=1, train_loss_step=2.090, val_loss=2.760, train_loss_epoch=1.250]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 283:  61%|████▎  | 41/67 [00:10<00:06,  3.78it/s, loss=2.03, v_num=1, train_loss_step=2.090, val_loss=2.760, train_loss_epoch=1.250]\u001b[A\n",
      "Epoch 283:  63%|████▍  | 42/67 [00:11<00:06,  3.77it/s, loss=2.03, v_num=1, train_loss_step=2.090, val_loss=2.760, train_loss_epoch=1.250]\u001b[A\n",
      "Epoch 283:  64%|████▍  | 43/67 [00:11<00:06,  3.76it/s, loss=2.03, v_num=1, train_loss_step=2.090, val_loss=2.760, train_loss_epoch=1.250]\u001b[A\n",
      "Epoch 283:  66%|████▌  | 44/67 [00:11<00:06,  3.75it/s, loss=2.03, v_num=1, train_loss_step=2.090, val_loss=2.760, train_loss_epoch=1.250]\u001b[A\n",
      "Epoch 283:  67%|████▋  | 45/67 [00:12<00:05,  3.73it/s, loss=2.03, v_num=1, train_loss_step=2.090, val_loss=2.760, train_loss_epoch=1.250]\u001b[A\n",
      "Epoch 283:  69%|████▊  | 46/67 [00:12<00:05,  3.71it/s, loss=2.03, v_num=1, train_loss_step=2.090, val_loss=2.760, train_loss_epoch=1.250]\u001b[A\n",
      "Epoch 283:  70%|████▉  | 47/67 [00:12<00:05,  3.70it/s, loss=2.03, v_num=1, train_loss_step=2.090, val_loss=2.760, train_loss_epoch=1.250]\u001b[A\n",
      "Epoch 283:  72%|█████  | 48/67 [00:13<00:05,  3.69it/s, loss=2.03, v_num=1, train_loss_step=2.090, val_loss=2.760, train_loss_epoch=1.250]\u001b[A\n",
      "Epoch 283:  73%|█████  | 49/67 [00:13<00:04,  3.67it/s, loss=2.03, v_num=1, train_loss_step=2.090, val_loss=2.760, train_loss_epoch=1.250]\u001b[A\n",
      "Epoch 283:  75%|█████▏ | 50/67 [00:13<00:04,  3.67it/s, loss=2.03, v_num=1, train_loss_step=2.090, val_loss=2.760, train_loss_epoch=1.250]\u001b[A\n",
      "Epoch 283:  76%|█████▎ | 51/67 [00:14<00:04,  3.63it/s, loss=2.03, v_num=1, train_loss_step=2.090, val_loss=2.760, train_loss_epoch=1.250]\u001b[A\n",
      "Epoch 283:  78%|█████▍ | 52/67 [00:14<00:04,  3.62it/s, loss=2.03, v_num=1, train_loss_step=2.090, val_loss=2.760, train_loss_epoch=1.250]\u001b[A\n",
      "Epoch 283:  79%|█████▌ | 53/67 [00:14<00:03,  3.61it/s, loss=2.03, v_num=1, train_loss_step=2.090, val_loss=2.760, train_loss_epoch=1.250]\u001b[A\n",
      "Epoch 283:  81%|█████▋ | 54/67 [00:15<00:03,  3.60it/s, loss=2.03, v_num=1, train_loss_step=2.090, val_loss=2.760, train_loss_epoch=1.250]\u001b[A\n",
      "Epoch 283:  82%|█████▋ | 55/67 [00:15<00:03,  3.59it/s, loss=2.03, v_num=1, train_loss_step=2.090, val_loss=2.760, train_loss_epoch=1.250]\u001b[A\n",
      "Epoch 283:  84%|█████▊ | 56/67 [00:15<00:03,  3.58it/s, loss=2.03, v_num=1, train_loss_step=2.090, val_loss=2.760, train_loss_epoch=1.250]\u001b[A\n",
      "Epoch 283:  85%|█████▉ | 57/67 [00:15<00:02,  3.58it/s, loss=2.03, v_num=1, train_loss_step=2.090, val_loss=2.760, train_loss_epoch=1.250]\u001b[A\n",
      "Epoch 283:  87%|██████ | 58/67 [00:16<00:02,  3.58it/s, loss=2.03, v_num=1, train_loss_step=2.090, val_loss=2.760, train_loss_epoch=1.250]\u001b[A\n",
      "Epoch 283:  88%|██████▏| 59/67 [00:16<00:02,  3.57it/s, loss=2.03, v_num=1, train_loss_step=2.090, val_loss=2.760, train_loss_epoch=1.250]\u001b[A\n",
      "Epoch 283:  90%|██████▎| 60/67 [00:16<00:01,  3.57it/s, loss=2.03, v_num=1, train_loss_step=2.090, val_loss=2.760, train_loss_epoch=1.250]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 283:  91%|██████▎| 61/67 [00:17<00:01,  3.54it/s, loss=2.03, v_num=1, train_loss_step=2.090, val_loss=2.760, train_loss_epoch=1.250]\u001b[A\n",
      "Epoch 283:  93%|██████▍| 62/67 [00:17<00:01,  3.53it/s, loss=2.03, v_num=1, train_loss_step=2.090, val_loss=2.760, train_loss_epoch=1.250]\u001b[A\n",
      "Epoch 283:  94%|██████▌| 63/67 [00:17<00:01,  3.53it/s, loss=2.03, v_num=1, train_loss_step=2.090, val_loss=2.760, train_loss_epoch=1.250]\u001b[A\n",
      "Epoch 283:  96%|██████▋| 64/67 [00:18<00:00,  3.53it/s, loss=2.03, v_num=1, train_loss_step=2.090, val_loss=2.760, train_loss_epoch=1.250]\u001b[A\n",
      "Epoch 283:  97%|██████▊| 65/67 [00:18<00:00,  3.53it/s, loss=2.03, v_num=1, train_loss_step=2.090, val_loss=2.760, train_loss_epoch=1.250]\u001b[A\n",
      "Epoch 283:  99%|██████▉| 66/67 [00:18<00:00,  3.52it/s, loss=2.03, v_num=1, train_loss_step=2.090, val_loss=2.760, train_loss_epoch=1.250]\u001b[A\n",
      "Epoch 283: 100%|███████| 67/67 [00:19<00:00,  3.46it/s, loss=2.03, v_num=1, train_loss_step=2.090, val_loss=2.760, train_loss_epoch=1.250]\u001b[A\n",
      "Epoch 284:  45%|███▏   | 30/67 [00:05<00:06,  5.74it/s, loss=2.31, v_num=1, train_loss_step=1.550, val_loss=2.760, train_loss_epoch=2.050]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 284:  46%|███▏   | 31/67 [00:07<00:08,  4.30it/s, loss=2.31, v_num=1, train_loss_step=1.550, val_loss=2.760, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 284:  48%|███▎   | 32/67 [00:07<00:08,  4.25it/s, loss=2.31, v_num=1, train_loss_step=1.550, val_loss=2.760, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 284:  49%|███▍   | 33/67 [00:07<00:08,  4.20it/s, loss=2.31, v_num=1, train_loss_step=1.550, val_loss=2.760, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 284:  51%|███▌   | 34/67 [00:08<00:07,  4.15it/s, loss=2.31, v_num=1, train_loss_step=1.550, val_loss=2.760, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 284:  52%|███▋   | 35/67 [00:08<00:07,  4.11it/s, loss=2.31, v_num=1, train_loss_step=1.550, val_loss=2.760, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 284:  54%|███▊   | 36/67 [00:08<00:07,  4.07it/s, loss=2.31, v_num=1, train_loss_step=1.550, val_loss=2.760, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 284:  55%|███▊   | 37/67 [00:09<00:07,  4.05it/s, loss=2.31, v_num=1, train_loss_step=1.550, val_loss=2.760, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 284:  57%|███▉   | 38/67 [00:09<00:07,  4.01it/s, loss=2.31, v_num=1, train_loss_step=1.550, val_loss=2.760, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 284:  58%|████   | 39/67 [00:09<00:07,  3.98it/s, loss=2.31, v_num=1, train_loss_step=1.550, val_loss=2.760, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 284:  60%|████▏  | 40/67 [00:10<00:06,  3.95it/s, loss=2.31, v_num=1, train_loss_step=1.550, val_loss=2.760, train_loss_epoch=2.050]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 284:  61%|████▎  | 41/67 [00:10<00:06,  3.86it/s, loss=2.31, v_num=1, train_loss_step=1.550, val_loss=2.760, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 284:  63%|████▍  | 42/67 [00:10<00:06,  3.84it/s, loss=2.31, v_num=1, train_loss_step=1.550, val_loss=2.760, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 284:  64%|████▍  | 43/67 [00:11<00:06,  3.83it/s, loss=2.31, v_num=1, train_loss_step=1.550, val_loss=2.760, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 284:  66%|████▌  | 44/67 [00:11<00:06,  3.81it/s, loss=2.31, v_num=1, train_loss_step=1.550, val_loss=2.760, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 284:  67%|████▋  | 45/67 [00:11<00:05,  3.80it/s, loss=2.31, v_num=1, train_loss_step=1.550, val_loss=2.760, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 284:  69%|████▊  | 46/67 [00:12<00:05,  3.79it/s, loss=2.31, v_num=1, train_loss_step=1.550, val_loss=2.760, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 284:  70%|████▉  | 47/67 [00:12<00:05,  3.77it/s, loss=2.31, v_num=1, train_loss_step=1.550, val_loss=2.760, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 284:  72%|█████  | 48/67 [00:12<00:05,  3.76it/s, loss=2.31, v_num=1, train_loss_step=1.550, val_loss=2.760, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 284:  73%|█████  | 49/67 [00:13<00:04,  3.75it/s, loss=2.31, v_num=1, train_loss_step=1.550, val_loss=2.760, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 284:  75%|█████▏ | 50/67 [00:13<00:04,  3.73it/s, loss=2.31, v_num=1, train_loss_step=1.550, val_loss=2.760, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 284:  76%|█████▎ | 51/67 [00:13<00:04,  3.68it/s, loss=2.31, v_num=1, train_loss_step=1.550, val_loss=2.760, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 284:  78%|█████▍ | 52/67 [00:14<00:04,  3.67it/s, loss=2.31, v_num=1, train_loss_step=1.550, val_loss=2.760, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 284:  79%|█████▌ | 53/67 [00:14<00:03,  3.66it/s, loss=2.31, v_num=1, train_loss_step=1.550, val_loss=2.760, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 284:  81%|█████▋ | 54/67 [00:14<00:03,  3.66it/s, loss=2.31, v_num=1, train_loss_step=1.550, val_loss=2.760, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 284:  82%|█████▋ | 55/67 [00:15<00:03,  3.65it/s, loss=2.31, v_num=1, train_loss_step=1.550, val_loss=2.760, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 284:  84%|█████▊ | 56/67 [00:15<00:03,  3.64it/s, loss=2.31, v_num=1, train_loss_step=1.550, val_loss=2.760, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 284:  85%|█████▉ | 57/67 [00:15<00:02,  3.63it/s, loss=2.31, v_num=1, train_loss_step=1.550, val_loss=2.760, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 284:  87%|██████ | 58/67 [00:16<00:02,  3.62it/s, loss=2.31, v_num=1, train_loss_step=1.550, val_loss=2.760, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 284:  88%|██████▏| 59/67 [00:16<00:02,  3.62it/s, loss=2.31, v_num=1, train_loss_step=1.550, val_loss=2.760, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 284:  90%|██████▎| 60/67 [00:16<00:01,  3.61it/s, loss=2.31, v_num=1, train_loss_step=1.550, val_loss=2.760, train_loss_epoch=2.050]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 284:  91%|██████▎| 61/67 [00:17<00:01,  3.57it/s, loss=2.31, v_num=1, train_loss_step=1.550, val_loss=2.760, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 284:  93%|██████▍| 62/67 [00:17<00:01,  3.57it/s, loss=2.31, v_num=1, train_loss_step=1.550, val_loss=2.760, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 284:  94%|██████▌| 63/67 [00:17<00:01,  3.56it/s, loss=2.31, v_num=1, train_loss_step=1.550, val_loss=2.760, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 284:  96%|██████▋| 64/67 [00:17<00:00,  3.56it/s, loss=2.31, v_num=1, train_loss_step=1.550, val_loss=2.760, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 284:  97%|██████▊| 65/67 [00:18<00:00,  3.55it/s, loss=2.31, v_num=1, train_loss_step=1.550, val_loss=2.760, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 284:  99%|██████▉| 66/67 [00:18<00:00,  3.54it/s, loss=2.31, v_num=1, train_loss_step=1.550, val_loss=2.760, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 284: 100%|███████| 67/67 [00:19<00:00,  3.46it/s, loss=2.31, v_num=1, train_loss_step=1.550, val_loss=2.760, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 285:  45%|███▏   | 30/67 [00:05<00:06,  5.36it/s, loss=1.73, v_num=1, train_loss_step=2.690, val_loss=2.760, train_loss_epoch=1.950]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 285:  46%|███▏   | 31/67 [00:07<00:08,  4.07it/s, loss=1.73, v_num=1, train_loss_step=2.690, val_loss=2.760, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 285:  48%|███▎   | 32/67 [00:07<00:08,  4.04it/s, loss=1.73, v_num=1, train_loss_step=2.690, val_loss=2.760, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 285:  49%|███▍   | 33/67 [00:08<00:08,  4.01it/s, loss=1.73, v_num=1, train_loss_step=2.690, val_loss=2.760, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 285:  51%|███▌   | 34/67 [00:08<00:08,  3.98it/s, loss=1.73, v_num=1, train_loss_step=2.690, val_loss=2.760, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 285:  52%|███▋   | 35/67 [00:08<00:08,  3.95it/s, loss=1.73, v_num=1, train_loss_step=2.690, val_loss=2.760, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 285:  54%|███▊   | 36/67 [00:09<00:07,  3.93it/s, loss=1.73, v_num=1, train_loss_step=2.690, val_loss=2.760, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 285:  55%|███▊   | 37/67 [00:09<00:07,  3.90it/s, loss=1.73, v_num=1, train_loss_step=2.690, val_loss=2.760, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 285:  57%|███▉   | 38/67 [00:09<00:07,  3.87it/s, loss=1.73, v_num=1, train_loss_step=2.690, val_loss=2.760, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 285:  58%|████   | 39/67 [00:10<00:07,  3.85it/s, loss=1.73, v_num=1, train_loss_step=2.690, val_loss=2.760, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 285:  60%|████▏  | 40/67 [00:10<00:07,  3.82it/s, loss=1.73, v_num=1, train_loss_step=2.690, val_loss=2.760, train_loss_epoch=1.950]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 285:  61%|████▎  | 41/67 [00:10<00:06,  3.76it/s, loss=1.73, v_num=1, train_loss_step=2.690, val_loss=2.760, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 285:  63%|████▍  | 42/67 [00:11<00:06,  3.74it/s, loss=1.73, v_num=1, train_loss_step=2.690, val_loss=2.760, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 285:  64%|████▍  | 43/67 [00:11<00:06,  3.71it/s, loss=1.73, v_num=1, train_loss_step=2.690, val_loss=2.760, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 285:  66%|████▌  | 44/67 [00:11<00:06,  3.69it/s, loss=1.73, v_num=1, train_loss_step=2.690, val_loss=2.760, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 285:  67%|████▋  | 45/67 [00:12<00:05,  3.68it/s, loss=1.73, v_num=1, train_loss_step=2.690, val_loss=2.760, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 285:  69%|████▊  | 46/67 [00:12<00:05,  3.66it/s, loss=1.73, v_num=1, train_loss_step=2.690, val_loss=2.760, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 285:  70%|████▉  | 47/67 [00:12<00:05,  3.66it/s, loss=1.73, v_num=1, train_loss_step=2.690, val_loss=2.760, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 285:  72%|█████  | 48/67 [00:13<00:05,  3.64it/s, loss=1.73, v_num=1, train_loss_step=2.690, val_loss=2.760, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 285:  73%|█████  | 49/67 [00:13<00:04,  3.64it/s, loss=1.73, v_num=1, train_loss_step=2.690, val_loss=2.760, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 285:  75%|█████▏ | 50/67 [00:13<00:04,  3.63it/s, loss=1.73, v_num=1, train_loss_step=2.690, val_loss=2.760, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 285:  76%|█████▎ | 51/67 [00:14<00:04,  3.59it/s, loss=1.73, v_num=1, train_loss_step=2.690, val_loss=2.760, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 285:  78%|█████▍ | 52/67 [00:14<00:04,  3.58it/s, loss=1.73, v_num=1, train_loss_step=2.690, val_loss=2.760, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 285:  79%|█████▌ | 53/67 [00:14<00:03,  3.58it/s, loss=1.73, v_num=1, train_loss_step=2.690, val_loss=2.760, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 285:  81%|█████▋ | 54/67 [00:15<00:03,  3.57it/s, loss=1.73, v_num=1, train_loss_step=2.690, val_loss=2.760, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 285:  82%|█████▋ | 55/67 [00:15<00:03,  3.57it/s, loss=1.73, v_num=1, train_loss_step=2.690, val_loss=2.760, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 285:  84%|█████▊ | 56/67 [00:15<00:03,  3.56it/s, loss=1.73, v_num=1, train_loss_step=2.690, val_loss=2.760, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 285:  85%|█████▉ | 57/67 [00:16<00:02,  3.56it/s, loss=1.73, v_num=1, train_loss_step=2.690, val_loss=2.760, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 285:  87%|██████ | 58/67 [00:16<00:02,  3.55it/s, loss=1.73, v_num=1, train_loss_step=2.690, val_loss=2.760, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 285:  88%|██████▏| 59/67 [00:16<00:02,  3.54it/s, loss=1.73, v_num=1, train_loss_step=2.690, val_loss=2.760, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 285:  90%|██████▎| 60/67 [00:16<00:01,  3.54it/s, loss=1.73, v_num=1, train_loss_step=2.690, val_loss=2.760, train_loss_epoch=1.950]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 285:  91%|██████▎| 61/67 [00:17<00:01,  3.50it/s, loss=1.73, v_num=1, train_loss_step=2.690, val_loss=2.760, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 285:  93%|██████▍| 62/67 [00:17<00:01,  3.50it/s, loss=1.73, v_num=1, train_loss_step=2.690, val_loss=2.760, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 285:  94%|██████▌| 63/67 [00:18<00:01,  3.50it/s, loss=1.73, v_num=1, train_loss_step=2.690, val_loss=2.760, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 285:  96%|██████▋| 64/67 [00:18<00:00,  3.49it/s, loss=1.73, v_num=1, train_loss_step=2.690, val_loss=2.760, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 285:  97%|██████▊| 65/67 [00:18<00:00,  3.49it/s, loss=1.73, v_num=1, train_loss_step=2.690, val_loss=2.760, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 285:  99%|██████▉| 66/67 [00:18<00:00,  3.49it/s, loss=1.73, v_num=1, train_loss_step=2.690, val_loss=2.760, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 285: 100%|███████| 67/67 [00:19<00:00,  3.41it/s, loss=1.73, v_num=1, train_loss_step=2.690, val_loss=2.760, train_loss_epoch=1.950]\u001b[A\n",
      "Epoch 286:  45%|███▏   | 30/67 [00:05<00:06,  5.50it/s, loss=1.97, v_num=1, train_loss_step=0.535, val_loss=2.760, train_loss_epoch=1.890]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 286:  46%|███▏   | 31/67 [00:07<00:08,  4.14it/s, loss=1.97, v_num=1, train_loss_step=0.535, val_loss=2.760, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 286:  48%|███▎   | 32/67 [00:07<00:08,  4.10it/s, loss=1.97, v_num=1, train_loss_step=0.535, val_loss=2.760, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 286:  49%|███▍   | 33/67 [00:08<00:08,  4.07it/s, loss=1.97, v_num=1, train_loss_step=0.535, val_loss=2.760, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 286:  51%|███▌   | 34/67 [00:08<00:08,  4.04it/s, loss=1.97, v_num=1, train_loss_step=0.535, val_loss=2.760, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 286:  52%|███▋   | 35/67 [00:08<00:07,  4.01it/s, loss=1.97, v_num=1, train_loss_step=0.535, val_loss=2.760, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 286:  54%|███▊   | 36/67 [00:09<00:07,  3.98it/s, loss=1.97, v_num=1, train_loss_step=0.535, val_loss=2.760, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 286:  55%|███▊   | 37/67 [00:09<00:07,  3.95it/s, loss=1.97, v_num=1, train_loss_step=0.535, val_loss=2.760, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 286:  57%|███▉   | 38/67 [00:09<00:07,  3.93it/s, loss=1.97, v_num=1, train_loss_step=0.535, val_loss=2.760, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 286:  58%|████   | 39/67 [00:10<00:07,  3.90it/s, loss=1.97, v_num=1, train_loss_step=0.535, val_loss=2.760, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 286:  60%|████▏  | 40/67 [00:10<00:06,  3.87it/s, loss=1.97, v_num=1, train_loss_step=0.535, val_loss=2.760, train_loss_epoch=1.890]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 286:  61%|████▎  | 41/67 [00:10<00:06,  3.80it/s, loss=1.97, v_num=1, train_loss_step=0.535, val_loss=2.760, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 286:  63%|████▍  | 42/67 [00:11<00:06,  3.78it/s, loss=1.97, v_num=1, train_loss_step=0.535, val_loss=2.760, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 286:  64%|████▍  | 43/67 [00:11<00:06,  3.76it/s, loss=1.97, v_num=1, train_loss_step=0.535, val_loss=2.760, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 286:  66%|████▌  | 44/67 [00:11<00:06,  3.75it/s, loss=1.97, v_num=1, train_loss_step=0.535, val_loss=2.760, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 286:  67%|████▋  | 45/67 [00:12<00:05,  3.74it/s, loss=1.97, v_num=1, train_loss_step=0.535, val_loss=2.760, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 286:  69%|████▊  | 46/67 [00:12<00:05,  3.72it/s, loss=1.97, v_num=1, train_loss_step=0.535, val_loss=2.760, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 286:  70%|████▉  | 47/67 [00:12<00:05,  3.71it/s, loss=1.97, v_num=1, train_loss_step=0.535, val_loss=2.760, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 286:  72%|█████  | 48/67 [00:12<00:05,  3.70it/s, loss=1.97, v_num=1, train_loss_step=0.535, val_loss=2.760, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 286:  73%|█████  | 49/67 [00:13<00:04,  3.69it/s, loss=1.97, v_num=1, train_loss_step=0.535, val_loss=2.760, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 286:  75%|█████▏ | 50/67 [00:13<00:04,  3.68it/s, loss=1.97, v_num=1, train_loss_step=0.535, val_loss=2.760, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 286:  76%|█████▎ | 51/67 [00:14<00:04,  3.63it/s, loss=1.97, v_num=1, train_loss_step=0.535, val_loss=2.760, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 286:  78%|█████▍ | 52/67 [00:14<00:04,  3.62it/s, loss=1.97, v_num=1, train_loss_step=0.535, val_loss=2.760, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 286:  79%|█████▌ | 53/67 [00:14<00:03,  3.61it/s, loss=1.97, v_num=1, train_loss_step=0.535, val_loss=2.760, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 286:  81%|█████▋ | 54/67 [00:14<00:03,  3.60it/s, loss=1.97, v_num=1, train_loss_step=0.535, val_loss=2.760, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 286:  82%|█████▋ | 55/67 [00:15<00:03,  3.60it/s, loss=1.97, v_num=1, train_loss_step=0.535, val_loss=2.760, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 286:  84%|█████▊ | 56/67 [00:15<00:03,  3.59it/s, loss=1.97, v_num=1, train_loss_step=0.535, val_loss=2.760, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 286:  85%|█████▉ | 57/67 [00:15<00:02,  3.58it/s, loss=1.97, v_num=1, train_loss_step=0.535, val_loss=2.760, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 286:  87%|██████ | 58/67 [00:16<00:02,  3.57it/s, loss=1.97, v_num=1, train_loss_step=0.535, val_loss=2.760, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 286:  88%|██████▏| 59/67 [00:16<00:02,  3.57it/s, loss=1.97, v_num=1, train_loss_step=0.535, val_loss=2.760, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 286:  90%|██████▎| 60/67 [00:16<00:01,  3.56it/s, loss=1.97, v_num=1, train_loss_step=0.535, val_loss=2.760, train_loss_epoch=1.890]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 286:  91%|██████▎| 61/67 [00:17<00:01,  3.53it/s, loss=1.97, v_num=1, train_loss_step=0.535, val_loss=2.760, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 286:  93%|██████▍| 62/67 [00:17<00:01,  3.52it/s, loss=1.97, v_num=1, train_loss_step=0.535, val_loss=2.760, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 286:  94%|██████▌| 63/67 [00:17<00:01,  3.52it/s, loss=1.97, v_num=1, train_loss_step=0.535, val_loss=2.760, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 286:  96%|██████▋| 64/67 [00:18<00:00,  3.51it/s, loss=1.97, v_num=1, train_loss_step=0.535, val_loss=2.760, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 286:  97%|██████▊| 65/67 [00:18<00:00,  3.51it/s, loss=1.97, v_num=1, train_loss_step=0.535, val_loss=2.760, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 286:  99%|██████▉| 66/67 [00:18<00:00,  3.50it/s, loss=1.97, v_num=1, train_loss_step=0.535, val_loss=2.760, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 286: 100%|███████| 67/67 [00:19<00:00,  3.43it/s, loss=1.97, v_num=1, train_loss_step=0.535, val_loss=2.760, train_loss_epoch=1.890]\u001b[A\n",
      "Epoch 287:  45%|███▏   | 30/67 [00:05<00:06,  5.66it/s, loss=2.17, v_num=1, train_loss_step=6.190, val_loss=2.760, train_loss_epoch=1.740]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 287:  46%|███▏   | 31/67 [00:07<00:08,  4.19it/s, loss=2.17, v_num=1, train_loss_step=6.190, val_loss=2.760, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 287:  48%|███▎   | 32/67 [00:07<00:08,  4.14it/s, loss=2.17, v_num=1, train_loss_step=6.190, val_loss=2.760, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 287:  49%|███▍   | 33/67 [00:08<00:08,  4.10it/s, loss=2.17, v_num=1, train_loss_step=6.190, val_loss=2.760, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 287:  51%|███▌   | 34/67 [00:08<00:08,  4.05it/s, loss=2.17, v_num=1, train_loss_step=6.190, val_loss=2.760, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 287:  52%|███▋   | 35/67 [00:08<00:07,  4.02it/s, loss=2.17, v_num=1, train_loss_step=6.190, val_loss=2.760, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 287:  54%|███▊   | 36/67 [00:09<00:07,  4.00it/s, loss=2.17, v_num=1, train_loss_step=6.190, val_loss=2.760, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 287:  55%|███▊   | 37/67 [00:09<00:07,  3.97it/s, loss=2.17, v_num=1, train_loss_step=6.190, val_loss=2.760, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 287:  57%|███▉   | 38/67 [00:09<00:07,  3.95it/s, loss=2.17, v_num=1, train_loss_step=6.190, val_loss=2.760, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 287:  58%|████   | 39/67 [00:09<00:07,  3.94it/s, loss=2.17, v_num=1, train_loss_step=6.190, val_loss=2.760, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 287:  60%|████▏  | 40/67 [00:10<00:06,  3.91it/s, loss=2.17, v_num=1, train_loss_step=6.190, val_loss=2.760, train_loss_epoch=1.740]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 287:  61%|████▎  | 41/67 [00:10<00:06,  3.85it/s, loss=2.17, v_num=1, train_loss_step=6.190, val_loss=2.760, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 287:  63%|████▍  | 42/67 [00:10<00:06,  3.83it/s, loss=2.17, v_num=1, train_loss_step=6.190, val_loss=2.760, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 287:  64%|████▍  | 43/67 [00:11<00:06,  3.81it/s, loss=2.17, v_num=1, train_loss_step=6.190, val_loss=2.760, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 287:  66%|████▌  | 44/67 [00:11<00:06,  3.79it/s, loss=2.17, v_num=1, train_loss_step=6.190, val_loss=2.760, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 287:  67%|████▋  | 45/67 [00:11<00:05,  3.78it/s, loss=2.17, v_num=1, train_loss_step=6.190, val_loss=2.760, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 287:  69%|████▊  | 46/67 [00:12<00:05,  3.76it/s, loss=2.17, v_num=1, train_loss_step=6.190, val_loss=2.760, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 287:  70%|████▉  | 47/67 [00:12<00:05,  3.75it/s, loss=2.17, v_num=1, train_loss_step=6.190, val_loss=2.760, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 287:  72%|█████  | 48/67 [00:12<00:05,  3.73it/s, loss=2.17, v_num=1, train_loss_step=6.190, val_loss=2.760, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 287:  73%|█████  | 49/67 [00:13<00:04,  3.71it/s, loss=2.17, v_num=1, train_loss_step=6.190, val_loss=2.760, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 287:  75%|█████▏ | 50/67 [00:13<00:04,  3.70it/s, loss=2.17, v_num=1, train_loss_step=6.190, val_loss=2.760, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 287:  76%|█████▎ | 51/67 [00:13<00:04,  3.65it/s, loss=2.17, v_num=1, train_loss_step=6.190, val_loss=2.760, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 287:  78%|█████▍ | 52/67 [00:14<00:04,  3.64it/s, loss=2.17, v_num=1, train_loss_step=6.190, val_loss=2.760, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 287:  79%|█████▌ | 53/67 [00:14<00:03,  3.63it/s, loss=2.17, v_num=1, train_loss_step=6.190, val_loss=2.760, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 287:  81%|█████▋ | 54/67 [00:14<00:03,  3.62it/s, loss=2.17, v_num=1, train_loss_step=6.190, val_loss=2.760, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 287:  82%|█████▋ | 55/67 [00:15<00:03,  3.61it/s, loss=2.17, v_num=1, train_loss_step=6.190, val_loss=2.760, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 287:  84%|█████▊ | 56/67 [00:15<00:03,  3.60it/s, loss=2.17, v_num=1, train_loss_step=6.190, val_loss=2.760, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 287:  85%|█████▉ | 57/67 [00:15<00:02,  3.59it/s, loss=2.17, v_num=1, train_loss_step=6.190, val_loss=2.760, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 287:  87%|██████ | 58/67 [00:16<00:02,  3.58it/s, loss=2.17, v_num=1, train_loss_step=6.190, val_loss=2.760, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 287:  88%|██████▏| 59/67 [00:16<00:02,  3.58it/s, loss=2.17, v_num=1, train_loss_step=6.190, val_loss=2.760, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 287:  90%|██████▎| 60/67 [00:16<00:01,  3.57it/s, loss=2.17, v_num=1, train_loss_step=6.190, val_loss=2.760, train_loss_epoch=1.740]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 287:  91%|██████▎| 61/67 [00:17<00:01,  3.53it/s, loss=2.17, v_num=1, train_loss_step=6.190, val_loss=2.760, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 287:  93%|██████▍| 62/67 [00:17<00:01,  3.53it/s, loss=2.17, v_num=1, train_loss_step=6.190, val_loss=2.760, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 287:  94%|██████▌| 63/67 [00:17<00:01,  3.52it/s, loss=2.17, v_num=1, train_loss_step=6.190, val_loss=2.760, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 287:  96%|██████▋| 64/67 [00:18<00:00,  3.52it/s, loss=2.17, v_num=1, train_loss_step=6.190, val_loss=2.760, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 287:  97%|██████▊| 65/67 [00:18<00:00,  3.51it/s, loss=2.17, v_num=1, train_loss_step=6.190, val_loss=2.760, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 287:  99%|██████▉| 66/67 [00:18<00:00,  3.51it/s, loss=2.17, v_num=1, train_loss_step=6.190, val_loss=2.760, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 287: 100%|███████| 67/67 [00:19<00:00,  3.43it/s, loss=2.17, v_num=1, train_loss_step=6.190, val_loss=2.760, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 288:  45%|███▌    | 30/67 [00:05<00:06,  5.56it/s, loss=1.6, v_num=1, train_loss_step=1.360, val_loss=2.760, train_loss_epoch=2.000]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 288:  46%|███▋    | 31/67 [00:07<00:08,  4.21it/s, loss=1.6, v_num=1, train_loss_step=1.360, val_loss=2.760, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 288:  48%|███▊    | 32/67 [00:07<00:08,  4.18it/s, loss=1.6, v_num=1, train_loss_step=1.360, val_loss=2.760, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 288:  49%|███▉    | 33/67 [00:07<00:08,  4.14it/s, loss=1.6, v_num=1, train_loss_step=1.360, val_loss=2.760, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 288:  51%|████    | 34/67 [00:08<00:08,  4.11it/s, loss=1.6, v_num=1, train_loss_step=1.360, val_loss=2.760, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 288:  52%|████▏   | 35/67 [00:08<00:07,  4.08it/s, loss=1.6, v_num=1, train_loss_step=1.360, val_loss=2.760, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 288:  54%|████▎   | 36/67 [00:08<00:07,  4.05it/s, loss=1.6, v_num=1, train_loss_step=1.360, val_loss=2.760, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 288:  55%|████▍   | 37/67 [00:09<00:07,  4.02it/s, loss=1.6, v_num=1, train_loss_step=1.360, val_loss=2.760, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 288:  57%|████▌   | 38/67 [00:09<00:07,  3.99it/s, loss=1.6, v_num=1, train_loss_step=1.360, val_loss=2.760, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 288:  58%|████▋   | 39/67 [00:09<00:07,  3.97it/s, loss=1.6, v_num=1, train_loss_step=1.360, val_loss=2.760, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 288:  60%|████▊   | 40/67 [00:10<00:06,  3.94it/s, loss=1.6, v_num=1, train_loss_step=1.360, val_loss=2.760, train_loss_epoch=2.000]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 288:  61%|████▉   | 41/67 [00:10<00:06,  3.87it/s, loss=1.6, v_num=1, train_loss_step=1.360, val_loss=2.760, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 288:  63%|█████   | 42/67 [00:10<00:06,  3.85it/s, loss=1.6, v_num=1, train_loss_step=1.360, val_loss=2.760, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 288:  64%|█████▏  | 43/67 [00:11<00:06,  3.83it/s, loss=1.6, v_num=1, train_loss_step=1.360, val_loss=2.760, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 288:  66%|█████▎  | 44/67 [00:11<00:06,  3.82it/s, loss=1.6, v_num=1, train_loss_step=1.360, val_loss=2.760, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 288:  67%|█████▎  | 45/67 [00:11<00:05,  3.80it/s, loss=1.6, v_num=1, train_loss_step=1.360, val_loss=2.760, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 288:  69%|█████▍  | 46/67 [00:12<00:05,  3.79it/s, loss=1.6, v_num=1, train_loss_step=1.360, val_loss=2.760, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 288:  70%|█████▌  | 47/67 [00:12<00:05,  3.77it/s, loss=1.6, v_num=1, train_loss_step=1.360, val_loss=2.760, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 288:  72%|█████▋  | 48/67 [00:12<00:05,  3.75it/s, loss=1.6, v_num=1, train_loss_step=1.360, val_loss=2.760, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 288:  73%|█████▊  | 49/67 [00:13<00:04,  3.74it/s, loss=1.6, v_num=1, train_loss_step=1.360, val_loss=2.760, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 288:  75%|█████▉  | 50/67 [00:13<00:04,  3.73it/s, loss=1.6, v_num=1, train_loss_step=1.360, val_loss=2.760, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 288:  76%|██████  | 51/67 [00:13<00:04,  3.68it/s, loss=1.6, v_num=1, train_loss_step=1.360, val_loss=2.760, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 288:  78%|██████▏ | 52/67 [00:14<00:04,  3.66it/s, loss=1.6, v_num=1, train_loss_step=1.360, val_loss=2.760, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 288:  79%|██████▎ | 53/67 [00:14<00:03,  3.65it/s, loss=1.6, v_num=1, train_loss_step=1.360, val_loss=2.760, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 288:  81%|██████▍ | 54/67 [00:14<00:03,  3.65it/s, loss=1.6, v_num=1, train_loss_step=1.360, val_loss=2.760, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 288:  82%|██████▌ | 55/67 [00:15<00:03,  3.64it/s, loss=1.6, v_num=1, train_loss_step=1.360, val_loss=2.760, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 288:  84%|██████▋ | 56/67 [00:15<00:03,  3.63it/s, loss=1.6, v_num=1, train_loss_step=1.360, val_loss=2.760, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 288:  85%|██████▊ | 57/67 [00:15<00:02,  3.62it/s, loss=1.6, v_num=1, train_loss_step=1.360, val_loss=2.760, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 288:  87%|██████▉ | 58/67 [00:16<00:02,  3.61it/s, loss=1.6, v_num=1, train_loss_step=1.360, val_loss=2.760, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 288:  88%|███████ | 59/67 [00:16<00:02,  3.61it/s, loss=1.6, v_num=1, train_loss_step=1.360, val_loss=2.760, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 288:  90%|███████▏| 60/67 [00:16<00:01,  3.60it/s, loss=1.6, v_num=1, train_loss_step=1.360, val_loss=2.760, train_loss_epoch=2.000]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 288:  91%|███████▎| 61/67 [00:17<00:01,  3.56it/s, loss=1.6, v_num=1, train_loss_step=1.360, val_loss=2.760, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 288:  93%|███████▍| 62/67 [00:17<00:01,  3.55it/s, loss=1.6, v_num=1, train_loss_step=1.360, val_loss=2.760, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 288:  94%|███████▌| 63/67 [00:17<00:01,  3.55it/s, loss=1.6, v_num=1, train_loss_step=1.360, val_loss=2.760, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 288:  96%|███████▋| 64/67 [00:18<00:00,  3.54it/s, loss=1.6, v_num=1, train_loss_step=1.360, val_loss=2.760, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 288:  97%|███████▊| 65/67 [00:18<00:00,  3.54it/s, loss=1.6, v_num=1, train_loss_step=1.360, val_loss=2.760, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 288:  99%|███████▉| 66/67 [00:18<00:00,  3.54it/s, loss=1.6, v_num=1, train_loss_step=1.360, val_loss=2.760, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 288: 100%|████████| 67/67 [00:19<00:00,  3.46it/s, loss=1.6, v_num=1, train_loss_step=1.360, val_loss=2.760, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 289:  45%|███▏   | 30/67 [00:05<00:06,  5.30it/s, loss=3.62, v_num=1, train_loss_step=1.100, val_loss=2.760, train_loss_epoch=1.720]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 289:  46%|███▏   | 31/67 [00:07<00:08,  4.01it/s, loss=3.62, v_num=1, train_loss_step=1.100, val_loss=2.760, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 289:  48%|███▎   | 32/67 [00:08<00:08,  3.97it/s, loss=3.62, v_num=1, train_loss_step=1.100, val_loss=2.760, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 289:  49%|███▍   | 33/67 [00:08<00:08,  3.94it/s, loss=3.62, v_num=1, train_loss_step=1.100, val_loss=2.760, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 289:  51%|███▌   | 34/67 [00:08<00:08,  3.90it/s, loss=3.62, v_num=1, train_loss_step=1.100, val_loss=2.760, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 289:  52%|███▋   | 35/67 [00:09<00:08,  3.87it/s, loss=3.62, v_num=1, train_loss_step=1.100, val_loss=2.760, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 289:  54%|███▊   | 36/67 [00:09<00:08,  3.85it/s, loss=3.62, v_num=1, train_loss_step=1.100, val_loss=2.760, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 289:  55%|███▊   | 37/67 [00:09<00:07,  3.82it/s, loss=3.62, v_num=1, train_loss_step=1.100, val_loss=2.760, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 289:  57%|███▉   | 38/67 [00:10<00:07,  3.79it/s, loss=3.62, v_num=1, train_loss_step=1.100, val_loss=2.760, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 289:  58%|████   | 39/67 [00:10<00:07,  3.77it/s, loss=3.62, v_num=1, train_loss_step=1.100, val_loss=2.760, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 289:  60%|████▏  | 40/67 [00:10<00:07,  3.75it/s, loss=3.62, v_num=1, train_loss_step=1.100, val_loss=2.760, train_loss_epoch=1.720]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 289:  61%|████▎  | 41/67 [00:11<00:07,  3.68it/s, loss=3.62, v_num=1, train_loss_step=1.100, val_loss=2.760, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 289:  63%|████▍  | 42/67 [00:11<00:06,  3.67it/s, loss=3.62, v_num=1, train_loss_step=1.100, val_loss=2.760, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 289:  64%|████▍  | 43/67 [00:11<00:06,  3.65it/s, loss=3.62, v_num=1, train_loss_step=1.100, val_loss=2.760, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 289:  66%|████▌  | 44/67 [00:12<00:06,  3.64it/s, loss=3.62, v_num=1, train_loss_step=1.100, val_loss=2.760, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 289:  67%|████▋  | 45/67 [00:12<00:06,  3.63it/s, loss=3.62, v_num=1, train_loss_step=1.100, val_loss=2.760, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 289:  69%|████▊  | 46/67 [00:12<00:05,  3.61it/s, loss=3.62, v_num=1, train_loss_step=1.100, val_loss=2.760, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 289:  70%|████▉  | 47/67 [00:13<00:05,  3.60it/s, loss=3.62, v_num=1, train_loss_step=1.100, val_loss=2.760, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 289:  72%|█████  | 48/67 [00:13<00:05,  3.59it/s, loss=3.62, v_num=1, train_loss_step=1.100, val_loss=2.760, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 289:  73%|█████  | 49/67 [00:13<00:05,  3.58it/s, loss=3.62, v_num=1, train_loss_step=1.100, val_loss=2.760, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 289:  75%|█████▏ | 50/67 [00:14<00:04,  3.57it/s, loss=3.62, v_num=1, train_loss_step=1.100, val_loss=2.760, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 289:  76%|█████▎ | 51/67 [00:14<00:04,  3.53it/s, loss=3.62, v_num=1, train_loss_step=1.100, val_loss=2.760, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 289:  78%|█████▍ | 52/67 [00:14<00:04,  3.52it/s, loss=3.62, v_num=1, train_loss_step=1.100, val_loss=2.760, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 289:  79%|█████▌ | 53/67 [00:15<00:03,  3.51it/s, loss=3.62, v_num=1, train_loss_step=1.100, val_loss=2.760, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 289:  81%|█████▋ | 54/67 [00:15<00:03,  3.50it/s, loss=3.62, v_num=1, train_loss_step=1.100, val_loss=2.760, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 289:  82%|█████▋ | 55/67 [00:15<00:03,  3.49it/s, loss=3.62, v_num=1, train_loss_step=1.100, val_loss=2.760, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 289:  84%|█████▊ | 56/67 [00:16<00:03,  3.49it/s, loss=3.62, v_num=1, train_loss_step=1.100, val_loss=2.760, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 289:  85%|█████▉ | 57/67 [00:16<00:02,  3.48it/s, loss=3.62, v_num=1, train_loss_step=1.100, val_loss=2.760, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 289:  87%|██████ | 58/67 [00:16<00:02,  3.47it/s, loss=3.62, v_num=1, train_loss_step=1.100, val_loss=2.760, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 289:  88%|██████▏| 59/67 [00:17<00:02,  3.47it/s, loss=3.62, v_num=1, train_loss_step=1.100, val_loss=2.760, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 289:  90%|██████▎| 60/67 [00:17<00:02,  3.46it/s, loss=3.62, v_num=1, train_loss_step=1.100, val_loss=2.760, train_loss_epoch=1.720]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 289:  91%|██████▎| 61/67 [00:17<00:01,  3.43it/s, loss=3.62, v_num=1, train_loss_step=1.100, val_loss=2.760, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 289:  93%|██████▍| 62/67 [00:18<00:01,  3.42it/s, loss=3.62, v_num=1, train_loss_step=1.100, val_loss=2.760, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 289:  94%|██████▌| 63/67 [00:18<00:01,  3.42it/s, loss=3.62, v_num=1, train_loss_step=1.100, val_loss=2.760, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 289:  96%|██████▋| 64/67 [00:18<00:00,  3.41it/s, loss=3.62, v_num=1, train_loss_step=1.100, val_loss=2.760, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 289:  97%|██████▊| 65/67 [00:19<00:00,  3.41it/s, loss=3.62, v_num=1, train_loss_step=1.100, val_loss=2.760, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 289:  99%|██████▉| 66/67 [00:19<00:00,  3.41it/s, loss=3.62, v_num=1, train_loss_step=1.100, val_loss=2.760, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 289: 100%|███████| 67/67 [00:20<00:00,  3.33it/s, loss=3.62, v_num=1, train_loss_step=1.100, val_loss=2.760, train_loss_epoch=1.720]\u001b[A\n",
      "Epoch 290:  45%|███▏   | 30/67 [00:05<00:06,  5.57it/s, loss=2.16, v_num=1, train_loss_step=0.946, val_loss=2.760, train_loss_epoch=3.130]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 290:  46%|███▏   | 31/67 [00:07<00:08,  4.19it/s, loss=2.16, v_num=1, train_loss_step=0.946, val_loss=2.760, train_loss_epoch=3.130]\u001b[A\n",
      "Epoch 290:  48%|███▎   | 32/67 [00:07<00:08,  4.15it/s, loss=2.16, v_num=1, train_loss_step=0.946, val_loss=2.760, train_loss_epoch=3.130]\u001b[A\n",
      "Epoch 290:  49%|███▍   | 33/67 [00:08<00:08,  4.10it/s, loss=2.16, v_num=1, train_loss_step=0.946, val_loss=2.760, train_loss_epoch=3.130]\u001b[A\n",
      "Epoch 290:  51%|███▌   | 34/67 [00:08<00:08,  4.07it/s, loss=2.16, v_num=1, train_loss_step=0.946, val_loss=2.760, train_loss_epoch=3.130]\u001b[A\n",
      "Epoch 290:  52%|███▋   | 35/67 [00:08<00:07,  4.04it/s, loss=2.16, v_num=1, train_loss_step=0.946, val_loss=2.760, train_loss_epoch=3.130]\u001b[A\n",
      "Epoch 290:  54%|███▊   | 36/67 [00:08<00:07,  4.00it/s, loss=2.16, v_num=1, train_loss_step=0.946, val_loss=2.760, train_loss_epoch=3.130]\u001b[A\n",
      "Epoch 290:  55%|███▊   | 37/67 [00:09<00:07,  3.97it/s, loss=2.16, v_num=1, train_loss_step=0.946, val_loss=2.760, train_loss_epoch=3.130]\u001b[A\n",
      "Epoch 290:  57%|███▉   | 38/67 [00:09<00:07,  3.94it/s, loss=2.16, v_num=1, train_loss_step=0.946, val_loss=2.760, train_loss_epoch=3.130]\u001b[A\n",
      "Epoch 290:  58%|████   | 39/67 [00:09<00:07,  3.92it/s, loss=2.16, v_num=1, train_loss_step=0.946, val_loss=2.760, train_loss_epoch=3.130]\u001b[A\n",
      "Epoch 290:  60%|████▏  | 40/67 [00:10<00:06,  3.90it/s, loss=2.16, v_num=1, train_loss_step=0.946, val_loss=2.760, train_loss_epoch=3.130]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 290:  61%|████▎  | 41/67 [00:10<00:06,  3.83it/s, loss=2.16, v_num=1, train_loss_step=0.946, val_loss=2.760, train_loss_epoch=3.130]\u001b[A\n",
      "Epoch 290:  63%|████▍  | 42/67 [00:11<00:06,  3.81it/s, loss=2.16, v_num=1, train_loss_step=0.946, val_loss=2.760, train_loss_epoch=3.130]\u001b[A\n",
      "Epoch 290:  64%|████▍  | 43/67 [00:11<00:06,  3.80it/s, loss=2.16, v_num=1, train_loss_step=0.946, val_loss=2.760, train_loss_epoch=3.130]\u001b[A\n",
      "Epoch 290:  66%|████▌  | 44/67 [00:11<00:06,  3.78it/s, loss=2.16, v_num=1, train_loss_step=0.946, val_loss=2.760, train_loss_epoch=3.130]\u001b[A\n",
      "Epoch 290:  67%|████▋  | 45/67 [00:11<00:05,  3.75it/s, loss=2.16, v_num=1, train_loss_step=0.946, val_loss=2.760, train_loss_epoch=3.130]\u001b[A\n",
      "Epoch 290:  69%|████▊  | 46/67 [00:12<00:05,  3.73it/s, loss=2.16, v_num=1, train_loss_step=0.946, val_loss=2.760, train_loss_epoch=3.130]\u001b[A\n",
      "Epoch 290:  70%|████▉  | 47/67 [00:12<00:05,  3.72it/s, loss=2.16, v_num=1, train_loss_step=0.946, val_loss=2.760, train_loss_epoch=3.130]\u001b[A\n",
      "Epoch 290:  72%|█████  | 48/67 [00:12<00:05,  3.71it/s, loss=2.16, v_num=1, train_loss_step=0.946, val_loss=2.760, train_loss_epoch=3.130]\u001b[A\n",
      "Epoch 290:  73%|█████  | 49/67 [00:13<00:04,  3.70it/s, loss=2.16, v_num=1, train_loss_step=0.946, val_loss=2.760, train_loss_epoch=3.130]\u001b[A\n",
      "Epoch 290:  75%|█████▏ | 50/67 [00:13<00:04,  3.68it/s, loss=2.16, v_num=1, train_loss_step=0.946, val_loss=2.760, train_loss_epoch=3.130]\u001b[A\n",
      "Epoch 290:  76%|█████▎ | 51/67 [00:14<00:04,  3.62it/s, loss=2.16, v_num=1, train_loss_step=0.946, val_loss=2.760, train_loss_epoch=3.130]\u001b[A\n",
      "Epoch 290:  78%|█████▍ | 52/67 [00:14<00:04,  3.61it/s, loss=2.16, v_num=1, train_loss_step=0.946, val_loss=2.760, train_loss_epoch=3.130]\u001b[A\n",
      "Epoch 290:  79%|█████▌ | 53/67 [00:14<00:03,  3.60it/s, loss=2.16, v_num=1, train_loss_step=0.946, val_loss=2.760, train_loss_epoch=3.130]\u001b[A\n",
      "Epoch 290:  81%|█████▋ | 54/67 [00:15<00:03,  3.59it/s, loss=2.16, v_num=1, train_loss_step=0.946, val_loss=2.760, train_loss_epoch=3.130]\u001b[A\n",
      "Epoch 290:  82%|█████▋ | 55/67 [00:15<00:03,  3.59it/s, loss=2.16, v_num=1, train_loss_step=0.946, val_loss=2.760, train_loss_epoch=3.130]\u001b[A\n",
      "Epoch 290:  84%|█████▊ | 56/67 [00:15<00:03,  3.57it/s, loss=2.16, v_num=1, train_loss_step=0.946, val_loss=2.760, train_loss_epoch=3.130]\u001b[A\n",
      "Epoch 290:  85%|█████▉ | 57/67 [00:15<00:02,  3.57it/s, loss=2.16, v_num=1, train_loss_step=0.946, val_loss=2.760, train_loss_epoch=3.130]\u001b[A\n",
      "Epoch 290:  87%|██████ | 58/67 [00:16<00:02,  3.56it/s, loss=2.16, v_num=1, train_loss_step=0.946, val_loss=2.760, train_loss_epoch=3.130]\u001b[A\n",
      "Epoch 290:  88%|██████▏| 59/67 [00:16<00:02,  3.55it/s, loss=2.16, v_num=1, train_loss_step=0.946, val_loss=2.760, train_loss_epoch=3.130]\u001b[A\n",
      "Epoch 290:  90%|██████▎| 60/67 [00:16<00:01,  3.54it/s, loss=2.16, v_num=1, train_loss_step=0.946, val_loss=2.760, train_loss_epoch=3.130]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 290:  91%|██████▎| 61/67 [00:17<00:01,  3.50it/s, loss=2.16, v_num=1, train_loss_step=0.946, val_loss=2.760, train_loss_epoch=3.130]\u001b[A\n",
      "Epoch 290:  93%|██████▍| 62/67 [00:17<00:01,  3.49it/s, loss=2.16, v_num=1, train_loss_step=0.946, val_loss=2.760, train_loss_epoch=3.130]\u001b[A\n",
      "Epoch 290:  94%|██████▌| 63/67 [00:18<00:01,  3.48it/s, loss=2.16, v_num=1, train_loss_step=0.946, val_loss=2.760, train_loss_epoch=3.130]\u001b[A\n",
      "Epoch 290:  96%|██████▋| 64/67 [00:18<00:00,  3.48it/s, loss=2.16, v_num=1, train_loss_step=0.946, val_loss=2.760, train_loss_epoch=3.130]\u001b[A\n",
      "Epoch 290:  97%|██████▊| 65/67 [00:18<00:00,  3.47it/s, loss=2.16, v_num=1, train_loss_step=0.946, val_loss=2.760, train_loss_epoch=3.130]\u001b[A\n",
      "Epoch 290:  99%|██████▉| 66/67 [00:19<00:00,  3.47it/s, loss=2.16, v_num=1, train_loss_step=0.946, val_loss=2.760, train_loss_epoch=3.130]\u001b[A\n",
      "Epoch 290: 100%|███████| 67/67 [00:19<00:00,  3.39it/s, loss=2.16, v_num=1, train_loss_step=0.946, val_loss=2.750, train_loss_epoch=3.130]\u001b[A\n",
      "Epoch 291:  45%|███▏   | 30/67 [00:05<00:06,  5.57it/s, loss=2.08, v_num=1, train_loss_step=5.780, val_loss=2.750, train_loss_epoch=2.010]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 291:  46%|███▏   | 31/67 [00:07<00:08,  4.22it/s, loss=2.08, v_num=1, train_loss_step=5.780, val_loss=2.750, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 291:  48%|███▎   | 32/67 [00:07<00:08,  4.18it/s, loss=2.08, v_num=1, train_loss_step=5.780, val_loss=2.750, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 291:  49%|███▍   | 33/67 [00:07<00:08,  4.15it/s, loss=2.08, v_num=1, train_loss_step=5.780, val_loss=2.750, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 291:  51%|███▌   | 34/67 [00:08<00:08,  4.11it/s, loss=2.08, v_num=1, train_loss_step=5.780, val_loss=2.750, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 291:  52%|███▋   | 35/67 [00:08<00:07,  4.09it/s, loss=2.08, v_num=1, train_loss_step=5.780, val_loss=2.750, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 291:  54%|███▊   | 36/67 [00:08<00:07,  4.05it/s, loss=2.08, v_num=1, train_loss_step=5.780, val_loss=2.750, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 291:  55%|███▊   | 37/67 [00:09<00:07,  4.01it/s, loss=2.08, v_num=1, train_loss_step=5.780, val_loss=2.750, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 291:  57%|███▉   | 38/67 [00:09<00:07,  3.99it/s, loss=2.08, v_num=1, train_loss_step=5.780, val_loss=2.750, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 291:  58%|████   | 39/67 [00:09<00:07,  3.96it/s, loss=2.08, v_num=1, train_loss_step=5.780, val_loss=2.750, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 291:  60%|████▏  | 40/67 [00:10<00:06,  3.93it/s, loss=2.08, v_num=1, train_loss_step=5.780, val_loss=2.750, train_loss_epoch=2.010]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 291:  61%|████▎  | 41/67 [00:10<00:06,  3.86it/s, loss=2.08, v_num=1, train_loss_step=5.780, val_loss=2.750, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 291:  63%|████▍  | 42/67 [00:10<00:06,  3.85it/s, loss=2.08, v_num=1, train_loss_step=5.780, val_loss=2.750, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 291:  64%|████▍  | 43/67 [00:11<00:06,  3.82it/s, loss=2.08, v_num=1, train_loss_step=5.780, val_loss=2.750, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 291:  66%|████▌  | 44/67 [00:11<00:06,  3.80it/s, loss=2.08, v_num=1, train_loss_step=5.780, val_loss=2.750, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 291:  67%|████▋  | 45/67 [00:11<00:05,  3.78it/s, loss=2.08, v_num=1, train_loss_step=5.780, val_loss=2.750, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 291:  69%|████▊  | 46/67 [00:12<00:05,  3.76it/s, loss=2.08, v_num=1, train_loss_step=5.780, val_loss=2.750, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 291:  70%|████▉  | 47/67 [00:12<00:05,  3.75it/s, loss=2.08, v_num=1, train_loss_step=5.780, val_loss=2.750, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 291:  72%|█████  | 48/67 [00:12<00:05,  3.73it/s, loss=2.08, v_num=1, train_loss_step=5.780, val_loss=2.750, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 291:  73%|█████  | 49/67 [00:13<00:04,  3.72it/s, loss=2.08, v_num=1, train_loss_step=5.780, val_loss=2.750, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 291:  75%|█████▏ | 50/67 [00:13<00:04,  3.70it/s, loss=2.08, v_num=1, train_loss_step=5.780, val_loss=2.750, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 291:  76%|█████▎ | 51/67 [00:13<00:04,  3.66it/s, loss=2.08, v_num=1, train_loss_step=5.780, val_loss=2.750, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 291:  78%|█████▍ | 52/67 [00:14<00:04,  3.64it/s, loss=2.08, v_num=1, train_loss_step=5.780, val_loss=2.750, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 291:  79%|█████▌ | 53/67 [00:14<00:03,  3.63it/s, loss=2.08, v_num=1, train_loss_step=5.780, val_loss=2.750, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 291:  81%|█████▋ | 54/67 [00:14<00:03,  3.62it/s, loss=2.08, v_num=1, train_loss_step=5.780, val_loss=2.750, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 291:  82%|█████▋ | 55/67 [00:15<00:03,  3.60it/s, loss=2.08, v_num=1, train_loss_step=5.780, val_loss=2.750, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 291:  84%|█████▊ | 56/67 [00:15<00:03,  3.59it/s, loss=2.08, v_num=1, train_loss_step=5.780, val_loss=2.750, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 291:  85%|█████▉ | 57/67 [00:15<00:02,  3.59it/s, loss=2.08, v_num=1, train_loss_step=5.780, val_loss=2.750, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 291:  87%|██████ | 58/67 [00:16<00:02,  3.58it/s, loss=2.08, v_num=1, train_loss_step=5.780, val_loss=2.750, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 291:  88%|██████▏| 59/67 [00:16<00:02,  3.57it/s, loss=2.08, v_num=1, train_loss_step=5.780, val_loss=2.750, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 291:  90%|██████▎| 60/67 [00:16<00:01,  3.56it/s, loss=2.08, v_num=1, train_loss_step=5.780, val_loss=2.750, train_loss_epoch=2.010]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 291:  91%|██████▎| 61/67 [00:17<00:01,  3.53it/s, loss=2.08, v_num=1, train_loss_step=5.780, val_loss=2.750, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 291:  93%|██████▍| 62/67 [00:17<00:01,  3.52it/s, loss=2.08, v_num=1, train_loss_step=5.780, val_loss=2.750, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 291:  94%|██████▌| 63/67 [00:17<00:01,  3.51it/s, loss=2.08, v_num=1, train_loss_step=5.780, val_loss=2.750, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 291:  96%|██████▋| 64/67 [00:18<00:00,  3.51it/s, loss=2.08, v_num=1, train_loss_step=5.780, val_loss=2.750, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 291:  97%|██████▊| 65/67 [00:18<00:00,  3.50it/s, loss=2.08, v_num=1, train_loss_step=5.780, val_loss=2.750, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 291:  99%|██████▉| 66/67 [00:18<00:00,  3.50it/s, loss=2.08, v_num=1, train_loss_step=5.780, val_loss=2.750, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 291: 100%|███████| 67/67 [00:19<00:00,  3.42it/s, loss=2.08, v_num=1, train_loss_step=5.780, val_loss=2.750, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 292:  45%|███▏   | 30/67 [00:05<00:07,  5.21it/s, loss=1.52, v_num=1, train_loss_step=0.888, val_loss=2.750, train_loss_epoch=1.980]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 292:  46%|███▏   | 31/67 [00:07<00:08,  4.02it/s, loss=1.52, v_num=1, train_loss_step=0.888, val_loss=2.750, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 292:  48%|███▎   | 32/67 [00:08<00:08,  3.99it/s, loss=1.52, v_num=1, train_loss_step=0.888, val_loss=2.750, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 292:  49%|███▍   | 33/67 [00:08<00:08,  3.96it/s, loss=1.52, v_num=1, train_loss_step=0.888, val_loss=2.750, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 292:  51%|███▌   | 34/67 [00:08<00:08,  3.94it/s, loss=1.52, v_num=1, train_loss_step=0.888, val_loss=2.750, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 292:  52%|███▋   | 35/67 [00:08<00:08,  3.91it/s, loss=1.52, v_num=1, train_loss_step=0.888, val_loss=2.750, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 292:  54%|███▊   | 36/67 [00:09<00:07,  3.88it/s, loss=1.52, v_num=1, train_loss_step=0.888, val_loss=2.750, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 292:  55%|███▊   | 37/67 [00:09<00:07,  3.86it/s, loss=1.52, v_num=1, train_loss_step=0.888, val_loss=2.750, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 292:  57%|███▉   | 38/67 [00:09<00:07,  3.85it/s, loss=1.52, v_num=1, train_loss_step=0.888, val_loss=2.750, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 292:  58%|████   | 39/67 [00:10<00:07,  3.83it/s, loss=1.52, v_num=1, train_loss_step=0.888, val_loss=2.750, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 292:  60%|████▏  | 40/67 [00:10<00:07,  3.81it/s, loss=1.52, v_num=1, train_loss_step=0.888, val_loss=2.750, train_loss_epoch=1.980]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 292:  61%|████▎  | 41/67 [00:10<00:06,  3.75it/s, loss=1.52, v_num=1, train_loss_step=0.888, val_loss=2.750, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 292:  63%|████▍  | 42/67 [00:11<00:06,  3.74it/s, loss=1.52, v_num=1, train_loss_step=0.888, val_loss=2.750, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 292:  64%|████▍  | 43/67 [00:11<00:06,  3.73it/s, loss=1.52, v_num=1, train_loss_step=0.888, val_loss=2.750, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 292:  66%|████▌  | 44/67 [00:11<00:06,  3.72it/s, loss=1.52, v_num=1, train_loss_step=0.888, val_loss=2.750, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 292:  67%|████▋  | 45/67 [00:12<00:05,  3.71it/s, loss=1.52, v_num=1, train_loss_step=0.888, val_loss=2.750, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 292:  69%|████▊  | 46/67 [00:12<00:05,  3.70it/s, loss=1.52, v_num=1, train_loss_step=0.888, val_loss=2.750, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 292:  70%|████▉  | 47/67 [00:12<00:05,  3.69it/s, loss=1.52, v_num=1, train_loss_step=0.888, val_loss=2.750, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 292:  72%|█████  | 48/67 [00:13<00:05,  3.68it/s, loss=1.52, v_num=1, train_loss_step=0.888, val_loss=2.750, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 292:  73%|█████  | 49/67 [00:13<00:04,  3.67it/s, loss=1.52, v_num=1, train_loss_step=0.888, val_loss=2.750, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 292:  75%|█████▏ | 50/67 [00:13<00:04,  3.66it/s, loss=1.52, v_num=1, train_loss_step=0.888, val_loss=2.750, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 292:  76%|█████▎ | 51/67 [00:14<00:04,  3.61it/s, loss=1.52, v_num=1, train_loss_step=0.888, val_loss=2.750, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 292:  78%|█████▍ | 52/67 [00:14<00:04,  3.61it/s, loss=1.52, v_num=1, train_loss_step=0.888, val_loss=2.750, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 292:  79%|█████▌ | 53/67 [00:14<00:03,  3.60it/s, loss=1.52, v_num=1, train_loss_step=0.888, val_loss=2.750, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 292:  81%|█████▋ | 54/67 [00:15<00:03,  3.59it/s, loss=1.52, v_num=1, train_loss_step=0.888, val_loss=2.750, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 292:  82%|█████▋ | 55/67 [00:15<00:03,  3.59it/s, loss=1.52, v_num=1, train_loss_step=0.888, val_loss=2.750, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 292:  84%|█████▊ | 56/67 [00:15<00:03,  3.58it/s, loss=1.52, v_num=1, train_loss_step=0.888, val_loss=2.750, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 292:  85%|█████▉ | 57/67 [00:15<00:02,  3.57it/s, loss=1.52, v_num=1, train_loss_step=0.888, val_loss=2.750, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 292:  87%|██████ | 58/67 [00:16<00:02,  3.57it/s, loss=1.52, v_num=1, train_loss_step=0.888, val_loss=2.750, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 292:  88%|██████▏| 59/67 [00:16<00:02,  3.56it/s, loss=1.52, v_num=1, train_loss_step=0.888, val_loss=2.750, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 292:  90%|██████▎| 60/67 [00:16<00:01,  3.56it/s, loss=1.52, v_num=1, train_loss_step=0.888, val_loss=2.750, train_loss_epoch=1.980]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 292:  91%|██████▎| 61/67 [00:17<00:01,  3.52it/s, loss=1.52, v_num=1, train_loss_step=0.888, val_loss=2.750, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 292:  93%|██████▍| 62/67 [00:17<00:01,  3.52it/s, loss=1.52, v_num=1, train_loss_step=0.888, val_loss=2.750, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 292:  94%|██████▌| 63/67 [00:17<00:01,  3.52it/s, loss=1.52, v_num=1, train_loss_step=0.888, val_loss=2.750, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 292:  96%|██████▋| 64/67 [00:18<00:00,  3.51it/s, loss=1.52, v_num=1, train_loss_step=0.888, val_loss=2.750, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 292:  97%|██████▊| 65/67 [00:18<00:00,  3.51it/s, loss=1.52, v_num=1, train_loss_step=0.888, val_loss=2.750, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 292:  99%|██████▉| 66/67 [00:18<00:00,  3.51it/s, loss=1.52, v_num=1, train_loss_step=0.888, val_loss=2.750, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 292: 100%|███████| 67/67 [00:19<00:00,  3.44it/s, loss=1.52, v_num=1, train_loss_step=0.888, val_loss=2.750, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 293:  45%|███▏   | 30/67 [00:05<00:06,  5.55it/s, loss=2.27, v_num=1, train_loss_step=2.260, val_loss=2.750, train_loss_epoch=1.990]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 293:  46%|███▏   | 31/67 [00:07<00:08,  4.21it/s, loss=2.27, v_num=1, train_loss_step=2.260, val_loss=2.750, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 293:  48%|███▎   | 32/67 [00:07<00:08,  4.17it/s, loss=2.27, v_num=1, train_loss_step=2.260, val_loss=2.750, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 293:  49%|███▍   | 33/67 [00:07<00:08,  4.13it/s, loss=2.27, v_num=1, train_loss_step=2.260, val_loss=2.750, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 293:  51%|███▌   | 34/67 [00:08<00:08,  4.10it/s, loss=2.27, v_num=1, train_loss_step=2.260, val_loss=2.750, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 293:  52%|███▋   | 35/67 [00:08<00:07,  4.07it/s, loss=2.27, v_num=1, train_loss_step=2.260, val_loss=2.750, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 293:  54%|███▊   | 36/67 [00:08<00:07,  4.04it/s, loss=2.27, v_num=1, train_loss_step=2.260, val_loss=2.750, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 293:  55%|███▊   | 37/67 [00:09<00:07,  4.01it/s, loss=2.27, v_num=1, train_loss_step=2.260, val_loss=2.750, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 293:  57%|███▉   | 38/67 [00:09<00:07,  3.98it/s, loss=2.27, v_num=1, train_loss_step=2.260, val_loss=2.750, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 293:  58%|████   | 39/67 [00:09<00:07,  3.94it/s, loss=2.27, v_num=1, train_loss_step=2.260, val_loss=2.750, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 293:  60%|████▏  | 40/67 [00:10<00:06,  3.92it/s, loss=2.27, v_num=1, train_loss_step=2.260, val_loss=2.750, train_loss_epoch=1.990]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 293:  61%|████▎  | 41/67 [00:10<00:06,  3.85it/s, loss=2.27, v_num=1, train_loss_step=2.260, val_loss=2.750, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 293:  63%|████▍  | 42/67 [00:10<00:06,  3.83it/s, loss=2.27, v_num=1, train_loss_step=2.260, val_loss=2.750, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 293:  64%|████▍  | 43/67 [00:11<00:06,  3.81it/s, loss=2.27, v_num=1, train_loss_step=2.260, val_loss=2.750, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 293:  66%|████▌  | 44/67 [00:11<00:06,  3.80it/s, loss=2.27, v_num=1, train_loss_step=2.260, val_loss=2.750, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 293:  67%|████▋  | 45/67 [00:11<00:05,  3.78it/s, loss=2.27, v_num=1, train_loss_step=2.260, val_loss=2.750, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 293:  69%|████▊  | 46/67 [00:12<00:05,  3.77it/s, loss=2.27, v_num=1, train_loss_step=2.260, val_loss=2.750, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 293:  70%|████▉  | 47/67 [00:12<00:05,  3.75it/s, loss=2.27, v_num=1, train_loss_step=2.260, val_loss=2.750, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 293:  72%|█████  | 48/67 [00:12<00:05,  3.74it/s, loss=2.27, v_num=1, train_loss_step=2.260, val_loss=2.750, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 293:  73%|█████  | 49/67 [00:13<00:04,  3.71it/s, loss=2.27, v_num=1, train_loss_step=2.260, val_loss=2.750, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 293:  75%|█████▏ | 50/67 [00:13<00:04,  3.70it/s, loss=2.27, v_num=1, train_loss_step=2.260, val_loss=2.750, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 293:  76%|█████▎ | 51/67 [00:14<00:04,  3.64it/s, loss=2.27, v_num=1, train_loss_step=2.260, val_loss=2.750, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 293:  78%|█████▍ | 52/67 [00:14<00:04,  3.62it/s, loss=2.27, v_num=1, train_loss_step=2.260, val_loss=2.750, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 293:  79%|█████▌ | 53/67 [00:14<00:03,  3.60it/s, loss=2.27, v_num=1, train_loss_step=2.260, val_loss=2.750, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 293:  81%|█████▋ | 54/67 [00:15<00:03,  3.59it/s, loss=2.27, v_num=1, train_loss_step=2.260, val_loss=2.750, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 293:  82%|█████▋ | 55/67 [00:15<00:03,  3.57it/s, loss=2.27, v_num=1, train_loss_step=2.260, val_loss=2.750, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 293:  84%|█████▊ | 56/67 [00:15<00:03,  3.56it/s, loss=2.27, v_num=1, train_loss_step=2.260, val_loss=2.750, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 293:  85%|█████▉ | 57/67 [00:16<00:02,  3.56it/s, loss=2.27, v_num=1, train_loss_step=2.260, val_loss=2.750, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 293:  87%|██████ | 58/67 [00:16<00:02,  3.55it/s, loss=2.27, v_num=1, train_loss_step=2.260, val_loss=2.750, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 293:  88%|██████▏| 59/67 [00:16<00:02,  3.54it/s, loss=2.27, v_num=1, train_loss_step=2.260, val_loss=2.750, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 293:  90%|██████▎| 60/67 [00:16<00:01,  3.54it/s, loss=2.27, v_num=1, train_loss_step=2.260, val_loss=2.750, train_loss_epoch=1.990]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 293:  91%|██████▎| 61/67 [00:17<00:01,  3.51it/s, loss=2.27, v_num=1, train_loss_step=2.260, val_loss=2.750, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 293:  93%|██████▍| 62/67 [00:17<00:01,  3.51it/s, loss=2.27, v_num=1, train_loss_step=2.260, val_loss=2.750, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 293:  94%|██████▌| 63/67 [00:17<00:01,  3.50it/s, loss=2.27, v_num=1, train_loss_step=2.260, val_loss=2.750, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 293:  96%|██████▋| 64/67 [00:18<00:00,  3.50it/s, loss=2.27, v_num=1, train_loss_step=2.260, val_loss=2.750, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 293:  97%|██████▊| 65/67 [00:18<00:00,  3.50it/s, loss=2.27, v_num=1, train_loss_step=2.260, val_loss=2.750, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 293:  99%|██████▉| 66/67 [00:18<00:00,  3.49it/s, loss=2.27, v_num=1, train_loss_step=2.260, val_loss=2.750, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 293: 100%|███████| 67/67 [00:19<00:00,  3.41it/s, loss=2.27, v_num=1, train_loss_step=2.260, val_loss=2.750, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 294:  45%|███▏   | 30/67 [00:05<00:06,  5.54it/s, loss=2.04, v_num=1, train_loss_step=1.360, val_loss=2.750, train_loss_epoch=2.200]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 294:  46%|███▏   | 31/67 [00:07<00:08,  4.14it/s, loss=2.04, v_num=1, train_loss_step=1.360, val_loss=2.750, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 294:  48%|███▎   | 32/67 [00:07<00:08,  4.08it/s, loss=2.04, v_num=1, train_loss_step=1.360, val_loss=2.750, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 294:  49%|███▍   | 33/67 [00:08<00:08,  4.04it/s, loss=2.04, v_num=1, train_loss_step=1.360, val_loss=2.750, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 294:  51%|███▌   | 34/67 [00:08<00:08,  4.00it/s, loss=2.04, v_num=1, train_loss_step=1.360, val_loss=2.750, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 294:  52%|███▋   | 35/67 [00:08<00:08,  3.94it/s, loss=2.04, v_num=1, train_loss_step=1.360, val_loss=2.750, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 294:  54%|███▊   | 36/67 [00:09<00:07,  3.91it/s, loss=2.04, v_num=1, train_loss_step=1.360, val_loss=2.750, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 294:  55%|███▊   | 37/67 [00:09<00:07,  3.87it/s, loss=2.04, v_num=1, train_loss_step=1.360, val_loss=2.750, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 294:  57%|███▉   | 38/67 [00:09<00:07,  3.84it/s, loss=2.04, v_num=1, train_loss_step=1.360, val_loss=2.750, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 294:  58%|████   | 39/67 [00:10<00:07,  3.81it/s, loss=2.04, v_num=1, train_loss_step=1.360, val_loss=2.750, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 294:  60%|████▏  | 40/67 [00:10<00:07,  3.79it/s, loss=2.04, v_num=1, train_loss_step=1.360, val_loss=2.750, train_loss_epoch=2.200]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 294:  61%|████▎  | 41/67 [00:11<00:06,  3.72it/s, loss=2.04, v_num=1, train_loss_step=1.360, val_loss=2.750, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 294:  63%|████▍  | 42/67 [00:11<00:06,  3.70it/s, loss=2.04, v_num=1, train_loss_step=1.360, val_loss=2.750, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 294:  64%|████▍  | 43/67 [00:11<00:06,  3.68it/s, loss=2.04, v_num=1, train_loss_step=1.360, val_loss=2.750, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 294:  66%|████▌  | 44/67 [00:12<00:06,  3.65it/s, loss=2.04, v_num=1, train_loss_step=1.360, val_loss=2.750, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 294:  67%|████▋  | 45/67 [00:12<00:06,  3.64it/s, loss=2.04, v_num=1, train_loss_step=1.360, val_loss=2.750, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 294:  69%|████▊  | 46/67 [00:12<00:05,  3.62it/s, loss=2.04, v_num=1, train_loss_step=1.360, val_loss=2.750, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 294:  70%|████▉  | 47/67 [00:13<00:05,  3.60it/s, loss=2.04, v_num=1, train_loss_step=1.360, val_loss=2.750, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 294:  72%|█████  | 48/67 [00:13<00:05,  3.58it/s, loss=2.04, v_num=1, train_loss_step=1.360, val_loss=2.750, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 294:  73%|█████  | 49/67 [00:13<00:05,  3.56it/s, loss=2.04, v_num=1, train_loss_step=1.360, val_loss=2.750, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 294:  75%|█████▏ | 50/67 [00:14<00:04,  3.55it/s, loss=2.04, v_num=1, train_loss_step=1.360, val_loss=2.750, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 294:  76%|█████▎ | 51/67 [00:14<00:04,  3.50it/s, loss=2.04, v_num=1, train_loss_step=1.360, val_loss=2.750, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 294:  78%|█████▍ | 52/67 [00:14<00:04,  3.49it/s, loss=2.04, v_num=1, train_loss_step=1.360, val_loss=2.750, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 294:  79%|█████▌ | 53/67 [00:15<00:04,  3.48it/s, loss=2.04, v_num=1, train_loss_step=1.360, val_loss=2.750, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 294:  81%|█████▋ | 54/67 [00:15<00:03,  3.47it/s, loss=2.04, v_num=1, train_loss_step=1.360, val_loss=2.750, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 294:  82%|█████▋ | 55/67 [00:15<00:03,  3.46it/s, loss=2.04, v_num=1, train_loss_step=1.360, val_loss=2.750, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 294:  84%|█████▊ | 56/67 [00:16<00:03,  3.45it/s, loss=2.04, v_num=1, train_loss_step=1.360, val_loss=2.750, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 294:  85%|█████▉ | 57/67 [00:16<00:02,  3.44it/s, loss=2.04, v_num=1, train_loss_step=1.360, val_loss=2.750, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 294:  87%|██████ | 58/67 [00:16<00:02,  3.44it/s, loss=2.04, v_num=1, train_loss_step=1.360, val_loss=2.750, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 294:  88%|██████▏| 59/67 [00:17<00:02,  3.43it/s, loss=2.04, v_num=1, train_loss_step=1.360, val_loss=2.750, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 294:  90%|██████▎| 60/67 [00:17<00:02,  3.41it/s, loss=2.04, v_num=1, train_loss_step=1.360, val_loss=2.750, train_loss_epoch=2.200]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 294:  91%|██████▎| 61/67 [00:18<00:01,  3.38it/s, loss=2.04, v_num=1, train_loss_step=1.360, val_loss=2.750, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 294:  93%|██████▍| 62/67 [00:18<00:01,  3.37it/s, loss=2.04, v_num=1, train_loss_step=1.360, val_loss=2.750, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 294:  94%|██████▌| 63/67 [00:18<00:01,  3.37it/s, loss=2.04, v_num=1, train_loss_step=1.360, val_loss=2.750, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 294:  96%|██████▋| 64/67 [00:19<00:00,  3.36it/s, loss=2.04, v_num=1, train_loss_step=1.360, val_loss=2.750, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 294:  97%|██████▊| 65/67 [00:19<00:00,  3.36it/s, loss=2.04, v_num=1, train_loss_step=1.360, val_loss=2.750, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 294:  99%|██████▉| 66/67 [00:19<00:00,  3.35it/s, loss=2.04, v_num=1, train_loss_step=1.360, val_loss=2.750, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 294: 100%|███████| 67/67 [00:20<00:00,  3.28it/s, loss=2.04, v_num=1, train_loss_step=1.360, val_loss=2.750, train_loss_epoch=2.200]\u001b[A\n",
      "Epoch 295:  45%|███▏   | 30/67 [00:05<00:07,  5.26it/s, loss=2.01, v_num=1, train_loss_step=1.280, val_loss=2.750, train_loss_epoch=2.040]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 295:  46%|███▏   | 31/67 [00:07<00:08,  4.01it/s, loss=2.01, v_num=1, train_loss_step=1.280, val_loss=2.750, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 295:  48%|███▎   | 32/67 [00:08<00:08,  3.98it/s, loss=2.01, v_num=1, train_loss_step=1.280, val_loss=2.750, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 295:  49%|███▍   | 33/67 [00:08<00:08,  3.94it/s, loss=2.01, v_num=1, train_loss_step=1.280, val_loss=2.750, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 295:  51%|███▌   | 34/67 [00:08<00:08,  3.91it/s, loss=2.01, v_num=1, train_loss_step=1.280, val_loss=2.750, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 295:  52%|███▋   | 35/67 [00:09<00:08,  3.88it/s, loss=2.01, v_num=1, train_loss_step=1.280, val_loss=2.750, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 295:  54%|███▊   | 36/67 [00:09<00:08,  3.85it/s, loss=2.01, v_num=1, train_loss_step=1.280, val_loss=2.750, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 295:  55%|███▊   | 37/67 [00:09<00:07,  3.82it/s, loss=2.01, v_num=1, train_loss_step=1.280, val_loss=2.750, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 295:  57%|███▉   | 38/67 [00:10<00:07,  3.79it/s, loss=2.01, v_num=1, train_loss_step=1.280, val_loss=2.750, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 295:  58%|████   | 39/67 [00:10<00:07,  3.77it/s, loss=2.01, v_num=1, train_loss_step=1.280, val_loss=2.750, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 295:  60%|████▏  | 40/67 [00:10<00:07,  3.74it/s, loss=2.01, v_num=1, train_loss_step=1.280, val_loss=2.750, train_loss_epoch=2.040]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 295:  61%|████▎  | 41/67 [00:11<00:07,  3.67it/s, loss=2.01, v_num=1, train_loss_step=1.280, val_loss=2.750, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 295:  63%|████▍  | 42/67 [00:11<00:06,  3.65it/s, loss=2.01, v_num=1, train_loss_step=1.280, val_loss=2.750, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 295:  64%|████▍  | 43/67 [00:11<00:06,  3.64it/s, loss=2.01, v_num=1, train_loss_step=1.280, val_loss=2.750, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 295:  66%|████▌  | 44/67 [00:12<00:06,  3.62it/s, loss=2.01, v_num=1, train_loss_step=1.280, val_loss=2.750, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 295:  67%|████▋  | 45/67 [00:12<00:06,  3.61it/s, loss=2.01, v_num=1, train_loss_step=1.280, val_loss=2.750, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 295:  69%|████▊  | 46/67 [00:12<00:05,  3.59it/s, loss=2.01, v_num=1, train_loss_step=1.280, val_loss=2.750, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 295:  70%|████▉  | 47/67 [00:13<00:05,  3.58it/s, loss=2.01, v_num=1, train_loss_step=1.280, val_loss=2.750, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 295:  72%|█████  | 48/67 [00:13<00:05,  3.56it/s, loss=2.01, v_num=1, train_loss_step=1.280, val_loss=2.750, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 295:  73%|█████  | 49/67 [00:13<00:05,  3.55it/s, loss=2.01, v_num=1, train_loss_step=1.280, val_loss=2.750, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 295:  75%|█████▏ | 50/67 [00:14<00:04,  3.54it/s, loss=2.01, v_num=1, train_loss_step=1.280, val_loss=2.750, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 295:  76%|█████▎ | 51/67 [00:14<00:04,  3.49it/s, loss=2.01, v_num=1, train_loss_step=1.280, val_loss=2.750, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 295:  78%|█████▍ | 52/67 [00:14<00:04,  3.47it/s, loss=2.01, v_num=1, train_loss_step=1.280, val_loss=2.750, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 295:  79%|█████▌ | 53/67 [00:15<00:04,  3.47it/s, loss=2.01, v_num=1, train_loss_step=1.280, val_loss=2.750, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 295:  81%|█████▋ | 54/67 [00:15<00:03,  3.46it/s, loss=2.01, v_num=1, train_loss_step=1.280, val_loss=2.750, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 295:  82%|█████▋ | 55/67 [00:15<00:03,  3.45it/s, loss=2.01, v_num=1, train_loss_step=1.280, val_loss=2.750, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 295:  84%|█████▊ | 56/67 [00:16<00:03,  3.44it/s, loss=2.01, v_num=1, train_loss_step=1.280, val_loss=2.750, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 295:  85%|█████▉ | 57/67 [00:16<00:02,  3.43it/s, loss=2.01, v_num=1, train_loss_step=1.280, val_loss=2.750, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 295:  87%|██████ | 58/67 [00:16<00:02,  3.43it/s, loss=2.01, v_num=1, train_loss_step=1.280, val_loss=2.750, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 295:  88%|██████▏| 59/67 [00:17<00:02,  3.42it/s, loss=2.01, v_num=1, train_loss_step=1.280, val_loss=2.750, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 295:  90%|██████▎| 60/67 [00:17<00:02,  3.41it/s, loss=2.01, v_num=1, train_loss_step=1.280, val_loss=2.750, train_loss_epoch=2.040]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 295:  91%|██████▎| 61/67 [00:18<00:01,  3.38it/s, loss=2.01, v_num=1, train_loss_step=1.280, val_loss=2.750, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 295:  93%|██████▍| 62/67 [00:18<00:01,  3.38it/s, loss=2.01, v_num=1, train_loss_step=1.280, val_loss=2.750, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 295:  94%|██████▌| 63/67 [00:18<00:01,  3.37it/s, loss=2.01, v_num=1, train_loss_step=1.280, val_loss=2.750, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 295:  96%|██████▋| 64/67 [00:18<00:00,  3.37it/s, loss=2.01, v_num=1, train_loss_step=1.280, val_loss=2.750, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 295:  97%|██████▊| 65/67 [00:19<00:00,  3.37it/s, loss=2.01, v_num=1, train_loss_step=1.280, val_loss=2.750, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 295:  99%|██████▉| 66/67 [00:19<00:00,  3.36it/s, loss=2.01, v_num=1, train_loss_step=1.280, val_loss=2.750, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 295: 100%|███████| 67/67 [00:20<00:00,  3.29it/s, loss=2.01, v_num=1, train_loss_step=1.280, val_loss=2.750, train_loss_epoch=2.040]\u001b[A\n",
      "Epoch 296:  45%|███▏   | 30/67 [00:05<00:07,  5.25it/s, loss=1.67, v_num=1, train_loss_step=0.327, val_loss=2.750, train_loss_epoch=2.110]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 296:  46%|███▏   | 31/67 [00:07<00:09,  3.98it/s, loss=1.67, v_num=1, train_loss_step=0.327, val_loss=2.750, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 296:  48%|███▎   | 32/67 [00:08<00:08,  3.94it/s, loss=1.67, v_num=1, train_loss_step=0.327, val_loss=2.750, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 296:  49%|███▍   | 33/67 [00:08<00:08,  3.91it/s, loss=1.67, v_num=1, train_loss_step=0.327, val_loss=2.750, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 296:  51%|███▌   | 34/67 [00:08<00:08,  3.88it/s, loss=1.67, v_num=1, train_loss_step=0.327, val_loss=2.750, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 296:  52%|███▋   | 35/67 [00:09<00:08,  3.85it/s, loss=1.67, v_num=1, train_loss_step=0.327, val_loss=2.750, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 296:  54%|███▊   | 36/67 [00:09<00:08,  3.82it/s, loss=1.67, v_num=1, train_loss_step=0.327, val_loss=2.750, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 296:  55%|███▊   | 37/67 [00:09<00:07,  3.80it/s, loss=1.67, v_num=1, train_loss_step=0.327, val_loss=2.750, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 296:  57%|███▉   | 38/67 [00:10<00:07,  3.77it/s, loss=1.67, v_num=1, train_loss_step=0.327, val_loss=2.750, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 296:  58%|████   | 39/67 [00:10<00:07,  3.75it/s, loss=1.67, v_num=1, train_loss_step=0.327, val_loss=2.750, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 296:  60%|████▏  | 40/67 [00:10<00:07,  3.72it/s, loss=1.67, v_num=1, train_loss_step=0.327, val_loss=2.750, train_loss_epoch=2.110]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 296:  61%|████▎  | 41/67 [00:11<00:07,  3.66it/s, loss=1.67, v_num=1, train_loss_step=0.327, val_loss=2.750, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 296:  63%|████▍  | 42/67 [00:11<00:06,  3.64it/s, loss=1.67, v_num=1, train_loss_step=0.327, val_loss=2.750, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 296:  64%|████▍  | 43/67 [00:11<00:06,  3.62it/s, loss=1.67, v_num=1, train_loss_step=0.327, val_loss=2.750, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 296:  66%|████▌  | 44/67 [00:12<00:06,  3.61it/s, loss=1.67, v_num=1, train_loss_step=0.327, val_loss=2.750, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 296:  67%|████▋  | 45/67 [00:12<00:06,  3.59it/s, loss=1.67, v_num=1, train_loss_step=0.327, val_loss=2.750, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 296:  69%|████▊  | 46/67 [00:12<00:05,  3.58it/s, loss=1.67, v_num=1, train_loss_step=0.327, val_loss=2.750, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 296:  70%|████▉  | 47/67 [00:13<00:05,  3.56it/s, loss=1.67, v_num=1, train_loss_step=0.327, val_loss=2.750, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 296:  72%|█████  | 48/67 [00:13<00:05,  3.55it/s, loss=1.67, v_num=1, train_loss_step=0.327, val_loss=2.750, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 296:  73%|█████  | 49/67 [00:13<00:05,  3.54it/s, loss=1.67, v_num=1, train_loss_step=0.327, val_loss=2.750, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 296:  75%|█████▏ | 50/67 [00:14<00:04,  3.53it/s, loss=1.67, v_num=1, train_loss_step=0.327, val_loss=2.750, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 296:  76%|█████▎ | 51/67 [00:14<00:04,  3.48it/s, loss=1.67, v_num=1, train_loss_step=0.327, val_loss=2.750, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 296:  78%|█████▍ | 52/67 [00:14<00:04,  3.47it/s, loss=1.67, v_num=1, train_loss_step=0.327, val_loss=2.750, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 296:  79%|█████▌ | 53/67 [00:15<00:04,  3.46it/s, loss=1.67, v_num=1, train_loss_step=0.327, val_loss=2.750, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 296:  81%|█████▋ | 54/67 [00:15<00:03,  3.45it/s, loss=1.67, v_num=1, train_loss_step=0.327, val_loss=2.750, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 296:  82%|█████▋ | 55/67 [00:15<00:03,  3.44it/s, loss=1.67, v_num=1, train_loss_step=0.327, val_loss=2.750, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 296:  84%|█████▊ | 56/67 [00:16<00:03,  3.43it/s, loss=1.67, v_num=1, train_loss_step=0.327, val_loss=2.750, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 296:  85%|█████▉ | 57/67 [00:16<00:02,  3.43it/s, loss=1.67, v_num=1, train_loss_step=0.327, val_loss=2.750, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 296:  87%|██████ | 58/67 [00:16<00:02,  3.42it/s, loss=1.67, v_num=1, train_loss_step=0.327, val_loss=2.750, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 296:  88%|██████▏| 59/67 [00:17<00:02,  3.41it/s, loss=1.67, v_num=1, train_loss_step=0.327, val_loss=2.750, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 296:  90%|██████▎| 60/67 [00:17<00:02,  3.40it/s, loss=1.67, v_num=1, train_loss_step=0.327, val_loss=2.750, train_loss_epoch=2.110]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 296:  91%|██████▎| 61/67 [00:18<00:01,  3.37it/s, loss=1.67, v_num=1, train_loss_step=0.327, val_loss=2.750, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 296:  93%|██████▍| 62/67 [00:18<00:01,  3.37it/s, loss=1.67, v_num=1, train_loss_step=0.327, val_loss=2.750, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 296:  94%|██████▌| 63/67 [00:18<00:01,  3.36it/s, loss=1.67, v_num=1, train_loss_step=0.327, val_loss=2.750, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 296:  96%|██████▋| 64/67 [00:19<00:00,  3.36it/s, loss=1.67, v_num=1, train_loss_step=0.327, val_loss=2.750, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 296:  97%|██████▊| 65/67 [00:19<00:00,  3.36it/s, loss=1.67, v_num=1, train_loss_step=0.327, val_loss=2.750, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 296:  99%|██████▉| 66/67 [00:19<00:00,  3.35it/s, loss=1.67, v_num=1, train_loss_step=0.327, val_loss=2.750, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 296: 100%|███████| 67/67 [00:20<00:00,  3.28it/s, loss=1.67, v_num=1, train_loss_step=0.327, val_loss=2.750, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 297:  45%|███▌    | 30/67 [00:05<00:06,  5.31it/s, loss=1.6, v_num=1, train_loss_step=1.230, val_loss=2.750, train_loss_epoch=1.910]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 297:  46%|███▋    | 31/67 [00:07<00:08,  4.03it/s, loss=1.6, v_num=1, train_loss_step=1.230, val_loss=2.750, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 297:  48%|███▊    | 32/67 [00:08<00:08,  3.99it/s, loss=1.6, v_num=1, train_loss_step=1.230, val_loss=2.750, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 297:  49%|███▉    | 33/67 [00:08<00:08,  3.96it/s, loss=1.6, v_num=1, train_loss_step=1.230, val_loss=2.750, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 297:  51%|████    | 34/67 [00:08<00:08,  3.92it/s, loss=1.6, v_num=1, train_loss_step=1.230, val_loss=2.750, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 297:  52%|████▏   | 35/67 [00:08<00:08,  3.89it/s, loss=1.6, v_num=1, train_loss_step=1.230, val_loss=2.750, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 297:  54%|████▎   | 36/67 [00:09<00:08,  3.86it/s, loss=1.6, v_num=1, train_loss_step=1.230, val_loss=2.750, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 297:  55%|████▍   | 37/67 [00:09<00:07,  3.83it/s, loss=1.6, v_num=1, train_loss_step=1.230, val_loss=2.750, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 297:  57%|████▌   | 38/67 [00:09<00:07,  3.81it/s, loss=1.6, v_num=1, train_loss_step=1.230, val_loss=2.750, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 297:  58%|████▋   | 39/67 [00:10<00:07,  3.79it/s, loss=1.6, v_num=1, train_loss_step=1.230, val_loss=2.750, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 297:  60%|████▊   | 40/67 [00:10<00:07,  3.76it/s, loss=1.6, v_num=1, train_loss_step=1.230, val_loss=2.750, train_loss_epoch=1.910]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 297:  61%|████▉   | 41/67 [00:11<00:07,  3.70it/s, loss=1.6, v_num=1, train_loss_step=1.230, val_loss=2.750, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 297:  63%|█████   | 42/67 [00:11<00:06,  3.68it/s, loss=1.6, v_num=1, train_loss_step=1.230, val_loss=2.750, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 297:  64%|█████▏  | 43/67 [00:11<00:06,  3.65it/s, loss=1.6, v_num=1, train_loss_step=1.230, val_loss=2.750, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 297:  66%|█████▎  | 44/67 [00:12<00:06,  3.63it/s, loss=1.6, v_num=1, train_loss_step=1.230, val_loss=2.750, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 297:  67%|█████▎  | 45/67 [00:12<00:06,  3.62it/s, loss=1.6, v_num=1, train_loss_step=1.230, val_loss=2.750, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 297:  69%|█████▍  | 46/67 [00:12<00:05,  3.61it/s, loss=1.6, v_num=1, train_loss_step=1.230, val_loss=2.750, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 297:  70%|█████▌  | 47/67 [00:13<00:05,  3.59it/s, loss=1.6, v_num=1, train_loss_step=1.230, val_loss=2.750, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 297:  72%|█████▋  | 48/67 [00:13<00:05,  3.58it/s, loss=1.6, v_num=1, train_loss_step=1.230, val_loss=2.750, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 297:  73%|█████▊  | 49/67 [00:13<00:05,  3.57it/s, loss=1.6, v_num=1, train_loss_step=1.230, val_loss=2.750, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 297:  75%|█████▉  | 50/67 [00:14<00:04,  3.55it/s, loss=1.6, v_num=1, train_loss_step=1.230, val_loss=2.750, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 297:  76%|██████  | 51/67 [00:14<00:04,  3.51it/s, loss=1.6, v_num=1, train_loss_step=1.230, val_loss=2.750, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 297:  78%|██████▏ | 52/67 [00:14<00:04,  3.50it/s, loss=1.6, v_num=1, train_loss_step=1.230, val_loss=2.750, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 297:  79%|██████▎ | 53/67 [00:15<00:04,  3.48it/s, loss=1.6, v_num=1, train_loss_step=1.230, val_loss=2.750, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 297:  81%|██████▍ | 54/67 [00:15<00:03,  3.47it/s, loss=1.6, v_num=1, train_loss_step=1.230, val_loss=2.750, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 297:  82%|██████▌ | 55/67 [00:15<00:03,  3.46it/s, loss=1.6, v_num=1, train_loss_step=1.230, val_loss=2.750, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 297:  84%|██████▋ | 56/67 [00:16<00:03,  3.45it/s, loss=1.6, v_num=1, train_loss_step=1.230, val_loss=2.750, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 297:  85%|██████▊ | 57/67 [00:16<00:02,  3.45it/s, loss=1.6, v_num=1, train_loss_step=1.230, val_loss=2.750, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 297:  87%|██████▉ | 58/67 [00:16<00:02,  3.44it/s, loss=1.6, v_num=1, train_loss_step=1.230, val_loss=2.750, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 297:  88%|███████ | 59/67 [00:17<00:02,  3.43it/s, loss=1.6, v_num=1, train_loss_step=1.230, val_loss=2.750, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 297:  90%|███████▏| 60/67 [00:17<00:02,  3.42it/s, loss=1.6, v_num=1, train_loss_step=1.230, val_loss=2.750, train_loss_epoch=1.910]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 297:  91%|███████▎| 61/67 [00:18<00:01,  3.39it/s, loss=1.6, v_num=1, train_loss_step=1.230, val_loss=2.750, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 297:  93%|███████▍| 62/67 [00:18<00:01,  3.38it/s, loss=1.6, v_num=1, train_loss_step=1.230, val_loss=2.750, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 297:  94%|███████▌| 63/67 [00:18<00:01,  3.38it/s, loss=1.6, v_num=1, train_loss_step=1.230, val_loss=2.750, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 297:  96%|███████▋| 64/67 [00:18<00:00,  3.37it/s, loss=1.6, v_num=1, train_loss_step=1.230, val_loss=2.750, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 297:  97%|███████▊| 65/67 [00:19<00:00,  3.37it/s, loss=1.6, v_num=1, train_loss_step=1.230, val_loss=2.750, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 297:  99%|███████▉| 66/67 [00:19<00:00,  3.36it/s, loss=1.6, v_num=1, train_loss_step=1.230, val_loss=2.750, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 297: 100%|████████| 67/67 [00:20<00:00,  3.29it/s, loss=1.6, v_num=1, train_loss_step=1.230, val_loss=2.750, train_loss_epoch=1.910]\u001b[A\n",
      "Epoch 298:  45%|███▏   | 30/67 [00:05<00:06,  5.33it/s, loss=2.19, v_num=1, train_loss_step=5.290, val_loss=2.750, train_loss_epoch=1.740]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 298:  46%|███▏   | 31/67 [00:07<00:08,  4.06it/s, loss=2.19, v_num=1, train_loss_step=5.290, val_loss=2.750, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 298:  48%|███▎   | 32/67 [00:07<00:08,  4.02it/s, loss=2.19, v_num=1, train_loss_step=5.290, val_loss=2.750, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 298:  49%|███▍   | 33/67 [00:08<00:08,  3.98it/s, loss=2.19, v_num=1, train_loss_step=5.290, val_loss=2.750, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 298:  51%|███▌   | 34/67 [00:08<00:08,  3.94it/s, loss=2.19, v_num=1, train_loss_step=5.290, val_loss=2.750, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 298:  52%|███▋   | 35/67 [00:08<00:08,  3.91it/s, loss=2.19, v_num=1, train_loss_step=5.290, val_loss=2.750, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 298:  54%|███▊   | 36/67 [00:09<00:07,  3.88it/s, loss=2.19, v_num=1, train_loss_step=5.290, val_loss=2.750, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 298:  55%|███▊   | 37/67 [00:09<00:07,  3.85it/s, loss=2.19, v_num=1, train_loss_step=5.290, val_loss=2.750, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 298:  57%|███▉   | 38/67 [00:09<00:07,  3.82it/s, loss=2.19, v_num=1, train_loss_step=5.290, val_loss=2.750, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 298:  58%|████   | 39/67 [00:10<00:07,  3.80it/s, loss=2.19, v_num=1, train_loss_step=5.290, val_loss=2.750, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 298:  60%|████▏  | 40/67 [00:10<00:07,  3.78it/s, loss=2.19, v_num=1, train_loss_step=5.290, val_loss=2.750, train_loss_epoch=1.740]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 298:  61%|████▎  | 41/67 [00:11<00:07,  3.71it/s, loss=2.19, v_num=1, train_loss_step=5.290, val_loss=2.750, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 298:  63%|████▍  | 42/67 [00:11<00:06,  3.69it/s, loss=2.19, v_num=1, train_loss_step=5.290, val_loss=2.750, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 298:  64%|████▍  | 43/67 [00:11<00:06,  3.67it/s, loss=2.19, v_num=1, train_loss_step=5.290, val_loss=2.750, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 298:  66%|████▌  | 44/67 [00:12<00:06,  3.66it/s, loss=2.19, v_num=1, train_loss_step=5.290, val_loss=2.750, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 298:  67%|████▋  | 45/67 [00:12<00:06,  3.64it/s, loss=2.19, v_num=1, train_loss_step=5.290, val_loss=2.750, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 298:  69%|████▊  | 46/67 [00:12<00:05,  3.63it/s, loss=2.19, v_num=1, train_loss_step=5.290, val_loss=2.750, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 298:  70%|████▉  | 47/67 [00:13<00:05,  3.61it/s, loss=2.19, v_num=1, train_loss_step=5.290, val_loss=2.750, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 298:  72%|█████  | 48/67 [00:13<00:05,  3.60it/s, loss=2.19, v_num=1, train_loss_step=5.290, val_loss=2.750, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 298:  73%|█████  | 49/67 [00:13<00:05,  3.59it/s, loss=2.19, v_num=1, train_loss_step=5.290, val_loss=2.750, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 298:  75%|█████▏ | 50/67 [00:13<00:04,  3.57it/s, loss=2.19, v_num=1, train_loss_step=5.290, val_loss=2.750, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 298:  76%|█████▎ | 51/67 [00:14<00:04,  3.53it/s, loss=2.19, v_num=1, train_loss_step=5.290, val_loss=2.750, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 298:  78%|█████▍ | 52/67 [00:14<00:04,  3.52it/s, loss=2.19, v_num=1, train_loss_step=5.290, val_loss=2.750, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 298:  79%|█████▌ | 53/67 [00:15<00:03,  3.51it/s, loss=2.19, v_num=1, train_loss_step=5.290, val_loss=2.750, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 298:  81%|█████▋ | 54/67 [00:15<00:03,  3.50it/s, loss=2.19, v_num=1, train_loss_step=5.290, val_loss=2.750, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 298:  82%|█████▋ | 55/67 [00:15<00:03,  3.49it/s, loss=2.19, v_num=1, train_loss_step=5.290, val_loss=2.750, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 298:  84%|█████▊ | 56/67 [00:16<00:03,  3.48it/s, loss=2.19, v_num=1, train_loss_step=5.290, val_loss=2.750, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 298:  85%|█████▉ | 57/67 [00:16<00:02,  3.47it/s, loss=2.19, v_num=1, train_loss_step=5.290, val_loss=2.750, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 298:  87%|██████ | 58/67 [00:16<00:02,  3.47it/s, loss=2.19, v_num=1, train_loss_step=5.290, val_loss=2.750, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 298:  88%|██████▏| 59/67 [00:17<00:02,  3.46it/s, loss=2.19, v_num=1, train_loss_step=5.290, val_loss=2.750, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 298:  90%|██████▎| 60/67 [00:17<00:02,  3.45it/s, loss=2.19, v_num=1, train_loss_step=5.290, val_loss=2.750, train_loss_epoch=1.740]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 298:  91%|██████▎| 61/67 [00:17<00:01,  3.42it/s, loss=2.19, v_num=1, train_loss_step=5.290, val_loss=2.750, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 298:  93%|██████▍| 62/67 [00:18<00:01,  3.41it/s, loss=2.19, v_num=1, train_loss_step=5.290, val_loss=2.750, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 298:  94%|██████▌| 63/67 [00:18<00:01,  3.41it/s, loss=2.19, v_num=1, train_loss_step=5.290, val_loss=2.750, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 298:  96%|██████▋| 64/67 [00:18<00:00,  3.40it/s, loss=2.19, v_num=1, train_loss_step=5.290, val_loss=2.750, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 298:  97%|██████▊| 65/67 [00:19<00:00,  3.40it/s, loss=2.19, v_num=1, train_loss_step=5.290, val_loss=2.750, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 298:  99%|██████▉| 66/67 [00:19<00:00,  3.39it/s, loss=2.19, v_num=1, train_loss_step=5.290, val_loss=2.750, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 298: 100%|███████| 67/67 [00:20<00:00,  3.32it/s, loss=2.19, v_num=1, train_loss_step=5.290, val_loss=2.740, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 299:  45%|███▏   | 30/67 [00:05<00:07,  5.28it/s, loss=2.02, v_num=1, train_loss_step=1.330, val_loss=2.740, train_loss_epoch=2.410]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 299:  46%|███▏   | 31/67 [00:07<00:08,  4.04it/s, loss=2.02, v_num=1, train_loss_step=1.330, val_loss=2.740, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 299:  48%|███▎   | 32/67 [00:07<00:08,  4.00it/s, loss=2.02, v_num=1, train_loss_step=1.330, val_loss=2.740, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 299:  49%|███▍   | 33/67 [00:08<00:08,  3.96it/s, loss=2.02, v_num=1, train_loss_step=1.330, val_loss=2.740, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 299:  51%|███▌   | 34/67 [00:08<00:08,  3.92it/s, loss=2.02, v_num=1, train_loss_step=1.330, val_loss=2.740, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 299:  52%|███▋   | 35/67 [00:09<00:08,  3.89it/s, loss=2.02, v_num=1, train_loss_step=1.330, val_loss=2.740, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 299:  54%|███▊   | 36/67 [00:09<00:08,  3.85it/s, loss=2.02, v_num=1, train_loss_step=1.330, val_loss=2.740, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 299:  55%|███▊   | 37/67 [00:09<00:07,  3.82it/s, loss=2.02, v_num=1, train_loss_step=1.330, val_loss=2.740, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 299:  57%|███▉   | 38/67 [00:10<00:07,  3.79it/s, loss=2.02, v_num=1, train_loss_step=1.330, val_loss=2.740, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 299:  58%|████   | 39/67 [00:10<00:07,  3.77it/s, loss=2.02, v_num=1, train_loss_step=1.330, val_loss=2.740, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 299:  60%|████▏  | 40/67 [00:10<00:07,  3.74it/s, loss=2.02, v_num=1, train_loss_step=1.330, val_loss=2.740, train_loss_epoch=2.410]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 299:  61%|████▎  | 41/67 [00:11<00:07,  3.68it/s, loss=2.02, v_num=1, train_loss_step=1.330, val_loss=2.740, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 299:  63%|████▍  | 42/67 [00:11<00:06,  3.66it/s, loss=2.02, v_num=1, train_loss_step=1.330, val_loss=2.740, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 299:  64%|████▍  | 43/67 [00:11<00:06,  3.64it/s, loss=2.02, v_num=1, train_loss_step=1.330, val_loss=2.740, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 299:  66%|████▌  | 44/67 [00:12<00:06,  3.62it/s, loss=2.02, v_num=1, train_loss_step=1.330, val_loss=2.740, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 299:  67%|████▋  | 45/67 [00:12<00:06,  3.61it/s, loss=2.02, v_num=1, train_loss_step=1.330, val_loss=2.740, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 299:  69%|████▊  | 46/67 [00:12<00:05,  3.59it/s, loss=2.02, v_num=1, train_loss_step=1.330, val_loss=2.740, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 299:  70%|████▉  | 47/67 [00:13<00:05,  3.58it/s, loss=2.02, v_num=1, train_loss_step=1.330, val_loss=2.740, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 299:  72%|█████  | 48/67 [00:13<00:05,  3.56it/s, loss=2.02, v_num=1, train_loss_step=1.330, val_loss=2.740, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 299:  73%|█████  | 49/67 [00:13<00:05,  3.55it/s, loss=2.02, v_num=1, train_loss_step=1.330, val_loss=2.740, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 299:  75%|█████▏ | 50/67 [00:14<00:04,  3.54it/s, loss=2.02, v_num=1, train_loss_step=1.330, val_loss=2.740, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 299:  76%|█████▎ | 51/67 [00:14<00:04,  3.49it/s, loss=2.02, v_num=1, train_loss_step=1.330, val_loss=2.740, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 299:  78%|█████▍ | 52/67 [00:14<00:04,  3.48it/s, loss=2.02, v_num=1, train_loss_step=1.330, val_loss=2.740, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 299:  79%|█████▌ | 53/67 [00:15<00:04,  3.47it/s, loss=2.02, v_num=1, train_loss_step=1.330, val_loss=2.740, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 299:  81%|█████▋ | 54/67 [00:15<00:03,  3.46it/s, loss=2.02, v_num=1, train_loss_step=1.330, val_loss=2.740, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 299:  82%|█████▋ | 55/67 [00:15<00:03,  3.45it/s, loss=2.02, v_num=1, train_loss_step=1.330, val_loss=2.740, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 299:  84%|█████▊ | 56/67 [00:16<00:03,  3.44it/s, loss=2.02, v_num=1, train_loss_step=1.330, val_loss=2.740, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 299:  85%|█████▉ | 57/67 [00:16<00:02,  3.44it/s, loss=2.02, v_num=1, train_loss_step=1.330, val_loss=2.740, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 299:  87%|██████ | 58/67 [00:16<00:02,  3.43it/s, loss=2.02, v_num=1, train_loss_step=1.330, val_loss=2.740, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 299:  88%|██████▏| 59/67 [00:17<00:02,  3.42it/s, loss=2.02, v_num=1, train_loss_step=1.330, val_loss=2.740, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 299:  90%|██████▎| 60/67 [00:17<00:02,  3.41it/s, loss=2.02, v_num=1, train_loss_step=1.330, val_loss=2.740, train_loss_epoch=2.410]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 299:  91%|██████▎| 61/67 [00:18<00:01,  3.38it/s, loss=2.02, v_num=1, train_loss_step=1.330, val_loss=2.740, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 299:  93%|██████▍| 62/67 [00:18<00:01,  3.38it/s, loss=2.02, v_num=1, train_loss_step=1.330, val_loss=2.740, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 299:  94%|██████▌| 63/67 [00:18<00:01,  3.37it/s, loss=2.02, v_num=1, train_loss_step=1.330, val_loss=2.740, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 299:  96%|██████▋| 64/67 [00:19<00:00,  3.37it/s, loss=2.02, v_num=1, train_loss_step=1.330, val_loss=2.740, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 299:  97%|██████▊| 65/67 [00:19<00:00,  3.36it/s, loss=2.02, v_num=1, train_loss_step=1.330, val_loss=2.740, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 299:  99%|██████▉| 66/67 [00:19<00:00,  3.36it/s, loss=2.02, v_num=1, train_loss_step=1.330, val_loss=2.740, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 299: 100%|███████| 67/67 [00:20<00:00,  3.28it/s, loss=2.02, v_num=1, train_loss_step=1.330, val_loss=2.740, train_loss_epoch=2.410]\u001b[A\n",
      "Epoch 300:  45%|███▏   | 30/67 [00:05<00:07,  5.24it/s, loss=1.82, v_num=1, train_loss_step=0.992, val_loss=2.740, train_loss_epoch=2.280]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 300:  46%|███▏   | 31/67 [00:07<00:09,  3.97it/s, loss=1.82, v_num=1, train_loss_step=0.992, val_loss=2.740, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 300:  48%|███▎   | 32/67 [00:08<00:08,  3.93it/s, loss=1.82, v_num=1, train_loss_step=0.992, val_loss=2.740, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 300:  49%|███▍   | 33/67 [00:08<00:08,  3.90it/s, loss=1.82, v_num=1, train_loss_step=0.992, val_loss=2.740, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 300:  51%|███▌   | 34/67 [00:08<00:08,  3.86it/s, loss=1.82, v_num=1, train_loss_step=0.992, val_loss=2.740, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 300:  52%|███▋   | 35/67 [00:09<00:08,  3.82it/s, loss=1.82, v_num=1, train_loss_step=0.992, val_loss=2.740, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 300:  54%|███▊   | 36/67 [00:09<00:08,  3.80it/s, loss=1.82, v_num=1, train_loss_step=0.992, val_loss=2.740, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 300:  55%|███▊   | 37/67 [00:09<00:07,  3.77it/s, loss=1.82, v_num=1, train_loss_step=0.992, val_loss=2.740, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 300:  57%|███▉   | 38/67 [00:10<00:07,  3.74it/s, loss=1.82, v_num=1, train_loss_step=0.992, val_loss=2.740, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 300:  58%|████   | 39/67 [00:10<00:07,  3.72it/s, loss=1.82, v_num=1, train_loss_step=0.992, val_loss=2.740, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 300:  60%|████▏  | 40/67 [00:10<00:07,  3.70it/s, loss=1.82, v_num=1, train_loss_step=0.992, val_loss=2.740, train_loss_epoch=2.280]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 300:  61%|████▎  | 41/67 [00:11<00:07,  3.63it/s, loss=1.82, v_num=1, train_loss_step=0.992, val_loss=2.740, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 300:  63%|████▍  | 42/67 [00:11<00:06,  3.62it/s, loss=1.82, v_num=1, train_loss_step=0.992, val_loss=2.740, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 300:  64%|████▍  | 43/67 [00:11<00:06,  3.60it/s, loss=1.82, v_num=1, train_loss_step=0.992, val_loss=2.740, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 300:  66%|████▌  | 44/67 [00:12<00:06,  3.58it/s, loss=1.82, v_num=1, train_loss_step=0.992, val_loss=2.740, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 300:  67%|████▋  | 45/67 [00:12<00:06,  3.57it/s, loss=1.82, v_num=1, train_loss_step=0.992, val_loss=2.740, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 300:  69%|████▊  | 46/67 [00:12<00:05,  3.55it/s, loss=1.82, v_num=1, train_loss_step=0.992, val_loss=2.740, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 300:  70%|████▉  | 47/67 [00:13<00:05,  3.54it/s, loss=1.82, v_num=1, train_loss_step=0.992, val_loss=2.740, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 300:  72%|█████  | 48/67 [00:13<00:05,  3.53it/s, loss=1.82, v_num=1, train_loss_step=0.992, val_loss=2.740, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 300:  73%|█████  | 49/67 [00:13<00:05,  3.52it/s, loss=1.82, v_num=1, train_loss_step=0.992, val_loss=2.740, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 300:  75%|█████▏ | 50/67 [00:14<00:04,  3.50it/s, loss=1.82, v_num=1, train_loss_step=0.992, val_loss=2.740, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 300:  76%|█████▎ | 51/67 [00:14<00:04,  3.46it/s, loss=1.82, v_num=1, train_loss_step=0.992, val_loss=2.740, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 300:  78%|█████▍ | 52/67 [00:15<00:04,  3.45it/s, loss=1.82, v_num=1, train_loss_step=0.992, val_loss=2.740, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 300:  79%|█████▌ | 53/67 [00:15<00:04,  3.44it/s, loss=1.82, v_num=1, train_loss_step=0.992, val_loss=2.740, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 300:  81%|█████▋ | 54/67 [00:15<00:03,  3.43it/s, loss=1.82, v_num=1, train_loss_step=0.992, val_loss=2.740, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 300:  82%|█████▋ | 55/67 [00:16<00:03,  3.43it/s, loss=1.82, v_num=1, train_loss_step=0.992, val_loss=2.740, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 300:  84%|█████▊ | 56/67 [00:16<00:03,  3.42it/s, loss=1.82, v_num=1, train_loss_step=0.992, val_loss=2.740, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 300:  85%|█████▉ | 57/67 [00:16<00:02,  3.41it/s, loss=1.82, v_num=1, train_loss_step=0.992, val_loss=2.740, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 300:  87%|██████ | 58/67 [00:17<00:02,  3.40it/s, loss=1.82, v_num=1, train_loss_step=0.992, val_loss=2.740, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 300:  88%|██████▏| 59/67 [00:17<00:02,  3.39it/s, loss=1.82, v_num=1, train_loss_step=0.992, val_loss=2.740, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 300:  90%|██████▎| 60/67 [00:17<00:02,  3.39it/s, loss=1.82, v_num=1, train_loss_step=0.992, val_loss=2.740, train_loss_epoch=2.280]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 300:  91%|██████▎| 61/67 [00:18<00:01,  3.35it/s, loss=1.82, v_num=1, train_loss_step=0.992, val_loss=2.740, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 300:  93%|██████▍| 62/67 [00:18<00:01,  3.35it/s, loss=1.82, v_num=1, train_loss_step=0.992, val_loss=2.740, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 300:  94%|██████▌| 63/67 [00:18<00:01,  3.34it/s, loss=1.82, v_num=1, train_loss_step=0.992, val_loss=2.740, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 300:  96%|██████▋| 64/67 [00:19<00:00,  3.34it/s, loss=1.82, v_num=1, train_loss_step=0.992, val_loss=2.740, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 300:  97%|██████▊| 65/67 [00:19<00:00,  3.33it/s, loss=1.82, v_num=1, train_loss_step=0.992, val_loss=2.740, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 300:  99%|██████▉| 66/67 [00:19<00:00,  3.33it/s, loss=1.82, v_num=1, train_loss_step=0.992, val_loss=2.740, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 300: 100%|███████| 67/67 [00:20<00:00,  3.26it/s, loss=1.82, v_num=1, train_loss_step=0.992, val_loss=2.740, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 301:  45%|███▏   | 30/67 [00:05<00:07,  5.25it/s, loss=2.59, v_num=1, train_loss_step=1.620, val_loss=2.740, train_loss_epoch=2.140]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 301:  46%|███▏   | 31/67 [00:07<00:08,  4.02it/s, loss=2.59, v_num=1, train_loss_step=1.620, val_loss=2.740, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 301:  48%|███▎   | 32/67 [00:08<00:08,  3.98it/s, loss=2.59, v_num=1, train_loss_step=1.620, val_loss=2.740, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 301:  49%|███▍   | 33/67 [00:08<00:08,  3.94it/s, loss=2.59, v_num=1, train_loss_step=1.620, val_loss=2.740, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 301:  51%|███▌   | 34/67 [00:08<00:08,  3.90it/s, loss=2.59, v_num=1, train_loss_step=1.620, val_loss=2.740, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 301:  52%|███▋   | 35/67 [00:09<00:08,  3.87it/s, loss=2.59, v_num=1, train_loss_step=1.620, val_loss=2.740, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 301:  54%|███▊   | 36/67 [00:09<00:08,  3.83it/s, loss=2.59, v_num=1, train_loss_step=1.620, val_loss=2.740, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 301:  55%|███▊   | 37/67 [00:09<00:07,  3.80it/s, loss=2.59, v_num=1, train_loss_step=1.620, val_loss=2.740, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 301:  57%|███▉   | 38/67 [00:10<00:07,  3.78it/s, loss=2.59, v_num=1, train_loss_step=1.620, val_loss=2.740, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 301:  58%|████   | 39/67 [00:10<00:07,  3.75it/s, loss=2.59, v_num=1, train_loss_step=1.620, val_loss=2.740, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 301:  60%|████▏  | 40/67 [00:10<00:07,  3.72it/s, loss=2.59, v_num=1, train_loss_step=1.620, val_loss=2.740, train_loss_epoch=2.140]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 301:  61%|████▎  | 41/67 [00:11<00:07,  3.66it/s, loss=2.59, v_num=1, train_loss_step=1.620, val_loss=2.740, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 301:  63%|████▍  | 42/67 [00:11<00:06,  3.64it/s, loss=2.59, v_num=1, train_loss_step=1.620, val_loss=2.740, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 301:  64%|████▍  | 43/67 [00:11<00:06,  3.62it/s, loss=2.59, v_num=1, train_loss_step=1.620, val_loss=2.740, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 301:  66%|████▌  | 44/67 [00:12<00:06,  3.61it/s, loss=2.59, v_num=1, train_loss_step=1.620, val_loss=2.740, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 301:  67%|████▋  | 45/67 [00:12<00:06,  3.59it/s, loss=2.59, v_num=1, train_loss_step=1.620, val_loss=2.740, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 301:  69%|████▊  | 46/67 [00:12<00:05,  3.58it/s, loss=2.59, v_num=1, train_loss_step=1.620, val_loss=2.740, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 301:  70%|████▉  | 47/67 [00:13<00:05,  3.57it/s, loss=2.59, v_num=1, train_loss_step=1.620, val_loss=2.740, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 301:  72%|█████  | 48/67 [00:13<00:05,  3.55it/s, loss=2.59, v_num=1, train_loss_step=1.620, val_loss=2.740, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 301:  73%|█████  | 49/67 [00:13<00:05,  3.54it/s, loss=2.59, v_num=1, train_loss_step=1.620, val_loss=2.740, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 301:  75%|█████▏ | 50/67 [00:14<00:04,  3.53it/s, loss=2.59, v_num=1, train_loss_step=1.620, val_loss=2.740, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 301:  76%|█████▎ | 51/67 [00:14<00:04,  3.48it/s, loss=2.59, v_num=1, train_loss_step=1.620, val_loss=2.740, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 301:  78%|█████▍ | 52/67 [00:15<00:04,  3.46it/s, loss=2.59, v_num=1, train_loss_step=1.620, val_loss=2.740, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 301:  79%|█████▌ | 53/67 [00:15<00:04,  3.45it/s, loss=2.59, v_num=1, train_loss_step=1.620, val_loss=2.740, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 301:  81%|█████▋ | 54/67 [00:15<00:03,  3.44it/s, loss=2.59, v_num=1, train_loss_step=1.620, val_loss=2.740, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 301:  82%|█████▋ | 55/67 [00:16<00:03,  3.43it/s, loss=2.59, v_num=1, train_loss_step=1.620, val_loss=2.740, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 301:  84%|█████▊ | 56/67 [00:16<00:03,  3.42it/s, loss=2.59, v_num=1, train_loss_step=1.620, val_loss=2.740, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 301:  85%|█████▉ | 57/67 [00:16<00:02,  3.42it/s, loss=2.59, v_num=1, train_loss_step=1.620, val_loss=2.740, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 301:  87%|██████ | 58/67 [00:17<00:02,  3.41it/s, loss=2.59, v_num=1, train_loss_step=1.620, val_loss=2.740, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 301:  88%|██████▏| 59/67 [00:17<00:02,  3.40it/s, loss=2.59, v_num=1, train_loss_step=1.620, val_loss=2.740, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 301:  90%|██████▎| 60/67 [00:17<00:02,  3.39it/s, loss=2.59, v_num=1, train_loss_step=1.620, val_loss=2.740, train_loss_epoch=2.140]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 301:  91%|██████▎| 61/67 [00:18<00:01,  3.36it/s, loss=2.59, v_num=1, train_loss_step=1.620, val_loss=2.740, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 301:  93%|██████▍| 62/67 [00:18<00:01,  3.35it/s, loss=2.59, v_num=1, train_loss_step=1.620, val_loss=2.740, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 301:  94%|██████▌| 63/67 [00:18<00:01,  3.35it/s, loss=2.59, v_num=1, train_loss_step=1.620, val_loss=2.740, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 301:  96%|██████▋| 64/67 [00:19<00:00,  3.34it/s, loss=2.59, v_num=1, train_loss_step=1.620, val_loss=2.740, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 301:  97%|██████▊| 65/67 [00:19<00:00,  3.34it/s, loss=2.59, v_num=1, train_loss_step=1.620, val_loss=2.740, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 301:  99%|██████▉| 66/67 [00:19<00:00,  3.34it/s, loss=2.59, v_num=1, train_loss_step=1.620, val_loss=2.740, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 301: 100%|███████| 67/67 [00:20<00:00,  3.27it/s, loss=2.59, v_num=1, train_loss_step=1.620, val_loss=2.740, train_loss_epoch=2.140]\u001b[A\n",
      "Epoch 302:  45%|███▏   | 30/67 [00:05<00:06,  5.29it/s, loss=1.46, v_num=1, train_loss_step=1.600, val_loss=2.740, train_loss_epoch=2.480]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 302:  46%|███▏   | 31/67 [00:07<00:08,  4.04it/s, loss=1.46, v_num=1, train_loss_step=1.600, val_loss=2.740, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 302:  48%|███▎   | 32/67 [00:07<00:08,  4.00it/s, loss=1.46, v_num=1, train_loss_step=1.600, val_loss=2.740, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 302:  49%|███▍   | 33/67 [00:08<00:08,  3.96it/s, loss=1.46, v_num=1, train_loss_step=1.600, val_loss=2.740, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 302:  51%|███▌   | 34/67 [00:08<00:08,  3.92it/s, loss=1.46, v_num=1, train_loss_step=1.600, val_loss=2.740, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 302:  52%|███▋   | 35/67 [00:08<00:08,  3.89it/s, loss=1.46, v_num=1, train_loss_step=1.600, val_loss=2.740, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 302:  54%|███▊   | 36/67 [00:09<00:08,  3.86it/s, loss=1.46, v_num=1, train_loss_step=1.600, val_loss=2.740, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 302:  55%|███▊   | 37/67 [00:09<00:07,  3.83it/s, loss=1.46, v_num=1, train_loss_step=1.600, val_loss=2.740, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 302:  57%|███▉   | 38/67 [00:09<00:07,  3.81it/s, loss=1.46, v_num=1, train_loss_step=1.600, val_loss=2.740, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 302:  58%|████   | 39/67 [00:10<00:07,  3.79it/s, loss=1.46, v_num=1, train_loss_step=1.600, val_loss=2.740, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 302:  60%|████▏  | 40/67 [00:10<00:07,  3.76it/s, loss=1.46, v_num=1, train_loss_step=1.600, val_loss=2.740, train_loss_epoch=2.480]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 302:  61%|████▎  | 41/67 [00:11<00:07,  3.69it/s, loss=1.46, v_num=1, train_loss_step=1.600, val_loss=2.740, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 302:  63%|████▍  | 42/67 [00:11<00:06,  3.67it/s, loss=1.46, v_num=1, train_loss_step=1.600, val_loss=2.740, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 302:  64%|████▍  | 43/67 [00:11<00:06,  3.65it/s, loss=1.46, v_num=1, train_loss_step=1.600, val_loss=2.740, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 302:  66%|████▌  | 44/67 [00:12<00:06,  3.64it/s, loss=1.46, v_num=1, train_loss_step=1.600, val_loss=2.740, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 302:  67%|████▋  | 45/67 [00:12<00:06,  3.62it/s, loss=1.46, v_num=1, train_loss_step=1.600, val_loss=2.740, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 302:  69%|████▊  | 46/67 [00:12<00:05,  3.61it/s, loss=1.46, v_num=1, train_loss_step=1.600, val_loss=2.740, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 302:  70%|████▉  | 47/67 [00:13<00:05,  3.60it/s, loss=1.46, v_num=1, train_loss_step=1.600, val_loss=2.740, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 302:  72%|█████  | 48/67 [00:13<00:05,  3.58it/s, loss=1.46, v_num=1, train_loss_step=1.600, val_loss=2.740, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 302:  73%|█████  | 49/67 [00:13<00:05,  3.56it/s, loss=1.46, v_num=1, train_loss_step=1.600, val_loss=2.740, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 302:  75%|█████▏ | 50/67 [00:14<00:04,  3.55it/s, loss=1.46, v_num=1, train_loss_step=1.600, val_loss=2.740, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 302:  76%|█████▎ | 51/67 [00:14<00:04,  3.50it/s, loss=1.46, v_num=1, train_loss_step=1.600, val_loss=2.740, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 302:  78%|█████▍ | 52/67 [00:14<00:04,  3.49it/s, loss=1.46, v_num=1, train_loss_step=1.600, val_loss=2.740, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 302:  79%|█████▌ | 53/67 [00:15<00:04,  3.48it/s, loss=1.46, v_num=1, train_loss_step=1.600, val_loss=2.740, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 302:  81%|█████▋ | 54/67 [00:15<00:03,  3.47it/s, loss=1.46, v_num=1, train_loss_step=1.600, val_loss=2.740, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 302:  82%|█████▋ | 55/67 [00:15<00:03,  3.46it/s, loss=1.46, v_num=1, train_loss_step=1.600, val_loss=2.740, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 302:  84%|█████▊ | 56/67 [00:16<00:03,  3.46it/s, loss=1.46, v_num=1, train_loss_step=1.600, val_loss=2.740, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 302:  85%|█████▉ | 57/67 [00:16<00:02,  3.45it/s, loss=1.46, v_num=1, train_loss_step=1.600, val_loss=2.740, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 302:  87%|██████ | 58/67 [00:16<00:02,  3.44it/s, loss=1.46, v_num=1, train_loss_step=1.600, val_loss=2.740, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 302:  88%|██████▏| 59/67 [00:17<00:02,  3.43it/s, loss=1.46, v_num=1, train_loss_step=1.600, val_loss=2.740, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 302:  90%|██████▎| 60/67 [00:17<00:02,  3.43it/s, loss=1.46, v_num=1, train_loss_step=1.600, val_loss=2.740, train_loss_epoch=2.480]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 302:  91%|██████▎| 61/67 [00:17<00:01,  3.39it/s, loss=1.46, v_num=1, train_loss_step=1.600, val_loss=2.740, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 302:  93%|██████▍| 62/67 [00:18<00:01,  3.38it/s, loss=1.46, v_num=1, train_loss_step=1.600, val_loss=2.740, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 302:  94%|██████▌| 63/67 [00:18<00:01,  3.38it/s, loss=1.46, v_num=1, train_loss_step=1.600, val_loss=2.740, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 302:  96%|██████▋| 64/67 [00:18<00:00,  3.37it/s, loss=1.46, v_num=1, train_loss_step=1.600, val_loss=2.740, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 302:  97%|██████▊| 65/67 [00:19<00:00,  3.37it/s, loss=1.46, v_num=1, train_loss_step=1.600, val_loss=2.740, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 302:  99%|██████▉| 66/67 [00:19<00:00,  3.37it/s, loss=1.46, v_num=1, train_loss_step=1.600, val_loss=2.740, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 302: 100%|███████| 67/67 [00:20<00:00,  3.30it/s, loss=1.46, v_num=1, train_loss_step=1.600, val_loss=2.740, train_loss_epoch=2.480]\u001b[A\n",
      "Epoch 303:  45%|███▏   | 30/67 [00:05<00:07,  5.27it/s, loss=2.27, v_num=1, train_loss_step=2.450, val_loss=2.740, train_loss_epoch=1.580]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 303:  46%|███▏   | 31/67 [00:07<00:09,  3.98it/s, loss=2.27, v_num=1, train_loss_step=2.450, val_loss=2.740, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 303:  48%|███▎   | 32/67 [00:08<00:08,  3.95it/s, loss=2.27, v_num=1, train_loss_step=2.450, val_loss=2.740, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 303:  49%|███▍   | 33/67 [00:08<00:08,  3.91it/s, loss=2.27, v_num=1, train_loss_step=2.450, val_loss=2.740, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 303:  51%|███▌   | 34/67 [00:08<00:08,  3.88it/s, loss=2.27, v_num=1, train_loss_step=2.450, val_loss=2.740, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 303:  52%|███▋   | 35/67 [00:09<00:08,  3.85it/s, loss=2.27, v_num=1, train_loss_step=2.450, val_loss=2.740, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 303:  54%|███▊   | 36/67 [00:09<00:08,  3.82it/s, loss=2.27, v_num=1, train_loss_step=2.450, val_loss=2.740, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 303:  55%|███▊   | 37/67 [00:09<00:07,  3.79it/s, loss=2.27, v_num=1, train_loss_step=2.450, val_loss=2.740, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 303:  57%|███▉   | 38/67 [00:10<00:07,  3.77it/s, loss=2.27, v_num=1, train_loss_step=2.450, val_loss=2.740, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 303:  58%|████   | 39/67 [00:10<00:07,  3.75it/s, loss=2.27, v_num=1, train_loss_step=2.450, val_loss=2.740, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 303:  60%|████▏  | 40/67 [00:10<00:07,  3.72it/s, loss=2.27, v_num=1, train_loss_step=2.450, val_loss=2.740, train_loss_epoch=1.580]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 303:  61%|████▎  | 41/67 [00:11<00:07,  3.62it/s, loss=2.27, v_num=1, train_loss_step=2.450, val_loss=2.740, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 303:  63%|████▍  | 42/67 [00:11<00:06,  3.61it/s, loss=2.27, v_num=1, train_loss_step=2.450, val_loss=2.740, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 303:  64%|████▍  | 43/67 [00:11<00:06,  3.59it/s, loss=2.27, v_num=1, train_loss_step=2.450, val_loss=2.740, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 303:  66%|████▌  | 44/67 [00:12<00:06,  3.58it/s, loss=2.27, v_num=1, train_loss_step=2.450, val_loss=2.740, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 303:  67%|████▋  | 45/67 [00:12<00:06,  3.57it/s, loss=2.27, v_num=1, train_loss_step=2.450, val_loss=2.740, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 303:  69%|████▊  | 46/67 [00:12<00:05,  3.55it/s, loss=2.27, v_num=1, train_loss_step=2.450, val_loss=2.740, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 303:  70%|████▉  | 47/67 [00:13<00:05,  3.54it/s, loss=2.27, v_num=1, train_loss_step=2.450, val_loss=2.740, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 303:  72%|█████  | 48/67 [00:13<00:05,  3.53it/s, loss=2.27, v_num=1, train_loss_step=2.450, val_loss=2.740, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 303:  73%|█████  | 49/67 [00:13<00:05,  3.51it/s, loss=2.27, v_num=1, train_loss_step=2.450, val_loss=2.740, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 303:  75%|█████▏ | 50/67 [00:14<00:04,  3.50it/s, loss=2.27, v_num=1, train_loss_step=2.450, val_loss=2.740, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 303:  76%|█████▎ | 51/67 [00:14<00:04,  3.45it/s, loss=2.27, v_num=1, train_loss_step=2.450, val_loss=2.740, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 303:  78%|█████▍ | 52/67 [00:15<00:04,  3.44it/s, loss=2.27, v_num=1, train_loss_step=2.450, val_loss=2.740, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 303:  79%|█████▌ | 53/67 [00:15<00:04,  3.43it/s, loss=2.27, v_num=1, train_loss_step=2.450, val_loss=2.740, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 303:  81%|█████▋ | 54/67 [00:15<00:03,  3.43it/s, loss=2.27, v_num=1, train_loss_step=2.450, val_loss=2.740, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 303:  82%|█████▋ | 55/67 [00:16<00:03,  3.42it/s, loss=2.27, v_num=1, train_loss_step=2.450, val_loss=2.740, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 303:  84%|█████▊ | 56/67 [00:16<00:03,  3.41it/s, loss=2.27, v_num=1, train_loss_step=2.450, val_loss=2.740, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 303:  85%|█████▉ | 57/67 [00:16<00:02,  3.40it/s, loss=2.27, v_num=1, train_loss_step=2.450, val_loss=2.740, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 303:  87%|██████ | 58/67 [00:17<00:02,  3.40it/s, loss=2.27, v_num=1, train_loss_step=2.450, val_loss=2.740, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 303:  88%|██████▏| 59/67 [00:17<00:02,  3.39it/s, loss=2.27, v_num=1, train_loss_step=2.450, val_loss=2.740, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 303:  90%|██████▎| 60/67 [00:17<00:02,  3.38it/s, loss=2.27, v_num=1, train_loss_step=2.450, val_loss=2.740, train_loss_epoch=1.580]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 303:  91%|██████▎| 61/67 [00:18<00:01,  3.34it/s, loss=2.27, v_num=1, train_loss_step=2.450, val_loss=2.740, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 303:  93%|██████▍| 62/67 [00:18<00:01,  3.34it/s, loss=2.27, v_num=1, train_loss_step=2.450, val_loss=2.740, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 303:  94%|██████▌| 63/67 [00:18<00:01,  3.33it/s, loss=2.27, v_num=1, train_loss_step=2.450, val_loss=2.740, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 303:  96%|██████▋| 64/67 [00:19<00:00,  3.33it/s, loss=2.27, v_num=1, train_loss_step=2.450, val_loss=2.740, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 303:  97%|██████▊| 65/67 [00:19<00:00,  3.33it/s, loss=2.27, v_num=1, train_loss_step=2.450, val_loss=2.740, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 303:  99%|██████▉| 66/67 [00:19<00:00,  3.32it/s, loss=2.27, v_num=1, train_loss_step=2.450, val_loss=2.740, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 303: 100%|███████| 67/67 [00:20<00:00,  3.25it/s, loss=2.27, v_num=1, train_loss_step=2.450, val_loss=2.740, train_loss_epoch=1.580]\u001b[A\n",
      "Epoch 304:  45%|███▏   | 30/67 [00:05<00:07,  5.19it/s, loss=1.76, v_num=1, train_loss_step=1.480, val_loss=2.740, train_loss_epoch=2.050]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 304:  46%|███▏   | 31/67 [00:07<00:09,  3.97it/s, loss=1.76, v_num=1, train_loss_step=1.480, val_loss=2.740, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 304:  48%|███▎   | 32/67 [00:08<00:08,  3.93it/s, loss=1.76, v_num=1, train_loss_step=1.480, val_loss=2.740, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 304:  49%|███▍   | 33/67 [00:08<00:08,  3.89it/s, loss=1.76, v_num=1, train_loss_step=1.480, val_loss=2.740, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 304:  51%|███▌   | 34/67 [00:08<00:08,  3.86it/s, loss=1.76, v_num=1, train_loss_step=1.480, val_loss=2.740, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 304:  52%|███▋   | 35/67 [00:09<00:08,  3.83it/s, loss=1.76, v_num=1, train_loss_step=1.480, val_loss=2.740, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 304:  54%|███▊   | 36/67 [00:09<00:08,  3.80it/s, loss=1.76, v_num=1, train_loss_step=1.480, val_loss=2.740, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 304:  55%|███▊   | 37/67 [00:09<00:07,  3.78it/s, loss=1.76, v_num=1, train_loss_step=1.480, val_loss=2.740, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 304:  57%|███▉   | 38/67 [00:10<00:07,  3.75it/s, loss=1.76, v_num=1, train_loss_step=1.480, val_loss=2.740, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 304:  58%|████   | 39/67 [00:10<00:07,  3.73it/s, loss=1.76, v_num=1, train_loss_step=1.480, val_loss=2.740, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 304:  60%|████▏  | 40/67 [00:10<00:07,  3.71it/s, loss=1.76, v_num=1, train_loss_step=1.480, val_loss=2.740, train_loss_epoch=2.050]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 304:  61%|████▎  | 41/67 [00:11<00:07,  3.64it/s, loss=1.76, v_num=1, train_loss_step=1.480, val_loss=2.740, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 304:  63%|████▍  | 42/67 [00:11<00:06,  3.62it/s, loss=1.76, v_num=1, train_loss_step=1.480, val_loss=2.740, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 304:  64%|████▍  | 43/67 [00:11<00:06,  3.60it/s, loss=1.76, v_num=1, train_loss_step=1.480, val_loss=2.740, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 304:  66%|████▌  | 44/67 [00:12<00:06,  3.58it/s, loss=1.76, v_num=1, train_loss_step=1.480, val_loss=2.740, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 304:  67%|████▋  | 45/67 [00:12<00:06,  3.57it/s, loss=1.76, v_num=1, train_loss_step=1.480, val_loss=2.740, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 304:  69%|████▊  | 46/67 [00:12<00:05,  3.55it/s, loss=1.76, v_num=1, train_loss_step=1.480, val_loss=2.740, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 304:  70%|████▉  | 47/67 [00:13<00:05,  3.54it/s, loss=1.76, v_num=1, train_loss_step=1.480, val_loss=2.740, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 304:  72%|█████  | 48/67 [00:13<00:05,  3.53it/s, loss=1.76, v_num=1, train_loss_step=1.480, val_loss=2.740, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 304:  73%|█████  | 49/67 [00:13<00:05,  3.52it/s, loss=1.76, v_num=1, train_loss_step=1.480, val_loss=2.740, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 304:  75%|█████▏ | 50/67 [00:14<00:04,  3.50it/s, loss=1.76, v_num=1, train_loss_step=1.480, val_loss=2.740, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 304:  76%|█████▎ | 51/67 [00:14<00:04,  3.45it/s, loss=1.76, v_num=1, train_loss_step=1.480, val_loss=2.740, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 304:  78%|█████▍ | 52/67 [00:15<00:04,  3.44it/s, loss=1.76, v_num=1, train_loss_step=1.480, val_loss=2.740, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 304:  79%|█████▌ | 53/67 [00:15<00:04,  3.43it/s, loss=1.76, v_num=1, train_loss_step=1.480, val_loss=2.740, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 304:  81%|█████▋ | 54/67 [00:15<00:03,  3.42it/s, loss=1.76, v_num=1, train_loss_step=1.480, val_loss=2.740, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 304:  82%|█████▋ | 55/67 [00:16<00:03,  3.41it/s, loss=1.76, v_num=1, train_loss_step=1.480, val_loss=2.740, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 304:  84%|█████▊ | 56/67 [00:16<00:03,  3.40it/s, loss=1.76, v_num=1, train_loss_step=1.480, val_loss=2.740, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 304:  85%|█████▉ | 57/67 [00:16<00:02,  3.40it/s, loss=1.76, v_num=1, train_loss_step=1.480, val_loss=2.740, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 304:  87%|██████ | 58/67 [00:17<00:02,  3.39it/s, loss=1.76, v_num=1, train_loss_step=1.480, val_loss=2.740, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 304:  88%|██████▏| 59/67 [00:17<00:02,  3.38it/s, loss=1.76, v_num=1, train_loss_step=1.480, val_loss=2.740, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 304:  90%|██████▎| 60/67 [00:17<00:02,  3.37it/s, loss=1.76, v_num=1, train_loss_step=1.480, val_loss=2.740, train_loss_epoch=2.050]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 304:  91%|██████▎| 61/67 [00:18<00:01,  3.34it/s, loss=1.76, v_num=1, train_loss_step=1.480, val_loss=2.740, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 304:  93%|██████▍| 62/67 [00:18<00:01,  3.33it/s, loss=1.76, v_num=1, train_loss_step=1.480, val_loss=2.740, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 304:  94%|██████▌| 63/67 [00:18<00:01,  3.33it/s, loss=1.76, v_num=1, train_loss_step=1.480, val_loss=2.740, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 304:  96%|██████▋| 64/67 [00:19<00:00,  3.32it/s, loss=1.76, v_num=1, train_loss_step=1.480, val_loss=2.740, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 304:  97%|██████▊| 65/67 [00:19<00:00,  3.32it/s, loss=1.76, v_num=1, train_loss_step=1.480, val_loss=2.740, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 304:  99%|██████▉| 66/67 [00:19<00:00,  3.32it/s, loss=1.76, v_num=1, train_loss_step=1.480, val_loss=2.740, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 304: 100%|███████| 67/67 [00:20<00:00,  3.25it/s, loss=1.76, v_num=1, train_loss_step=1.480, val_loss=2.740, train_loss_epoch=2.050]\u001b[A\n",
      "Epoch 305:  45%|███▏   | 30/67 [00:05<00:07,  5.15it/s, loss=2.13, v_num=1, train_loss_step=0.656, val_loss=2.740, train_loss_epoch=1.680]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 305:  46%|███▏   | 31/67 [00:07<00:09,  3.92it/s, loss=2.13, v_num=1, train_loss_step=0.656, val_loss=2.740, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 305:  48%|███▎   | 32/67 [00:08<00:08,  3.89it/s, loss=2.13, v_num=1, train_loss_step=0.656, val_loss=2.740, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 305:  49%|███▍   | 33/67 [00:08<00:08,  3.86it/s, loss=2.13, v_num=1, train_loss_step=0.656, val_loss=2.740, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 305:  51%|███▌   | 34/67 [00:08<00:08,  3.82it/s, loss=2.13, v_num=1, train_loss_step=0.656, val_loss=2.740, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 305:  52%|███▋   | 35/67 [00:09<00:08,  3.80it/s, loss=2.13, v_num=1, train_loss_step=0.656, val_loss=2.740, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 305:  54%|███▊   | 36/67 [00:09<00:08,  3.77it/s, loss=2.13, v_num=1, train_loss_step=0.656, val_loss=2.740, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 305:  55%|███▊   | 37/67 [00:09<00:08,  3.74it/s, loss=2.13, v_num=1, train_loss_step=0.656, val_loss=2.740, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 305:  57%|███▉   | 38/67 [00:10<00:07,  3.72it/s, loss=2.13, v_num=1, train_loss_step=0.656, val_loss=2.740, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 305:  58%|████   | 39/67 [00:10<00:07,  3.70it/s, loss=2.13, v_num=1, train_loss_step=0.656, val_loss=2.740, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 305:  60%|████▏  | 40/67 [00:10<00:07,  3.67it/s, loss=2.13, v_num=1, train_loss_step=0.656, val_loss=2.740, train_loss_epoch=1.680]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 305:  61%|████▎  | 41/67 [00:11<00:07,  3.59it/s, loss=2.13, v_num=1, train_loss_step=0.656, val_loss=2.740, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 305:  63%|████▍  | 42/67 [00:11<00:06,  3.58it/s, loss=2.13, v_num=1, train_loss_step=0.656, val_loss=2.740, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 305:  64%|████▍  | 43/67 [00:12<00:06,  3.56it/s, loss=2.13, v_num=1, train_loss_step=0.656, val_loss=2.740, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 305:  66%|████▌  | 44/67 [00:12<00:06,  3.55it/s, loss=2.13, v_num=1, train_loss_step=0.656, val_loss=2.740, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 305:  67%|████▋  | 45/67 [00:12<00:06,  3.54it/s, loss=2.13, v_num=1, train_loss_step=0.656, val_loss=2.740, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 305:  69%|████▊  | 46/67 [00:13<00:05,  3.53it/s, loss=2.13, v_num=1, train_loss_step=0.656, val_loss=2.740, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 305:  70%|████▉  | 47/67 [00:13<00:05,  3.51it/s, loss=2.13, v_num=1, train_loss_step=0.656, val_loss=2.740, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 305:  72%|█████  | 48/67 [00:13<00:05,  3.50it/s, loss=2.13, v_num=1, train_loss_step=0.656, val_loss=2.740, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 305:  73%|█████  | 49/67 [00:14<00:05,  3.49it/s, loss=2.13, v_num=1, train_loss_step=0.656, val_loss=2.740, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 305:  75%|█████▏ | 50/67 [00:14<00:04,  3.48it/s, loss=2.13, v_num=1, train_loss_step=0.656, val_loss=2.740, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 305:  76%|█████▎ | 51/67 [00:14<00:04,  3.43it/s, loss=2.13, v_num=1, train_loss_step=0.656, val_loss=2.740, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 305:  78%|█████▍ | 52/67 [00:15<00:04,  3.42it/s, loss=2.13, v_num=1, train_loss_step=0.656, val_loss=2.740, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 305:  79%|█████▌ | 53/67 [00:15<00:04,  3.41it/s, loss=2.13, v_num=1, train_loss_step=0.656, val_loss=2.740, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 305:  81%|█████▋ | 54/67 [00:15<00:03,  3.40it/s, loss=2.13, v_num=1, train_loss_step=0.656, val_loss=2.740, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 305:  82%|█████▋ | 55/67 [00:16<00:03,  3.39it/s, loss=2.13, v_num=1, train_loss_step=0.656, val_loss=2.740, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 305:  84%|█████▊ | 56/67 [00:16<00:03,  3.38it/s, loss=2.13, v_num=1, train_loss_step=0.656, val_loss=2.740, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 305:  85%|█████▉ | 57/67 [00:16<00:02,  3.37it/s, loss=2.13, v_num=1, train_loss_step=0.656, val_loss=2.740, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 305:  87%|██████ | 58/67 [00:17<00:02,  3.37it/s, loss=2.13, v_num=1, train_loss_step=0.656, val_loss=2.740, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 305:  88%|██████▏| 59/67 [00:17<00:02,  3.36it/s, loss=2.13, v_num=1, train_loss_step=0.656, val_loss=2.740, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 305:  90%|██████▎| 60/67 [00:17<00:02,  3.35it/s, loss=2.13, v_num=1, train_loss_step=0.656, val_loss=2.740, train_loss_epoch=1.680]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 305:  91%|██████▎| 61/67 [00:18<00:01,  3.32it/s, loss=2.13, v_num=1, train_loss_step=0.656, val_loss=2.740, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 305:  93%|██████▍| 62/67 [00:18<00:01,  3.32it/s, loss=2.13, v_num=1, train_loss_step=0.656, val_loss=2.740, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 305:  94%|██████▌| 63/67 [00:19<00:01,  3.31it/s, loss=2.13, v_num=1, train_loss_step=0.656, val_loss=2.740, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 305:  96%|██████▋| 64/67 [00:19<00:00,  3.31it/s, loss=2.13, v_num=1, train_loss_step=0.656, val_loss=2.740, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 305:  97%|██████▊| 65/67 [00:19<00:00,  3.31it/s, loss=2.13, v_num=1, train_loss_step=0.656, val_loss=2.740, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 305:  99%|██████▉| 66/67 [00:19<00:00,  3.30it/s, loss=2.13, v_num=1, train_loss_step=0.656, val_loss=2.740, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 305: 100%|███████| 67/67 [00:20<00:00,  3.23it/s, loss=2.13, v_num=1, train_loss_step=0.656, val_loss=2.740, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 306:  45%|███▏   | 30/67 [00:05<00:07,  5.15it/s, loss=2.23, v_num=1, train_loss_step=2.650, val_loss=2.740, train_loss_epoch=2.250]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 306:  46%|███▏   | 31/67 [00:07<00:09,  3.95it/s, loss=2.23, v_num=1, train_loss_step=2.650, val_loss=2.740, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 306:  48%|███▎   | 32/67 [00:08<00:08,  3.91it/s, loss=2.23, v_num=1, train_loss_step=2.650, val_loss=2.740, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 306:  49%|███▍   | 33/67 [00:08<00:08,  3.87it/s, loss=2.23, v_num=1, train_loss_step=2.650, val_loss=2.740, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 306:  51%|███▌   | 34/67 [00:08<00:08,  3.84it/s, loss=2.23, v_num=1, train_loss_step=2.650, val_loss=2.740, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 306:  52%|███▋   | 35/67 [00:09<00:08,  3.81it/s, loss=2.23, v_num=1, train_loss_step=2.650, val_loss=2.740, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 306:  54%|███▊   | 36/67 [00:09<00:08,  3.78it/s, loss=2.23, v_num=1, train_loss_step=2.650, val_loss=2.740, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 306:  55%|███▊   | 37/67 [00:09<00:08,  3.75it/s, loss=2.23, v_num=1, train_loss_step=2.650, val_loss=2.740, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 306:  57%|███▉   | 38/67 [00:10<00:07,  3.72it/s, loss=2.23, v_num=1, train_loss_step=2.650, val_loss=2.740, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 306:  58%|████   | 39/67 [00:10<00:07,  3.70it/s, loss=2.23, v_num=1, train_loss_step=2.650, val_loss=2.740, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 306:  60%|████▏  | 40/67 [00:10<00:07,  3.68it/s, loss=2.23, v_num=1, train_loss_step=2.650, val_loss=2.740, train_loss_epoch=2.250]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 306:  61%|████▎  | 41/67 [00:11<00:07,  3.61it/s, loss=2.23, v_num=1, train_loss_step=2.650, val_loss=2.740, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 306:  63%|████▍  | 42/67 [00:11<00:06,  3.60it/s, loss=2.23, v_num=1, train_loss_step=2.650, val_loss=2.740, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 306:  64%|████▍  | 43/67 [00:12<00:06,  3.58it/s, loss=2.23, v_num=1, train_loss_step=2.650, val_loss=2.740, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 306:  66%|████▌  | 44/67 [00:12<00:06,  3.56it/s, loss=2.23, v_num=1, train_loss_step=2.650, val_loss=2.740, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 306:  67%|████▋  | 45/67 [00:12<00:06,  3.55it/s, loss=2.23, v_num=1, train_loss_step=2.650, val_loss=2.740, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 306:  69%|████▊  | 46/67 [00:13<00:05,  3.53it/s, loss=2.23, v_num=1, train_loss_step=2.650, val_loss=2.740, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 306:  70%|████▉  | 47/67 [00:13<00:05,  3.52it/s, loss=2.23, v_num=1, train_loss_step=2.650, val_loss=2.740, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 306:  72%|█████  | 48/67 [00:13<00:05,  3.51it/s, loss=2.23, v_num=1, train_loss_step=2.650, val_loss=2.740, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 306:  73%|█████  | 49/67 [00:14<00:05,  3.50it/s, loss=2.23, v_num=1, train_loss_step=2.650, val_loss=2.740, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 306:  75%|█████▏ | 50/67 [00:14<00:04,  3.49it/s, loss=2.23, v_num=1, train_loss_step=2.650, val_loss=2.740, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 306:  76%|█████▎ | 51/67 [00:14<00:04,  3.44it/s, loss=2.23, v_num=1, train_loss_step=2.650, val_loss=2.740, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 306:  78%|█████▍ | 52/67 [00:15<00:04,  3.43it/s, loss=2.23, v_num=1, train_loss_step=2.650, val_loss=2.740, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 306:  79%|█████▌ | 53/67 [00:15<00:04,  3.42it/s, loss=2.23, v_num=1, train_loss_step=2.650, val_loss=2.740, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 306:  81%|█████▋ | 54/67 [00:15<00:03,  3.42it/s, loss=2.23, v_num=1, train_loss_step=2.650, val_loss=2.740, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 306:  82%|█████▋ | 55/67 [00:16<00:03,  3.40it/s, loss=2.23, v_num=1, train_loss_step=2.650, val_loss=2.740, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 306:  84%|█████▊ | 56/67 [00:16<00:03,  3.39it/s, loss=2.23, v_num=1, train_loss_step=2.650, val_loss=2.740, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 306:  85%|█████▉ | 57/67 [00:16<00:02,  3.38it/s, loss=2.23, v_num=1, train_loss_step=2.650, val_loss=2.740, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 306:  87%|██████ | 58/67 [00:17<00:02,  3.37it/s, loss=2.23, v_num=1, train_loss_step=2.650, val_loss=2.740, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 306:  88%|██████▏| 59/67 [00:17<00:02,  3.37it/s, loss=2.23, v_num=1, train_loss_step=2.650, val_loss=2.740, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 306:  90%|██████▎| 60/67 [00:17<00:02,  3.36it/s, loss=2.23, v_num=1, train_loss_step=2.650, val_loss=2.740, train_loss_epoch=2.250]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 306:  91%|██████▎| 61/67 [00:18<00:01,  3.33it/s, loss=2.23, v_num=1, train_loss_step=2.650, val_loss=2.740, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 306:  93%|██████▍| 62/67 [00:18<00:01,  3.32it/s, loss=2.23, v_num=1, train_loss_step=2.650, val_loss=2.740, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 306:  94%|██████▌| 63/67 [00:18<00:01,  3.32it/s, loss=2.23, v_num=1, train_loss_step=2.650, val_loss=2.740, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 306:  96%|██████▋| 64/67 [00:19<00:00,  3.31it/s, loss=2.23, v_num=1, train_loss_step=2.650, val_loss=2.740, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 306:  97%|██████▊| 65/67 [00:19<00:00,  3.31it/s, loss=2.23, v_num=1, train_loss_step=2.650, val_loss=2.740, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 306:  99%|██████▉| 66/67 [00:19<00:00,  3.31it/s, loss=2.23, v_num=1, train_loss_step=2.650, val_loss=2.740, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 306: 100%|███████| 67/67 [00:20<00:00,  3.23it/s, loss=2.23, v_num=1, train_loss_step=2.650, val_loss=2.740, train_loss_epoch=2.250]\u001b[A\n",
      "Epoch 307:  45%|███▏   | 30/67 [00:05<00:07,  5.14it/s, loss=1.65, v_num=1, train_loss_step=1.740, val_loss=2.740, train_loss_epoch=2.020]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 307:  46%|███▏   | 31/67 [00:07<00:09,  3.90it/s, loss=1.65, v_num=1, train_loss_step=1.740, val_loss=2.740, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 307:  48%|███▎   | 32/67 [00:08<00:09,  3.86it/s, loss=1.65, v_num=1, train_loss_step=1.740, val_loss=2.740, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 307:  49%|███▍   | 33/67 [00:08<00:08,  3.82it/s, loss=1.65, v_num=1, train_loss_step=1.740, val_loss=2.740, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 307:  51%|███▌   | 34/67 [00:08<00:08,  3.79it/s, loss=1.65, v_num=1, train_loss_step=1.740, val_loss=2.740, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 307:  52%|███▋   | 35/67 [00:09<00:08,  3.76it/s, loss=1.65, v_num=1, train_loss_step=1.740, val_loss=2.740, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 307:  54%|███▊   | 36/67 [00:09<00:08,  3.73it/s, loss=1.65, v_num=1, train_loss_step=1.740, val_loss=2.740, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 307:  55%|███▊   | 37/67 [00:09<00:08,  3.71it/s, loss=1.65, v_num=1, train_loss_step=1.740, val_loss=2.740, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 307:  57%|███▉   | 38/67 [00:10<00:07,  3.69it/s, loss=1.65, v_num=1, train_loss_step=1.740, val_loss=2.740, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 307:  58%|████   | 39/67 [00:10<00:07,  3.67it/s, loss=1.65, v_num=1, train_loss_step=1.740, val_loss=2.740, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 307:  60%|████▏  | 40/67 [00:10<00:07,  3.65it/s, loss=1.65, v_num=1, train_loss_step=1.740, val_loss=2.740, train_loss_epoch=2.020]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 307:  61%|████▎  | 41/67 [00:11<00:07,  3.58it/s, loss=1.65, v_num=1, train_loss_step=1.740, val_loss=2.740, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 307:  63%|████▍  | 42/67 [00:11<00:07,  3.56it/s, loss=1.65, v_num=1, train_loss_step=1.740, val_loss=2.740, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 307:  64%|████▍  | 43/67 [00:12<00:06,  3.55it/s, loss=1.65, v_num=1, train_loss_step=1.740, val_loss=2.740, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 307:  66%|████▌  | 44/67 [00:12<00:06,  3.53it/s, loss=1.65, v_num=1, train_loss_step=1.740, val_loss=2.740, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 307:  67%|████▋  | 45/67 [00:12<00:06,  3.52it/s, loss=1.65, v_num=1, train_loss_step=1.740, val_loss=2.740, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 307:  69%|████▊  | 46/67 [00:13<00:05,  3.50it/s, loss=1.65, v_num=1, train_loss_step=1.740, val_loss=2.740, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 307:  70%|████▉  | 47/67 [00:13<00:05,  3.49it/s, loss=1.65, v_num=1, train_loss_step=1.740, val_loss=2.740, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 307:  72%|█████  | 48/67 [00:13<00:05,  3.48it/s, loss=1.65, v_num=1, train_loss_step=1.740, val_loss=2.740, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 307:  73%|█████  | 49/67 [00:14<00:05,  3.47it/s, loss=1.65, v_num=1, train_loss_step=1.740, val_loss=2.740, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 307:  75%|█████▏ | 50/67 [00:14<00:04,  3.46it/s, loss=1.65, v_num=1, train_loss_step=1.740, val_loss=2.740, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 307:  76%|█████▎ | 51/67 [00:14<00:04,  3.41it/s, loss=1.65, v_num=1, train_loss_step=1.740, val_loss=2.740, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 307:  78%|█████▍ | 52/67 [00:15<00:04,  3.40it/s, loss=1.65, v_num=1, train_loss_step=1.740, val_loss=2.740, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 307:  79%|█████▌ | 53/67 [00:15<00:04,  3.38it/s, loss=1.65, v_num=1, train_loss_step=1.740, val_loss=2.740, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 307:  81%|█████▋ | 54/67 [00:15<00:03,  3.38it/s, loss=1.65, v_num=1, train_loss_step=1.740, val_loss=2.740, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 307:  82%|█████▋ | 55/67 [00:16<00:03,  3.37it/s, loss=1.65, v_num=1, train_loss_step=1.740, val_loss=2.740, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 307:  84%|█████▊ | 56/67 [00:16<00:03,  3.36it/s, loss=1.65, v_num=1, train_loss_step=1.740, val_loss=2.740, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 307:  85%|█████▉ | 57/67 [00:16<00:02,  3.36it/s, loss=1.65, v_num=1, train_loss_step=1.740, val_loss=2.740, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 307:  87%|██████ | 58/67 [00:17<00:02,  3.35it/s, loss=1.65, v_num=1, train_loss_step=1.740, val_loss=2.740, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 307:  88%|██████▏| 59/67 [00:17<00:02,  3.34it/s, loss=1.65, v_num=1, train_loss_step=1.740, val_loss=2.740, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 307:  90%|██████▎| 60/67 [00:17<00:02,  3.34it/s, loss=1.65, v_num=1, train_loss_step=1.740, val_loss=2.740, train_loss_epoch=2.020]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 307:  91%|██████▎| 61/67 [00:18<00:01,  3.30it/s, loss=1.65, v_num=1, train_loss_step=1.740, val_loss=2.740, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 307:  93%|██████▍| 62/67 [00:18<00:01,  3.30it/s, loss=1.65, v_num=1, train_loss_step=1.740, val_loss=2.740, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 307:  94%|██████▌| 63/67 [00:19<00:01,  3.29it/s, loss=1.65, v_num=1, train_loss_step=1.740, val_loss=2.740, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 307:  96%|██████▋| 64/67 [00:19<00:00,  3.29it/s, loss=1.65, v_num=1, train_loss_step=1.740, val_loss=2.740, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 307:  97%|██████▊| 65/67 [00:19<00:00,  3.29it/s, loss=1.65, v_num=1, train_loss_step=1.740, val_loss=2.740, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 307:  99%|██████▉| 66/67 [00:20<00:00,  3.28it/s, loss=1.65, v_num=1, train_loss_step=1.740, val_loss=2.740, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 307: 100%|███████| 67/67 [00:20<00:00,  3.21it/s, loss=1.65, v_num=1, train_loss_step=1.740, val_loss=2.730, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 308:  45%|███▏   | 30/67 [00:05<00:07,  5.09it/s, loss=1.68, v_num=1, train_loss_step=2.230, val_loss=2.730, train_loss_epoch=1.700]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 308:  46%|███▏   | 31/67 [00:08<00:09,  3.87it/s, loss=1.68, v_num=1, train_loss_step=2.230, val_loss=2.730, train_loss_epoch=1.700]\u001b[A\n",
      "Epoch 308:  48%|███▎   | 32/67 [00:08<00:09,  3.84it/s, loss=1.68, v_num=1, train_loss_step=2.230, val_loss=2.730, train_loss_epoch=1.700]\u001b[A\n",
      "Epoch 308:  49%|███▍   | 33/67 [00:08<00:08,  3.81it/s, loss=1.68, v_num=1, train_loss_step=2.230, val_loss=2.730, train_loss_epoch=1.700]\u001b[A\n",
      "Epoch 308:  51%|███▌   | 34/67 [00:09<00:08,  3.78it/s, loss=1.68, v_num=1, train_loss_step=2.230, val_loss=2.730, train_loss_epoch=1.700]\u001b[A\n",
      "Epoch 308:  52%|███▋   | 35/67 [00:09<00:08,  3.75it/s, loss=1.68, v_num=1, train_loss_step=2.230, val_loss=2.730, train_loss_epoch=1.700]\u001b[A\n",
      "Epoch 308:  54%|███▊   | 36/67 [00:09<00:08,  3.73it/s, loss=1.68, v_num=1, train_loss_step=2.230, val_loss=2.730, train_loss_epoch=1.700]\u001b[A\n",
      "Epoch 308:  55%|███▊   | 37/67 [00:09<00:08,  3.70it/s, loss=1.68, v_num=1, train_loss_step=2.230, val_loss=2.730, train_loss_epoch=1.700]\u001b[A\n",
      "Epoch 308:  57%|███▉   | 38/67 [00:10<00:07,  3.68it/s, loss=1.68, v_num=1, train_loss_step=2.230, val_loss=2.730, train_loss_epoch=1.700]\u001b[A\n",
      "Epoch 308:  58%|████   | 39/67 [00:10<00:07,  3.66it/s, loss=1.68, v_num=1, train_loss_step=2.230, val_loss=2.730, train_loss_epoch=1.700]\u001b[A\n",
      "Epoch 308:  60%|████▏  | 40/67 [00:11<00:07,  3.63it/s, loss=1.68, v_num=1, train_loss_step=2.230, val_loss=2.730, train_loss_epoch=1.700]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 308:  61%|████▎  | 41/67 [00:11<00:07,  3.57it/s, loss=1.68, v_num=1, train_loss_step=2.230, val_loss=2.730, train_loss_epoch=1.700]\u001b[A\n",
      "Epoch 308:  63%|████▍  | 42/67 [00:11<00:07,  3.55it/s, loss=1.68, v_num=1, train_loss_step=2.230, val_loss=2.730, train_loss_epoch=1.700]\u001b[A\n",
      "Epoch 308:  64%|████▍  | 43/67 [00:12<00:06,  3.53it/s, loss=1.68, v_num=1, train_loss_step=2.230, val_loss=2.730, train_loss_epoch=1.700]\u001b[A\n",
      "Epoch 308:  66%|████▌  | 44/67 [00:12<00:06,  3.52it/s, loss=1.68, v_num=1, train_loss_step=2.230, val_loss=2.730, train_loss_epoch=1.700]\u001b[A\n",
      "Epoch 308:  67%|████▋  | 45/67 [00:12<00:06,  3.51it/s, loss=1.68, v_num=1, train_loss_step=2.230, val_loss=2.730, train_loss_epoch=1.700]\u001b[A\n",
      "Epoch 308:  69%|████▊  | 46/67 [00:13<00:06,  3.49it/s, loss=1.68, v_num=1, train_loss_step=2.230, val_loss=2.730, train_loss_epoch=1.700]\u001b[A\n",
      "Epoch 308:  70%|████▉  | 47/67 [00:13<00:05,  3.48it/s, loss=1.68, v_num=1, train_loss_step=2.230, val_loss=2.730, train_loss_epoch=1.700]\u001b[A\n",
      "Epoch 308:  72%|█████  | 48/67 [00:13<00:05,  3.47it/s, loss=1.68, v_num=1, train_loss_step=2.230, val_loss=2.730, train_loss_epoch=1.700]\u001b[A\n",
      "Epoch 308:  73%|█████  | 49/67 [00:14<00:05,  3.46it/s, loss=1.68, v_num=1, train_loss_step=2.230, val_loss=2.730, train_loss_epoch=1.700]\u001b[A\n",
      "Epoch 308:  75%|█████▏ | 50/67 [00:14<00:04,  3.45it/s, loss=1.68, v_num=1, train_loss_step=2.230, val_loss=2.730, train_loss_epoch=1.700]\u001b[A\n",
      "Epoch 308:  76%|█████▎ | 51/67 [00:14<00:04,  3.40it/s, loss=1.68, v_num=1, train_loss_step=2.230, val_loss=2.730, train_loss_epoch=1.700]\u001b[A\n",
      "Epoch 308:  78%|█████▍ | 52/67 [00:15<00:04,  3.39it/s, loss=1.68, v_num=1, train_loss_step=2.230, val_loss=2.730, train_loss_epoch=1.700]\u001b[A\n",
      "Epoch 308:  79%|█████▌ | 53/67 [00:15<00:04,  3.38it/s, loss=1.68, v_num=1, train_loss_step=2.230, val_loss=2.730, train_loss_epoch=1.700]\u001b[A\n",
      "Epoch 308:  81%|█████▋ | 54/67 [00:16<00:03,  3.37it/s, loss=1.68, v_num=1, train_loss_step=2.230, val_loss=2.730, train_loss_epoch=1.700]\u001b[A\n",
      "Epoch 308:  82%|█████▋ | 55/67 [00:16<00:03,  3.36it/s, loss=1.68, v_num=1, train_loss_step=2.230, val_loss=2.730, train_loss_epoch=1.700]\u001b[A\n",
      "Epoch 308:  84%|█████▊ | 56/67 [00:16<00:03,  3.35it/s, loss=1.68, v_num=1, train_loss_step=2.230, val_loss=2.730, train_loss_epoch=1.700]\u001b[A\n",
      "Epoch 308:  85%|█████▉ | 57/67 [00:17<00:02,  3.35it/s, loss=1.68, v_num=1, train_loss_step=2.230, val_loss=2.730, train_loss_epoch=1.700]\u001b[A\n",
      "Epoch 308:  87%|██████ | 58/67 [00:17<00:02,  3.34it/s, loss=1.68, v_num=1, train_loss_step=2.230, val_loss=2.730, train_loss_epoch=1.700]\u001b[A\n",
      "Epoch 308:  88%|██████▏| 59/67 [00:17<00:02,  3.33it/s, loss=1.68, v_num=1, train_loss_step=2.230, val_loss=2.730, train_loss_epoch=1.700]\u001b[A\n",
      "Epoch 308:  90%|██████▎| 60/67 [00:18<00:02,  3.33it/s, loss=1.68, v_num=1, train_loss_step=2.230, val_loss=2.730, train_loss_epoch=1.700]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 308:  91%|██████▎| 61/67 [00:18<00:01,  3.29it/s, loss=1.68, v_num=1, train_loss_step=2.230, val_loss=2.730, train_loss_epoch=1.700]\u001b[A\n",
      "Epoch 308:  93%|██████▍| 62/67 [00:18<00:01,  3.29it/s, loss=1.68, v_num=1, train_loss_step=2.230, val_loss=2.730, train_loss_epoch=1.700]\u001b[A\n",
      "Epoch 308:  94%|██████▌| 63/67 [00:19<00:01,  3.28it/s, loss=1.68, v_num=1, train_loss_step=2.230, val_loss=2.730, train_loss_epoch=1.700]\u001b[A\n",
      "Epoch 308:  96%|██████▋| 64/67 [00:19<00:00,  3.28it/s, loss=1.68, v_num=1, train_loss_step=2.230, val_loss=2.730, train_loss_epoch=1.700]\u001b[A\n",
      "Epoch 308:  97%|██████▊| 65/67 [00:19<00:00,  3.28it/s, loss=1.68, v_num=1, train_loss_step=2.230, val_loss=2.730, train_loss_epoch=1.700]\u001b[A\n",
      "Epoch 308:  99%|██████▉| 66/67 [00:20<00:00,  3.27it/s, loss=1.68, v_num=1, train_loss_step=2.230, val_loss=2.730, train_loss_epoch=1.700]\u001b[A\n",
      "Epoch 308: 100%|███████| 67/67 [00:20<00:00,  3.21it/s, loss=1.68, v_num=1, train_loss_step=2.230, val_loss=2.730, train_loss_epoch=1.700]\u001b[A\n",
      "Epoch 309:  45%|███▏   | 30/67 [00:05<00:07,  5.23it/s, loss=1.67, v_num=1, train_loss_step=1.050, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 309:  46%|███▏   | 31/67 [00:07<00:09,  3.94it/s, loss=1.67, v_num=1, train_loss_step=1.050, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 309:  48%|███▎   | 32/67 [00:08<00:08,  3.91it/s, loss=1.67, v_num=1, train_loss_step=1.050, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 309:  49%|███▍   | 33/67 [00:08<00:08,  3.87it/s, loss=1.67, v_num=1, train_loss_step=1.050, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 309:  51%|███▌   | 34/67 [00:08<00:08,  3.83it/s, loss=1.67, v_num=1, train_loss_step=1.050, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 309:  52%|███▋   | 35/67 [00:09<00:08,  3.81it/s, loss=1.67, v_num=1, train_loss_step=1.050, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 309:  54%|███▊   | 36/67 [00:09<00:08,  3.76it/s, loss=1.67, v_num=1, train_loss_step=1.050, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 309:  55%|███▊   | 37/67 [00:09<00:08,  3.73it/s, loss=1.67, v_num=1, train_loss_step=1.050, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 309:  57%|███▉   | 38/67 [00:10<00:07,  3.71it/s, loss=1.67, v_num=1, train_loss_step=1.050, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 309:  58%|████   | 39/67 [00:10<00:07,  3.69it/s, loss=1.67, v_num=1, train_loss_step=1.050, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 309:  60%|████▏  | 40/67 [00:10<00:07,  3.67it/s, loss=1.67, v_num=1, train_loss_step=1.050, val_loss=2.730, train_loss_epoch=1.740]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 309:  61%|████▎  | 41/67 [00:11<00:07,  3.59it/s, loss=1.67, v_num=1, train_loss_step=1.050, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 309:  63%|████▍  | 42/67 [00:11<00:06,  3.57it/s, loss=1.67, v_num=1, train_loss_step=1.050, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 309:  64%|████▍  | 43/67 [00:12<00:06,  3.56it/s, loss=1.67, v_num=1, train_loss_step=1.050, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 309:  66%|████▌  | 44/67 [00:12<00:06,  3.54it/s, loss=1.67, v_num=1, train_loss_step=1.050, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 309:  67%|████▋  | 45/67 [00:12<00:06,  3.53it/s, loss=1.67, v_num=1, train_loss_step=1.050, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 309:  69%|████▊  | 46/67 [00:13<00:05,  3.52it/s, loss=1.67, v_num=1, train_loss_step=1.050, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 309:  70%|████▉  | 47/67 [00:13<00:05,  3.50it/s, loss=1.67, v_num=1, train_loss_step=1.050, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 309:  72%|█████  | 48/67 [00:13<00:05,  3.49it/s, loss=1.67, v_num=1, train_loss_step=1.050, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 309:  73%|█████  | 49/67 [00:14<00:05,  3.48it/s, loss=1.67, v_num=1, train_loss_step=1.050, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 309:  75%|█████▏ | 50/67 [00:14<00:04,  3.47it/s, loss=1.67, v_num=1, train_loss_step=1.050, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 309:  76%|█████▎ | 51/67 [00:14<00:04,  3.42it/s, loss=1.67, v_num=1, train_loss_step=1.050, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 309:  78%|█████▍ | 52/67 [00:15<00:04,  3.41it/s, loss=1.67, v_num=1, train_loss_step=1.050, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 309:  79%|█████▌ | 53/67 [00:15<00:04,  3.40it/s, loss=1.67, v_num=1, train_loss_step=1.050, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 309:  81%|█████▋ | 54/67 [00:15<00:03,  3.40it/s, loss=1.67, v_num=1, train_loss_step=1.050, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 309:  82%|█████▋ | 55/67 [00:16<00:03,  3.39it/s, loss=1.67, v_num=1, train_loss_step=1.050, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 309:  84%|█████▊ | 56/67 [00:16<00:03,  3.38it/s, loss=1.67, v_num=1, train_loss_step=1.050, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 309:  85%|█████▉ | 57/67 [00:16<00:02,  3.37it/s, loss=1.67, v_num=1, train_loss_step=1.050, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 309:  87%|██████ | 58/67 [00:17<00:02,  3.37it/s, loss=1.67, v_num=1, train_loss_step=1.050, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 309:  88%|██████▏| 59/67 [00:17<00:02,  3.36it/s, loss=1.67, v_num=1, train_loss_step=1.050, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 309:  90%|██████▎| 60/67 [00:17<00:02,  3.35it/s, loss=1.67, v_num=1, train_loss_step=1.050, val_loss=2.730, train_loss_epoch=1.740]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 309:  91%|██████▎| 61/67 [00:18<00:01,  3.30it/s, loss=1.67, v_num=1, train_loss_step=1.050, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 309:  93%|██████▍| 62/67 [00:18<00:01,  3.30it/s, loss=1.67, v_num=1, train_loss_step=1.050, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 309:  94%|██████▌| 63/67 [00:19<00:01,  3.29it/s, loss=1.67, v_num=1, train_loss_step=1.050, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 309:  96%|██████▋| 64/67 [00:19<00:00,  3.29it/s, loss=1.67, v_num=1, train_loss_step=1.050, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 309:  97%|██████▊| 65/67 [00:19<00:00,  3.29it/s, loss=1.67, v_num=1, train_loss_step=1.050, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 309:  99%|██████▉| 66/67 [00:20<00:00,  3.29it/s, loss=1.67, v_num=1, train_loss_step=1.050, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 309: 100%|███████| 67/67 [00:20<00:00,  3.22it/s, loss=1.67, v_num=1, train_loss_step=1.050, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 310:  45%|███▏   | 30/67 [00:05<00:07,  5.25it/s, loss=1.51, v_num=1, train_loss_step=0.491, val_loss=2.730, train_loss_epoch=1.590]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 310:  46%|███▏   | 31/67 [00:07<00:08,  4.01it/s, loss=1.51, v_num=1, train_loss_step=0.491, val_loss=2.730, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 310:  48%|███▎   | 32/67 [00:08<00:08,  3.97it/s, loss=1.51, v_num=1, train_loss_step=0.491, val_loss=2.730, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 310:  49%|███▍   | 33/67 [00:08<00:08,  3.93it/s, loss=1.51, v_num=1, train_loss_step=0.491, val_loss=2.730, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 310:  51%|███▌   | 34/67 [00:08<00:08,  3.90it/s, loss=1.51, v_num=1, train_loss_step=0.491, val_loss=2.730, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 310:  52%|███▋   | 35/67 [00:09<00:08,  3.87it/s, loss=1.51, v_num=1, train_loss_step=0.491, val_loss=2.730, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 310:  54%|███▊   | 36/67 [00:09<00:08,  3.84it/s, loss=1.51, v_num=1, train_loss_step=0.491, val_loss=2.730, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 310:  55%|███▊   | 37/67 [00:09<00:07,  3.81it/s, loss=1.51, v_num=1, train_loss_step=0.491, val_loss=2.730, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 310:  57%|███▉   | 38/67 [00:10<00:07,  3.78it/s, loss=1.51, v_num=1, train_loss_step=0.491, val_loss=2.730, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 310:  58%|████   | 39/67 [00:10<00:07,  3.76it/s, loss=1.51, v_num=1, train_loss_step=0.491, val_loss=2.730, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 310:  60%|████▏  | 40/67 [00:10<00:07,  3.73it/s, loss=1.51, v_num=1, train_loss_step=0.491, val_loss=2.730, train_loss_epoch=1.590]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 310:  61%|████▎  | 41/67 [00:11<00:07,  3.66it/s, loss=1.51, v_num=1, train_loss_step=0.491, val_loss=2.730, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 310:  63%|████▍  | 42/67 [00:11<00:06,  3.64it/s, loss=1.51, v_num=1, train_loss_step=0.491, val_loss=2.730, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 310:  64%|████▍  | 43/67 [00:11<00:06,  3.63it/s, loss=1.51, v_num=1, train_loss_step=0.491, val_loss=2.730, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 310:  66%|████▌  | 44/67 [00:12<00:06,  3.61it/s, loss=1.51, v_num=1, train_loss_step=0.491, val_loss=2.730, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 310:  67%|████▋  | 45/67 [00:12<00:06,  3.59it/s, loss=1.51, v_num=1, train_loss_step=0.491, val_loss=2.730, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 310:  69%|████▊  | 46/67 [00:12<00:05,  3.58it/s, loss=1.51, v_num=1, train_loss_step=0.491, val_loss=2.730, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 310:  70%|████▉  | 47/67 [00:13<00:05,  3.56it/s, loss=1.51, v_num=1, train_loss_step=0.491, val_loss=2.730, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 310:  72%|█████  | 48/67 [00:13<00:05,  3.55it/s, loss=1.51, v_num=1, train_loss_step=0.491, val_loss=2.730, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 310:  73%|█████  | 49/67 [00:13<00:05,  3.53it/s, loss=1.51, v_num=1, train_loss_step=0.491, val_loss=2.730, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 310:  75%|█████▏ | 50/67 [00:14<00:04,  3.52it/s, loss=1.51, v_num=1, train_loss_step=0.491, val_loss=2.730, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 310:  76%|█████▎ | 51/67 [00:14<00:04,  3.47it/s, loss=1.51, v_num=1, train_loss_step=0.491, val_loss=2.730, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 310:  78%|█████▍ | 52/67 [00:15<00:04,  3.46it/s, loss=1.51, v_num=1, train_loss_step=0.491, val_loss=2.730, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 310:  79%|█████▌ | 53/67 [00:15<00:04,  3.45it/s, loss=1.51, v_num=1, train_loss_step=0.491, val_loss=2.730, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 310:  81%|█████▋ | 54/67 [00:15<00:03,  3.44it/s, loss=1.51, v_num=1, train_loss_step=0.491, val_loss=2.730, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 310:  82%|█████▋ | 55/67 [00:16<00:03,  3.43it/s, loss=1.51, v_num=1, train_loss_step=0.491, val_loss=2.730, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 310:  84%|█████▊ | 56/67 [00:16<00:03,  3.42it/s, loss=1.51, v_num=1, train_loss_step=0.491, val_loss=2.730, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 310:  85%|█████▉ | 57/67 [00:16<00:02,  3.42it/s, loss=1.51, v_num=1, train_loss_step=0.491, val_loss=2.730, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 310:  87%|██████ | 58/67 [00:17<00:02,  3.41it/s, loss=1.51, v_num=1, train_loss_step=0.491, val_loss=2.730, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 310:  88%|██████▏| 59/67 [00:17<00:02,  3.40it/s, loss=1.51, v_num=1, train_loss_step=0.491, val_loss=2.730, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 310:  90%|██████▎| 60/67 [00:17<00:02,  3.39it/s, loss=1.51, v_num=1, train_loss_step=0.491, val_loss=2.730, train_loss_epoch=1.590]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 310:  91%|██████▎| 61/67 [00:18<00:01,  3.35it/s, loss=1.51, v_num=1, train_loss_step=0.491, val_loss=2.730, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 310:  93%|██████▍| 62/67 [00:18<00:01,  3.34it/s, loss=1.51, v_num=1, train_loss_step=0.491, val_loss=2.730, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 310:  94%|██████▌| 63/67 [00:18<00:01,  3.34it/s, loss=1.51, v_num=1, train_loss_step=0.491, val_loss=2.730, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 310:  96%|██████▋| 64/67 [00:19<00:00,  3.33it/s, loss=1.51, v_num=1, train_loss_step=0.491, val_loss=2.730, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 310:  97%|██████▊| 65/67 [00:19<00:00,  3.33it/s, loss=1.51, v_num=1, train_loss_step=0.491, val_loss=2.730, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 310:  99%|██████▉| 66/67 [00:19<00:00,  3.32it/s, loss=1.51, v_num=1, train_loss_step=0.491, val_loss=2.730, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 310: 100%|███████| 67/67 [00:20<00:00,  3.25it/s, loss=1.51, v_num=1, train_loss_step=0.491, val_loss=2.730, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 311:  45%|███▏   | 30/67 [00:05<00:07,  5.26it/s, loss=1.72, v_num=1, train_loss_step=2.140, val_loss=2.730, train_loss_epoch=1.710]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 311:  46%|███▏   | 31/67 [00:07<00:09,  3.98it/s, loss=1.72, v_num=1, train_loss_step=2.140, val_loss=2.730, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 311:  48%|███▎   | 32/67 [00:08<00:08,  3.95it/s, loss=1.72, v_num=1, train_loss_step=2.140, val_loss=2.730, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 311:  49%|███▍   | 33/67 [00:08<00:08,  3.91it/s, loss=1.72, v_num=1, train_loss_step=2.140, val_loss=2.730, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 311:  51%|███▌   | 34/67 [00:08<00:08,  3.87it/s, loss=1.72, v_num=1, train_loss_step=2.140, val_loss=2.730, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 311:  52%|███▋   | 35/67 [00:09<00:08,  3.84it/s, loss=1.72, v_num=1, train_loss_step=2.140, val_loss=2.730, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 311:  54%|███▊   | 36/67 [00:09<00:08,  3.82it/s, loss=1.72, v_num=1, train_loss_step=2.140, val_loss=2.730, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 311:  55%|███▊   | 37/67 [00:09<00:07,  3.78it/s, loss=1.72, v_num=1, train_loss_step=2.140, val_loss=2.730, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 311:  57%|███▉   | 38/67 [00:10<00:07,  3.76it/s, loss=1.72, v_num=1, train_loss_step=2.140, val_loss=2.730, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 311:  58%|████   | 39/67 [00:10<00:07,  3.74it/s, loss=1.72, v_num=1, train_loss_step=2.140, val_loss=2.730, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 311:  60%|████▏  | 40/67 [00:10<00:07,  3.71it/s, loss=1.72, v_num=1, train_loss_step=2.140, val_loss=2.730, train_loss_epoch=1.710]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 311:  61%|████▎  | 41/67 [00:11<00:07,  3.63it/s, loss=1.72, v_num=1, train_loss_step=2.140, val_loss=2.730, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 311:  63%|████▍  | 42/67 [00:11<00:06,  3.61it/s, loss=1.72, v_num=1, train_loss_step=2.140, val_loss=2.730, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 311:  64%|████▍  | 43/67 [00:11<00:06,  3.59it/s, loss=1.72, v_num=1, train_loss_step=2.140, val_loss=2.730, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 311:  66%|████▌  | 44/67 [00:12<00:06,  3.58it/s, loss=1.72, v_num=1, train_loss_step=2.140, val_loss=2.730, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 311:  67%|████▋  | 45/67 [00:12<00:06,  3.56it/s, loss=1.72, v_num=1, train_loss_step=2.140, val_loss=2.730, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 311:  69%|████▊  | 46/67 [00:12<00:05,  3.55it/s, loss=1.72, v_num=1, train_loss_step=2.140, val_loss=2.730, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 311:  70%|████▉  | 47/67 [00:13<00:05,  3.53it/s, loss=1.72, v_num=1, train_loss_step=2.140, val_loss=2.730, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 311:  72%|█████  | 48/67 [00:13<00:05,  3.52it/s, loss=1.72, v_num=1, train_loss_step=2.140, val_loss=2.730, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 311:  73%|█████  | 49/67 [00:13<00:05,  3.51it/s, loss=1.72, v_num=1, train_loss_step=2.140, val_loss=2.730, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 311:  75%|█████▏ | 50/67 [00:14<00:04,  3.50it/s, loss=1.72, v_num=1, train_loss_step=2.140, val_loss=2.730, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 311:  76%|█████▎ | 51/67 [00:14<00:04,  3.46it/s, loss=1.72, v_num=1, train_loss_step=2.140, val_loss=2.730, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 311:  78%|█████▍ | 52/67 [00:15<00:04,  3.44it/s, loss=1.72, v_num=1, train_loss_step=2.140, val_loss=2.730, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 311:  79%|█████▌ | 53/67 [00:15<00:04,  3.44it/s, loss=1.72, v_num=1, train_loss_step=2.140, val_loss=2.730, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 311:  81%|█████▋ | 54/67 [00:15<00:03,  3.43it/s, loss=1.72, v_num=1, train_loss_step=2.140, val_loss=2.730, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 311:  82%|█████▋ | 55/67 [00:16<00:03,  3.42it/s, loss=1.72, v_num=1, train_loss_step=2.140, val_loss=2.730, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 311:  84%|█████▊ | 56/67 [00:16<00:03,  3.41it/s, loss=1.72, v_num=1, train_loss_step=2.140, val_loss=2.730, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 311:  85%|█████▉ | 57/67 [00:16<00:02,  3.40it/s, loss=1.72, v_num=1, train_loss_step=2.140, val_loss=2.730, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 311:  87%|██████ | 58/67 [00:17<00:02,  3.39it/s, loss=1.72, v_num=1, train_loss_step=2.140, val_loss=2.730, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 311:  88%|██████▏| 59/67 [00:17<00:02,  3.39it/s, loss=1.72, v_num=1, train_loss_step=2.140, val_loss=2.730, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 311:  90%|██████▎| 60/67 [00:17<00:02,  3.38it/s, loss=1.72, v_num=1, train_loss_step=2.140, val_loss=2.730, train_loss_epoch=1.710]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 311:  91%|██████▎| 61/67 [00:18<00:01,  3.34it/s, loss=1.72, v_num=1, train_loss_step=2.140, val_loss=2.730, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 311:  93%|██████▍| 62/67 [00:18<00:01,  3.34it/s, loss=1.72, v_num=1, train_loss_step=2.140, val_loss=2.730, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 311:  94%|██████▌| 63/67 [00:18<00:01,  3.33it/s, loss=1.72, v_num=1, train_loss_step=2.140, val_loss=2.730, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 311:  96%|██████▋| 64/67 [00:19<00:00,  3.33it/s, loss=1.72, v_num=1, train_loss_step=2.140, val_loss=2.730, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 311:  97%|██████▊| 65/67 [00:19<00:00,  3.32it/s, loss=1.72, v_num=1, train_loss_step=2.140, val_loss=2.730, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 311:  99%|██████▉| 66/67 [00:19<00:00,  3.32it/s, loss=1.72, v_num=1, train_loss_step=2.140, val_loss=2.730, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 311: 100%|███████| 67/67 [00:20<00:00,  3.24it/s, loss=1.72, v_num=1, train_loss_step=2.140, val_loss=2.730, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 312:  45%|███▏   | 30/67 [00:05<00:07,  5.21it/s, loss=2.58, v_num=1, train_loss_step=15.20, val_loss=2.730, train_loss_epoch=1.520]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 312:  46%|███▏   | 31/67 [00:07<00:09,  3.94it/s, loss=2.58, v_num=1, train_loss_step=15.20, val_loss=2.730, train_loss_epoch=1.520]\u001b[A\n",
      "Epoch 312:  48%|███▎   | 32/67 [00:08<00:08,  3.90it/s, loss=2.58, v_num=1, train_loss_step=15.20, val_loss=2.730, train_loss_epoch=1.520]\u001b[A\n",
      "Epoch 312:  49%|███▍   | 33/67 [00:08<00:08,  3.86it/s, loss=2.58, v_num=1, train_loss_step=15.20, val_loss=2.730, train_loss_epoch=1.520]\u001b[A\n",
      "Epoch 312:  51%|███▌   | 34/67 [00:08<00:08,  3.83it/s, loss=2.58, v_num=1, train_loss_step=15.20, val_loss=2.730, train_loss_epoch=1.520]\u001b[A\n",
      "Epoch 312:  52%|███▋   | 35/67 [00:09<00:08,  3.79it/s, loss=2.58, v_num=1, train_loss_step=15.20, val_loss=2.730, train_loss_epoch=1.520]\u001b[A\n",
      "Epoch 312:  54%|███▊   | 36/67 [00:09<00:08,  3.77it/s, loss=2.58, v_num=1, train_loss_step=15.20, val_loss=2.730, train_loss_epoch=1.520]\u001b[A\n",
      "Epoch 312:  55%|███▊   | 37/67 [00:09<00:08,  3.74it/s, loss=2.58, v_num=1, train_loss_step=15.20, val_loss=2.730, train_loss_epoch=1.520]\u001b[A\n",
      "Epoch 312:  57%|███▉   | 38/67 [00:10<00:07,  3.72it/s, loss=2.58, v_num=1, train_loss_step=15.20, val_loss=2.730, train_loss_epoch=1.520]\u001b[A\n",
      "Epoch 312:  58%|████   | 39/67 [00:10<00:07,  3.69it/s, loss=2.58, v_num=1, train_loss_step=15.20, val_loss=2.730, train_loss_epoch=1.520]\u001b[A\n",
      "Epoch 312:  60%|████▏  | 40/67 [00:10<00:07,  3.67it/s, loss=2.58, v_num=1, train_loss_step=15.20, val_loss=2.730, train_loss_epoch=1.520]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 312:  61%|████▎  | 41/67 [00:11<00:07,  3.60it/s, loss=2.58, v_num=1, train_loss_step=15.20, val_loss=2.730, train_loss_epoch=1.520]\u001b[A\n",
      "Epoch 312:  63%|████▍  | 42/67 [00:11<00:06,  3.58it/s, loss=2.58, v_num=1, train_loss_step=15.20, val_loss=2.730, train_loss_epoch=1.520]\u001b[A\n",
      "Epoch 312:  64%|████▍  | 43/67 [00:12<00:06,  3.57it/s, loss=2.58, v_num=1, train_loss_step=15.20, val_loss=2.730, train_loss_epoch=1.520]\u001b[A\n",
      "Epoch 312:  66%|████▌  | 44/67 [00:12<00:06,  3.55it/s, loss=2.58, v_num=1, train_loss_step=15.20, val_loss=2.730, train_loss_epoch=1.520]\u001b[A\n",
      "Epoch 312:  67%|████▋  | 45/67 [00:12<00:06,  3.54it/s, loss=2.58, v_num=1, train_loss_step=15.20, val_loss=2.730, train_loss_epoch=1.520]\u001b[A\n",
      "Epoch 312:  69%|████▊  | 46/67 [00:13<00:05,  3.53it/s, loss=2.58, v_num=1, train_loss_step=15.20, val_loss=2.730, train_loss_epoch=1.520]\u001b[A\n",
      "Epoch 312:  70%|████▉  | 47/67 [00:13<00:05,  3.51it/s, loss=2.58, v_num=1, train_loss_step=15.20, val_loss=2.730, train_loss_epoch=1.520]\u001b[A\n",
      "Epoch 312:  72%|█████  | 48/67 [00:13<00:05,  3.50it/s, loss=2.58, v_num=1, train_loss_step=15.20, val_loss=2.730, train_loss_epoch=1.520]\u001b[A\n",
      "Epoch 312:  73%|█████  | 49/67 [00:14<00:05,  3.49it/s, loss=2.58, v_num=1, train_loss_step=15.20, val_loss=2.730, train_loss_epoch=1.520]\u001b[A\n",
      "Epoch 312:  75%|█████▏ | 50/67 [00:14<00:04,  3.48it/s, loss=2.58, v_num=1, train_loss_step=15.20, val_loss=2.730, train_loss_epoch=1.520]\u001b[A\n",
      "Epoch 312:  76%|█████▎ | 51/67 [00:14<00:04,  3.42it/s, loss=2.58, v_num=1, train_loss_step=15.20, val_loss=2.730, train_loss_epoch=1.520]\u001b[A\n",
      "Epoch 312:  78%|█████▍ | 52/67 [00:15<00:04,  3.41it/s, loss=2.58, v_num=1, train_loss_step=15.20, val_loss=2.730, train_loss_epoch=1.520]\u001b[A\n",
      "Epoch 312:  79%|█████▌ | 53/67 [00:15<00:04,  3.40it/s, loss=2.58, v_num=1, train_loss_step=15.20, val_loss=2.730, train_loss_epoch=1.520]\u001b[A\n",
      "Epoch 312:  81%|█████▋ | 54/67 [00:15<00:03,  3.39it/s, loss=2.58, v_num=1, train_loss_step=15.20, val_loss=2.730, train_loss_epoch=1.520]\u001b[A\n",
      "Epoch 312:  82%|█████▋ | 55/67 [00:16<00:03,  3.38it/s, loss=2.58, v_num=1, train_loss_step=15.20, val_loss=2.730, train_loss_epoch=1.520]\u001b[A\n",
      "Epoch 312:  84%|█████▊ | 56/67 [00:16<00:03,  3.38it/s, loss=2.58, v_num=1, train_loss_step=15.20, val_loss=2.730, train_loss_epoch=1.520]\u001b[A\n",
      "Epoch 312:  85%|█████▉ | 57/67 [00:16<00:02,  3.37it/s, loss=2.58, v_num=1, train_loss_step=15.20, val_loss=2.730, train_loss_epoch=1.520]\u001b[A\n",
      "Epoch 312:  87%|██████ | 58/67 [00:17<00:02,  3.36it/s, loss=2.58, v_num=1, train_loss_step=15.20, val_loss=2.730, train_loss_epoch=1.520]\u001b[A\n",
      "Epoch 312:  88%|██████▏| 59/67 [00:17<00:02,  3.35it/s, loss=2.58, v_num=1, train_loss_step=15.20, val_loss=2.730, train_loss_epoch=1.520]\u001b[A\n",
      "Epoch 312:  90%|██████▎| 60/67 [00:17<00:02,  3.35it/s, loss=2.58, v_num=1, train_loss_step=15.20, val_loss=2.730, train_loss_epoch=1.520]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 312:  91%|██████▎| 61/67 [00:18<00:01,  3.30it/s, loss=2.58, v_num=1, train_loss_step=15.20, val_loss=2.730, train_loss_epoch=1.520]\u001b[A\n",
      "Epoch 312:  93%|██████▍| 62/67 [00:18<00:01,  3.30it/s, loss=2.58, v_num=1, train_loss_step=15.20, val_loss=2.730, train_loss_epoch=1.520]\u001b[A\n",
      "Epoch 312:  94%|██████▌| 63/67 [00:19<00:01,  3.29it/s, loss=2.58, v_num=1, train_loss_step=15.20, val_loss=2.730, train_loss_epoch=1.520]\u001b[A\n",
      "Epoch 312:  96%|██████▋| 64/67 [00:19<00:00,  3.29it/s, loss=2.58, v_num=1, train_loss_step=15.20, val_loss=2.730, train_loss_epoch=1.520]\u001b[A\n",
      "Epoch 312:  97%|██████▊| 65/67 [00:19<00:00,  3.28it/s, loss=2.58, v_num=1, train_loss_step=15.20, val_loss=2.730, train_loss_epoch=1.520]\u001b[A\n",
      "Epoch 312:  99%|██████▉| 66/67 [00:20<00:00,  3.28it/s, loss=2.58, v_num=1, train_loss_step=15.20, val_loss=2.730, train_loss_epoch=1.520]\u001b[A\n",
      "Epoch 312: 100%|███████| 67/67 [00:20<00:00,  3.21it/s, loss=2.58, v_num=1, train_loss_step=15.20, val_loss=2.730, train_loss_epoch=1.520]\u001b[A\n",
      "Epoch 313:  45%|███▏   | 30/67 [00:05<00:07,  5.15it/s, loss=1.67, v_num=1, train_loss_step=2.250, val_loss=2.730, train_loss_epoch=2.320]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 313:  46%|███▏   | 31/67 [00:07<00:09,  3.88it/s, loss=1.67, v_num=1, train_loss_step=2.250, val_loss=2.730, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 313:  48%|███▎   | 32/67 [00:08<00:09,  3.85it/s, loss=1.67, v_num=1, train_loss_step=2.250, val_loss=2.730, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 313:  49%|███▍   | 33/67 [00:08<00:08,  3.81it/s, loss=1.67, v_num=1, train_loss_step=2.250, val_loss=2.730, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 313:  51%|███▌   | 34/67 [00:09<00:08,  3.78it/s, loss=1.67, v_num=1, train_loss_step=2.250, val_loss=2.730, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 313:  52%|███▋   | 35/67 [00:09<00:08,  3.75it/s, loss=1.67, v_num=1, train_loss_step=2.250, val_loss=2.730, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 313:  54%|███▊   | 36/67 [00:09<00:08,  3.72it/s, loss=1.67, v_num=1, train_loss_step=2.250, val_loss=2.730, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 313:  55%|███▊   | 37/67 [00:10<00:08,  3.69it/s, loss=1.67, v_num=1, train_loss_step=2.250, val_loss=2.730, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 313:  57%|███▉   | 38/67 [00:10<00:07,  3.67it/s, loss=1.67, v_num=1, train_loss_step=2.250, val_loss=2.730, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 313:  58%|████   | 39/67 [00:10<00:07,  3.65it/s, loss=1.67, v_num=1, train_loss_step=2.250, val_loss=2.730, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 313:  60%|████▏  | 40/67 [00:11<00:07,  3.63it/s, loss=1.67, v_num=1, train_loss_step=2.250, val_loss=2.730, train_loss_epoch=2.320]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 313:  61%|████▎  | 41/67 [00:11<00:07,  3.56it/s, loss=1.67, v_num=1, train_loss_step=2.250, val_loss=2.730, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 313:  63%|████▍  | 42/67 [00:11<00:07,  3.54it/s, loss=1.67, v_num=1, train_loss_step=2.250, val_loss=2.730, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 313:  64%|████▍  | 43/67 [00:12<00:06,  3.52it/s, loss=1.67, v_num=1, train_loss_step=2.250, val_loss=2.730, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 313:  66%|████▌  | 44/67 [00:12<00:06,  3.51it/s, loss=1.67, v_num=1, train_loss_step=2.250, val_loss=2.730, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 313:  67%|████▋  | 45/67 [00:12<00:06,  3.50it/s, loss=1.67, v_num=1, train_loss_step=2.250, val_loss=2.730, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 313:  69%|████▊  | 46/67 [00:13<00:06,  3.48it/s, loss=1.67, v_num=1, train_loss_step=2.250, val_loss=2.730, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 313:  70%|████▉  | 47/67 [00:13<00:05,  3.47it/s, loss=1.67, v_num=1, train_loss_step=2.250, val_loss=2.730, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 313:  72%|█████  | 48/67 [00:13<00:05,  3.46it/s, loss=1.67, v_num=1, train_loss_step=2.250, val_loss=2.730, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 313:  73%|█████  | 49/67 [00:14<00:05,  3.45it/s, loss=1.67, v_num=1, train_loss_step=2.250, val_loss=2.730, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 313:  75%|█████▏ | 50/67 [00:14<00:04,  3.44it/s, loss=1.67, v_num=1, train_loss_step=2.250, val_loss=2.730, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 313:  76%|█████▎ | 51/67 [00:15<00:04,  3.39it/s, loss=1.67, v_num=1, train_loss_step=2.250, val_loss=2.730, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 313:  78%|█████▍ | 52/67 [00:15<00:04,  3.38it/s, loss=1.67, v_num=1, train_loss_step=2.250, val_loss=2.730, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 313:  79%|█████▌ | 53/67 [00:15<00:04,  3.37it/s, loss=1.67, v_num=1, train_loss_step=2.250, val_loss=2.730, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 313:  81%|█████▋ | 54/67 [00:16<00:03,  3.36it/s, loss=1.67, v_num=1, train_loss_step=2.250, val_loss=2.730, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 313:  82%|█████▋ | 55/67 [00:16<00:03,  3.36it/s, loss=1.67, v_num=1, train_loss_step=2.250, val_loss=2.730, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 313:  84%|█████▊ | 56/67 [00:16<00:03,  3.35it/s, loss=1.67, v_num=1, train_loss_step=2.250, val_loss=2.730, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 313:  85%|█████▉ | 57/67 [00:17<00:02,  3.34it/s, loss=1.67, v_num=1, train_loss_step=2.250, val_loss=2.730, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 313:  87%|██████ | 58/67 [00:17<00:02,  3.34it/s, loss=1.67, v_num=1, train_loss_step=2.250, val_loss=2.730, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 313:  88%|██████▏| 59/67 [00:17<00:02,  3.33it/s, loss=1.67, v_num=1, train_loss_step=2.250, val_loss=2.730, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 313:  90%|██████▎| 60/67 [00:18<00:02,  3.33it/s, loss=1.67, v_num=1, train_loss_step=2.250, val_loss=2.730, train_loss_epoch=2.320]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 313:  91%|██████▎| 61/67 [00:18<00:01,  3.28it/s, loss=1.67, v_num=1, train_loss_step=2.250, val_loss=2.730, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 313:  93%|██████▍| 62/67 [00:18<00:01,  3.28it/s, loss=1.67, v_num=1, train_loss_step=2.250, val_loss=2.730, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 313:  94%|██████▌| 63/67 [00:19<00:01,  3.27it/s, loss=1.67, v_num=1, train_loss_step=2.250, val_loss=2.730, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 313:  96%|██████▋| 64/67 [00:19<00:00,  3.27it/s, loss=1.67, v_num=1, train_loss_step=2.250, val_loss=2.730, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 313:  97%|██████▊| 65/67 [00:19<00:00,  3.27it/s, loss=1.67, v_num=1, train_loss_step=2.250, val_loss=2.730, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 313:  99%|██████▉| 66/67 [00:20<00:00,  3.26it/s, loss=1.67, v_num=1, train_loss_step=2.250, val_loss=2.730, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 313: 100%|███████| 67/67 [00:21<00:00,  3.19it/s, loss=1.67, v_num=1, train_loss_step=2.250, val_loss=2.730, train_loss_epoch=2.320]\u001b[A\n",
      "Epoch 314:  45%|███▏   | 30/67 [00:05<00:07,  5.18it/s, loss=2.13, v_num=1, train_loss_step=1.980, val_loss=2.730, train_loss_epoch=1.730]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 314:  46%|███▏   | 31/67 [00:07<00:09,  3.92it/s, loss=2.13, v_num=1, train_loss_step=1.980, val_loss=2.730, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 314:  48%|███▎   | 32/67 [00:08<00:09,  3.88it/s, loss=2.13, v_num=1, train_loss_step=1.980, val_loss=2.730, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 314:  49%|███▍   | 33/67 [00:08<00:08,  3.85it/s, loss=2.13, v_num=1, train_loss_step=1.980, val_loss=2.730, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 314:  51%|███▌   | 34/67 [00:08<00:08,  3.82it/s, loss=2.13, v_num=1, train_loss_step=1.980, val_loss=2.730, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 314:  52%|███▋   | 35/67 [00:09<00:08,  3.79it/s, loss=2.13, v_num=1, train_loss_step=1.980, val_loss=2.730, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 314:  54%|███▊   | 36/67 [00:09<00:08,  3.77it/s, loss=2.13, v_num=1, train_loss_step=1.980, val_loss=2.730, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 314:  55%|███▊   | 37/67 [00:09<00:08,  3.74it/s, loss=2.13, v_num=1, train_loss_step=1.980, val_loss=2.730, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 314:  57%|███▉   | 38/67 [00:10<00:07,  3.72it/s, loss=2.13, v_num=1, train_loss_step=1.980, val_loss=2.730, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 314:  58%|████   | 39/67 [00:10<00:07,  3.70it/s, loss=2.13, v_num=1, train_loss_step=1.980, val_loss=2.730, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 314:  60%|████▏  | 40/67 [00:10<00:07,  3.67it/s, loss=2.13, v_num=1, train_loss_step=1.980, val_loss=2.730, train_loss_epoch=1.730]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 314:  61%|████▎  | 41/67 [00:11<00:07,  3.59it/s, loss=2.13, v_num=1, train_loss_step=1.980, val_loss=2.730, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 314:  63%|████▍  | 42/67 [00:11<00:06,  3.58it/s, loss=2.13, v_num=1, train_loss_step=1.980, val_loss=2.730, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 314:  64%|████▍  | 43/67 [00:12<00:06,  3.56it/s, loss=2.13, v_num=1, train_loss_step=1.980, val_loss=2.730, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 314:  66%|████▌  | 44/67 [00:12<00:06,  3.55it/s, loss=2.13, v_num=1, train_loss_step=1.980, val_loss=2.730, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 314:  67%|████▋  | 45/67 [00:12<00:06,  3.54it/s, loss=2.13, v_num=1, train_loss_step=1.980, val_loss=2.730, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 314:  69%|████▊  | 46/67 [00:13<00:05,  3.52it/s, loss=2.13, v_num=1, train_loss_step=1.980, val_loss=2.730, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 314:  70%|████▉  | 47/67 [00:13<00:05,  3.51it/s, loss=2.13, v_num=1, train_loss_step=1.980, val_loss=2.730, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 314:  72%|█████  | 48/67 [00:13<00:05,  3.50it/s, loss=2.13, v_num=1, train_loss_step=1.980, val_loss=2.730, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 314:  73%|█████  | 49/67 [00:14<00:05,  3.49it/s, loss=2.13, v_num=1, train_loss_step=1.980, val_loss=2.730, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 314:  75%|█████▏ | 50/67 [00:14<00:04,  3.48it/s, loss=2.13, v_num=1, train_loss_step=1.980, val_loss=2.730, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 314:  76%|█████▎ | 51/67 [00:14<00:04,  3.43it/s, loss=2.13, v_num=1, train_loss_step=1.980, val_loss=2.730, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 314:  78%|█████▍ | 52/67 [00:15<00:04,  3.42it/s, loss=2.13, v_num=1, train_loss_step=1.980, val_loss=2.730, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 314:  79%|█████▌ | 53/67 [00:15<00:04,  3.42it/s, loss=2.13, v_num=1, train_loss_step=1.980, val_loss=2.730, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 314:  81%|█████▋ | 54/67 [00:15<00:03,  3.41it/s, loss=2.13, v_num=1, train_loss_step=1.980, val_loss=2.730, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 314:  82%|█████▋ | 55/67 [00:16<00:03,  3.40it/s, loss=2.13, v_num=1, train_loss_step=1.980, val_loss=2.730, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 314:  84%|█████▊ | 56/67 [00:16<00:03,  3.40it/s, loss=2.13, v_num=1, train_loss_step=1.980, val_loss=2.730, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 314:  85%|█████▉ | 57/67 [00:16<00:02,  3.39it/s, loss=2.13, v_num=1, train_loss_step=1.980, val_loss=2.730, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 314:  87%|██████ | 58/67 [00:17<00:02,  3.38it/s, loss=2.13, v_num=1, train_loss_step=1.980, val_loss=2.730, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 314:  88%|██████▏| 59/67 [00:17<00:02,  3.38it/s, loss=2.13, v_num=1, train_loss_step=1.980, val_loss=2.730, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 314:  90%|██████▎| 60/67 [00:17<00:02,  3.37it/s, loss=2.13, v_num=1, train_loss_step=1.980, val_loss=2.730, train_loss_epoch=1.730]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 314:  91%|██████▎| 61/67 [00:18<00:01,  3.33it/s, loss=2.13, v_num=1, train_loss_step=1.980, val_loss=2.730, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 314:  93%|██████▍| 62/67 [00:18<00:01,  3.33it/s, loss=2.13, v_num=1, train_loss_step=1.980, val_loss=2.730, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 314:  94%|██████▌| 63/67 [00:18<00:01,  3.32it/s, loss=2.13, v_num=1, train_loss_step=1.980, val_loss=2.730, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 314:  96%|██████▋| 64/67 [00:19<00:00,  3.32it/s, loss=2.13, v_num=1, train_loss_step=1.980, val_loss=2.730, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 314:  97%|██████▊| 65/67 [00:19<00:00,  3.32it/s, loss=2.13, v_num=1, train_loss_step=1.980, val_loss=2.730, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 314:  99%|██████▉| 66/67 [00:19<00:00,  3.31it/s, loss=2.13, v_num=1, train_loss_step=1.980, val_loss=2.730, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 314: 100%|███████| 67/67 [00:20<00:00,  3.24it/s, loss=2.13, v_num=1, train_loss_step=1.980, val_loss=2.730, train_loss_epoch=1.730]\u001b[A\n",
      "Epoch 315:  45%|███▏   | 30/67 [00:05<00:07,  5.18it/s, loss=1.67, v_num=1, train_loss_step=3.450, val_loss=2.730, train_loss_epoch=2.330]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 315:  46%|███▏   | 31/67 [00:07<00:09,  3.91it/s, loss=1.67, v_num=1, train_loss_step=3.450, val_loss=2.730, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 315:  48%|███▎   | 32/67 [00:08<00:09,  3.87it/s, loss=1.67, v_num=1, train_loss_step=3.450, val_loss=2.730, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 315:  49%|███▍   | 33/67 [00:08<00:08,  3.84it/s, loss=1.67, v_num=1, train_loss_step=3.450, val_loss=2.730, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 315:  51%|███▌   | 34/67 [00:08<00:08,  3.80it/s, loss=1.67, v_num=1, train_loss_step=3.450, val_loss=2.730, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 315:  52%|███▋   | 35/67 [00:09<00:08,  3.78it/s, loss=1.67, v_num=1, train_loss_step=3.450, val_loss=2.730, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 315:  54%|███▊   | 36/67 [00:09<00:08,  3.76it/s, loss=1.67, v_num=1, train_loss_step=3.450, val_loss=2.730, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 315:  55%|███▊   | 37/67 [00:09<00:08,  3.72it/s, loss=1.67, v_num=1, train_loss_step=3.450, val_loss=2.730, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 315:  57%|███▉   | 38/67 [00:10<00:07,  3.70it/s, loss=1.67, v_num=1, train_loss_step=3.450, val_loss=2.730, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 315:  58%|████   | 39/67 [00:10<00:07,  3.68it/s, loss=1.67, v_num=1, train_loss_step=3.450, val_loss=2.730, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 315:  60%|████▏  | 40/67 [00:10<00:07,  3.66it/s, loss=1.67, v_num=1, train_loss_step=3.450, val_loss=2.730, train_loss_epoch=2.330]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 315:  61%|████▎  | 41/67 [00:11<00:07,  3.58it/s, loss=1.67, v_num=1, train_loss_step=3.450, val_loss=2.730, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 315:  63%|████▍  | 42/67 [00:11<00:07,  3.57it/s, loss=1.67, v_num=1, train_loss_step=3.450, val_loss=2.730, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 315:  64%|████▍  | 43/67 [00:12<00:06,  3.55it/s, loss=1.67, v_num=1, train_loss_step=3.450, val_loss=2.730, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 315:  66%|████▌  | 44/67 [00:12<00:06,  3.54it/s, loss=1.67, v_num=1, train_loss_step=3.450, val_loss=2.730, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 315:  67%|████▋  | 45/67 [00:12<00:06,  3.53it/s, loss=1.67, v_num=1, train_loss_step=3.450, val_loss=2.730, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 315:  69%|████▊  | 46/67 [00:13<00:05,  3.51it/s, loss=1.67, v_num=1, train_loss_step=3.450, val_loss=2.730, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 315:  70%|████▉  | 47/67 [00:13<00:05,  3.50it/s, loss=1.67, v_num=1, train_loss_step=3.450, val_loss=2.730, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 315:  72%|█████  | 48/67 [00:13<00:05,  3.49it/s, loss=1.67, v_num=1, train_loss_step=3.450, val_loss=2.730, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 315:  73%|█████  | 49/67 [00:14<00:05,  3.48it/s, loss=1.67, v_num=1, train_loss_step=3.450, val_loss=2.730, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 315:  75%|█████▏ | 50/67 [00:14<00:04,  3.47it/s, loss=1.67, v_num=1, train_loss_step=3.450, val_loss=2.730, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 315:  76%|█████▎ | 51/67 [00:14<00:04,  3.42it/s, loss=1.67, v_num=1, train_loss_step=3.450, val_loss=2.730, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 315:  78%|█████▍ | 52/67 [00:15<00:04,  3.41it/s, loss=1.67, v_num=1, train_loss_step=3.450, val_loss=2.730, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 315:  79%|█████▌ | 53/67 [00:15<00:04,  3.40it/s, loss=1.67, v_num=1, train_loss_step=3.450, val_loss=2.730, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 315:  81%|█████▋ | 54/67 [00:15<00:03,  3.39it/s, loss=1.67, v_num=1, train_loss_step=3.450, val_loss=2.730, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 315:  82%|█████▋ | 55/67 [00:16<00:03,  3.39it/s, loss=1.67, v_num=1, train_loss_step=3.450, val_loss=2.730, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 315:  84%|█████▊ | 56/67 [00:16<00:03,  3.37it/s, loss=1.67, v_num=1, train_loss_step=3.450, val_loss=2.730, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 315:  85%|█████▉ | 57/67 [00:16<00:02,  3.36it/s, loss=1.67, v_num=1, train_loss_step=3.450, val_loss=2.730, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 315:  87%|██████ | 58/67 [00:17<00:02,  3.36it/s, loss=1.67, v_num=1, train_loss_step=3.450, val_loss=2.730, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 315:  88%|██████▏| 59/67 [00:17<00:02,  3.35it/s, loss=1.67, v_num=1, train_loss_step=3.450, val_loss=2.730, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 315:  90%|██████▎| 60/67 [00:17<00:02,  3.34it/s, loss=1.67, v_num=1, train_loss_step=3.450, val_loss=2.730, train_loss_epoch=2.330]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 315:  91%|██████▎| 61/67 [00:18<00:01,  3.30it/s, loss=1.67, v_num=1, train_loss_step=3.450, val_loss=2.730, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 315:  93%|██████▍| 62/67 [00:18<00:01,  3.30it/s, loss=1.67, v_num=1, train_loss_step=3.450, val_loss=2.730, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 315:  94%|██████▌| 63/67 [00:19<00:01,  3.29it/s, loss=1.67, v_num=1, train_loss_step=3.450, val_loss=2.730, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 315:  96%|██████▋| 64/67 [00:19<00:00,  3.29it/s, loss=1.67, v_num=1, train_loss_step=3.450, val_loss=2.730, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 315:  97%|██████▊| 65/67 [00:19<00:00,  3.28it/s, loss=1.67, v_num=1, train_loss_step=3.450, val_loss=2.730, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 315:  99%|██████▉| 66/67 [00:20<00:00,  3.28it/s, loss=1.67, v_num=1, train_loss_step=3.450, val_loss=2.730, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 315: 100%|███████| 67/67 [00:20<00:00,  3.20it/s, loss=1.67, v_num=1, train_loss_step=3.450, val_loss=2.730, train_loss_epoch=2.330]\u001b[A\n",
      "Epoch 316:  45%|███▌    | 30/67 [00:05<00:07,  5.17it/s, loss=1.7, v_num=1, train_loss_step=0.821, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 316:  46%|███▋    | 31/67 [00:07<00:09,  3.89it/s, loss=1.7, v_num=1, train_loss_step=0.821, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 316:  48%|███▊    | 32/67 [00:08<00:09,  3.85it/s, loss=1.7, v_num=1, train_loss_step=0.821, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 316:  49%|███▉    | 33/67 [00:08<00:08,  3.82it/s, loss=1.7, v_num=1, train_loss_step=0.821, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 316:  51%|████    | 34/67 [00:08<00:08,  3.78it/s, loss=1.7, v_num=1, train_loss_step=0.821, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 316:  52%|████▏   | 35/67 [00:09<00:08,  3.76it/s, loss=1.7, v_num=1, train_loss_step=0.821, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 316:  54%|████▎   | 36/67 [00:09<00:08,  3.73it/s, loss=1.7, v_num=1, train_loss_step=0.821, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 316:  55%|████▍   | 37/67 [00:09<00:08,  3.70it/s, loss=1.7, v_num=1, train_loss_step=0.821, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 316:  57%|████▌   | 38/67 [00:10<00:07,  3.68it/s, loss=1.7, v_num=1, train_loss_step=0.821, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 316:  58%|████▋   | 39/67 [00:10<00:07,  3.66it/s, loss=1.7, v_num=1, train_loss_step=0.821, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 316:  60%|████▊   | 40/67 [00:10<00:07,  3.64it/s, loss=1.7, v_num=1, train_loss_step=0.821, val_loss=2.730, train_loss_epoch=1.740]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 316:  61%|████▉   | 41/67 [00:11<00:07,  3.56it/s, loss=1.7, v_num=1, train_loss_step=0.821, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 316:  63%|█████   | 42/67 [00:11<00:07,  3.55it/s, loss=1.7, v_num=1, train_loss_step=0.821, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 316:  64%|█████▏  | 43/67 [00:12<00:06,  3.53it/s, loss=1.7, v_num=1, train_loss_step=0.821, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 316:  66%|█████▎  | 44/67 [00:12<00:06,  3.51it/s, loss=1.7, v_num=1, train_loss_step=0.821, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 316:  67%|█████▎  | 45/67 [00:12<00:06,  3.50it/s, loss=1.7, v_num=1, train_loss_step=0.821, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 316:  69%|█████▍  | 46/67 [00:13<00:06,  3.49it/s, loss=1.7, v_num=1, train_loss_step=0.821, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 316:  70%|█████▌  | 47/67 [00:13<00:05,  3.47it/s, loss=1.7, v_num=1, train_loss_step=0.821, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 316:  72%|█████▋  | 48/67 [00:13<00:05,  3.46it/s, loss=1.7, v_num=1, train_loss_step=0.821, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 316:  73%|█████▊  | 49/67 [00:14<00:05,  3.45it/s, loss=1.7, v_num=1, train_loss_step=0.821, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 316:  75%|█████▉  | 50/67 [00:14<00:04,  3.44it/s, loss=1.7, v_num=1, train_loss_step=0.821, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 316:  76%|██████  | 51/67 [00:15<00:04,  3.39it/s, loss=1.7, v_num=1, train_loss_step=0.821, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 316:  78%|██████▏ | 52/67 [00:15<00:04,  3.38it/s, loss=1.7, v_num=1, train_loss_step=0.821, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 316:  79%|██████▎ | 53/67 [00:15<00:04,  3.37it/s, loss=1.7, v_num=1, train_loss_step=0.821, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 316:  81%|██████▍ | 54/67 [00:16<00:03,  3.36it/s, loss=1.7, v_num=1, train_loss_step=0.821, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 316:  82%|██████▌ | 55/67 [00:16<00:03,  3.35it/s, loss=1.7, v_num=1, train_loss_step=0.821, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 316:  84%|██████▋ | 56/67 [00:16<00:03,  3.35it/s, loss=1.7, v_num=1, train_loss_step=0.821, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 316:  85%|██████▊ | 57/67 [00:17<00:02,  3.34it/s, loss=1.7, v_num=1, train_loss_step=0.821, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 316:  87%|██████▉ | 58/67 [00:17<00:02,  3.33it/s, loss=1.7, v_num=1, train_loss_step=0.821, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 316:  88%|███████ | 59/67 [00:17<00:02,  3.33it/s, loss=1.7, v_num=1, train_loss_step=0.821, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 316:  90%|███████▏| 60/67 [00:18<00:02,  3.32it/s, loss=1.7, v_num=1, train_loss_step=0.821, val_loss=2.730, train_loss_epoch=1.740]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 316:  91%|███████▎| 61/67 [00:18<00:01,  3.29it/s, loss=1.7, v_num=1, train_loss_step=0.821, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 316:  93%|███████▍| 62/67 [00:18<00:01,  3.28it/s, loss=1.7, v_num=1, train_loss_step=0.821, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 316:  94%|███████▌| 63/67 [00:19<00:01,  3.28it/s, loss=1.7, v_num=1, train_loss_step=0.821, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 316:  96%|███████▋| 64/67 [00:19<00:00,  3.28it/s, loss=1.7, v_num=1, train_loss_step=0.821, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 316:  97%|███████▊| 65/67 [00:19<00:00,  3.27it/s, loss=1.7, v_num=1, train_loss_step=0.821, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 316:  99%|███████▉| 66/67 [00:20<00:00,  3.27it/s, loss=1.7, v_num=1, train_loss_step=0.821, val_loss=2.730, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 316: 100%|████████| 67/67 [00:20<00:00,  3.20it/s, loss=1.7, v_num=1, train_loss_step=0.821, val_loss=2.720, train_loss_epoch=1.740]\u001b[A\n",
      "Epoch 317:  45%|███▏   | 30/67 [00:05<00:07,  5.11it/s, loss=2.08, v_num=1, train_loss_step=7.400, val_loss=2.720, train_loss_epoch=2.510]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 317:  46%|███▏   | 31/67 [00:08<00:09,  3.86it/s, loss=2.08, v_num=1, train_loss_step=7.400, val_loss=2.720, train_loss_epoch=2.510]\u001b[A\n",
      "Epoch 317:  48%|███▎   | 32/67 [00:08<00:09,  3.82it/s, loss=2.08, v_num=1, train_loss_step=7.400, val_loss=2.720, train_loss_epoch=2.510]\u001b[A\n",
      "Epoch 317:  49%|███▍   | 33/67 [00:08<00:08,  3.79it/s, loss=2.08, v_num=1, train_loss_step=7.400, val_loss=2.720, train_loss_epoch=2.510]\u001b[A\n",
      "Epoch 317:  51%|███▌   | 34/67 [00:09<00:08,  3.76it/s, loss=2.08, v_num=1, train_loss_step=7.400, val_loss=2.720, train_loss_epoch=2.510]\u001b[A\n",
      "Epoch 317:  52%|███▋   | 35/67 [00:09<00:08,  3.74it/s, loss=2.08, v_num=1, train_loss_step=7.400, val_loss=2.720, train_loss_epoch=2.510]\u001b[A\n",
      "Epoch 317:  54%|███▊   | 36/67 [00:09<00:08,  3.71it/s, loss=2.08, v_num=1, train_loss_step=7.400, val_loss=2.720, train_loss_epoch=2.510]\u001b[A\n",
      "Epoch 317:  55%|███▊   | 37/67 [00:10<00:08,  3.68it/s, loss=2.08, v_num=1, train_loss_step=7.400, val_loss=2.720, train_loss_epoch=2.510]\u001b[A\n",
      "Epoch 317:  57%|███▉   | 38/67 [00:10<00:07,  3.66it/s, loss=2.08, v_num=1, train_loss_step=7.400, val_loss=2.720, train_loss_epoch=2.510]\u001b[A\n",
      "Epoch 317:  58%|████   | 39/67 [00:10<00:07,  3.64it/s, loss=2.08, v_num=1, train_loss_step=7.400, val_loss=2.720, train_loss_epoch=2.510]\u001b[A\n",
      "Epoch 317:  60%|████▏  | 40/67 [00:11<00:07,  3.62it/s, loss=2.08, v_num=1, train_loss_step=7.400, val_loss=2.720, train_loss_epoch=2.510]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 317:  61%|████▎  | 41/67 [00:11<00:07,  3.55it/s, loss=2.08, v_num=1, train_loss_step=7.400, val_loss=2.720, train_loss_epoch=2.510]\u001b[A\n",
      "Epoch 317:  63%|████▍  | 42/67 [00:11<00:07,  3.53it/s, loss=2.08, v_num=1, train_loss_step=7.400, val_loss=2.720, train_loss_epoch=2.510]\u001b[A\n",
      "Epoch 317:  64%|████▍  | 43/67 [00:12<00:06,  3.52it/s, loss=2.08, v_num=1, train_loss_step=7.400, val_loss=2.720, train_loss_epoch=2.510]\u001b[A\n",
      "Epoch 317:  66%|████▌  | 44/67 [00:12<00:06,  3.50it/s, loss=2.08, v_num=1, train_loss_step=7.400, val_loss=2.720, train_loss_epoch=2.510]\u001b[A\n",
      "Epoch 317:  67%|████▋  | 45/67 [00:12<00:06,  3.49it/s, loss=2.08, v_num=1, train_loss_step=7.400, val_loss=2.720, train_loss_epoch=2.510]\u001b[A\n",
      "Epoch 317:  69%|████▊  | 46/67 [00:13<00:06,  3.48it/s, loss=2.08, v_num=1, train_loss_step=7.400, val_loss=2.720, train_loss_epoch=2.510]\u001b[A\n",
      "Epoch 317:  70%|████▉  | 47/67 [00:13<00:05,  3.47it/s, loss=2.08, v_num=1, train_loss_step=7.400, val_loss=2.720, train_loss_epoch=2.510]\u001b[A\n",
      "Epoch 317:  72%|█████  | 48/67 [00:13<00:05,  3.46it/s, loss=2.08, v_num=1, train_loss_step=7.400, val_loss=2.720, train_loss_epoch=2.510]\u001b[A\n",
      "Epoch 317:  73%|█████  | 49/67 [00:14<00:05,  3.45it/s, loss=2.08, v_num=1, train_loss_step=7.400, val_loss=2.720, train_loss_epoch=2.510]\u001b[A\n",
      "Epoch 317:  75%|█████▏ | 50/67 [00:14<00:04,  3.44it/s, loss=2.08, v_num=1, train_loss_step=7.400, val_loss=2.720, train_loss_epoch=2.510]\u001b[A\n",
      "Epoch 317:  76%|█████▎ | 51/67 [00:15<00:04,  3.39it/s, loss=2.08, v_num=1, train_loss_step=7.400, val_loss=2.720, train_loss_epoch=2.510]\u001b[A\n",
      "Epoch 317:  78%|█████▍ | 52/67 [00:15<00:04,  3.38it/s, loss=2.08, v_num=1, train_loss_step=7.400, val_loss=2.720, train_loss_epoch=2.510]\u001b[A\n",
      "Epoch 317:  79%|█████▌ | 53/67 [00:15<00:04,  3.38it/s, loss=2.08, v_num=1, train_loss_step=7.400, val_loss=2.720, train_loss_epoch=2.510]\u001b[A\n",
      "Epoch 317:  81%|█████▋ | 54/67 [00:16<00:03,  3.37it/s, loss=2.08, v_num=1, train_loss_step=7.400, val_loss=2.720, train_loss_epoch=2.510]\u001b[A\n",
      "Epoch 317:  82%|█████▋ | 55/67 [00:16<00:03,  3.36it/s, loss=2.08, v_num=1, train_loss_step=7.400, val_loss=2.720, train_loss_epoch=2.510]\u001b[A\n",
      "Epoch 317:  84%|█████▊ | 56/67 [00:16<00:03,  3.35it/s, loss=2.08, v_num=1, train_loss_step=7.400, val_loss=2.720, train_loss_epoch=2.510]\u001b[A\n",
      "Epoch 317:  85%|█████▉ | 57/67 [00:17<00:02,  3.35it/s, loss=2.08, v_num=1, train_loss_step=7.400, val_loss=2.720, train_loss_epoch=2.510]\u001b[A\n",
      "Epoch 317:  87%|██████ | 58/67 [00:17<00:02,  3.34it/s, loss=2.08, v_num=1, train_loss_step=7.400, val_loss=2.720, train_loss_epoch=2.510]\u001b[A\n",
      "Epoch 317:  88%|██████▏| 59/67 [00:17<00:02,  3.33it/s, loss=2.08, v_num=1, train_loss_step=7.400, val_loss=2.720, train_loss_epoch=2.510]\u001b[A\n",
      "Epoch 317:  90%|██████▎| 60/67 [00:18<00:02,  3.33it/s, loss=2.08, v_num=1, train_loss_step=7.400, val_loss=2.720, train_loss_epoch=2.510]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 317:  91%|██████▎| 61/67 [00:18<00:01,  3.29it/s, loss=2.08, v_num=1, train_loss_step=7.400, val_loss=2.720, train_loss_epoch=2.510]\u001b[A\n",
      "Epoch 317:  93%|██████▍| 62/67 [00:18<00:01,  3.28it/s, loss=2.08, v_num=1, train_loss_step=7.400, val_loss=2.720, train_loss_epoch=2.510]\u001b[A\n",
      "Epoch 317:  94%|██████▌| 63/67 [00:19<00:01,  3.28it/s, loss=2.08, v_num=1, train_loss_step=7.400, val_loss=2.720, train_loss_epoch=2.510]\u001b[A\n",
      "Epoch 317:  96%|██████▋| 64/67 [00:19<00:00,  3.28it/s, loss=2.08, v_num=1, train_loss_step=7.400, val_loss=2.720, train_loss_epoch=2.510]\u001b[A\n",
      "Epoch 317:  97%|██████▊| 65/67 [00:19<00:00,  3.27it/s, loss=2.08, v_num=1, train_loss_step=7.400, val_loss=2.720, train_loss_epoch=2.510]\u001b[A\n",
      "Epoch 317:  99%|██████▉| 66/67 [00:20<00:00,  3.27it/s, loss=2.08, v_num=1, train_loss_step=7.400, val_loss=2.720, train_loss_epoch=2.510]\u001b[A\n",
      "Epoch 317: 100%|███████| 67/67 [00:20<00:00,  3.19it/s, loss=2.08, v_num=1, train_loss_step=7.400, val_loss=2.730, train_loss_epoch=2.510]\u001b[A\n",
      "Epoch 318:  45%|███▏   | 30/67 [00:05<00:07,  5.04it/s, loss=2.02, v_num=1, train_loss_step=2.310, val_loss=2.730, train_loss_epoch=2.180]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 318:  46%|███▏   | 31/67 [00:08<00:09,  3.84it/s, loss=2.02, v_num=1, train_loss_step=2.310, val_loss=2.730, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 318:  48%|███▎   | 32/67 [00:08<00:09,  3.81it/s, loss=2.02, v_num=1, train_loss_step=2.310, val_loss=2.730, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 318:  49%|███▍   | 33/67 [00:08<00:09,  3.78it/s, loss=2.02, v_num=1, train_loss_step=2.310, val_loss=2.730, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 318:  51%|███▌   | 34/67 [00:09<00:08,  3.75it/s, loss=2.02, v_num=1, train_loss_step=2.310, val_loss=2.730, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 318:  52%|███▋   | 35/67 [00:09<00:08,  3.72it/s, loss=2.02, v_num=1, train_loss_step=2.310, val_loss=2.730, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 318:  54%|███▊   | 36/67 [00:09<00:08,  3.69it/s, loss=2.02, v_num=1, train_loss_step=2.310, val_loss=2.730, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 318:  55%|███▊   | 37/67 [00:10<00:08,  3.67it/s, loss=2.02, v_num=1, train_loss_step=2.310, val_loss=2.730, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 318:  57%|███▉   | 38/67 [00:10<00:07,  3.65it/s, loss=2.02, v_num=1, train_loss_step=2.310, val_loss=2.730, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 318:  58%|████   | 39/67 [00:10<00:07,  3.64it/s, loss=2.02, v_num=1, train_loss_step=2.310, val_loss=2.730, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 318:  60%|████▏  | 40/67 [00:11<00:07,  3.62it/s, loss=2.02, v_num=1, train_loss_step=2.310, val_loss=2.730, train_loss_epoch=2.180]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 318:  61%|████▎  | 41/67 [00:11<00:07,  3.55it/s, loss=2.02, v_num=1, train_loss_step=2.310, val_loss=2.730, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 318:  63%|████▍  | 42/67 [00:11<00:07,  3.53it/s, loss=2.02, v_num=1, train_loss_step=2.310, val_loss=2.730, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 318:  64%|████▍  | 43/67 [00:12<00:06,  3.52it/s, loss=2.02, v_num=1, train_loss_step=2.310, val_loss=2.730, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 318:  66%|████▌  | 44/67 [00:12<00:06,  3.50it/s, loss=2.02, v_num=1, train_loss_step=2.310, val_loss=2.730, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 318:  67%|████▋  | 45/67 [00:12<00:06,  3.49it/s, loss=2.02, v_num=1, train_loss_step=2.310, val_loss=2.730, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 318:  69%|████▊  | 46/67 [00:13<00:06,  3.48it/s, loss=2.02, v_num=1, train_loss_step=2.310, val_loss=2.730, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 318:  70%|████▉  | 47/67 [00:13<00:05,  3.47it/s, loss=2.02, v_num=1, train_loss_step=2.310, val_loss=2.730, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 318:  72%|█████  | 48/67 [00:13<00:05,  3.46it/s, loss=2.02, v_num=1, train_loss_step=2.310, val_loss=2.730, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 318:  73%|█████  | 49/67 [00:14<00:05,  3.45it/s, loss=2.02, v_num=1, train_loss_step=2.310, val_loss=2.730, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 318:  75%|█████▏ | 50/67 [00:14<00:04,  3.44it/s, loss=2.02, v_num=1, train_loss_step=2.310, val_loss=2.730, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 318:  76%|█████▎ | 51/67 [00:15<00:04,  3.38it/s, loss=2.02, v_num=1, train_loss_step=2.310, val_loss=2.730, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 318:  78%|█████▍ | 52/67 [00:15<00:04,  3.37it/s, loss=2.02, v_num=1, train_loss_step=2.310, val_loss=2.730, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 318:  79%|█████▌ | 53/67 [00:15<00:04,  3.37it/s, loss=2.02, v_num=1, train_loss_step=2.310, val_loss=2.730, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 318:  81%|█████▋ | 54/67 [00:16<00:03,  3.36it/s, loss=2.02, v_num=1, train_loss_step=2.310, val_loss=2.730, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 318:  82%|█████▋ | 55/67 [00:16<00:03,  3.35it/s, loss=2.02, v_num=1, train_loss_step=2.310, val_loss=2.730, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 318:  84%|█████▊ | 56/67 [00:16<00:03,  3.35it/s, loss=2.02, v_num=1, train_loss_step=2.310, val_loss=2.730, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 318:  85%|█████▉ | 57/67 [00:17<00:02,  3.34it/s, loss=2.02, v_num=1, train_loss_step=2.310, val_loss=2.730, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 318:  87%|██████ | 58/67 [00:17<00:02,  3.33it/s, loss=2.02, v_num=1, train_loss_step=2.310, val_loss=2.730, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 318:  88%|██████▏| 59/67 [00:17<00:02,  3.33it/s, loss=2.02, v_num=1, train_loss_step=2.310, val_loss=2.730, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 318:  90%|██████▎| 60/67 [00:18<00:02,  3.32it/s, loss=2.02, v_num=1, train_loss_step=2.310, val_loss=2.730, train_loss_epoch=2.180]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 318:  91%|██████▎| 61/67 [00:18<00:01,  3.26it/s, loss=2.02, v_num=1, train_loss_step=2.310, val_loss=2.730, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 318:  93%|██████▍| 62/67 [00:50<00:04,  1.22it/s, loss=2.02, v_num=1, train_loss_step=2.310, val_loss=2.730, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 318:  94%|██████▌| 63/67 [00:51<00:03,  1.23it/s, loss=2.02, v_num=1, train_loss_step=2.310, val_loss=2.730, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 318:  96%|██████▋| 64/67 [00:51<00:02,  1.24it/s, loss=2.02, v_num=1, train_loss_step=2.310, val_loss=2.730, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 318:  97%|██████▊| 65/67 [00:51<00:01,  1.26it/s, loss=2.02, v_num=1, train_loss_step=2.310, val_loss=2.730, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 318:  99%|██████▉| 66/67 [00:52<00:00,  1.27it/s, loss=2.02, v_num=1, train_loss_step=2.310, val_loss=2.730, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 318: 100%|███████| 67/67 [00:52<00:00,  1.27it/s, loss=2.02, v_num=1, train_loss_step=2.310, val_loss=2.720, train_loss_epoch=2.180]\u001b[A\n",
      "Epoch 319:  45%|███▏   | 30/67 [00:06<00:07,  4.86it/s, loss=2.06, v_num=1, train_loss_step=1.320, val_loss=2.720, train_loss_epoch=1.990]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 319:  46%|███▏   | 31/67 [00:08<00:09,  3.65it/s, loss=2.06, v_num=1, train_loss_step=1.320, val_loss=2.720, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 319:  48%|███▎   | 32/67 [00:08<00:09,  3.63it/s, loss=2.06, v_num=1, train_loss_step=1.320, val_loss=2.720, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 319:  49%|███▍   | 33/67 [00:09<00:09,  3.62it/s, loss=2.06, v_num=1, train_loss_step=1.320, val_loss=2.720, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 319:  51%|███▌   | 34/67 [00:09<00:09,  3.61it/s, loss=2.06, v_num=1, train_loss_step=1.320, val_loss=2.720, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 319:  52%|███▋   | 35/67 [00:09<00:08,  3.59it/s, loss=2.06, v_num=1, train_loss_step=1.320, val_loss=2.720, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 319:  54%|███▊   | 36/67 [00:10<00:08,  3.59it/s, loss=2.06, v_num=1, train_loss_step=1.320, val_loss=2.720, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 319:  55%|███▊   | 37/67 [00:10<00:08,  3.58it/s, loss=2.06, v_num=1, train_loss_step=1.320, val_loss=2.720, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 319:  57%|███▉   | 38/67 [00:10<00:08,  3.56it/s, loss=2.06, v_num=1, train_loss_step=1.320, val_loss=2.720, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 319:  58%|████   | 39/67 [00:10<00:07,  3.55it/s, loss=2.06, v_num=1, train_loss_step=1.320, val_loss=2.720, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 319:  60%|████▏  | 40/67 [00:11<00:07,  3.53it/s, loss=2.06, v_num=1, train_loss_step=1.320, val_loss=2.720, train_loss_epoch=1.990]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 319:  61%|████▎  | 41/67 [00:11<00:07,  3.47it/s, loss=2.06, v_num=1, train_loss_step=1.320, val_loss=2.720, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 319:  63%|████▍  | 42/67 [00:12<00:07,  3.47it/s, loss=2.06, v_num=1, train_loss_step=1.320, val_loss=2.720, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 319:  64%|████▍  | 43/67 [00:12<00:06,  3.46it/s, loss=2.06, v_num=1, train_loss_step=1.320, val_loss=2.720, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 319:  66%|████▌  | 44/67 [00:12<00:06,  3.46it/s, loss=2.06, v_num=1, train_loss_step=1.320, val_loss=2.720, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 319:  67%|████▋  | 45/67 [00:13<00:06,  3.45it/s, loss=2.06, v_num=1, train_loss_step=1.320, val_loss=2.720, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 319:  69%|████▊  | 46/67 [00:13<00:06,  3.40it/s, loss=2.06, v_num=1, train_loss_step=1.320, val_loss=2.720, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 319:  70%|████▉  | 47/67 [00:13<00:05,  3.40it/s, loss=2.06, v_num=1, train_loss_step=1.320, val_loss=2.720, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 319:  72%|█████  | 48/67 [00:14<00:05,  3.40it/s, loss=2.06, v_num=1, train_loss_step=1.320, val_loss=2.720, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 319:  73%|█████  | 49/67 [00:14<00:05,  3.38it/s, loss=2.06, v_num=1, train_loss_step=1.320, val_loss=2.720, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 319:  75%|█████▏ | 50/67 [00:14<00:05,  3.38it/s, loss=2.06, v_num=1, train_loss_step=1.320, val_loss=2.720, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 319:  76%|█████▎ | 51/67 [00:15<00:04,  3.34it/s, loss=2.06, v_num=1, train_loss_step=1.320, val_loss=2.720, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 319:  78%|█████▍ | 52/67 [00:15<00:04,  3.34it/s, loss=2.06, v_num=1, train_loss_step=1.320, val_loss=2.720, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 319:  79%|█████▌ | 53/67 [00:15<00:04,  3.34it/s, loss=2.06, v_num=1, train_loss_step=1.320, val_loss=2.720, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 319:  81%|█████▋ | 54/67 [00:16<00:03,  3.34it/s, loss=2.06, v_num=1, train_loss_step=1.320, val_loss=2.720, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 319:  82%|█████▋ | 55/67 [00:22<00:04,  2.48it/s, loss=2.06, v_num=1, train_loss_step=1.320, val_loss=2.720, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 319:  84%|█████▊ | 56/67 [00:22<00:04,  2.49it/s, loss=2.06, v_num=1, train_loss_step=1.320, val_loss=2.720, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 319:  85%|█████▉ | 57/67 [00:22<00:03,  2.50it/s, loss=2.06, v_num=1, train_loss_step=1.320, val_loss=2.720, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 319:  87%|██████ | 58/67 [00:23<00:03,  2.42it/s, loss=2.06, v_num=1, train_loss_step=1.320, val_loss=2.720, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 319:  88%|██████▏| 59/67 [00:24<00:03,  2.43it/s, loss=2.06, v_num=1, train_loss_step=1.320, val_loss=2.720, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 319:  90%|██████▎| 60/67 [00:24<00:02,  2.44it/s, loss=2.06, v_num=1, train_loss_step=1.320, val_loss=2.720, train_loss_epoch=1.990]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 319:  91%|██████▎| 61/67 [00:25<00:02,  2.42it/s, loss=2.06, v_num=1, train_loss_step=1.320, val_loss=2.720, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 319:  93%|██████▍| 62/67 [00:25<00:02,  2.43it/s, loss=2.06, v_num=1, train_loss_step=1.320, val_loss=2.720, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 319:  94%|██████▌| 63/67 [00:25<00:01,  2.44it/s, loss=2.06, v_num=1, train_loss_step=1.320, val_loss=2.720, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 319:  96%|██████▋| 64/67 [00:26<00:01,  2.45it/s, loss=2.06, v_num=1, train_loss_step=1.320, val_loss=2.720, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 319:  97%|██████▊| 65/67 [00:26<00:00,  2.46it/s, loss=2.06, v_num=1, train_loss_step=1.320, val_loss=2.720, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 319:  99%|██████▉| 66/67 [00:26<00:00,  2.46it/s, loss=2.06, v_num=1, train_loss_step=1.320, val_loss=2.720, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 319: 100%|███████| 67/67 [00:27<00:00,  2.43it/s, loss=2.06, v_num=1, train_loss_step=1.320, val_loss=2.720, train_loss_epoch=1.990]\u001b[A\n",
      "Epoch 320:  45%|███▏   | 30/67 [00:05<00:07,  5.07it/s, loss=1.73, v_num=1, train_loss_step=0.538, val_loss=2.720, train_loss_epoch=2.130]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 320:  46%|███▏   | 31/67 [00:08<00:09,  3.83it/s, loss=1.73, v_num=1, train_loss_step=0.538, val_loss=2.720, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 320:  48%|███▎   | 32/67 [00:08<00:09,  3.81it/s, loss=1.73, v_num=1, train_loss_step=0.538, val_loss=2.720, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 320:  49%|███▍   | 33/67 [00:08<00:08,  3.78it/s, loss=1.73, v_num=1, train_loss_step=0.538, val_loss=2.720, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 320:  51%|███▌   | 34/67 [00:09<00:08,  3.77it/s, loss=1.73, v_num=1, train_loss_step=0.538, val_loss=2.720, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 320:  52%|███▋   | 35/67 [00:09<00:08,  3.75it/s, loss=1.73, v_num=1, train_loss_step=0.538, val_loss=2.720, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 320:  54%|███▊   | 36/67 [00:09<00:08,  3.73it/s, loss=1.73, v_num=1, train_loss_step=0.538, val_loss=2.720, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 320:  55%|███▊   | 37/67 [00:09<00:08,  3.71it/s, loss=1.73, v_num=1, train_loss_step=0.538, val_loss=2.720, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 320:  57%|███▉   | 38/67 [00:10<00:07,  3.70it/s, loss=1.73, v_num=1, train_loss_step=0.538, val_loss=2.720, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 320:  58%|████   | 39/67 [00:10<00:07,  3.69it/s, loss=1.73, v_num=1, train_loss_step=0.538, val_loss=2.720, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 320:  60%|████▏  | 40/67 [00:10<00:07,  3.68it/s, loss=1.73, v_num=1, train_loss_step=0.538, val_loss=2.720, train_loss_epoch=2.130]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 320:  61%|████▎  | 41/67 [00:11<00:07,  3.58it/s, loss=1.73, v_num=1, train_loss_step=0.538, val_loss=2.720, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 320:  63%|████▍  | 42/67 [00:11<00:06,  3.57it/s, loss=1.73, v_num=1, train_loss_step=0.538, val_loss=2.720, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 320:  64%|████▍  | 43/67 [00:12<00:06,  3.56it/s, loss=1.73, v_num=1, train_loss_step=0.538, val_loss=2.720, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 320:  66%|████▌  | 44/67 [00:17<00:09,  2.49it/s, loss=1.73, v_num=1, train_loss_step=0.538, val_loss=2.720, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 320:  67%|████▋  | 45/67 [00:18<00:08,  2.50it/s, loss=1.73, v_num=1, train_loss_step=0.538, val_loss=2.720, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 320:  69%|████▊  | 46/67 [00:18<00:08,  2.51it/s, loss=1.73, v_num=1, train_loss_step=0.538, val_loss=2.720, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 320:  70%|████▉  | 47/67 [00:18<00:07,  2.52it/s, loss=1.73, v_num=1, train_loss_step=0.538, val_loss=2.720, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 320:  72%|█████  | 48/67 [00:18<00:07,  2.53it/s, loss=1.73, v_num=1, train_loss_step=0.538, val_loss=2.720, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 320:  73%|█████  | 49/67 [00:19<00:07,  2.54it/s, loss=1.73, v_num=1, train_loss_step=0.538, val_loss=2.720, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 320:  75%|█████▏ | 50/67 [00:19<00:06,  2.54it/s, loss=1.73, v_num=1, train_loss_step=0.538, val_loss=2.720, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 320:  76%|█████▎ | 51/67 [00:20<00:06,  2.53it/s, loss=1.73, v_num=1, train_loss_step=0.538, val_loss=2.720, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 320:  78%|█████▍ | 52/67 [00:20<00:05,  2.54it/s, loss=1.73, v_num=1, train_loss_step=0.538, val_loss=2.720, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 320:  79%|█████▌ | 53/67 [00:20<00:05,  2.55it/s, loss=1.73, v_num=1, train_loss_step=0.538, val_loss=2.720, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 320:  81%|█████▋ | 54/67 [00:21<00:05,  2.56it/s, loss=1.73, v_num=1, train_loss_step=0.538, val_loss=2.720, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 320:  82%|█████▋ | 55/67 [00:21<00:04,  2.57it/s, loss=1.73, v_num=1, train_loss_step=0.538, val_loss=2.720, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 320:  84%|█████▊ | 56/67 [00:21<00:04,  2.58it/s, loss=1.73, v_num=1, train_loss_step=0.538, val_loss=2.720, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 320:  85%|█████▉ | 57/67 [00:22<00:03,  2.59it/s, loss=1.73, v_num=1, train_loss_step=0.538, val_loss=2.720, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 320:  87%|██████ | 58/67 [00:22<00:03,  2.59it/s, loss=1.73, v_num=1, train_loss_step=0.538, val_loss=2.720, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 320:  88%|██████▏| 59/67 [00:22<00:03,  2.60it/s, loss=1.73, v_num=1, train_loss_step=0.538, val_loss=2.720, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 320:  90%|██████▎| 60/67 [00:22<00:02,  2.61it/s, loss=1.73, v_num=1, train_loss_step=0.538, val_loss=2.720, train_loss_epoch=2.130]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 320:  91%|██████▎| 61/67 [00:23<00:02,  2.60it/s, loss=1.73, v_num=1, train_loss_step=0.538, val_loss=2.720, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 320:  93%|██████▍| 62/67 [00:23<00:01,  2.61it/s, loss=1.73, v_num=1, train_loss_step=0.538, val_loss=2.720, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 320:  94%|██████▌| 63/67 [00:24<00:01,  2.61it/s, loss=1.73, v_num=1, train_loss_step=0.538, val_loss=2.720, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 320:  96%|██████▋| 64/67 [00:24<00:01,  2.62it/s, loss=1.73, v_num=1, train_loss_step=0.538, val_loss=2.720, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 320:  97%|██████▊| 65/67 [00:24<00:00,  2.63it/s, loss=1.73, v_num=1, train_loss_step=0.538, val_loss=2.720, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 320:  99%|██████▉| 66/67 [00:24<00:00,  2.64it/s, loss=1.73, v_num=1, train_loss_step=0.538, val_loss=2.720, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 320: 100%|███████| 67/67 [00:25<00:00,  2.61it/s, loss=1.73, v_num=1, train_loss_step=0.538, val_loss=2.720, train_loss_epoch=2.130]\u001b[A\n",
      "Epoch 321:  45%|███▏   | 30/67 [00:05<00:06,  5.38it/s, loss=2.68, v_num=1, train_loss_step=1.050, val_loss=2.720, train_loss_epoch=2.090]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 321:  46%|███▏   | 31/67 [00:07<00:08,  4.04it/s, loss=2.68, v_num=1, train_loss_step=1.050, val_loss=2.720, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 321:  48%|███▎   | 32/67 [00:07<00:08,  4.01it/s, loss=2.68, v_num=1, train_loss_step=1.050, val_loss=2.720, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 321:  49%|███▍   | 33/67 [00:08<00:08,  3.98it/s, loss=2.68, v_num=1, train_loss_step=1.050, val_loss=2.720, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 321:  51%|███▌   | 34/67 [00:13<00:13,  2.51it/s, loss=2.68, v_num=1, train_loss_step=1.050, val_loss=2.720, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 321:  52%|███▋   | 35/67 [00:13<00:12,  2.53it/s, loss=2.68, v_num=1, train_loss_step=1.050, val_loss=2.720, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 321:  54%|███▊   | 36/67 [00:14<00:12,  2.54it/s, loss=2.68, v_num=1, train_loss_step=1.050, val_loss=2.720, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 321:  55%|███▊   | 37/67 [00:14<00:11,  2.56it/s, loss=2.68, v_num=1, train_loss_step=1.050, val_loss=2.720, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 321:  57%|███▉   | 38/67 [00:14<00:11,  2.57it/s, loss=2.68, v_num=1, train_loss_step=1.050, val_loss=2.720, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 321:  58%|████   | 39/67 [00:15<00:10,  2.59it/s, loss=2.68, v_num=1, train_loss_step=1.050, val_loss=2.720, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 321:  60%|████▏  | 40/67 [00:15<00:10,  2.60it/s, loss=2.68, v_num=1, train_loss_step=1.050, val_loss=2.720, train_loss_epoch=2.090]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 321:  61%|████▎  | 41/67 [00:15<00:10,  2.59it/s, loss=2.68, v_num=1, train_loss_step=1.050, val_loss=2.720, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 321:  63%|████▍  | 42/67 [00:16<00:09,  2.60it/s, loss=2.68, v_num=1, train_loss_step=1.050, val_loss=2.720, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 321:  64%|████▍  | 43/67 [00:16<00:09,  2.62it/s, loss=2.68, v_num=1, train_loss_step=1.050, val_loss=2.720, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 321:  66%|████▌  | 44/67 [00:16<00:08,  2.63it/s, loss=2.68, v_num=1, train_loss_step=1.050, val_loss=2.720, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 321:  67%|████▋  | 45/67 [00:17<00:08,  2.64it/s, loss=2.68, v_num=1, train_loss_step=1.050, val_loss=2.720, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 321:  69%|████▊  | 46/67 [00:17<00:07,  2.65it/s, loss=2.68, v_num=1, train_loss_step=1.050, val_loss=2.720, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 321:  70%|████▉  | 47/67 [00:17<00:07,  2.66it/s, loss=2.68, v_num=1, train_loss_step=1.050, val_loss=2.720, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 321:  72%|█████  | 48/67 [00:17<00:07,  2.67it/s, loss=2.68, v_num=1, train_loss_step=1.050, val_loss=2.720, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 321:  73%|█████  | 49/67 [00:18<00:06,  2.68it/s, loss=2.68, v_num=1, train_loss_step=1.050, val_loss=2.720, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 321:  75%|█████▏ | 50/67 [00:18<00:06,  2.69it/s, loss=2.68, v_num=1, train_loss_step=1.050, val_loss=2.720, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 321:  76%|█████▎ | 51/67 [00:19<00:05,  2.68it/s, loss=2.68, v_num=1, train_loss_step=1.050, val_loss=2.720, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 321:  78%|█████▍ | 52/67 [00:19<00:05,  2.69it/s, loss=2.68, v_num=1, train_loss_step=1.050, val_loss=2.720, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 321:  79%|█████▌ | 53/67 [00:19<00:05,  2.70it/s, loss=2.68, v_num=1, train_loss_step=1.050, val_loss=2.720, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 321:  81%|█████▋ | 54/67 [00:19<00:04,  2.71it/s, loss=2.68, v_num=1, train_loss_step=1.050, val_loss=2.720, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 321:  82%|█████▋ | 55/67 [00:20<00:04,  2.72it/s, loss=2.68, v_num=1, train_loss_step=1.050, val_loss=2.720, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 321:  84%|█████▊ | 56/67 [00:20<00:04,  2.73it/s, loss=2.68, v_num=1, train_loss_step=1.050, val_loss=2.720, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 321:  85%|█████▉ | 57/67 [00:20<00:03,  2.73it/s, loss=2.68, v_num=1, train_loss_step=1.050, val_loss=2.720, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 321:  87%|██████ | 58/67 [00:21<00:03,  2.74it/s, loss=2.68, v_num=1, train_loss_step=1.050, val_loss=2.720, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 321:  88%|██████▏| 59/67 [00:21<00:02,  2.75it/s, loss=2.68, v_num=1, train_loss_step=1.050, val_loss=2.720, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 321:  90%|██████▎| 60/67 [00:21<00:02,  2.76it/s, loss=2.68, v_num=1, train_loss_step=1.050, val_loss=2.720, train_loss_epoch=2.090]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 321:  91%|██████▎| 61/67 [00:22<00:02,  2.75it/s, loss=2.68, v_num=1, train_loss_step=1.050, val_loss=2.720, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 321:  93%|██████▍| 62/67 [00:22<00:01,  2.75it/s, loss=2.68, v_num=1, train_loss_step=1.050, val_loss=2.720, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 321:  94%|██████▌| 63/67 [00:22<00:01,  2.76it/s, loss=2.68, v_num=1, train_loss_step=1.050, val_loss=2.720, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 321:  96%|██████▋| 64/67 [00:23<00:01,  2.77it/s, loss=2.68, v_num=1, train_loss_step=1.050, val_loss=2.720, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 321:  97%|██████▊| 65/67 [00:23<00:00,  2.78it/s, loss=2.68, v_num=1, train_loss_step=1.050, val_loss=2.720, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 321:  99%|██████▉| 66/67 [00:23<00:00,  2.78it/s, loss=2.68, v_num=1, train_loss_step=1.050, val_loss=2.720, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 321: 100%|███████| 67/67 [00:45<00:00,  1.46it/s, loss=2.68, v_num=1, train_loss_step=1.050, val_loss=2.720, train_loss_epoch=2.090]\u001b[A\n",
      "Epoch 322:  45%|███▏   | 30/67 [00:06<00:07,  4.95it/s, loss=1.82, v_num=1, train_loss_step=2.510, val_loss=2.720, train_loss_epoch=2.430]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 322:  46%|███▏   | 31/67 [00:08<00:09,  3.79it/s, loss=1.82, v_num=1, train_loss_step=2.510, val_loss=2.720, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 322:  48%|███▎   | 32/67 [00:08<00:09,  3.77it/s, loss=1.82, v_num=1, train_loss_step=2.510, val_loss=2.720, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 322:  49%|███▍   | 33/67 [00:08<00:09,  3.75it/s, loss=1.82, v_num=1, train_loss_step=2.510, val_loss=2.720, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 322:  51%|███▌   | 34/67 [00:09<00:08,  3.73it/s, loss=1.82, v_num=1, train_loss_step=2.510, val_loss=2.720, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 322:  52%|███▋   | 35/67 [00:09<00:08,  3.72it/s, loss=1.82, v_num=1, train_loss_step=2.510, val_loss=2.720, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 322:  54%|███▊   | 36/67 [00:09<00:08,  3.70it/s, loss=1.82, v_num=1, train_loss_step=2.510, val_loss=2.720, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 322:  55%|███▊   | 37/67 [00:10<00:08,  3.69it/s, loss=1.82, v_num=1, train_loss_step=2.510, val_loss=2.720, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 322:  57%|███▉   | 38/67 [00:10<00:07,  3.68it/s, loss=1.82, v_num=1, train_loss_step=2.510, val_loss=2.720, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 322:  58%|████   | 39/67 [00:10<00:07,  3.66it/s, loss=1.82, v_num=1, train_loss_step=2.510, val_loss=2.720, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 322:  60%|████▏  | 40/67 [00:10<00:07,  3.65it/s, loss=1.82, v_num=1, train_loss_step=2.510, val_loss=2.720, train_loss_epoch=2.430]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 322:  61%|████▎  | 41/67 [00:11<00:07,  3.59it/s, loss=1.82, v_num=1, train_loss_step=2.510, val_loss=2.720, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 322:  63%|████▍  | 42/67 [00:11<00:06,  3.58it/s, loss=1.82, v_num=1, train_loss_step=2.510, val_loss=2.720, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 322:  64%|████▍  | 43/67 [00:12<00:06,  3.57it/s, loss=1.82, v_num=1, train_loss_step=2.510, val_loss=2.720, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 322:  66%|████▌  | 44/67 [00:12<00:06,  3.57it/s, loss=1.82, v_num=1, train_loss_step=2.510, val_loss=2.720, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 322:  67%|████▋  | 45/67 [00:12<00:06,  3.56it/s, loss=1.82, v_num=1, train_loss_step=2.510, val_loss=2.720, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 322:  69%|████▊  | 46/67 [00:12<00:05,  3.55it/s, loss=1.82, v_num=1, train_loss_step=2.510, val_loss=2.720, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 322:  70%|████▉  | 47/67 [00:13<00:05,  3.54it/s, loss=1.82, v_num=1, train_loss_step=2.510, val_loss=2.720, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 322:  72%|█████  | 48/67 [00:13<00:05,  3.53it/s, loss=1.82, v_num=1, train_loss_step=2.510, val_loss=2.720, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 322:  73%|█████  | 49/67 [00:13<00:05,  3.53it/s, loss=1.82, v_num=1, train_loss_step=2.510, val_loss=2.720, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 322:  75%|█████▏ | 50/67 [00:14<00:04,  3.52it/s, loss=1.82, v_num=1, train_loss_step=2.510, val_loss=2.720, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 322:  76%|█████▎ | 51/67 [00:14<00:04,  3.47it/s, loss=1.82, v_num=1, train_loss_step=2.510, val_loss=2.720, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 322:  78%|█████▍ | 52/67 [00:15<00:04,  3.47it/s, loss=1.82, v_num=1, train_loss_step=2.510, val_loss=2.720, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 322:  79%|█████▌ | 53/67 [00:15<00:04,  3.46it/s, loss=1.82, v_num=1, train_loss_step=2.510, val_loss=2.720, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 322:  81%|█████▋ | 54/67 [00:15<00:03,  3.45it/s, loss=1.82, v_num=1, train_loss_step=2.510, val_loss=2.720, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 322:  82%|█████▋ | 55/67 [00:15<00:03,  3.45it/s, loss=1.82, v_num=1, train_loss_step=2.510, val_loss=2.720, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 322:  84%|█████▊ | 56/67 [00:16<00:03,  3.44it/s, loss=1.82, v_num=1, train_loss_step=2.510, val_loss=2.720, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 322:  85%|█████▉ | 57/67 [00:16<00:02,  3.44it/s, loss=1.82, v_num=1, train_loss_step=2.510, val_loss=2.720, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 322:  87%|██████ | 58/67 [00:16<00:02,  3.43it/s, loss=1.82, v_num=1, train_loss_step=2.510, val_loss=2.720, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 322:  88%|██████▏| 59/67 [00:17<00:02,  3.42it/s, loss=1.82, v_num=1, train_loss_step=2.510, val_loss=2.720, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 322:  90%|██████▎| 60/67 [00:17<00:02,  3.42it/s, loss=1.82, v_num=1, train_loss_step=2.510, val_loss=2.720, train_loss_epoch=2.430]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 322:  91%|██████▎| 61/67 [00:18<00:01,  3.39it/s, loss=1.82, v_num=1, train_loss_step=2.510, val_loss=2.720, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 322:  93%|██████▍| 62/67 [00:18<00:01,  3.38it/s, loss=1.82, v_num=1, train_loss_step=2.510, val_loss=2.720, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 322:  94%|██████▌| 63/67 [00:18<00:01,  3.38it/s, loss=1.82, v_num=1, train_loss_step=2.510, val_loss=2.720, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 322:  96%|██████▋| 64/67 [00:18<00:00,  3.38it/s, loss=1.82, v_num=1, train_loss_step=2.510, val_loss=2.720, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 322:  97%|██████▊| 65/67 [00:19<00:00,  3.38it/s, loss=1.82, v_num=1, train_loss_step=2.510, val_loss=2.720, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 322:  99%|██████▉| 66/67 [00:19<00:00,  3.38it/s, loss=1.82, v_num=1, train_loss_step=2.510, val_loss=2.720, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 322: 100%|███████| 67/67 [00:20<00:00,  3.31it/s, loss=1.82, v_num=1, train_loss_step=2.510, val_loss=2.720, train_loss_epoch=2.430]\u001b[A\n",
      "Epoch 323:  45%|███▏   | 30/67 [00:05<00:06,  5.66it/s, loss=1.67, v_num=1, train_loss_step=0.784, val_loss=2.720, train_loss_epoch=1.820]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 323:  46%|███▏   | 31/67 [00:07<00:08,  4.22it/s, loss=1.67, v_num=1, train_loss_step=0.784, val_loss=2.720, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 323:  48%|███▎   | 32/67 [00:07<00:08,  4.18it/s, loss=1.67, v_num=1, train_loss_step=0.784, val_loss=2.720, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 323:  49%|███▍   | 33/67 [00:07<00:08,  4.14it/s, loss=1.67, v_num=1, train_loss_step=0.784, val_loss=2.720, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 323:  51%|███▌   | 34/67 [00:08<00:08,  4.09it/s, loss=1.67, v_num=1, train_loss_step=0.784, val_loss=2.720, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 323:  52%|███▋   | 35/67 [00:08<00:07,  4.06it/s, loss=1.67, v_num=1, train_loss_step=0.784, val_loss=2.720, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 323:  54%|███▊   | 36/67 [00:08<00:07,  4.02it/s, loss=1.67, v_num=1, train_loss_step=0.784, val_loss=2.720, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 323:  55%|███▊   | 37/67 [00:09<00:07,  3.98it/s, loss=1.67, v_num=1, train_loss_step=0.784, val_loss=2.720, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 323:  57%|███▉   | 38/67 [00:09<00:07,  3.95it/s, loss=1.67, v_num=1, train_loss_step=0.784, val_loss=2.720, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 323:  58%|████   | 39/67 [00:09<00:07,  3.93it/s, loss=1.67, v_num=1, train_loss_step=0.784, val_loss=2.720, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 323:  60%|████▏  | 40/67 [00:10<00:06,  3.91it/s, loss=1.67, v_num=1, train_loss_step=0.784, val_loss=2.720, train_loss_epoch=1.820]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 323:  61%|████▎  | 41/67 [00:10<00:06,  3.84it/s, loss=1.67, v_num=1, train_loss_step=0.784, val_loss=2.720, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 323:  63%|████▍  | 42/67 [00:10<00:06,  3.83it/s, loss=1.67, v_num=1, train_loss_step=0.784, val_loss=2.720, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 323:  64%|████▍  | 43/67 [00:11<00:06,  3.81it/s, loss=1.67, v_num=1, train_loss_step=0.784, val_loss=2.720, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 323:  66%|████▌  | 44/67 [00:11<00:06,  3.79it/s, loss=1.67, v_num=1, train_loss_step=0.784, val_loss=2.720, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 323:  67%|████▋  | 45/67 [00:11<00:05,  3.78it/s, loss=1.67, v_num=1, train_loss_step=0.784, val_loss=2.720, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 323:  69%|████▊  | 46/67 [00:12<00:05,  3.76it/s, loss=1.67, v_num=1, train_loss_step=0.784, val_loss=2.720, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 323:  70%|████▉  | 47/67 [00:12<00:05,  3.75it/s, loss=1.67, v_num=1, train_loss_step=0.784, val_loss=2.720, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 323:  72%|█████  | 48/67 [00:12<00:05,  3.73it/s, loss=1.67, v_num=1, train_loss_step=0.784, val_loss=2.720, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 323:  73%|█████  | 49/67 [00:13<00:04,  3.71it/s, loss=1.67, v_num=1, train_loss_step=0.784, val_loss=2.720, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 323:  75%|█████▏ | 50/67 [00:13<00:04,  3.70it/s, loss=1.67, v_num=1, train_loss_step=0.784, val_loss=2.720, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 323:  76%|█████▎ | 51/67 [00:13<00:04,  3.65it/s, loss=1.67, v_num=1, train_loss_step=0.784, val_loss=2.720, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 323:  78%|█████▍ | 52/67 [00:14<00:04,  3.64it/s, loss=1.67, v_num=1, train_loss_step=0.784, val_loss=2.720, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 323:  79%|█████▌ | 53/67 [00:14<00:03,  3.63it/s, loss=1.67, v_num=1, train_loss_step=0.784, val_loss=2.720, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 323:  81%|█████▋ | 54/67 [00:14<00:03,  3.63it/s, loss=1.67, v_num=1, train_loss_step=0.784, val_loss=2.720, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 323:  82%|█████▋ | 55/67 [00:15<00:03,  3.61it/s, loss=1.67, v_num=1, train_loss_step=0.784, val_loss=2.720, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 323:  84%|█████▊ | 56/67 [00:15<00:03,  3.61it/s, loss=1.67, v_num=1, train_loss_step=0.784, val_loss=2.720, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 323:  85%|█████▉ | 57/67 [00:15<00:02,  3.60it/s, loss=1.67, v_num=1, train_loss_step=0.784, val_loss=2.720, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 323:  87%|██████ | 58/67 [00:16<00:02,  3.60it/s, loss=1.67, v_num=1, train_loss_step=0.784, val_loss=2.720, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 323:  88%|██████▏| 59/67 [00:16<00:02,  3.59it/s, loss=1.67, v_num=1, train_loss_step=0.784, val_loss=2.720, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 323:  90%|██████▎| 60/67 [00:16<00:01,  3.58it/s, loss=1.67, v_num=1, train_loss_step=0.784, val_loss=2.720, train_loss_epoch=1.820]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 323:  91%|██████▎| 61/67 [00:17<00:01,  3.55it/s, loss=1.67, v_num=1, train_loss_step=0.784, val_loss=2.720, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 323:  93%|██████▍| 62/67 [00:17<00:01,  3.54it/s, loss=1.67, v_num=1, train_loss_step=0.784, val_loss=2.720, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 323:  94%|██████▌| 63/67 [00:17<00:01,  3.54it/s, loss=1.67, v_num=1, train_loss_step=0.784, val_loss=2.720, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 323:  96%|██████▋| 64/67 [00:18<00:00,  3.53it/s, loss=1.67, v_num=1, train_loss_step=0.784, val_loss=2.720, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 323:  97%|██████▊| 65/67 [00:18<00:00,  3.53it/s, loss=1.67, v_num=1, train_loss_step=0.784, val_loss=2.720, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 323:  99%|██████▉| 66/67 [00:18<00:00,  3.53it/s, loss=1.67, v_num=1, train_loss_step=0.784, val_loss=2.720, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 323: 100%|███████| 67/67 [00:19<00:00,  3.45it/s, loss=1.67, v_num=1, train_loss_step=0.784, val_loss=2.710, train_loss_epoch=1.820]\u001b[A\n",
      "Epoch 324:  45%|███▏   | 30/67 [00:05<00:06,  5.50it/s, loss=1.91, v_num=1, train_loss_step=0.520, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 324:  46%|███▏   | 31/67 [00:07<00:08,  4.16it/s, loss=1.91, v_num=1, train_loss_step=0.520, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 324:  48%|███▎   | 32/67 [00:07<00:08,  4.12it/s, loss=1.91, v_num=1, train_loss_step=0.520, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 324:  49%|███▍   | 33/67 [00:08<00:08,  4.08it/s, loss=1.91, v_num=1, train_loss_step=0.520, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 324:  51%|███▌   | 34/67 [00:08<00:08,  4.06it/s, loss=1.91, v_num=1, train_loss_step=0.520, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 324:  52%|███▋   | 35/67 [00:08<00:07,  4.02it/s, loss=1.91, v_num=1, train_loss_step=0.520, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 324:  54%|███▊   | 36/67 [00:09<00:07,  3.99it/s, loss=1.91, v_num=1, train_loss_step=0.520, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 324:  55%|███▊   | 37/67 [00:09<00:07,  3.96it/s, loss=1.91, v_num=1, train_loss_step=0.520, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 324:  57%|███▉   | 38/67 [00:09<00:07,  3.94it/s, loss=1.91, v_num=1, train_loss_step=0.520, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 324:  58%|████   | 39/67 [00:09<00:07,  3.92it/s, loss=1.91, v_num=1, train_loss_step=0.520, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 324:  60%|████▏  | 40/67 [00:10<00:06,  3.90it/s, loss=1.91, v_num=1, train_loss_step=0.520, val_loss=2.710, train_loss_epoch=1.850]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 324:  61%|████▎  | 41/67 [00:10<00:06,  3.83it/s, loss=1.91, v_num=1, train_loss_step=0.520, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 324:  63%|████▍  | 42/67 [00:11<00:06,  3.81it/s, loss=1.91, v_num=1, train_loss_step=0.520, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 324:  64%|████▍  | 43/67 [00:11<00:06,  3.80it/s, loss=1.91, v_num=1, train_loss_step=0.520, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 324:  66%|████▌  | 44/67 [00:11<00:06,  3.78it/s, loss=1.91, v_num=1, train_loss_step=0.520, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 324:  67%|████▋  | 45/67 [00:11<00:05,  3.77it/s, loss=1.91, v_num=1, train_loss_step=0.520, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 324:  69%|████▊  | 46/67 [00:12<00:05,  3.75it/s, loss=1.91, v_num=1, train_loss_step=0.520, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 324:  70%|████▉  | 47/67 [00:12<00:05,  3.73it/s, loss=1.91, v_num=1, train_loss_step=0.520, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 324:  72%|█████  | 48/67 [00:12<00:05,  3.72it/s, loss=1.91, v_num=1, train_loss_step=0.520, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 324:  73%|█████  | 49/67 [00:13<00:04,  3.71it/s, loss=1.91, v_num=1, train_loss_step=0.520, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 324:  75%|█████▏ | 50/67 [00:13<00:04,  3.70it/s, loss=1.91, v_num=1, train_loss_step=0.520, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 324:  76%|█████▎ | 51/67 [00:13<00:04,  3.64it/s, loss=1.91, v_num=1, train_loss_step=0.520, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 324:  78%|█████▍ | 52/67 [00:14<00:04,  3.63it/s, loss=1.91, v_num=1, train_loss_step=0.520, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 324:  79%|█████▌ | 53/67 [00:14<00:03,  3.63it/s, loss=1.91, v_num=1, train_loss_step=0.520, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 324:  81%|█████▋ | 54/67 [00:14<00:03,  3.62it/s, loss=1.91, v_num=1, train_loss_step=0.520, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 324:  82%|█████▋ | 55/67 [00:15<00:03,  3.61it/s, loss=1.91, v_num=1, train_loss_step=0.520, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 324:  84%|█████▊ | 56/67 [00:15<00:03,  3.60it/s, loss=1.91, v_num=1, train_loss_step=0.520, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 324:  85%|█████▉ | 57/67 [00:15<00:02,  3.60it/s, loss=1.91, v_num=1, train_loss_step=0.520, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 324:  87%|██████ | 58/67 [00:16<00:02,  3.59it/s, loss=1.91, v_num=1, train_loss_step=0.520, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 324:  88%|██████▏| 59/67 [00:16<00:02,  3.59it/s, loss=1.91, v_num=1, train_loss_step=0.520, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 324:  90%|██████▎| 60/67 [00:16<00:01,  3.58it/s, loss=1.91, v_num=1, train_loss_step=0.520, val_loss=2.710, train_loss_epoch=1.850]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 324:  91%|██████▎| 61/67 [00:17<00:01,  3.54it/s, loss=1.91, v_num=1, train_loss_step=0.520, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 324:  93%|██████▍| 62/67 [00:17<00:01,  3.53it/s, loss=1.91, v_num=1, train_loss_step=0.520, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 324:  94%|██████▌| 63/67 [00:17<00:01,  3.52it/s, loss=1.91, v_num=1, train_loss_step=0.520, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 324:  96%|██████▋| 64/67 [00:18<00:00,  3.52it/s, loss=1.91, v_num=1, train_loss_step=0.520, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 324:  97%|██████▊| 65/67 [00:18<00:00,  3.51it/s, loss=1.91, v_num=1, train_loss_step=0.520, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 324:  99%|██████▉| 66/67 [00:18<00:00,  3.50it/s, loss=1.91, v_num=1, train_loss_step=0.520, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 324: 100%|███████| 67/67 [00:19<00:00,  3.43it/s, loss=1.91, v_num=1, train_loss_step=0.520, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 325:  45%|███▏   | 30/67 [00:05<00:06,  5.56it/s, loss=1.81, v_num=1, train_loss_step=1.320, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 325:  46%|███▏   | 31/67 [00:07<00:08,  4.14it/s, loss=1.81, v_num=1, train_loss_step=1.320, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 325:  48%|███▎   | 32/67 [00:07<00:08,  4.10it/s, loss=1.81, v_num=1, train_loss_step=1.320, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 325:  49%|███▍   | 33/67 [00:08<00:08,  4.06it/s, loss=1.81, v_num=1, train_loss_step=1.320, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 325:  51%|███▌   | 34/67 [00:08<00:08,  4.03it/s, loss=1.81, v_num=1, train_loss_step=1.320, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 325:  52%|███▋   | 35/67 [00:08<00:07,  4.00it/s, loss=1.81, v_num=1, train_loss_step=1.320, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 325:  54%|███▊   | 36/67 [00:09<00:07,  3.98it/s, loss=1.81, v_num=1, train_loss_step=1.320, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 325:  55%|███▊   | 37/67 [00:09<00:07,  3.95it/s, loss=1.81, v_num=1, train_loss_step=1.320, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 325:  57%|███▉   | 38/67 [00:09<00:07,  3.93it/s, loss=1.81, v_num=1, train_loss_step=1.320, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 325:  58%|████   | 39/67 [00:09<00:07,  3.91it/s, loss=1.81, v_num=1, train_loss_step=1.320, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 325:  60%|████▏  | 40/67 [00:10<00:06,  3.89it/s, loss=1.81, v_num=1, train_loss_step=1.320, val_loss=2.710, train_loss_epoch=1.930]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 325:  61%|████▎  | 41/67 [00:10<00:06,  3.82it/s, loss=1.81, v_num=1, train_loss_step=1.320, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 325:  63%|████▍  | 42/67 [00:11<00:06,  3.80it/s, loss=1.81, v_num=1, train_loss_step=1.320, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 325:  64%|████▍  | 43/67 [00:11<00:06,  3.79it/s, loss=1.81, v_num=1, train_loss_step=1.320, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 325:  66%|████▌  | 44/67 [00:11<00:06,  3.77it/s, loss=1.81, v_num=1, train_loss_step=1.320, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 325:  67%|████▋  | 45/67 [00:11<00:05,  3.76it/s, loss=1.81, v_num=1, train_loss_step=1.320, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 325:  69%|████▊  | 46/67 [00:12<00:05,  3.75it/s, loss=1.81, v_num=1, train_loss_step=1.320, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 325:  70%|████▉  | 47/67 [00:12<00:05,  3.74it/s, loss=1.81, v_num=1, train_loss_step=1.320, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 325:  72%|█████  | 48/67 [00:12<00:05,  3.72it/s, loss=1.81, v_num=1, train_loss_step=1.320, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 325:  73%|█████  | 49/67 [00:13<00:04,  3.71it/s, loss=1.81, v_num=1, train_loss_step=1.320, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 325:  75%|█████▏ | 50/67 [00:13<00:04,  3.70it/s, loss=1.81, v_num=1, train_loss_step=1.320, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 325:  76%|█████▎ | 51/67 [00:13<00:04,  3.65it/s, loss=1.81, v_num=1, train_loss_step=1.320, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 325:  78%|█████▍ | 52/67 [00:14<00:04,  3.64it/s, loss=1.81, v_num=1, train_loss_step=1.320, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 325:  79%|█████▌ | 53/67 [00:14<00:03,  3.63it/s, loss=1.81, v_num=1, train_loss_step=1.320, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 325:  81%|█████▋ | 54/67 [00:14<00:03,  3.63it/s, loss=1.81, v_num=1, train_loss_step=1.320, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 325:  82%|█████▋ | 55/67 [00:15<00:03,  3.62it/s, loss=1.81, v_num=1, train_loss_step=1.320, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 325:  84%|█████▊ | 56/67 [00:15<00:03,  3.61it/s, loss=1.81, v_num=1, train_loss_step=1.320, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 325:  85%|█████▉ | 57/67 [00:15<00:02,  3.60it/s, loss=1.81, v_num=1, train_loss_step=1.320, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 325:  87%|██████ | 58/67 [00:16<00:02,  3.60it/s, loss=1.81, v_num=1, train_loss_step=1.320, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 325:  88%|██████▏| 59/67 [00:16<00:02,  3.59it/s, loss=1.81, v_num=1, train_loss_step=1.320, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 325:  90%|██████▎| 60/67 [00:16<00:01,  3.58it/s, loss=1.81, v_num=1, train_loss_step=1.320, val_loss=2.710, train_loss_epoch=1.930]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 325:  91%|██████▎| 61/67 [00:17<00:01,  3.55it/s, loss=1.81, v_num=1, train_loss_step=1.320, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 325:  93%|██████▍| 62/67 [00:17<00:01,  3.54it/s, loss=1.81, v_num=1, train_loss_step=1.320, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 325:  94%|██████▌| 63/67 [00:17<00:01,  3.54it/s, loss=1.81, v_num=1, train_loss_step=1.320, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 325:  96%|██████▋| 64/67 [00:18<00:00,  3.53it/s, loss=1.81, v_num=1, train_loss_step=1.320, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 325:  97%|██████▊| 65/67 [00:18<00:00,  3.53it/s, loss=1.81, v_num=1, train_loss_step=1.320, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 325:  99%|██████▉| 66/67 [00:18<00:00,  3.53it/s, loss=1.81, v_num=1, train_loss_step=1.320, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 325: 100%|███████| 67/67 [00:19<00:00,  3.45it/s, loss=1.81, v_num=1, train_loss_step=1.320, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 326:  45%|███▏   | 30/67 [00:05<00:06,  5.64it/s, loss=2.15, v_num=1, train_loss_step=1.920, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 326:  46%|███▏   | 31/67 [00:07<00:08,  4.16it/s, loss=2.15, v_num=1, train_loss_step=1.920, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 326:  48%|███▎   | 32/67 [00:07<00:08,  4.11it/s, loss=2.15, v_num=1, train_loss_step=1.920, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 326:  49%|███▍   | 33/67 [00:08<00:08,  4.07it/s, loss=2.15, v_num=1, train_loss_step=1.920, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 326:  51%|███▌   | 34/67 [00:08<00:08,  4.01it/s, loss=2.15, v_num=1, train_loss_step=1.920, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 326:  52%|███▋   | 35/67 [00:08<00:08,  3.98it/s, loss=2.15, v_num=1, train_loss_step=1.920, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 326:  54%|███▊   | 36/67 [00:09<00:07,  3.95it/s, loss=2.15, v_num=1, train_loss_step=1.920, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 326:  55%|███▊   | 37/67 [00:09<00:07,  3.93it/s, loss=2.15, v_num=1, train_loss_step=1.920, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 326:  57%|███▉   | 38/67 [00:09<00:07,  3.91it/s, loss=2.15, v_num=1, train_loss_step=1.920, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 326:  58%|████   | 39/67 [00:10<00:07,  3.89it/s, loss=2.15, v_num=1, train_loss_step=1.920, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 326:  60%|████▏  | 40/67 [00:10<00:06,  3.87it/s, loss=2.15, v_num=1, train_loss_step=1.920, val_loss=2.710, train_loss_epoch=1.850]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 326:  61%|████▎  | 41/67 [00:10<00:06,  3.80it/s, loss=2.15, v_num=1, train_loss_step=1.920, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 326:  63%|████▍  | 42/67 [00:11<00:06,  3.79it/s, loss=2.15, v_num=1, train_loss_step=1.920, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 326:  64%|████▍  | 43/67 [00:11<00:06,  3.77it/s, loss=2.15, v_num=1, train_loss_step=1.920, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 326:  66%|████▌  | 44/67 [00:11<00:06,  3.75it/s, loss=2.15, v_num=1, train_loss_step=1.920, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 326:  67%|████▋  | 45/67 [00:12<00:05,  3.74it/s, loss=2.15, v_num=1, train_loss_step=1.920, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 326:  69%|████▊  | 46/67 [00:12<00:05,  3.72it/s, loss=2.15, v_num=1, train_loss_step=1.920, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 326:  70%|████▉  | 47/67 [00:12<00:05,  3.71it/s, loss=2.15, v_num=1, train_loss_step=1.920, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 326:  72%|█████  | 48/67 [00:12<00:05,  3.70it/s, loss=2.15, v_num=1, train_loss_step=1.920, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 326:  73%|█████  | 49/67 [00:13<00:04,  3.68it/s, loss=2.15, v_num=1, train_loss_step=1.920, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 326:  75%|█████▏ | 50/67 [00:13<00:04,  3.68it/s, loss=2.15, v_num=1, train_loss_step=1.920, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 326:  76%|█████▎ | 51/67 [00:14<00:04,  3.61it/s, loss=2.15, v_num=1, train_loss_step=1.920, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 326:  78%|█████▍ | 52/67 [00:14<00:04,  3.60it/s, loss=2.15, v_num=1, train_loss_step=1.920, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 326:  79%|█████▌ | 53/67 [00:14<00:03,  3.60it/s, loss=2.15, v_num=1, train_loss_step=1.920, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 326:  81%|█████▋ | 54/67 [00:15<00:03,  3.59it/s, loss=2.15, v_num=1, train_loss_step=1.920, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 326:  82%|█████▋ | 55/67 [00:15<00:03,  3.57it/s, loss=2.15, v_num=1, train_loss_step=1.920, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 326:  84%|█████▊ | 56/67 [00:15<00:03,  3.57it/s, loss=2.15, v_num=1, train_loss_step=1.920, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 326:  85%|█████▉ | 57/67 [00:15<00:02,  3.56it/s, loss=2.15, v_num=1, train_loss_step=1.920, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 326:  87%|██████ | 58/67 [00:16<00:02,  3.56it/s, loss=2.15, v_num=1, train_loss_step=1.920, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 326:  88%|██████▏| 59/67 [00:16<00:02,  3.55it/s, loss=2.15, v_num=1, train_loss_step=1.920, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 326:  90%|██████▎| 60/67 [00:16<00:01,  3.54it/s, loss=2.15, v_num=1, train_loss_step=1.920, val_loss=2.710, train_loss_epoch=1.850]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 326:  91%|██████▎| 61/67 [00:17<00:01,  3.50it/s, loss=2.15, v_num=1, train_loss_step=1.920, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 326:  93%|██████▍| 62/67 [00:17<00:01,  3.49it/s, loss=2.15, v_num=1, train_loss_step=1.920, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 326:  94%|██████▌| 63/67 [00:18<00:01,  3.49it/s, loss=2.15, v_num=1, train_loss_step=1.920, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 326:  96%|██████▋| 64/67 [00:18<00:00,  3.48it/s, loss=2.15, v_num=1, train_loss_step=1.920, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 326:  97%|██████▊| 65/67 [00:18<00:00,  3.48it/s, loss=2.15, v_num=1, train_loss_step=1.920, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 326:  99%|██████▉| 66/67 [00:18<00:00,  3.48it/s, loss=2.15, v_num=1, train_loss_step=1.920, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 326: 100%|███████| 67/67 [00:19<00:00,  3.40it/s, loss=2.15, v_num=1, train_loss_step=1.920, val_loss=2.710, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 327:  45%|███▏   | 30/67 [00:05<00:06,  5.43it/s, loss=1.63, v_num=1, train_loss_step=1.820, val_loss=2.710, train_loss_epoch=1.980]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 327:  46%|███▏   | 31/67 [00:07<00:08,  4.09it/s, loss=1.63, v_num=1, train_loss_step=1.820, val_loss=2.710, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 327:  48%|███▎   | 32/67 [00:07<00:08,  4.04it/s, loss=1.63, v_num=1, train_loss_step=1.820, val_loss=2.710, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 327:  49%|███▍   | 33/67 [00:08<00:08,  4.00it/s, loss=1.63, v_num=1, train_loss_step=1.820, val_loss=2.710, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 327:  51%|███▌   | 34/67 [00:08<00:08,  3.96it/s, loss=1.63, v_num=1, train_loss_step=1.820, val_loss=2.710, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 327:  52%|███▋   | 35/67 [00:08<00:08,  3.92it/s, loss=1.63, v_num=1, train_loss_step=1.820, val_loss=2.710, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 327:  54%|███▊   | 36/67 [00:09<00:07,  3.90it/s, loss=1.63, v_num=1, train_loss_step=1.820, val_loss=2.710, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 327:  55%|███▊   | 37/67 [00:09<00:07,  3.87it/s, loss=1.63, v_num=1, train_loss_step=1.820, val_loss=2.710, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 327:  57%|███▉   | 38/67 [00:09<00:07,  3.86it/s, loss=1.63, v_num=1, train_loss_step=1.820, val_loss=2.710, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 327:  58%|████   | 39/67 [00:10<00:07,  3.84it/s, loss=1.63, v_num=1, train_loss_step=1.820, val_loss=2.710, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 327:  60%|████▏  | 40/67 [00:10<00:07,  3.82it/s, loss=1.63, v_num=1, train_loss_step=1.820, val_loss=2.710, train_loss_epoch=1.980]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 327:  61%|████▎  | 41/67 [00:10<00:06,  3.74it/s, loss=1.63, v_num=1, train_loss_step=1.820, val_loss=2.710, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 327:  63%|████▍  | 42/67 [00:11<00:06,  3.72it/s, loss=1.63, v_num=1, train_loss_step=1.820, val_loss=2.710, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 327:  64%|████▍  | 43/67 [00:11<00:06,  3.71it/s, loss=1.63, v_num=1, train_loss_step=1.820, val_loss=2.710, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 327:  66%|████▌  | 44/67 [00:11<00:06,  3.70it/s, loss=1.63, v_num=1, train_loss_step=1.820, val_loss=2.710, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 327:  67%|████▋  | 45/67 [00:12<00:05,  3.68it/s, loss=1.63, v_num=1, train_loss_step=1.820, val_loss=2.710, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 327:  69%|████▊  | 46/67 [00:12<00:05,  3.67it/s, loss=1.63, v_num=1, train_loss_step=1.820, val_loss=2.710, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 327:  70%|████▉  | 47/67 [00:12<00:05,  3.66it/s, loss=1.63, v_num=1, train_loss_step=1.820, val_loss=2.710, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 327:  72%|█████  | 48/67 [00:13<00:05,  3.64it/s, loss=1.63, v_num=1, train_loss_step=1.820, val_loss=2.710, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 327:  73%|█████  | 49/67 [00:13<00:04,  3.63it/s, loss=1.63, v_num=1, train_loss_step=1.820, val_loss=2.710, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 327:  75%|█████▏ | 50/67 [00:13<00:04,  3.62it/s, loss=1.63, v_num=1, train_loss_step=1.820, val_loss=2.710, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 327:  76%|█████▎ | 51/67 [00:14<00:04,  3.58it/s, loss=1.63, v_num=1, train_loss_step=1.820, val_loss=2.710, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 327:  78%|█████▍ | 52/67 [00:14<00:04,  3.56it/s, loss=1.63, v_num=1, train_loss_step=1.820, val_loss=2.710, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 327:  79%|█████▌ | 53/67 [00:14<00:03,  3.55it/s, loss=1.63, v_num=1, train_loss_step=1.820, val_loss=2.710, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 327:  81%|█████▋ | 54/67 [00:15<00:03,  3.54it/s, loss=1.63, v_num=1, train_loss_step=1.820, val_loss=2.710, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 327:  82%|█████▋ | 55/67 [00:15<00:03,  3.53it/s, loss=1.63, v_num=1, train_loss_step=1.820, val_loss=2.710, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 327:  84%|█████▊ | 56/67 [00:15<00:03,  3.53it/s, loss=1.63, v_num=1, train_loss_step=1.820, val_loss=2.710, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 327:  85%|█████▉ | 57/67 [00:16<00:02,  3.52it/s, loss=1.63, v_num=1, train_loss_step=1.820, val_loss=2.710, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 327:  87%|██████ | 58/67 [00:16<00:02,  3.51it/s, loss=1.63, v_num=1, train_loss_step=1.820, val_loss=2.710, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 327:  88%|██████▏| 59/67 [00:16<00:02,  3.50it/s, loss=1.63, v_num=1, train_loss_step=1.820, val_loss=2.710, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 327:  90%|██████▎| 60/67 [00:17<00:01,  3.50it/s, loss=1.63, v_num=1, train_loss_step=1.820, val_loss=2.710, train_loss_epoch=1.980]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 327:  91%|██████▎| 61/67 [00:17<00:01,  3.44it/s, loss=1.63, v_num=1, train_loss_step=1.820, val_loss=2.710, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 327:  93%|██████▍| 62/67 [00:18<00:01,  3.44it/s, loss=1.63, v_num=1, train_loss_step=1.820, val_loss=2.710, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 327:  94%|██████▌| 63/67 [00:18<00:01,  3.44it/s, loss=1.63, v_num=1, train_loss_step=1.820, val_loss=2.710, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 327:  96%|██████▋| 64/67 [00:18<00:00,  3.43it/s, loss=1.63, v_num=1, train_loss_step=1.820, val_loss=2.710, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 327:  97%|██████▊| 65/67 [00:18<00:00,  3.43it/s, loss=1.63, v_num=1, train_loss_step=1.820, val_loss=2.710, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 327:  99%|██████▉| 66/67 [00:19<00:00,  3.43it/s, loss=1.63, v_num=1, train_loss_step=1.820, val_loss=2.710, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 327: 100%|███████| 67/67 [00:19<00:00,  3.36it/s, loss=1.63, v_num=1, train_loss_step=1.820, val_loss=2.710, train_loss_epoch=1.980]\u001b[A\n",
      "Epoch 328:  45%|███▏   | 30/67 [00:05<00:06,  5.38it/s, loss=1.98, v_num=1, train_loss_step=3.020, val_loss=2.710, train_loss_epoch=2.230]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 328:  46%|███▏   | 31/67 [00:07<00:08,  4.06it/s, loss=1.98, v_num=1, train_loss_step=3.020, val_loss=2.710, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 328:  48%|███▎   | 32/67 [00:07<00:08,  4.02it/s, loss=1.98, v_num=1, train_loss_step=3.020, val_loss=2.710, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 328:  49%|███▍   | 33/67 [00:08<00:08,  4.00it/s, loss=1.98, v_num=1, train_loss_step=3.020, val_loss=2.710, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 328:  51%|███▌   | 34/67 [00:08<00:08,  3.97it/s, loss=1.98, v_num=1, train_loss_step=3.020, val_loss=2.710, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 328:  52%|███▋   | 35/67 [00:08<00:08,  3.95it/s, loss=1.98, v_num=1, train_loss_step=3.020, val_loss=2.710, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 328:  54%|███▊   | 36/67 [00:09<00:07,  3.93it/s, loss=1.98, v_num=1, train_loss_step=3.020, val_loss=2.710, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 328:  55%|███▊   | 37/67 [00:09<00:07,  3.91it/s, loss=1.98, v_num=1, train_loss_step=3.020, val_loss=2.710, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 328:  57%|███▉   | 38/67 [00:09<00:07,  3.89it/s, loss=1.98, v_num=1, train_loss_step=3.020, val_loss=2.710, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 328:  58%|████   | 39/67 [00:10<00:07,  3.87it/s, loss=1.98, v_num=1, train_loss_step=3.020, val_loss=2.710, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 328:  60%|████▏  | 40/67 [00:10<00:07,  3.85it/s, loss=1.98, v_num=1, train_loss_step=3.020, val_loss=2.710, train_loss_epoch=2.230]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 328:  61%|████▎  | 41/67 [00:10<00:06,  3.78it/s, loss=1.98, v_num=1, train_loss_step=3.020, val_loss=2.710, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 328:  63%|████▍  | 42/67 [00:11<00:06,  3.77it/s, loss=1.98, v_num=1, train_loss_step=3.020, val_loss=2.710, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 328:  64%|████▍  | 43/67 [00:11<00:06,  3.75it/s, loss=1.98, v_num=1, train_loss_step=3.020, val_loss=2.710, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 328:  66%|████▌  | 44/67 [00:11<00:06,  3.73it/s, loss=1.98, v_num=1, train_loss_step=3.020, val_loss=2.710, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 328:  67%|████▋  | 45/67 [00:12<00:05,  3.71it/s, loss=1.98, v_num=1, train_loss_step=3.020, val_loss=2.710, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 328:  69%|████▊  | 46/67 [00:12<00:05,  3.70it/s, loss=1.98, v_num=1, train_loss_step=3.020, val_loss=2.710, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 328:  70%|████▉  | 47/67 [00:12<00:05,  3.69it/s, loss=1.98, v_num=1, train_loss_step=3.020, val_loss=2.710, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 328:  72%|█████  | 48/67 [00:13<00:05,  3.67it/s, loss=1.98, v_num=1, train_loss_step=3.020, val_loss=2.710, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 328:  73%|█████  | 49/67 [00:13<00:04,  3.66it/s, loss=1.98, v_num=1, train_loss_step=3.020, val_loss=2.710, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 328:  75%|█████▏ | 50/67 [00:13<00:04,  3.65it/s, loss=1.98, v_num=1, train_loss_step=3.020, val_loss=2.710, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 328:  76%|█████▎ | 51/67 [00:14<00:04,  3.59it/s, loss=1.98, v_num=1, train_loss_step=3.020, val_loss=2.710, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 328:  78%|█████▍ | 52/67 [00:14<00:04,  3.57it/s, loss=1.98, v_num=1, train_loss_step=3.020, val_loss=2.710, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 328:  79%|█████▌ | 53/67 [00:14<00:03,  3.56it/s, loss=1.98, v_num=1, train_loss_step=3.020, val_loss=2.710, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 328:  81%|█████▋ | 54/67 [00:15<00:03,  3.55it/s, loss=1.98, v_num=1, train_loss_step=3.020, val_loss=2.710, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 328:  82%|█████▋ | 55/67 [00:15<00:03,  3.55it/s, loss=1.98, v_num=1, train_loss_step=3.020, val_loss=2.710, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 328:  84%|█████▊ | 56/67 [00:15<00:03,  3.54it/s, loss=1.98, v_num=1, train_loss_step=3.020, val_loss=2.710, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 328:  85%|█████▉ | 57/67 [00:16<00:02,  3.54it/s, loss=1.98, v_num=1, train_loss_step=3.020, val_loss=2.710, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 328:  87%|██████ | 58/67 [00:16<00:02,  3.53it/s, loss=1.98, v_num=1, train_loss_step=3.020, val_loss=2.710, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 328:  88%|██████▏| 59/67 [00:16<00:02,  3.52it/s, loss=1.98, v_num=1, train_loss_step=3.020, val_loss=2.710, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 328:  90%|██████▎| 60/67 [00:17<00:01,  3.52it/s, loss=1.98, v_num=1, train_loss_step=3.020, val_loss=2.710, train_loss_epoch=2.230]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 328:  91%|██████▎| 61/67 [00:17<00:01,  3.48it/s, loss=1.98, v_num=1, train_loss_step=3.020, val_loss=2.710, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 328:  93%|██████▍| 62/67 [00:17<00:01,  3.47it/s, loss=1.98, v_num=1, train_loss_step=3.020, val_loss=2.710, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 328:  94%|██████▌| 63/67 [00:18<00:01,  3.47it/s, loss=1.98, v_num=1, train_loss_step=3.020, val_loss=2.710, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 328:  96%|██████▋| 64/67 [00:18<00:00,  3.46it/s, loss=1.98, v_num=1, train_loss_step=3.020, val_loss=2.710, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 328:  97%|██████▊| 65/67 [00:18<00:00,  3.46it/s, loss=1.98, v_num=1, train_loss_step=3.020, val_loss=2.710, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 328:  99%|██████▉| 66/67 [00:19<00:00,  3.46it/s, loss=1.98, v_num=1, train_loss_step=3.020, val_loss=2.710, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 328: 100%|███████| 67/67 [00:19<00:00,  3.39it/s, loss=1.98, v_num=1, train_loss_step=3.020, val_loss=2.710, train_loss_epoch=2.230]\u001b[A\n",
      "Epoch 329:  45%|███▏   | 30/67 [00:05<00:06,  5.33it/s, loss=2.16, v_num=1, train_loss_step=1.790, val_loss=2.710, train_loss_epoch=1.900]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 329:  46%|███▏   | 31/67 [00:07<00:09,  3.99it/s, loss=2.16, v_num=1, train_loss_step=1.790, val_loss=2.710, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 329:  48%|███▎   | 32/67 [00:08<00:08,  3.96it/s, loss=2.16, v_num=1, train_loss_step=1.790, val_loss=2.710, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 329:  49%|███▍   | 33/67 [00:08<00:08,  3.93it/s, loss=2.16, v_num=1, train_loss_step=1.790, val_loss=2.710, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 329:  51%|███▌   | 34/67 [00:08<00:08,  3.91it/s, loss=2.16, v_num=1, train_loss_step=1.790, val_loss=2.710, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 329:  52%|███▋   | 35/67 [00:09<00:08,  3.89it/s, loss=2.16, v_num=1, train_loss_step=1.790, val_loss=2.710, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 329:  54%|███▊   | 36/67 [00:09<00:08,  3.86it/s, loss=2.16, v_num=1, train_loss_step=1.790, val_loss=2.710, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 329:  55%|███▊   | 37/67 [00:09<00:07,  3.84it/s, loss=2.16, v_num=1, train_loss_step=1.790, val_loss=2.710, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 329:  57%|███▉   | 38/67 [00:09<00:07,  3.83it/s, loss=2.16, v_num=1, train_loss_step=1.790, val_loss=2.710, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 329:  58%|████   | 39/67 [00:10<00:07,  3.81it/s, loss=2.16, v_num=1, train_loss_step=1.790, val_loss=2.710, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 329:  60%|████▏  | 40/67 [00:10<00:07,  3.79it/s, loss=2.16, v_num=1, train_loss_step=1.790, val_loss=2.710, train_loss_epoch=1.900]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 329:  61%|████▎  | 41/67 [00:10<00:06,  3.73it/s, loss=2.16, v_num=1, train_loss_step=1.790, val_loss=2.710, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 329:  63%|████▍  | 42/67 [00:11<00:06,  3.72it/s, loss=2.16, v_num=1, train_loss_step=1.790, val_loss=2.710, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 329:  64%|████▍  | 43/67 [00:11<00:06,  3.70it/s, loss=2.16, v_num=1, train_loss_step=1.790, val_loss=2.710, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 329:  66%|████▌  | 44/67 [00:11<00:06,  3.70it/s, loss=2.16, v_num=1, train_loss_step=1.790, val_loss=2.710, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 329:  67%|████▋  | 45/67 [00:12<00:05,  3.69it/s, loss=2.16, v_num=1, train_loss_step=1.790, val_loss=2.710, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 329:  69%|████▊  | 46/67 [00:12<00:05,  3.67it/s, loss=2.16, v_num=1, train_loss_step=1.790, val_loss=2.710, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 329:  70%|████▉  | 47/67 [00:12<00:05,  3.67it/s, loss=2.16, v_num=1, train_loss_step=1.790, val_loss=2.710, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 329:  72%|█████  | 48/67 [00:13<00:05,  3.66it/s, loss=2.16, v_num=1, train_loss_step=1.790, val_loss=2.710, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 329:  73%|█████  | 49/67 [00:13<00:04,  3.65it/s, loss=2.16, v_num=1, train_loss_step=1.790, val_loss=2.710, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 329:  75%|█████▏ | 50/67 [00:13<00:04,  3.64it/s, loss=2.16, v_num=1, train_loss_step=1.790, val_loss=2.710, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 329:  76%|█████▎ | 51/67 [00:14<00:04,  3.59it/s, loss=2.16, v_num=1, train_loss_step=1.790, val_loss=2.710, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 329:  78%|█████▍ | 52/67 [00:14<00:04,  3.59it/s, loss=2.16, v_num=1, train_loss_step=1.790, val_loss=2.710, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 329:  79%|█████▌ | 53/67 [00:14<00:03,  3.58it/s, loss=2.16, v_num=1, train_loss_step=1.790, val_loss=2.710, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 329:  81%|█████▋ | 54/67 [00:15<00:03,  3.58it/s, loss=2.16, v_num=1, train_loss_step=1.790, val_loss=2.710, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 329:  82%|█████▋ | 55/67 [00:15<00:03,  3.57it/s, loss=2.16, v_num=1, train_loss_step=1.790, val_loss=2.710, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 329:  84%|█████▊ | 56/67 [00:15<00:03,  3.56it/s, loss=2.16, v_num=1, train_loss_step=1.790, val_loss=2.710, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 329:  85%|█████▉ | 57/67 [00:16<00:02,  3.56it/s, loss=2.16, v_num=1, train_loss_step=1.790, val_loss=2.710, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 329:  87%|██████ | 58/67 [00:16<00:02,  3.55it/s, loss=2.16, v_num=1, train_loss_step=1.790, val_loss=2.710, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 329:  88%|██████▏| 59/67 [00:16<00:02,  3.55it/s, loss=2.16, v_num=1, train_loss_step=1.790, val_loss=2.710, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 329:  90%|██████▎| 60/67 [00:16<00:01,  3.54it/s, loss=2.16, v_num=1, train_loss_step=1.790, val_loss=2.710, train_loss_epoch=1.900]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 329:  91%|██████▎| 61/67 [00:17<00:01,  3.51it/s, loss=2.16, v_num=1, train_loss_step=1.790, val_loss=2.710, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 329:  93%|██████▍| 62/67 [00:17<00:01,  3.50it/s, loss=2.16, v_num=1, train_loss_step=1.790, val_loss=2.710, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 329:  94%|██████▌| 63/67 [00:17<00:01,  3.50it/s, loss=2.16, v_num=1, train_loss_step=1.790, val_loss=2.710, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 329:  96%|██████▋| 64/67 [00:18<00:00,  3.50it/s, loss=2.16, v_num=1, train_loss_step=1.790, val_loss=2.710, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 329:  97%|██████▊| 65/67 [00:18<00:00,  3.50it/s, loss=2.16, v_num=1, train_loss_step=1.790, val_loss=2.710, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 329:  99%|██████▉| 66/67 [00:18<00:00,  3.50it/s, loss=2.16, v_num=1, train_loss_step=1.790, val_loss=2.710, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 329: 100%|███████| 67/67 [00:19<00:00,  3.42it/s, loss=2.16, v_num=1, train_loss_step=1.790, val_loss=2.710, train_loss_epoch=1.900]\u001b[A\n",
      "Epoch 330:  45%|███▏   | 30/67 [00:05<00:06,  5.38it/s, loss=2.34, v_num=1, train_loss_step=1.900, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 330:  46%|███▏   | 31/67 [00:07<00:08,  4.03it/s, loss=2.34, v_num=1, train_loss_step=1.900, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 330:  48%|███▎   | 32/67 [00:08<00:08,  3.99it/s, loss=2.34, v_num=1, train_loss_step=1.900, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 330:  49%|███▍   | 33/67 [00:08<00:08,  3.95it/s, loss=2.34, v_num=1, train_loss_step=1.900, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 330:  51%|███▌   | 34/67 [00:08<00:08,  3.91it/s, loss=2.34, v_num=1, train_loss_step=1.900, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 330:  52%|███▋   | 35/67 [00:09<00:08,  3.88it/s, loss=2.34, v_num=1, train_loss_step=1.900, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 330:  54%|███▊   | 36/67 [00:09<00:08,  3.85it/s, loss=2.34, v_num=1, train_loss_step=1.900, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 330:  55%|███▊   | 37/67 [00:09<00:07,  3.82it/s, loss=2.34, v_num=1, train_loss_step=1.900, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 330:  57%|███▉   | 38/67 [00:10<00:07,  3.79it/s, loss=2.34, v_num=1, train_loss_step=1.900, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 330:  58%|████   | 39/67 [00:10<00:07,  3.77it/s, loss=2.34, v_num=1, train_loss_step=1.900, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 330:  60%|████▏  | 40/67 [00:10<00:07,  3.75it/s, loss=2.34, v_num=1, train_loss_step=1.900, val_loss=2.710, train_loss_epoch=1.930]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 330:  61%|████▎  | 41/67 [00:11<00:07,  3.68it/s, loss=2.34, v_num=1, train_loss_step=1.900, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 330:  63%|████▍  | 42/67 [00:11<00:06,  3.66it/s, loss=2.34, v_num=1, train_loss_step=1.900, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 330:  64%|████▍  | 43/67 [00:11<00:06,  3.64it/s, loss=2.34, v_num=1, train_loss_step=1.900, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 330:  66%|████▌  | 44/67 [00:12<00:06,  3.63it/s, loss=2.34, v_num=1, train_loss_step=1.900, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 330:  67%|████▋  | 45/67 [00:12<00:06,  3.62it/s, loss=2.34, v_num=1, train_loss_step=1.900, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 330:  69%|████▊  | 46/67 [00:12<00:05,  3.61it/s, loss=2.34, v_num=1, train_loss_step=1.900, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 330:  70%|████▉  | 47/67 [00:13<00:05,  3.60it/s, loss=2.34, v_num=1, train_loss_step=1.900, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 330:  72%|█████  | 48/67 [00:13<00:05,  3.60it/s, loss=2.34, v_num=1, train_loss_step=1.900, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 330:  73%|█████  | 49/67 [00:13<00:05,  3.58it/s, loss=2.34, v_num=1, train_loss_step=1.900, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 330:  75%|█████▏ | 50/67 [00:13<00:04,  3.58it/s, loss=2.34, v_num=1, train_loss_step=1.900, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 330:  76%|█████▎ | 51/67 [00:14<00:04,  3.53it/s, loss=2.34, v_num=1, train_loss_step=1.900, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 330:  78%|█████▍ | 52/67 [00:14<00:04,  3.53it/s, loss=2.34, v_num=1, train_loss_step=1.900, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 330:  79%|█████▌ | 53/67 [00:15<00:03,  3.52it/s, loss=2.34, v_num=1, train_loss_step=1.900, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 330:  81%|█████▋ | 54/67 [00:15<00:03,  3.52it/s, loss=2.34, v_num=1, train_loss_step=1.900, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 330:  82%|█████▋ | 55/67 [00:15<00:03,  3.51it/s, loss=2.34, v_num=1, train_loss_step=1.900, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 330:  84%|█████▊ | 56/67 [00:15<00:03,  3.50it/s, loss=2.34, v_num=1, train_loss_step=1.900, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 330:  85%|█████▉ | 57/67 [00:16<00:02,  3.50it/s, loss=2.34, v_num=1, train_loss_step=1.900, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 330:  87%|██████ | 58/67 [00:16<00:02,  3.48it/s, loss=2.34, v_num=1, train_loss_step=1.900, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 330:  88%|██████▏| 59/67 [00:16<00:02,  3.48it/s, loss=2.34, v_num=1, train_loss_step=1.900, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 330:  90%|██████▎| 60/67 [00:17<00:02,  3.47it/s, loss=2.34, v_num=1, train_loss_step=1.900, val_loss=2.710, train_loss_epoch=1.930]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 330:  91%|██████▎| 61/67 [00:17<00:01,  3.43it/s, loss=2.34, v_num=1, train_loss_step=1.900, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 330:  93%|██████▍| 62/67 [00:18<00:01,  3.43it/s, loss=2.34, v_num=1, train_loss_step=1.900, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 330:  94%|██████▌| 63/67 [00:18<00:01,  3.43it/s, loss=2.34, v_num=1, train_loss_step=1.900, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 330:  96%|██████▋| 64/67 [00:18<00:00,  3.43it/s, loss=2.34, v_num=1, train_loss_step=1.900, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 330:  97%|██████▊| 65/67 [00:18<00:00,  3.43it/s, loss=2.34, v_num=1, train_loss_step=1.900, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 330:  99%|██████▉| 66/67 [00:19<00:00,  3.43it/s, loss=2.34, v_num=1, train_loss_step=1.900, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 330: 100%|███████| 67/67 [00:19<00:00,  3.36it/s, loss=2.34, v_num=1, train_loss_step=1.900, val_loss=2.710, train_loss_epoch=1.930]\u001b[A\n",
      "Epoch 331:  45%|███▏   | 30/67 [00:05<00:06,  5.33it/s, loss=1.87, v_num=1, train_loss_step=0.624, val_loss=2.710, train_loss_epoch=2.490]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 331:  46%|███▏   | 31/67 [00:07<00:09,  4.00it/s, loss=1.87, v_num=1, train_loss_step=0.624, val_loss=2.710, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 331:  48%|███▎   | 32/67 [00:08<00:08,  3.97it/s, loss=1.87, v_num=1, train_loss_step=0.624, val_loss=2.710, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 331:  49%|███▍   | 33/67 [00:08<00:08,  3.95it/s, loss=1.87, v_num=1, train_loss_step=0.624, val_loss=2.710, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 331:  51%|███▌   | 34/67 [00:08<00:08,  3.92it/s, loss=1.87, v_num=1, train_loss_step=0.624, val_loss=2.710, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 331:  52%|███▋   | 35/67 [00:08<00:08,  3.90it/s, loss=1.87, v_num=1, train_loss_step=0.624, val_loss=2.710, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 331:  54%|███▊   | 36/67 [00:09<00:07,  3.88it/s, loss=1.87, v_num=1, train_loss_step=0.624, val_loss=2.710, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 331:  55%|███▊   | 37/67 [00:09<00:07,  3.86it/s, loss=1.87, v_num=1, train_loss_step=0.624, val_loss=2.710, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 331:  57%|███▉   | 38/67 [00:09<00:07,  3.84it/s, loss=1.87, v_num=1, train_loss_step=0.624, val_loss=2.710, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 331:  58%|████   | 39/67 [00:10<00:07,  3.81it/s, loss=1.87, v_num=1, train_loss_step=0.624, val_loss=2.710, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 331:  60%|████▏  | 40/67 [00:10<00:07,  3.79it/s, loss=1.87, v_num=1, train_loss_step=0.624, val_loss=2.710, train_loss_epoch=2.490]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 331:  61%|████▎  | 41/67 [00:11<00:07,  3.71it/s, loss=1.87, v_num=1, train_loss_step=0.624, val_loss=2.710, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 331:  63%|████▍  | 42/67 [00:11<00:06,  3.70it/s, loss=1.87, v_num=1, train_loss_step=0.624, val_loss=2.710, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 331:  64%|████▍  | 43/67 [00:11<00:06,  3.69it/s, loss=1.87, v_num=1, train_loss_step=0.624, val_loss=2.710, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 331:  66%|████▌  | 44/67 [00:11<00:06,  3.68it/s, loss=1.87, v_num=1, train_loss_step=0.624, val_loss=2.710, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 331:  67%|████▋  | 45/67 [00:12<00:05,  3.67it/s, loss=1.87, v_num=1, train_loss_step=0.624, val_loss=2.710, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 331:  69%|████▊  | 46/67 [00:12<00:05,  3.66it/s, loss=1.87, v_num=1, train_loss_step=0.624, val_loss=2.710, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 331:  70%|████▉  | 47/67 [00:12<00:05,  3.65it/s, loss=1.87, v_num=1, train_loss_step=0.624, val_loss=2.710, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 331:  72%|█████  | 48/67 [00:13<00:05,  3.64it/s, loss=1.87, v_num=1, train_loss_step=0.624, val_loss=2.710, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 331:  73%|█████  | 49/67 [00:13<00:04,  3.63it/s, loss=1.87, v_num=1, train_loss_step=0.624, val_loss=2.710, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 331:  75%|█████▏ | 50/67 [00:13<00:04,  3.61it/s, loss=1.87, v_num=1, train_loss_step=0.624, val_loss=2.710, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 331:  76%|█████▎ | 51/67 [00:14<00:04,  3.55it/s, loss=1.87, v_num=1, train_loss_step=0.624, val_loss=2.710, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 331:  78%|█████▍ | 52/67 [00:14<00:04,  3.54it/s, loss=1.87, v_num=1, train_loss_step=0.624, val_loss=2.710, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 331:  79%|█████▌ | 53/67 [00:15<00:03,  3.53it/s, loss=1.87, v_num=1, train_loss_step=0.624, val_loss=2.710, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 331:  81%|█████▋ | 54/67 [00:15<00:03,  3.52it/s, loss=1.87, v_num=1, train_loss_step=0.624, val_loss=2.710, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 331:  82%|█████▋ | 55/67 [00:15<00:03,  3.52it/s, loss=1.87, v_num=1, train_loss_step=0.624, val_loss=2.710, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 331:  84%|█████▊ | 56/67 [00:15<00:03,  3.51it/s, loss=1.87, v_num=1, train_loss_step=0.624, val_loss=2.710, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 331:  85%|█████▉ | 57/67 [00:16<00:02,  3.50it/s, loss=1.87, v_num=1, train_loss_step=0.624, val_loss=2.710, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 331:  87%|██████ | 58/67 [00:16<00:02,  3.49it/s, loss=1.87, v_num=1, train_loss_step=0.624, val_loss=2.710, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 331:  88%|██████▏| 59/67 [00:16<00:02,  3.49it/s, loss=1.87, v_num=1, train_loss_step=0.624, val_loss=2.710, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 331:  90%|██████▎| 60/67 [00:17<00:02,  3.48it/s, loss=1.87, v_num=1, train_loss_step=0.624, val_loss=2.710, train_loss_epoch=2.490]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 331:  91%|██████▎| 61/67 [00:17<00:01,  3.44it/s, loss=1.87, v_num=1, train_loss_step=0.624, val_loss=2.710, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 331:  93%|██████▍| 62/67 [00:18<00:01,  3.43it/s, loss=1.87, v_num=1, train_loss_step=0.624, val_loss=2.710, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 331:  94%|██████▌| 63/67 [00:18<00:01,  3.43it/s, loss=1.87, v_num=1, train_loss_step=0.624, val_loss=2.710, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 331:  96%|██████▋| 64/67 [00:18<00:00,  3.43it/s, loss=1.87, v_num=1, train_loss_step=0.624, val_loss=2.710, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 331:  97%|██████▊| 65/67 [00:18<00:00,  3.43it/s, loss=1.87, v_num=1, train_loss_step=0.624, val_loss=2.710, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 331:  99%|██████▉| 66/67 [00:19<00:00,  3.42it/s, loss=1.87, v_num=1, train_loss_step=0.624, val_loss=2.710, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 331: 100%|███████| 67/67 [00:19<00:00,  3.35it/s, loss=1.87, v_num=1, train_loss_step=0.624, val_loss=2.700, train_loss_epoch=2.490]\u001b[A\n",
      "Epoch 332:  45%|███▌    | 30/67 [00:05<00:06,  5.68it/s, loss=1.7, v_num=1, train_loss_step=1.280, val_loss=2.700, train_loss_epoch=1.630]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 332:  46%|███▋    | 31/67 [00:07<00:08,  4.20it/s, loss=1.7, v_num=1, train_loss_step=1.280, val_loss=2.700, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 332:  48%|███▊    | 32/67 [00:07<00:08,  4.17it/s, loss=1.7, v_num=1, train_loss_step=1.280, val_loss=2.700, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 332:  49%|███▉    | 33/67 [00:07<00:08,  4.14it/s, loss=1.7, v_num=1, train_loss_step=1.280, val_loss=2.700, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 332:  51%|████    | 34/67 [00:08<00:08,  4.10it/s, loss=1.7, v_num=1, train_loss_step=1.280, val_loss=2.700, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 332:  52%|████▏   | 35/67 [00:08<00:07,  4.07it/s, loss=1.7, v_num=1, train_loss_step=1.280, val_loss=2.700, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 332:  54%|████▎   | 36/67 [00:08<00:07,  4.05it/s, loss=1.7, v_num=1, train_loss_step=1.280, val_loss=2.700, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 332:  55%|████▍   | 37/67 [00:09<00:07,  4.02it/s, loss=1.7, v_num=1, train_loss_step=1.280, val_loss=2.700, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 332:  57%|████▌   | 38/67 [00:09<00:07,  4.00it/s, loss=1.7, v_num=1, train_loss_step=1.280, val_loss=2.700, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 332:  58%|████▋   | 39/67 [00:09<00:07,  3.97it/s, loss=1.7, v_num=1, train_loss_step=1.280, val_loss=2.700, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 332:  60%|████▊   | 40/67 [00:10<00:06,  3.95it/s, loss=1.7, v_num=1, train_loss_step=1.280, val_loss=2.700, train_loss_epoch=1.630]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 332:  61%|████▉   | 41/67 [00:10<00:06,  3.88it/s, loss=1.7, v_num=1, train_loss_step=1.280, val_loss=2.700, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 332:  63%|█████   | 42/67 [00:10<00:06,  3.87it/s, loss=1.7, v_num=1, train_loss_step=1.280, val_loss=2.700, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 332:  64%|█████▏  | 43/67 [00:11<00:06,  3.85it/s, loss=1.7, v_num=1, train_loss_step=1.280, val_loss=2.700, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 332:  66%|█████▎  | 44/67 [00:11<00:05,  3.84it/s, loss=1.7, v_num=1, train_loss_step=1.280, val_loss=2.700, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 332:  67%|█████▎  | 45/67 [00:11<00:05,  3.83it/s, loss=1.7, v_num=1, train_loss_step=1.280, val_loss=2.700, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 332:  69%|█████▍  | 46/67 [00:12<00:05,  3.81it/s, loss=1.7, v_num=1, train_loss_step=1.280, val_loss=2.700, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 332:  70%|█████▌  | 47/67 [00:12<00:05,  3.80it/s, loss=1.7, v_num=1, train_loss_step=1.280, val_loss=2.700, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 332:  72%|█████▋  | 48/67 [00:12<00:05,  3.79it/s, loss=1.7, v_num=1, train_loss_step=1.280, val_loss=2.700, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 332:  73%|█████▊  | 49/67 [00:12<00:04,  3.77it/s, loss=1.7, v_num=1, train_loss_step=1.280, val_loss=2.700, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 332:  75%|█████▉  | 50/67 [00:13<00:04,  3.75it/s, loss=1.7, v_num=1, train_loss_step=1.280, val_loss=2.700, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 332:  76%|██████  | 51/67 [00:13<00:04,  3.69it/s, loss=1.7, v_num=1, train_loss_step=1.280, val_loss=2.700, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 332:  78%|██████▏ | 52/67 [00:14<00:04,  3.68it/s, loss=1.7, v_num=1, train_loss_step=1.280, val_loss=2.700, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 332:  79%|██████▎ | 53/67 [00:14<00:03,  3.67it/s, loss=1.7, v_num=1, train_loss_step=1.280, val_loss=2.700, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 332:  81%|██████▍ | 54/67 [00:14<00:03,  3.66it/s, loss=1.7, v_num=1, train_loss_step=1.280, val_loss=2.700, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 332:  82%|██████▌ | 55/67 [00:15<00:03,  3.65it/s, loss=1.7, v_num=1, train_loss_step=1.280, val_loss=2.700, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 332:  84%|██████▋ | 56/67 [00:15<00:03,  3.63it/s, loss=1.7, v_num=1, train_loss_step=1.280, val_loss=2.700, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 332:  85%|██████▊ | 57/67 [00:15<00:02,  3.62it/s, loss=1.7, v_num=1, train_loss_step=1.280, val_loss=2.700, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 332:  87%|██████▉ | 58/67 [00:16<00:02,  3.61it/s, loss=1.7, v_num=1, train_loss_step=1.280, val_loss=2.700, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 332:  88%|███████ | 59/67 [00:16<00:02,  3.59it/s, loss=1.7, v_num=1, train_loss_step=1.280, val_loss=2.700, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 332:  90%|███████▏| 60/67 [00:16<00:01,  3.58it/s, loss=1.7, v_num=1, train_loss_step=1.280, val_loss=2.700, train_loss_epoch=1.630]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 332:  91%|███████▎| 61/67 [00:17<00:01,  3.52it/s, loss=1.7, v_num=1, train_loss_step=1.280, val_loss=2.700, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 332:  93%|███████▍| 62/67 [00:17<00:01,  3.51it/s, loss=1.7, v_num=1, train_loss_step=1.280, val_loss=2.700, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 332:  94%|███████▌| 63/67 [00:17<00:01,  3.50it/s, loss=1.7, v_num=1, train_loss_step=1.280, val_loss=2.700, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 332:  96%|███████▋| 64/67 [00:18<00:00,  3.49it/s, loss=1.7, v_num=1, train_loss_step=1.280, val_loss=2.700, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 332:  97%|███████▊| 65/67 [00:18<00:00,  3.48it/s, loss=1.7, v_num=1, train_loss_step=1.280, val_loss=2.700, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 332:  99%|███████▉| 66/67 [00:19<00:00,  3.47it/s, loss=1.7, v_num=1, train_loss_step=1.280, val_loss=2.700, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 332: 100%|████████| 67/67 [00:19<00:00,  3.40it/s, loss=1.7, v_num=1, train_loss_step=1.280, val_loss=2.700, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 333:  45%|███▏   | 30/67 [00:05<00:06,  5.50it/s, loss=1.49, v_num=1, train_loss_step=1.620, val_loss=2.700, train_loss_epoch=1.590]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 333:  46%|███▏   | 31/67 [00:07<00:08,  4.16it/s, loss=1.49, v_num=1, train_loss_step=1.620, val_loss=2.700, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 333:  48%|███▎   | 32/67 [00:07<00:08,  4.12it/s, loss=1.49, v_num=1, train_loss_step=1.620, val_loss=2.700, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 333:  49%|███▍   | 33/67 [00:08<00:08,  4.09it/s, loss=1.49, v_num=1, train_loss_step=1.620, val_loss=2.700, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 333:  51%|███▌   | 34/67 [00:08<00:08,  4.06it/s, loss=1.49, v_num=1, train_loss_step=1.620, val_loss=2.700, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 333:  52%|███▋   | 35/67 [00:08<00:07,  4.03it/s, loss=1.49, v_num=1, train_loss_step=1.620, val_loss=2.700, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 333:  54%|███▊   | 36/67 [00:08<00:07,  4.01it/s, loss=1.49, v_num=1, train_loss_step=1.620, val_loss=2.700, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 333:  55%|███▊   | 37/67 [00:09<00:07,  3.97it/s, loss=1.49, v_num=1, train_loss_step=1.620, val_loss=2.700, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 333:  57%|███▉   | 38/67 [00:09<00:07,  3.95it/s, loss=1.49, v_num=1, train_loss_step=1.620, val_loss=2.700, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 333:  58%|████   | 39/67 [00:09<00:07,  3.93it/s, loss=1.49, v_num=1, train_loss_step=1.620, val_loss=2.700, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 333:  60%|████▏  | 40/67 [00:10<00:06,  3.91it/s, loss=1.49, v_num=1, train_loss_step=1.620, val_loss=2.700, train_loss_epoch=1.590]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 333:  61%|████▎  | 41/67 [00:10<00:06,  3.83it/s, loss=1.49, v_num=1, train_loss_step=1.620, val_loss=2.700, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 333:  63%|████▍  | 42/67 [00:11<00:06,  3.81it/s, loss=1.49, v_num=1, train_loss_step=1.620, val_loss=2.700, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 333:  64%|████▍  | 43/67 [00:11<00:06,  3.79it/s, loss=1.49, v_num=1, train_loss_step=1.620, val_loss=2.700, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 333:  66%|████▌  | 44/67 [00:11<00:06,  3.77it/s, loss=1.49, v_num=1, train_loss_step=1.620, val_loss=2.700, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 333:  67%|████▋  | 45/67 [00:11<00:05,  3.76it/s, loss=1.49, v_num=1, train_loss_step=1.620, val_loss=2.700, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 333:  69%|████▊  | 46/67 [00:12<00:05,  3.74it/s, loss=1.49, v_num=1, train_loss_step=1.620, val_loss=2.700, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 333:  70%|████▉  | 47/67 [00:12<00:05,  3.72it/s, loss=1.49, v_num=1, train_loss_step=1.620, val_loss=2.700, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 333:  72%|█████  | 48/67 [00:12<00:05,  3.71it/s, loss=1.49, v_num=1, train_loss_step=1.620, val_loss=2.700, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 333:  73%|█████  | 49/67 [00:13<00:04,  3.71it/s, loss=1.49, v_num=1, train_loss_step=1.620, val_loss=2.700, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 333:  75%|█████▏ | 50/67 [00:13<00:04,  3.70it/s, loss=1.49, v_num=1, train_loss_step=1.620, val_loss=2.700, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 333:  76%|█████▎ | 51/67 [00:13<00:04,  3.64it/s, loss=1.49, v_num=1, train_loss_step=1.620, val_loss=2.700, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 333:  78%|█████▍ | 52/67 [00:14<00:04,  3.63it/s, loss=1.49, v_num=1, train_loss_step=1.620, val_loss=2.700, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 333:  79%|█████▌ | 53/67 [00:14<00:03,  3.63it/s, loss=1.49, v_num=1, train_loss_step=1.620, val_loss=2.700, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 333:  81%|█████▋ | 54/67 [00:14<00:03,  3.62it/s, loss=1.49, v_num=1, train_loss_step=1.620, val_loss=2.700, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 333:  82%|█████▋ | 55/67 [00:15<00:03,  3.61it/s, loss=1.49, v_num=1, train_loss_step=1.620, val_loss=2.700, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 333:  84%|█████▊ | 56/67 [00:15<00:03,  3.60it/s, loss=1.49, v_num=1, train_loss_step=1.620, val_loss=2.700, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 333:  85%|█████▉ | 57/67 [00:15<00:02,  3.58it/s, loss=1.49, v_num=1, train_loss_step=1.620, val_loss=2.700, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 333:  87%|██████ | 58/67 [00:16<00:02,  3.57it/s, loss=1.49, v_num=1, train_loss_step=1.620, val_loss=2.700, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 333:  88%|██████▏| 59/67 [00:16<00:02,  3.57it/s, loss=1.49, v_num=1, train_loss_step=1.620, val_loss=2.700, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 333:  90%|██████▎| 60/67 [00:16<00:01,  3.56it/s, loss=1.49, v_num=1, train_loss_step=1.620, val_loss=2.700, train_loss_epoch=1.590]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 333:  91%|██████▎| 61/67 [00:17<00:01,  3.50it/s, loss=1.49, v_num=1, train_loss_step=1.620, val_loss=2.700, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 333:  93%|██████▍| 62/67 [00:17<00:01,  3.49it/s, loss=1.49, v_num=1, train_loss_step=1.620, val_loss=2.700, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 333:  94%|██████▌| 63/67 [00:18<00:01,  3.48it/s, loss=1.49, v_num=1, train_loss_step=1.620, val_loss=2.700, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 333:  96%|██████▋| 64/67 [00:18<00:00,  3.48it/s, loss=1.49, v_num=1, train_loss_step=1.620, val_loss=2.700, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 333:  97%|██████▊| 65/67 [00:18<00:00,  3.47it/s, loss=1.49, v_num=1, train_loss_step=1.620, val_loss=2.700, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 333:  99%|██████▉| 66/67 [00:19<00:00,  3.47it/s, loss=1.49, v_num=1, train_loss_step=1.620, val_loss=2.700, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 333: 100%|███████| 67/67 [00:19<00:00,  3.39it/s, loss=1.49, v_num=1, train_loss_step=1.620, val_loss=2.700, train_loss_epoch=1.590]\u001b[A\n",
      "Epoch 334:  45%|███▏   | 30/67 [00:05<00:06,  5.35it/s, loss=1.36, v_num=1, train_loss_step=0.421, val_loss=2.700, train_loss_epoch=1.660]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 334:  46%|███▏   | 31/67 [00:07<00:08,  4.01it/s, loss=1.36, v_num=1, train_loss_step=0.421, val_loss=2.700, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 334:  48%|███▎   | 32/67 [00:08<00:08,  3.99it/s, loss=1.36, v_num=1, train_loss_step=0.421, val_loss=2.700, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 334:  49%|███▍   | 33/67 [00:08<00:08,  3.96it/s, loss=1.36, v_num=1, train_loss_step=0.421, val_loss=2.700, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 334:  51%|███▌   | 34/67 [00:08<00:08,  3.94it/s, loss=1.36, v_num=1, train_loss_step=0.421, val_loss=2.700, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 334:  52%|███▋   | 35/67 [00:08<00:08,  3.91it/s, loss=1.36, v_num=1, train_loss_step=0.421, val_loss=2.700, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 334:  54%|███▊   | 36/67 [00:09<00:07,  3.89it/s, loss=1.36, v_num=1, train_loss_step=0.421, val_loss=2.700, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 334:  55%|███▊   | 37/67 [00:09<00:07,  3.87it/s, loss=1.36, v_num=1, train_loss_step=0.421, val_loss=2.700, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 334:  57%|███▉   | 38/67 [00:09<00:07,  3.85it/s, loss=1.36, v_num=1, train_loss_step=0.421, val_loss=2.700, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 334:  58%|████   | 39/67 [00:10<00:07,  3.84it/s, loss=1.36, v_num=1, train_loss_step=0.421, val_loss=2.700, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 334:  60%|████▏  | 40/67 [00:10<00:07,  3.82it/s, loss=1.36, v_num=1, train_loss_step=0.421, val_loss=2.700, train_loss_epoch=1.660]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 334:  61%|████▎  | 41/67 [00:10<00:06,  3.75it/s, loss=1.36, v_num=1, train_loss_step=0.421, val_loss=2.700, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 334:  63%|████▍  | 42/67 [00:11<00:06,  3.74it/s, loss=1.36, v_num=1, train_loss_step=0.421, val_loss=2.700, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 334:  64%|████▍  | 43/67 [00:11<00:06,  3.72it/s, loss=1.36, v_num=1, train_loss_step=0.421, val_loss=2.700, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 334:  66%|████▌  | 44/67 [00:11<00:06,  3.71it/s, loss=1.36, v_num=1, train_loss_step=0.421, val_loss=2.700, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 334:  67%|████▋  | 45/67 [00:12<00:05,  3.70it/s, loss=1.36, v_num=1, train_loss_step=0.421, val_loss=2.700, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 334:  69%|████▊  | 46/67 [00:12<00:05,  3.69it/s, loss=1.36, v_num=1, train_loss_step=0.421, val_loss=2.700, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 334:  70%|████▉  | 47/67 [00:12<00:05,  3.68it/s, loss=1.36, v_num=1, train_loss_step=0.421, val_loss=2.700, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 334:  72%|█████  | 48/67 [00:13<00:05,  3.67it/s, loss=1.36, v_num=1, train_loss_step=0.421, val_loss=2.700, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 334:  73%|█████  | 49/67 [00:13<00:04,  3.66it/s, loss=1.36, v_num=1, train_loss_step=0.421, val_loss=2.700, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 334:  75%|█████▏ | 50/67 [00:13<00:04,  3.65it/s, loss=1.36, v_num=1, train_loss_step=0.421, val_loss=2.700, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 334:  76%|█████▎ | 51/67 [00:14<00:04,  3.60it/s, loss=1.36, v_num=1, train_loss_step=0.421, val_loss=2.700, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 334:  78%|█████▍ | 52/67 [00:14<00:04,  3.59it/s, loss=1.36, v_num=1, train_loss_step=0.421, val_loss=2.700, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 334:  79%|█████▌ | 53/67 [00:14<00:03,  3.59it/s, loss=1.36, v_num=1, train_loss_step=0.421, val_loss=2.700, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 334:  81%|█████▋ | 54/67 [00:15<00:03,  3.58it/s, loss=1.36, v_num=1, train_loss_step=0.421, val_loss=2.700, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 334:  82%|█████▋ | 55/67 [00:15<00:03,  3.57it/s, loss=1.36, v_num=1, train_loss_step=0.421, val_loss=2.700, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 334:  84%|█████▊ | 56/67 [00:15<00:03,  3.56it/s, loss=1.36, v_num=1, train_loss_step=0.421, val_loss=2.700, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 334:  85%|█████▉ | 57/67 [00:16<00:02,  3.56it/s, loss=1.36, v_num=1, train_loss_step=0.421, val_loss=2.700, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 334:  87%|██████ | 58/67 [00:16<00:02,  3.55it/s, loss=1.36, v_num=1, train_loss_step=0.421, val_loss=2.700, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 334:  88%|██████▏| 59/67 [00:16<00:02,  3.54it/s, loss=1.36, v_num=1, train_loss_step=0.421, val_loss=2.700, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 334:  90%|██████▎| 60/67 [00:16<00:01,  3.54it/s, loss=1.36, v_num=1, train_loss_step=0.421, val_loss=2.700, train_loss_epoch=1.660]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 334:  91%|██████▎| 61/67 [00:17<00:01,  3.50it/s, loss=1.36, v_num=1, train_loss_step=0.421, val_loss=2.700, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 334:  93%|██████▍| 62/67 [00:17<00:01,  3.50it/s, loss=1.36, v_num=1, train_loss_step=0.421, val_loss=2.700, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 334:  94%|██████▌| 63/67 [00:18<00:01,  3.50it/s, loss=1.36, v_num=1, train_loss_step=0.421, val_loss=2.700, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 334:  96%|██████▋| 64/67 [00:18<00:00,  3.49it/s, loss=1.36, v_num=1, train_loss_step=0.421, val_loss=2.700, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 334:  97%|██████▊| 65/67 [00:18<00:00,  3.49it/s, loss=1.36, v_num=1, train_loss_step=0.421, val_loss=2.700, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 334:  99%|██████▉| 66/67 [00:18<00:00,  3.49it/s, loss=1.36, v_num=1, train_loss_step=0.421, val_loss=2.700, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 334: 100%|███████| 67/67 [00:19<00:00,  3.41it/s, loss=1.36, v_num=1, train_loss_step=0.421, val_loss=2.700, train_loss_epoch=1.660]\u001b[A\n",
      "Epoch 335:  45%|███▏   | 30/67 [00:05<00:06,  5.46it/s, loss=1.81, v_num=1, train_loss_step=3.130, val_loss=2.700, train_loss_epoch=1.620]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 335:  46%|███▏   | 31/67 [00:07<00:08,  4.12it/s, loss=1.81, v_num=1, train_loss_step=3.130, val_loss=2.700, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 335:  48%|███▎   | 32/67 [00:07<00:08,  4.09it/s, loss=1.81, v_num=1, train_loss_step=3.130, val_loss=2.700, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 335:  49%|███▍   | 33/67 [00:08<00:08,  4.05it/s, loss=1.81, v_num=1, train_loss_step=3.130, val_loss=2.700, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 335:  51%|███▌   | 34/67 [00:08<00:08,  4.01it/s, loss=1.81, v_num=1, train_loss_step=3.130, val_loss=2.700, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 335:  52%|███▋   | 35/67 [00:08<00:08,  3.98it/s, loss=1.81, v_num=1, train_loss_step=3.130, val_loss=2.700, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 335:  54%|███▊   | 36/67 [00:09<00:07,  3.96it/s, loss=1.81, v_num=1, train_loss_step=3.130, val_loss=2.700, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 335:  55%|███▊   | 37/67 [00:09<00:07,  3.93it/s, loss=1.81, v_num=1, train_loss_step=3.130, val_loss=2.700, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 335:  57%|███▉   | 38/67 [00:09<00:07,  3.91it/s, loss=1.81, v_num=1, train_loss_step=3.130, val_loss=2.700, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 335:  58%|████   | 39/67 [00:10<00:07,  3.89it/s, loss=1.81, v_num=1, train_loss_step=3.130, val_loss=2.700, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 335:  60%|████▏  | 40/67 [00:10<00:06,  3.87it/s, loss=1.81, v_num=1, train_loss_step=3.130, val_loss=2.700, train_loss_epoch=1.620]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 335:  61%|████▎  | 41/67 [00:10<00:06,  3.79it/s, loss=1.81, v_num=1, train_loss_step=3.130, val_loss=2.700, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 335:  63%|████▍  | 42/67 [00:11<00:06,  3.78it/s, loss=1.81, v_num=1, train_loss_step=3.130, val_loss=2.700, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 335:  64%|████▍  | 43/67 [00:11<00:06,  3.75it/s, loss=1.81, v_num=1, train_loss_step=3.130, val_loss=2.700, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 335:  66%|████▌  | 44/67 [00:11<00:06,  3.74it/s, loss=1.81, v_num=1, train_loss_step=3.130, val_loss=2.700, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 335:  67%|████▋  | 45/67 [00:12<00:05,  3.73it/s, loss=1.81, v_num=1, train_loss_step=3.130, val_loss=2.700, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 335:  69%|████▊  | 46/67 [00:12<00:05,  3.71it/s, loss=1.81, v_num=1, train_loss_step=3.130, val_loss=2.700, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 335:  70%|████▉  | 47/67 [00:12<00:05,  3.70it/s, loss=1.81, v_num=1, train_loss_step=3.130, val_loss=2.700, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 335:  72%|█████  | 48/67 [00:12<00:05,  3.69it/s, loss=1.81, v_num=1, train_loss_step=3.130, val_loss=2.700, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 335:  73%|█████  | 49/67 [00:13<00:04,  3.68it/s, loss=1.81, v_num=1, train_loss_step=3.130, val_loss=2.700, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 335:  75%|█████▏ | 50/67 [00:13<00:04,  3.67it/s, loss=1.81, v_num=1, train_loss_step=3.130, val_loss=2.700, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 335:  76%|█████▎ | 51/67 [00:14<00:04,  3.61it/s, loss=1.81, v_num=1, train_loss_step=3.130, val_loss=2.700, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 335:  78%|█████▍ | 52/67 [00:14<00:04,  3.59it/s, loss=1.81, v_num=1, train_loss_step=3.130, val_loss=2.700, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 335:  79%|█████▌ | 53/67 [00:14<00:03,  3.59it/s, loss=1.81, v_num=1, train_loss_step=3.130, val_loss=2.700, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 335:  81%|█████▋ | 54/67 [00:15<00:03,  3.58it/s, loss=1.81, v_num=1, train_loss_step=3.130, val_loss=2.700, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 335:  82%|█████▋ | 55/67 [00:15<00:03,  3.57it/s, loss=1.81, v_num=1, train_loss_step=3.130, val_loss=2.700, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 335:  84%|█████▊ | 56/67 [00:15<00:03,  3.56it/s, loss=1.81, v_num=1, train_loss_step=3.130, val_loss=2.700, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 335:  85%|█████▉ | 57/67 [00:16<00:02,  3.55it/s, loss=1.81, v_num=1, train_loss_step=3.130, val_loss=2.700, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 335:  87%|██████ | 58/67 [00:16<00:02,  3.54it/s, loss=1.81, v_num=1, train_loss_step=3.130, val_loss=2.700, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 335:  88%|██████▏| 59/67 [00:16<00:02,  3.54it/s, loss=1.81, v_num=1, train_loss_step=3.130, val_loss=2.700, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 335:  90%|██████▎| 60/67 [00:16<00:01,  3.53it/s, loss=1.81, v_num=1, train_loss_step=3.130, val_loss=2.700, train_loss_epoch=1.620]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 335:  91%|██████▎| 61/67 [00:17<00:01,  3.50it/s, loss=1.81, v_num=1, train_loss_step=3.130, val_loss=2.700, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 335:  93%|██████▍| 62/67 [00:17<00:01,  3.49it/s, loss=1.81, v_num=1, train_loss_step=3.130, val_loss=2.700, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 335:  94%|██████▌| 63/67 [00:18<00:01,  3.48it/s, loss=1.81, v_num=1, train_loss_step=3.130, val_loss=2.700, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 335:  96%|██████▋| 64/67 [00:18<00:00,  3.48it/s, loss=1.81, v_num=1, train_loss_step=3.130, val_loss=2.700, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 335:  97%|██████▊| 65/67 [00:18<00:00,  3.47it/s, loss=1.81, v_num=1, train_loss_step=3.130, val_loss=2.700, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 335:  99%|██████▉| 66/67 [00:19<00:00,  3.46it/s, loss=1.81, v_num=1, train_loss_step=3.130, val_loss=2.700, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 335: 100%|███████| 67/67 [00:19<00:00,  3.37it/s, loss=1.81, v_num=1, train_loss_step=3.130, val_loss=2.700, train_loss_epoch=1.620]\u001b[A\n",
      "Epoch 336:  45%|███▏   | 30/67 [00:05<00:06,  5.38it/s, loss=1.96, v_num=1, train_loss_step=0.667, val_loss=2.700, train_loss_epoch=1.680]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 336:  46%|███▏   | 31/67 [00:07<00:08,  4.03it/s, loss=1.96, v_num=1, train_loss_step=0.667, val_loss=2.700, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 336:  48%|███▎   | 32/67 [00:07<00:08,  4.00it/s, loss=1.96, v_num=1, train_loss_step=0.667, val_loss=2.700, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 336:  49%|███▍   | 33/67 [00:08<00:08,  3.97it/s, loss=1.96, v_num=1, train_loss_step=0.667, val_loss=2.700, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 336:  51%|███▌   | 34/67 [00:08<00:08,  3.94it/s, loss=1.96, v_num=1, train_loss_step=0.667, val_loss=2.700, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 336:  52%|███▋   | 35/67 [00:08<00:08,  3.92it/s, loss=1.96, v_num=1, train_loss_step=0.667, val_loss=2.700, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 336:  54%|███▊   | 36/67 [00:09<00:07,  3.90it/s, loss=1.96, v_num=1, train_loss_step=0.667, val_loss=2.700, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 336:  55%|███▊   | 37/67 [00:09<00:07,  3.87it/s, loss=1.96, v_num=1, train_loss_step=0.667, val_loss=2.700, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 336:  57%|███▉   | 38/67 [00:09<00:07,  3.85it/s, loss=1.96, v_num=1, train_loss_step=0.667, val_loss=2.700, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 336:  58%|████   | 39/67 [00:10<00:07,  3.83it/s, loss=1.96, v_num=1, train_loss_step=0.667, val_loss=2.700, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 336:  60%|████▏  | 40/67 [00:10<00:07,  3.82it/s, loss=1.96, v_num=1, train_loss_step=0.667, val_loss=2.700, train_loss_epoch=1.680]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 336:  61%|████▎  | 41/67 [00:10<00:06,  3.73it/s, loss=1.96, v_num=1, train_loss_step=0.667, val_loss=2.700, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 336:  63%|████▍  | 42/67 [00:11<00:06,  3.72it/s, loss=1.96, v_num=1, train_loss_step=0.667, val_loss=2.700, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 336:  64%|████▍  | 43/67 [00:11<00:06,  3.70it/s, loss=1.96, v_num=1, train_loss_step=0.667, val_loss=2.700, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 336:  66%|████▌  | 44/67 [00:11<00:06,  3.68it/s, loss=1.96, v_num=1, train_loss_step=0.667, val_loss=2.700, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 336:  67%|████▋  | 45/67 [00:12<00:06,  3.66it/s, loss=1.96, v_num=1, train_loss_step=0.667, val_loss=2.700, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 336:  69%|████▊  | 46/67 [00:12<00:05,  3.64it/s, loss=1.96, v_num=1, train_loss_step=0.667, val_loss=2.700, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 336:  70%|████▉  | 47/67 [00:12<00:05,  3.62it/s, loss=1.96, v_num=1, train_loss_step=0.667, val_loss=2.700, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 336:  72%|█████  | 48/67 [00:13<00:05,  3.61it/s, loss=1.96, v_num=1, train_loss_step=0.667, val_loss=2.700, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 336:  73%|█████  | 49/67 [00:13<00:05,  3.55it/s, loss=1.96, v_num=1, train_loss_step=0.667, val_loss=2.700, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 336:  75%|█████▏ | 50/67 [00:14<00:04,  3.54it/s, loss=1.96, v_num=1, train_loss_step=0.667, val_loss=2.700, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 336:  76%|█████▎ | 51/67 [00:14<00:04,  3.49it/s, loss=1.96, v_num=1, train_loss_step=0.667, val_loss=2.700, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 336:  78%|█████▍ | 52/67 [00:14<00:04,  3.48it/s, loss=1.96, v_num=1, train_loss_step=0.667, val_loss=2.700, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 336:  79%|█████▌ | 53/67 [00:15<00:04,  3.47it/s, loss=1.96, v_num=1, train_loss_step=0.667, val_loss=2.700, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 336:  81%|█████▋ | 54/67 [00:15<00:03,  3.47it/s, loss=1.96, v_num=1, train_loss_step=0.667, val_loss=2.700, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 336:  82%|█████▋ | 55/67 [00:15<00:03,  3.45it/s, loss=1.96, v_num=1, train_loss_step=0.667, val_loss=2.700, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 336:  84%|█████▊ | 56/67 [00:16<00:03,  3.45it/s, loss=1.96, v_num=1, train_loss_step=0.667, val_loss=2.700, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 336:  85%|█████▉ | 57/67 [00:16<00:02,  3.44it/s, loss=1.96, v_num=1, train_loss_step=0.667, val_loss=2.700, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 336:  87%|██████ | 58/67 [00:16<00:02,  3.42it/s, loss=1.96, v_num=1, train_loss_step=0.667, val_loss=2.700, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 336:  88%|██████▏| 59/67 [00:17<00:02,  3.42it/s, loss=1.96, v_num=1, train_loss_step=0.667, val_loss=2.700, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 336:  90%|██████▎| 60/67 [00:17<00:02,  3.41it/s, loss=1.96, v_num=1, train_loss_step=0.667, val_loss=2.700, train_loss_epoch=1.680]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 336:  91%|██████▎| 61/67 [00:18<00:01,  3.35it/s, loss=1.96, v_num=1, train_loss_step=0.667, val_loss=2.700, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 336:  93%|██████▍| 62/67 [00:18<00:01,  3.35it/s, loss=1.96, v_num=1, train_loss_step=0.667, val_loss=2.700, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 336:  94%|██████▌| 63/67 [00:18<00:01,  3.35it/s, loss=1.96, v_num=1, train_loss_step=0.667, val_loss=2.700, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 336:  96%|██████▋| 64/67 [00:19<00:00,  3.34it/s, loss=1.96, v_num=1, train_loss_step=0.667, val_loss=2.700, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 336:  97%|██████▊| 65/67 [00:19<00:00,  3.35it/s, loss=1.96, v_num=1, train_loss_step=0.667, val_loss=2.700, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 336:  99%|██████▉| 66/67 [00:19<00:00,  3.35it/s, loss=1.96, v_num=1, train_loss_step=0.667, val_loss=2.700, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 336: 100%|███████| 67/67 [00:20<00:00,  3.28it/s, loss=1.96, v_num=1, train_loss_step=0.667, val_loss=2.700, train_loss_epoch=1.680]\u001b[A\n",
      "Epoch 337:  45%|███▏   | 30/67 [00:05<00:07,  5.28it/s, loss=1.87, v_num=1, train_loss_step=2.020, val_loss=2.700, train_loss_epoch=1.800]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 337:  46%|███▏   | 31/67 [00:08<00:09,  3.84it/s, loss=1.87, v_num=1, train_loss_step=2.020, val_loss=2.700, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 337:  48%|███▎   | 32/67 [00:08<00:09,  3.82it/s, loss=1.87, v_num=1, train_loss_step=2.020, val_loss=2.700, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 337:  49%|███▍   | 33/67 [00:08<00:08,  3.79it/s, loss=1.87, v_num=1, train_loss_step=2.020, val_loss=2.700, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 337:  51%|███▌   | 34/67 [00:09<00:08,  3.76it/s, loss=1.87, v_num=1, train_loss_step=2.020, val_loss=2.700, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 337:  52%|███▋   | 35/67 [00:09<00:08,  3.74it/s, loss=1.87, v_num=1, train_loss_step=2.020, val_loss=2.700, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 337:  54%|███▊   | 36/67 [00:09<00:08,  3.72it/s, loss=1.87, v_num=1, train_loss_step=2.020, val_loss=2.700, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 337:  55%|███▊   | 37/67 [00:10<00:08,  3.70it/s, loss=1.87, v_num=1, train_loss_step=2.020, val_loss=2.700, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 337:  57%|███▉   | 38/67 [00:10<00:07,  3.67it/s, loss=1.87, v_num=1, train_loss_step=2.020, val_loss=2.700, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 337:  58%|████   | 39/67 [00:10<00:07,  3.65it/s, loss=1.87, v_num=1, train_loss_step=2.020, val_loss=2.700, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 337:  60%|████▏  | 40/67 [00:11<00:07,  3.64it/s, loss=1.87, v_num=1, train_loss_step=2.020, val_loss=2.700, train_loss_epoch=1.800]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 337:  61%|████▎  | 41/67 [00:11<00:07,  3.57it/s, loss=1.87, v_num=1, train_loss_step=2.020, val_loss=2.700, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 337:  63%|████▍  | 42/67 [00:11<00:07,  3.56it/s, loss=1.87, v_num=1, train_loss_step=2.020, val_loss=2.700, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 337:  64%|████▍  | 43/67 [00:12<00:06,  3.54it/s, loss=1.87, v_num=1, train_loss_step=2.020, val_loss=2.700, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 337:  66%|████▌  | 44/67 [00:12<00:06,  3.53it/s, loss=1.87, v_num=1, train_loss_step=2.020, val_loss=2.700, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 337:  67%|████▋  | 45/67 [00:12<00:06,  3.52it/s, loss=1.87, v_num=1, train_loss_step=2.020, val_loss=2.700, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 337:  69%|████▊  | 46/67 [00:13<00:05,  3.52it/s, loss=1.87, v_num=1, train_loss_step=2.020, val_loss=2.700, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 337:  70%|████▉  | 47/67 [00:13<00:05,  3.51it/s, loss=1.87, v_num=1, train_loss_step=2.020, val_loss=2.700, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 337:  72%|█████  | 48/67 [00:13<00:05,  3.50it/s, loss=1.87, v_num=1, train_loss_step=2.020, val_loss=2.700, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 337:  73%|█████  | 49/67 [00:14<00:05,  3.49it/s, loss=1.87, v_num=1, train_loss_step=2.020, val_loss=2.700, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 337:  75%|█████▏ | 50/67 [00:14<00:04,  3.49it/s, loss=1.87, v_num=1, train_loss_step=2.020, val_loss=2.700, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 337:  76%|█████▎ | 51/67 [00:14<00:04,  3.45it/s, loss=1.87, v_num=1, train_loss_step=2.020, val_loss=2.700, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 337:  78%|█████▍ | 52/67 [00:15<00:04,  3.44it/s, loss=1.87, v_num=1, train_loss_step=2.020, val_loss=2.700, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 337:  79%|█████▌ | 53/67 [00:15<00:04,  3.43it/s, loss=1.87, v_num=1, train_loss_step=2.020, val_loss=2.700, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 337:  81%|█████▋ | 54/67 [00:15<00:03,  3.42it/s, loss=1.87, v_num=1, train_loss_step=2.020, val_loss=2.700, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 337:  82%|█████▋ | 55/67 [00:16<00:03,  3.42it/s, loss=1.87, v_num=1, train_loss_step=2.020, val_loss=2.700, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 337:  84%|█████▊ | 56/67 [00:16<00:03,  3.41it/s, loss=1.87, v_num=1, train_loss_step=2.020, val_loss=2.700, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 337:  85%|█████▉ | 57/67 [00:16<00:02,  3.41it/s, loss=1.87, v_num=1, train_loss_step=2.020, val_loss=2.700, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 337:  87%|██████ | 58/67 [00:17<00:02,  3.40it/s, loss=1.87, v_num=1, train_loss_step=2.020, val_loss=2.700, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 337:  88%|██████▏| 59/67 [00:17<00:02,  3.40it/s, loss=1.87, v_num=1, train_loss_step=2.020, val_loss=2.700, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 337:  90%|██████▎| 60/67 [00:17<00:02,  3.39it/s, loss=1.87, v_num=1, train_loss_step=2.020, val_loss=2.700, train_loss_epoch=1.800]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 337:  91%|██████▎| 61/67 [00:18<00:01,  3.35it/s, loss=1.87, v_num=1, train_loss_step=2.020, val_loss=2.700, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 337:  93%|██████▍| 62/67 [00:18<00:01,  3.35it/s, loss=1.87, v_num=1, train_loss_step=2.020, val_loss=2.700, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 337:  94%|██████▌| 63/67 [00:18<00:01,  3.35it/s, loss=1.87, v_num=1, train_loss_step=2.020, val_loss=2.700, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 337:  96%|██████▋| 64/67 [00:19<00:00,  3.34it/s, loss=1.87, v_num=1, train_loss_step=2.020, val_loss=2.700, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 337:  97%|██████▊| 65/67 [00:19<00:00,  3.34it/s, loss=1.87, v_num=1, train_loss_step=2.020, val_loss=2.700, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 337:  99%|██████▉| 66/67 [00:19<00:00,  3.34it/s, loss=1.87, v_num=1, train_loss_step=2.020, val_loss=2.700, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 337: 100%|███████| 67/67 [00:20<00:00,  3.28it/s, loss=1.87, v_num=1, train_loss_step=2.020, val_loss=2.700, train_loss_epoch=1.800]\u001b[A\n",
      "Epoch 338:  45%|███▏   | 30/67 [00:05<00:06,  5.35it/s, loss=1.76, v_num=1, train_loss_step=1.590, val_loss=2.700, train_loss_epoch=1.850]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 338:  46%|███▏   | 31/67 [00:07<00:08,  4.01it/s, loss=1.76, v_num=1, train_loss_step=1.590, val_loss=2.700, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 338:  48%|███▎   | 32/67 [00:08<00:08,  3.99it/s, loss=1.76, v_num=1, train_loss_step=1.590, val_loss=2.700, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 338:  49%|███▍   | 33/67 [00:08<00:08,  3.95it/s, loss=1.76, v_num=1, train_loss_step=1.590, val_loss=2.700, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 338:  51%|███▌   | 34/67 [00:08<00:08,  3.92it/s, loss=1.76, v_num=1, train_loss_step=1.590, val_loss=2.700, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 338:  52%|███▋   | 35/67 [00:08<00:08,  3.90it/s, loss=1.76, v_num=1, train_loss_step=1.590, val_loss=2.700, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 338:  54%|███▊   | 36/67 [00:09<00:07,  3.88it/s, loss=1.76, v_num=1, train_loss_step=1.590, val_loss=2.700, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 338:  55%|███▊   | 37/67 [00:09<00:07,  3.86it/s, loss=1.76, v_num=1, train_loss_step=1.590, val_loss=2.700, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 338:  57%|███▉   | 38/67 [00:09<00:07,  3.85it/s, loss=1.76, v_num=1, train_loss_step=1.590, val_loss=2.700, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 338:  58%|████   | 39/67 [00:10<00:07,  3.83it/s, loss=1.76, v_num=1, train_loss_step=1.590, val_loss=2.700, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 338:  60%|████▏  | 40/67 [00:10<00:07,  3.81it/s, loss=1.76, v_num=1, train_loss_step=1.590, val_loss=2.700, train_loss_epoch=1.850]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 338:  61%|████▎  | 41/67 [00:10<00:06,  3.74it/s, loss=1.76, v_num=1, train_loss_step=1.590, val_loss=2.700, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 338:  63%|████▍  | 42/67 [00:11<00:06,  3.73it/s, loss=1.76, v_num=1, train_loss_step=1.590, val_loss=2.700, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 338:  64%|████▍  | 43/67 [00:11<00:06,  3.72it/s, loss=1.76, v_num=1, train_loss_step=1.590, val_loss=2.700, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 338:  66%|████▌  | 44/67 [00:11<00:06,  3.71it/s, loss=1.76, v_num=1, train_loss_step=1.590, val_loss=2.700, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 338:  67%|████▋  | 45/67 [00:12<00:05,  3.70it/s, loss=1.76, v_num=1, train_loss_step=1.590, val_loss=2.700, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 338:  69%|████▊  | 46/67 [00:12<00:05,  3.68it/s, loss=1.76, v_num=1, train_loss_step=1.590, val_loss=2.700, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 338:  70%|████▉  | 47/67 [00:12<00:05,  3.68it/s, loss=1.76, v_num=1, train_loss_step=1.590, val_loss=2.700, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 338:  72%|█████  | 48/67 [00:13<00:05,  3.67it/s, loss=1.76, v_num=1, train_loss_step=1.590, val_loss=2.700, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 338:  73%|█████  | 49/67 [00:13<00:04,  3.66it/s, loss=1.76, v_num=1, train_loss_step=1.590, val_loss=2.700, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 338:  75%|█████▏ | 50/67 [00:13<00:04,  3.65it/s, loss=1.76, v_num=1, train_loss_step=1.590, val_loss=2.700, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 338:  76%|█████▎ | 51/67 [00:14<00:04,  3.60it/s, loss=1.76, v_num=1, train_loss_step=1.590, val_loss=2.700, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 338:  78%|█████▍ | 52/67 [00:14<00:04,  3.59it/s, loss=1.76, v_num=1, train_loss_step=1.590, val_loss=2.700, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 338:  79%|█████▌ | 53/67 [00:14<00:03,  3.59it/s, loss=1.76, v_num=1, train_loss_step=1.590, val_loss=2.700, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 338:  81%|█████▋ | 54/67 [00:15<00:03,  3.58it/s, loss=1.76, v_num=1, train_loss_step=1.590, val_loss=2.700, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 338:  82%|█████▋ | 55/67 [00:15<00:03,  3.58it/s, loss=1.76, v_num=1, train_loss_step=1.590, val_loss=2.700, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 338:  84%|█████▊ | 56/67 [00:15<00:03,  3.57it/s, loss=1.76, v_num=1, train_loss_step=1.590, val_loss=2.700, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 338:  85%|█████▉ | 57/67 [00:15<00:02,  3.56it/s, loss=1.76, v_num=1, train_loss_step=1.590, val_loss=2.700, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 338:  87%|██████ | 58/67 [00:16<00:02,  3.56it/s, loss=1.76, v_num=1, train_loss_step=1.590, val_loss=2.700, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 338:  88%|██████▏| 59/67 [00:16<00:02,  3.55it/s, loss=1.76, v_num=1, train_loss_step=1.590, val_loss=2.700, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 338:  90%|██████▎| 60/67 [00:16<00:01,  3.54it/s, loss=1.76, v_num=1, train_loss_step=1.590, val_loss=2.700, train_loss_epoch=1.850]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 338:  91%|██████▎| 61/67 [00:17<00:01,  3.50it/s, loss=1.76, v_num=1, train_loss_step=1.590, val_loss=2.700, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 338:  93%|██████▍| 62/67 [00:17<00:01,  3.49it/s, loss=1.76, v_num=1, train_loss_step=1.590, val_loss=2.700, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 338:  94%|██████▌| 63/67 [00:18<00:01,  3.49it/s, loss=1.76, v_num=1, train_loss_step=1.590, val_loss=2.700, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 338:  96%|██████▋| 64/67 [00:18<00:00,  3.48it/s, loss=1.76, v_num=1, train_loss_step=1.590, val_loss=2.700, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 338:  97%|██████▊| 65/67 [00:18<00:00,  3.47it/s, loss=1.76, v_num=1, train_loss_step=1.590, val_loss=2.700, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 338:  99%|██████▉| 66/67 [00:19<00:00,  3.46it/s, loss=1.76, v_num=1, train_loss_step=1.590, val_loss=2.700, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 338: 100%|███████| 67/67 [00:19<00:00,  3.38it/s, loss=1.76, v_num=1, train_loss_step=1.590, val_loss=2.700, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 339:  45%|███▏   | 30/67 [00:05<00:06,  5.35it/s, loss=2.23, v_num=1, train_loss_step=1.630, val_loss=2.700, train_loss_epoch=1.810]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 339:  46%|███▏   | 31/67 [00:07<00:08,  4.01it/s, loss=2.23, v_num=1, train_loss_step=1.630, val_loss=2.700, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 339:  48%|███▎   | 32/67 [00:08<00:08,  3.97it/s, loss=2.23, v_num=1, train_loss_step=1.630, val_loss=2.700, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 339:  49%|███▍   | 33/67 [00:08<00:08,  3.94it/s, loss=2.23, v_num=1, train_loss_step=1.630, val_loss=2.700, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 339:  51%|███▌   | 34/67 [00:08<00:08,  3.91it/s, loss=2.23, v_num=1, train_loss_step=1.630, val_loss=2.700, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 339:  52%|███▋   | 35/67 [00:09<00:08,  3.88it/s, loss=2.23, v_num=1, train_loss_step=1.630, val_loss=2.700, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 339:  54%|███▊   | 36/67 [00:09<00:08,  3.85it/s, loss=2.23, v_num=1, train_loss_step=1.630, val_loss=2.700, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 339:  55%|███▊   | 37/67 [00:09<00:07,  3.83it/s, loss=2.23, v_num=1, train_loss_step=1.630, val_loss=2.700, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 339:  57%|███▉   | 38/67 [00:09<00:07,  3.81it/s, loss=2.23, v_num=1, train_loss_step=1.630, val_loss=2.700, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 339:  58%|████   | 39/67 [00:10<00:07,  3.78it/s, loss=2.23, v_num=1, train_loss_step=1.630, val_loss=2.700, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 339:  60%|████▏  | 40/67 [00:10<00:07,  3.76it/s, loss=2.23, v_num=1, train_loss_step=1.630, val_loss=2.700, train_loss_epoch=1.810]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 339:  61%|████▎  | 41/67 [00:11<00:07,  3.67it/s, loss=2.23, v_num=1, train_loss_step=1.630, val_loss=2.700, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 339:  63%|████▍  | 42/67 [00:11<00:06,  3.66it/s, loss=2.23, v_num=1, train_loss_step=1.630, val_loss=2.700, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 339:  64%|████▍  | 43/67 [00:11<00:06,  3.65it/s, loss=2.23, v_num=1, train_loss_step=1.630, val_loss=2.700, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 339:  66%|████▌  | 44/67 [00:12<00:06,  3.64it/s, loss=2.23, v_num=1, train_loss_step=1.630, val_loss=2.700, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 339:  67%|████▋  | 45/67 [00:12<00:06,  3.63it/s, loss=2.23, v_num=1, train_loss_step=1.630, val_loss=2.700, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 339:  69%|████▊  | 46/67 [00:12<00:05,  3.62it/s, loss=2.23, v_num=1, train_loss_step=1.630, val_loss=2.700, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 339:  70%|████▉  | 47/67 [00:13<00:05,  3.61it/s, loss=2.23, v_num=1, train_loss_step=1.630, val_loss=2.700, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 339:  72%|█████  | 48/67 [00:13<00:05,  3.60it/s, loss=2.23, v_num=1, train_loss_step=1.630, val_loss=2.700, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 339:  73%|█████  | 49/67 [00:13<00:05,  3.58it/s, loss=2.23, v_num=1, train_loss_step=1.630, val_loss=2.700, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 339:  75%|█████▏ | 50/67 [00:14<00:04,  3.57it/s, loss=2.23, v_num=1, train_loss_step=1.630, val_loss=2.700, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 339:  76%|█████▎ | 51/67 [00:14<00:04,  3.53it/s, loss=2.23, v_num=1, train_loss_step=1.630, val_loss=2.700, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 339:  78%|█████▍ | 52/67 [00:14<00:04,  3.52it/s, loss=2.23, v_num=1, train_loss_step=1.630, val_loss=2.700, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 339:  79%|█████▌ | 53/67 [00:15<00:03,  3.52it/s, loss=2.23, v_num=1, train_loss_step=1.630, val_loss=2.700, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 339:  81%|█████▋ | 54/67 [00:15<00:03,  3.51it/s, loss=2.23, v_num=1, train_loss_step=1.630, val_loss=2.700, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 339:  82%|█████▋ | 55/67 [00:15<00:03,  3.50it/s, loss=2.23, v_num=1, train_loss_step=1.630, val_loss=2.700, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 339:  84%|█████▊ | 56/67 [00:16<00:03,  3.49it/s, loss=2.23, v_num=1, train_loss_step=1.630, val_loss=2.700, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 339:  85%|█████▉ | 57/67 [00:16<00:02,  3.49it/s, loss=2.23, v_num=1, train_loss_step=1.630, val_loss=2.700, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 339:  87%|██████ | 58/67 [00:16<00:02,  3.48it/s, loss=2.23, v_num=1, train_loss_step=1.630, val_loss=2.700, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 339:  88%|██████▏| 59/67 [00:16<00:02,  3.48it/s, loss=2.23, v_num=1, train_loss_step=1.630, val_loss=2.700, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 339:  90%|██████▎| 60/67 [00:17<00:02,  3.47it/s, loss=2.23, v_num=1, train_loss_step=1.630, val_loss=2.700, train_loss_epoch=1.810]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 339:  91%|██████▎| 61/67 [00:17<00:01,  3.43it/s, loss=2.23, v_num=1, train_loss_step=1.630, val_loss=2.700, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 339:  93%|██████▍| 62/67 [00:18<00:01,  3.42it/s, loss=2.23, v_num=1, train_loss_step=1.630, val_loss=2.700, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 339:  94%|██████▌| 63/67 [00:18<00:01,  3.42it/s, loss=2.23, v_num=1, train_loss_step=1.630, val_loss=2.700, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 339:  96%|██████▋| 64/67 [00:18<00:00,  3.42it/s, loss=2.23, v_num=1, train_loss_step=1.630, val_loss=2.700, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 339:  97%|██████▊| 65/67 [00:19<00:00,  3.42it/s, loss=2.23, v_num=1, train_loss_step=1.630, val_loss=2.700, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 339:  99%|██████▉| 66/67 [00:19<00:00,  3.41it/s, loss=2.23, v_num=1, train_loss_step=1.630, val_loss=2.700, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 339: 100%|███████| 67/67 [00:20<00:00,  3.33it/s, loss=2.23, v_num=1, train_loss_step=1.630, val_loss=2.700, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 340:  45%|███▏   | 30/67 [00:05<00:07,  5.28it/s, loss=1.73, v_num=1, train_loss_step=2.650, val_loss=2.700, train_loss_epoch=2.020]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 340:  46%|███▏   | 31/67 [00:07<00:09,  3.99it/s, loss=1.73, v_num=1, train_loss_step=2.650, val_loss=2.700, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 340:  48%|███▎   | 32/67 [00:08<00:08,  3.96it/s, loss=1.73, v_num=1, train_loss_step=2.650, val_loss=2.700, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 340:  49%|███▍   | 33/67 [00:08<00:08,  3.93it/s, loss=1.73, v_num=1, train_loss_step=2.650, val_loss=2.700, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 340:  51%|███▌   | 34/67 [00:08<00:08,  3.90it/s, loss=1.73, v_num=1, train_loss_step=2.650, val_loss=2.700, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 340:  52%|███▋   | 35/67 [00:09<00:08,  3.86it/s, loss=1.73, v_num=1, train_loss_step=2.650, val_loss=2.700, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 340:  54%|███▊   | 36/67 [00:09<00:08,  3.84it/s, loss=1.73, v_num=1, train_loss_step=2.650, val_loss=2.700, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 340:  55%|███▊   | 37/67 [00:09<00:07,  3.81it/s, loss=1.73, v_num=1, train_loss_step=2.650, val_loss=2.700, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 340:  57%|███▉   | 38/67 [00:10<00:07,  3.80it/s, loss=1.73, v_num=1, train_loss_step=2.650, val_loss=2.700, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 340:  58%|████   | 39/67 [00:10<00:07,  3.77it/s, loss=1.73, v_num=1, train_loss_step=2.650, val_loss=2.700, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 340:  60%|████▏  | 40/67 [00:10<00:07,  3.76it/s, loss=1.73, v_num=1, train_loss_step=2.650, val_loss=2.700, train_loss_epoch=2.020]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 340:  61%|████▎  | 41/67 [00:11<00:07,  3.68it/s, loss=1.73, v_num=1, train_loss_step=2.650, val_loss=2.700, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 340:  63%|████▍  | 42/67 [00:11<00:06,  3.67it/s, loss=1.73, v_num=1, train_loss_step=2.650, val_loss=2.700, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 340:  64%|████▍  | 43/67 [00:11<00:06,  3.66it/s, loss=1.73, v_num=1, train_loss_step=2.650, val_loss=2.700, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 340:  66%|████▌  | 44/67 [00:12<00:06,  3.65it/s, loss=1.73, v_num=1, train_loss_step=2.650, val_loss=2.700, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 340:  67%|████▋  | 45/67 [00:12<00:06,  3.64it/s, loss=1.73, v_num=1, train_loss_step=2.650, val_loss=2.700, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 340:  69%|████▊  | 46/67 [00:12<00:05,  3.63it/s, loss=1.73, v_num=1, train_loss_step=2.650, val_loss=2.700, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 340:  70%|████▉  | 47/67 [00:12<00:05,  3.62it/s, loss=1.73, v_num=1, train_loss_step=2.650, val_loss=2.700, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 340:  72%|█████  | 48/67 [00:13<00:05,  3.62it/s, loss=1.73, v_num=1, train_loss_step=2.650, val_loss=2.700, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 340:  73%|█████  | 49/67 [00:13<00:04,  3.61it/s, loss=1.73, v_num=1, train_loss_step=2.650, val_loss=2.700, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 340:  75%|█████▏ | 50/67 [00:13<00:04,  3.60it/s, loss=1.73, v_num=1, train_loss_step=2.650, val_loss=2.700, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 340:  76%|█████▎ | 51/67 [00:14<00:04,  3.55it/s, loss=1.73, v_num=1, train_loss_step=2.650, val_loss=2.700, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 340:  78%|█████▍ | 52/67 [00:14<00:04,  3.55it/s, loss=1.73, v_num=1, train_loss_step=2.650, val_loss=2.700, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 340:  79%|█████▌ | 53/67 [00:14<00:03,  3.54it/s, loss=1.73, v_num=1, train_loss_step=2.650, val_loss=2.700, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 340:  81%|█████▋ | 54/67 [00:15<00:03,  3.53it/s, loss=1.73, v_num=1, train_loss_step=2.650, val_loss=2.700, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 340:  82%|█████▋ | 55/67 [00:15<00:03,  3.52it/s, loss=1.73, v_num=1, train_loss_step=2.650, val_loss=2.700, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 340:  84%|█████▊ | 56/67 [00:15<00:03,  3.51it/s, loss=1.73, v_num=1, train_loss_step=2.650, val_loss=2.700, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 340:  85%|█████▉ | 57/67 [00:16<00:02,  3.50it/s, loss=1.73, v_num=1, train_loss_step=2.650, val_loss=2.700, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 340:  87%|██████ | 58/67 [00:16<00:02,  3.49it/s, loss=1.73, v_num=1, train_loss_step=2.650, val_loss=2.700, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 340:  88%|██████▏| 59/67 [00:16<00:02,  3.48it/s, loss=1.73, v_num=1, train_loss_step=2.650, val_loss=2.700, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 340:  90%|██████▎| 60/67 [00:17<00:02,  3.48it/s, loss=1.73, v_num=1, train_loss_step=2.650, val_loss=2.700, train_loss_epoch=2.020]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 340:  91%|██████▎| 61/67 [00:17<00:01,  3.44it/s, loss=1.73, v_num=1, train_loss_step=2.650, val_loss=2.700, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 340:  93%|██████▍| 62/67 [00:18<00:01,  3.44it/s, loss=1.73, v_num=1, train_loss_step=2.650, val_loss=2.700, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 340:  94%|██████▌| 63/67 [00:18<00:01,  3.43it/s, loss=1.73, v_num=1, train_loss_step=2.650, val_loss=2.700, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 340:  96%|██████▋| 64/67 [00:18<00:00,  3.43it/s, loss=1.73, v_num=1, train_loss_step=2.650, val_loss=2.700, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 340:  97%|██████▊| 65/67 [00:18<00:00,  3.42it/s, loss=1.73, v_num=1, train_loss_step=2.650, val_loss=2.700, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 340:  99%|██████▉| 66/67 [00:19<00:00,  3.42it/s, loss=1.73, v_num=1, train_loss_step=2.650, val_loss=2.700, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 340: 100%|███████| 67/67 [00:20<00:00,  3.35it/s, loss=1.73, v_num=1, train_loss_step=2.650, val_loss=2.690, train_loss_epoch=2.020]\u001b[A\n",
      "Epoch 341:  45%|███▏   | 30/67 [00:05<00:06,  5.42it/s, loss=1.67, v_num=1, train_loss_step=0.913, val_loss=2.690, train_loss_epoch=1.850]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 341:  46%|███▏   | 31/67 [00:07<00:08,  4.06it/s, loss=1.67, v_num=1, train_loss_step=0.913, val_loss=2.690, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 341:  48%|███▎   | 32/67 [00:07<00:08,  4.03it/s, loss=1.67, v_num=1, train_loss_step=0.913, val_loss=2.690, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 341:  49%|███▍   | 33/67 [00:08<00:08,  4.00it/s, loss=1.67, v_num=1, train_loss_step=0.913, val_loss=2.690, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 341:  51%|███▌   | 34/67 [00:08<00:08,  3.97it/s, loss=1.67, v_num=1, train_loss_step=0.913, val_loss=2.690, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 341:  52%|███▋   | 35/67 [00:08<00:08,  3.95it/s, loss=1.67, v_num=1, train_loss_step=0.913, val_loss=2.690, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 341:  54%|███▊   | 36/67 [00:09<00:07,  3.93it/s, loss=1.67, v_num=1, train_loss_step=0.913, val_loss=2.690, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 341:  55%|███▊   | 37/67 [00:09<00:07,  3.90it/s, loss=1.67, v_num=1, train_loss_step=0.913, val_loss=2.690, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 341:  57%|███▉   | 38/67 [00:09<00:07,  3.88it/s, loss=1.67, v_num=1, train_loss_step=0.913, val_loss=2.690, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 341:  58%|████   | 39/67 [00:10<00:07,  3.85it/s, loss=1.67, v_num=1, train_loss_step=0.913, val_loss=2.690, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 341:  60%|████▏  | 40/67 [00:10<00:07,  3.81it/s, loss=1.67, v_num=1, train_loss_step=0.913, val_loss=2.690, train_loss_epoch=1.850]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 341:  61%|████▎  | 41/67 [00:11<00:07,  3.70it/s, loss=1.67, v_num=1, train_loss_step=0.913, val_loss=2.690, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 341:  63%|████▍  | 42/67 [00:11<00:06,  3.68it/s, loss=1.67, v_num=1, train_loss_step=0.913, val_loss=2.690, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 341:  64%|████▍  | 43/67 [00:11<00:06,  3.65it/s, loss=1.67, v_num=1, train_loss_step=0.913, val_loss=2.690, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 341:  66%|████▌  | 44/67 [00:12<00:06,  3.64it/s, loss=1.67, v_num=1, train_loss_step=0.913, val_loss=2.690, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 341:  67%|████▋  | 45/67 [00:12<00:06,  3.62it/s, loss=1.67, v_num=1, train_loss_step=0.913, val_loss=2.690, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 341:  69%|████▊  | 46/67 [00:12<00:05,  3.60it/s, loss=1.67, v_num=1, train_loss_step=0.913, val_loss=2.690, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 341:  70%|████▉  | 47/67 [00:13<00:05,  3.59it/s, loss=1.67, v_num=1, train_loss_step=0.913, val_loss=2.690, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 341:  72%|█████  | 48/67 [00:13<00:05,  3.58it/s, loss=1.67, v_num=1, train_loss_step=0.913, val_loss=2.690, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 341:  73%|█████  | 49/67 [00:13<00:05,  3.57it/s, loss=1.67, v_num=1, train_loss_step=0.913, val_loss=2.690, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 341:  75%|█████▏ | 50/67 [00:14<00:04,  3.55it/s, loss=1.67, v_num=1, train_loss_step=0.913, val_loss=2.690, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 341:  76%|█████▎ | 51/67 [00:14<00:04,  3.50it/s, loss=1.67, v_num=1, train_loss_step=0.913, val_loss=2.690, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 341:  78%|█████▍ | 52/67 [00:14<00:04,  3.49it/s, loss=1.67, v_num=1, train_loss_step=0.913, val_loss=2.690, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 341:  79%|█████▌ | 53/67 [00:15<00:04,  3.48it/s, loss=1.67, v_num=1, train_loss_step=0.913, val_loss=2.690, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 341:  81%|█████▋ | 54/67 [00:15<00:03,  3.48it/s, loss=1.67, v_num=1, train_loss_step=0.913, val_loss=2.690, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 341:  82%|█████▋ | 55/67 [00:15<00:03,  3.47it/s, loss=1.67, v_num=1, train_loss_step=0.913, val_loss=2.690, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 341:  84%|█████▊ | 56/67 [00:16<00:03,  3.46it/s, loss=1.67, v_num=1, train_loss_step=0.913, val_loss=2.690, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 341:  85%|█████▉ | 57/67 [00:16<00:02,  3.46it/s, loss=1.67, v_num=1, train_loss_step=0.913, val_loss=2.690, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 341:  87%|██████ | 58/67 [00:16<00:02,  3.46it/s, loss=1.67, v_num=1, train_loss_step=0.913, val_loss=2.690, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 341:  88%|██████▏| 59/67 [00:17<00:02,  3.45it/s, loss=1.67, v_num=1, train_loss_step=0.913, val_loss=2.690, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 341:  90%|██████▎| 60/67 [00:17<00:02,  3.45it/s, loss=1.67, v_num=1, train_loss_step=0.913, val_loss=2.690, train_loss_epoch=1.850]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 341:  91%|██████▎| 61/67 [00:17<00:01,  3.41it/s, loss=1.67, v_num=1, train_loss_step=0.913, val_loss=2.690, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 341:  93%|██████▍| 62/67 [00:18<00:01,  3.41it/s, loss=1.67, v_num=1, train_loss_step=0.913, val_loss=2.690, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 341:  94%|██████▌| 63/67 [00:18<00:01,  3.41it/s, loss=1.67, v_num=1, train_loss_step=0.913, val_loss=2.690, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 341:  96%|██████▋| 64/67 [00:18<00:00,  3.40it/s, loss=1.67, v_num=1, train_loss_step=0.913, val_loss=2.690, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 341:  97%|██████▊| 65/67 [00:19<00:00,  3.40it/s, loss=1.67, v_num=1, train_loss_step=0.913, val_loss=2.690, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 341:  99%|██████▉| 66/67 [00:19<00:00,  3.40it/s, loss=1.67, v_num=1, train_loss_step=0.913, val_loss=2.690, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 341: 100%|███████| 67/67 [00:20<00:00,  3.33it/s, loss=1.67, v_num=1, train_loss_step=0.913, val_loss=2.700, train_loss_epoch=1.850]\u001b[A\n",
      "Epoch 342:  45%|███▏   | 30/67 [00:05<00:07,  5.25it/s, loss=2.54, v_num=1, train_loss_step=1.270, val_loss=2.700, train_loss_epoch=1.600]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 342:  46%|███▏   | 31/67 [00:07<00:09,  3.99it/s, loss=2.54, v_num=1, train_loss_step=1.270, val_loss=2.700, train_loss_epoch=1.600]\u001b[A\n",
      "Epoch 342:  48%|███▎   | 32/67 [00:08<00:08,  3.96it/s, loss=2.54, v_num=1, train_loss_step=1.270, val_loss=2.700, train_loss_epoch=1.600]\u001b[A\n",
      "Epoch 342:  49%|███▍   | 33/67 [00:08<00:08,  3.94it/s, loss=2.54, v_num=1, train_loss_step=1.270, val_loss=2.700, train_loss_epoch=1.600]\u001b[A\n",
      "Epoch 342:  51%|███▌   | 34/67 [00:08<00:08,  3.91it/s, loss=2.54, v_num=1, train_loss_step=1.270, val_loss=2.700, train_loss_epoch=1.600]\u001b[A\n",
      "Epoch 342:  52%|███▋   | 35/67 [00:08<00:08,  3.89it/s, loss=2.54, v_num=1, train_loss_step=1.270, val_loss=2.700, train_loss_epoch=1.600]\u001b[A\n",
      "Epoch 342:  54%|███▊   | 36/67 [00:09<00:08,  3.87it/s, loss=2.54, v_num=1, train_loss_step=1.270, val_loss=2.700, train_loss_epoch=1.600]\u001b[A\n",
      "Epoch 342:  55%|███▊   | 37/67 [00:09<00:07,  3.85it/s, loss=2.54, v_num=1, train_loss_step=1.270, val_loss=2.700, train_loss_epoch=1.600]\u001b[A\n",
      "Epoch 342:  57%|███▉   | 38/67 [00:09<00:07,  3.83it/s, loss=2.54, v_num=1, train_loss_step=1.270, val_loss=2.700, train_loss_epoch=1.600]\u001b[A\n",
      "Epoch 342:  58%|████   | 39/67 [00:10<00:07,  3.81it/s, loss=2.54, v_num=1, train_loss_step=1.270, val_loss=2.700, train_loss_epoch=1.600]\u001b[A\n",
      "Epoch 342:  60%|████▏  | 40/67 [00:10<00:07,  3.80it/s, loss=2.54, v_num=1, train_loss_step=1.270, val_loss=2.700, train_loss_epoch=1.600]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 342:  61%|████▎  | 41/67 [00:10<00:06,  3.73it/s, loss=2.54, v_num=1, train_loss_step=1.270, val_loss=2.700, train_loss_epoch=1.600]\u001b[A\n",
      "Epoch 342:  63%|████▍  | 42/67 [00:11<00:06,  3.71it/s, loss=2.54, v_num=1, train_loss_step=1.270, val_loss=2.700, train_loss_epoch=1.600]\u001b[A\n",
      "Epoch 342:  64%|████▍  | 43/67 [00:11<00:06,  3.70it/s, loss=2.54, v_num=1, train_loss_step=1.270, val_loss=2.700, train_loss_epoch=1.600]\u001b[A\n",
      "Epoch 342:  66%|████▌  | 44/67 [00:11<00:06,  3.69it/s, loss=2.54, v_num=1, train_loss_step=1.270, val_loss=2.700, train_loss_epoch=1.600]\u001b[A\n",
      "Epoch 342:  67%|████▋  | 45/67 [00:12<00:05,  3.67it/s, loss=2.54, v_num=1, train_loss_step=1.270, val_loss=2.700, train_loss_epoch=1.600]\u001b[A\n",
      "Epoch 342:  69%|████▊  | 46/67 [00:12<00:05,  3.66it/s, loss=2.54, v_num=1, train_loss_step=1.270, val_loss=2.700, train_loss_epoch=1.600]\u001b[A\n",
      "Epoch 342:  70%|████▉  | 47/67 [00:12<00:05,  3.64it/s, loss=2.54, v_num=1, train_loss_step=1.270, val_loss=2.700, train_loss_epoch=1.600]\u001b[A\n",
      "Epoch 342:  72%|█████  | 48/67 [00:13<00:05,  3.63it/s, loss=2.54, v_num=1, train_loss_step=1.270, val_loss=2.700, train_loss_epoch=1.600]\u001b[A\n",
      "Epoch 342:  73%|█████  | 49/67 [00:13<00:04,  3.62it/s, loss=2.54, v_num=1, train_loss_step=1.270, val_loss=2.700, train_loss_epoch=1.600]\u001b[A\n",
      "Epoch 342:  75%|█████▏ | 50/67 [00:13<00:04,  3.61it/s, loss=2.54, v_num=1, train_loss_step=1.270, val_loss=2.700, train_loss_epoch=1.600]\u001b[A\n",
      "Epoch 342:  76%|█████▎ | 51/67 [00:14<00:04,  3.57it/s, loss=2.54, v_num=1, train_loss_step=1.270, val_loss=2.700, train_loss_epoch=1.600]\u001b[A\n",
      "Epoch 342:  78%|█████▍ | 52/67 [00:14<00:04,  3.56it/s, loss=2.54, v_num=1, train_loss_step=1.270, val_loss=2.700, train_loss_epoch=1.600]\u001b[A\n",
      "Epoch 342:  79%|█████▌ | 53/67 [00:14<00:03,  3.55it/s, loss=2.54, v_num=1, train_loss_step=1.270, val_loss=2.700, train_loss_epoch=1.600]\u001b[A\n",
      "Epoch 342:  81%|█████▋ | 54/67 [00:15<00:03,  3.55it/s, loss=2.54, v_num=1, train_loss_step=1.270, val_loss=2.700, train_loss_epoch=1.600]\u001b[A\n",
      "Epoch 342:  82%|█████▋ | 55/67 [00:15<00:03,  3.54it/s, loss=2.54, v_num=1, train_loss_step=1.270, val_loss=2.700, train_loss_epoch=1.600]\u001b[A\n",
      "Epoch 342:  84%|█████▊ | 56/67 [00:15<00:03,  3.53it/s, loss=2.54, v_num=1, train_loss_step=1.270, val_loss=2.700, train_loss_epoch=1.600]\u001b[A\n",
      "Epoch 342:  85%|█████▉ | 57/67 [00:16<00:02,  3.53it/s, loss=2.54, v_num=1, train_loss_step=1.270, val_loss=2.700, train_loss_epoch=1.600]\u001b[A\n",
      "Epoch 342:  87%|██████ | 58/67 [00:16<00:02,  3.52it/s, loss=2.54, v_num=1, train_loss_step=1.270, val_loss=2.700, train_loss_epoch=1.600]\u001b[A\n",
      "Epoch 342:  88%|██████▏| 59/67 [00:16<00:02,  3.51it/s, loss=2.54, v_num=1, train_loss_step=1.270, val_loss=2.700, train_loss_epoch=1.600]\u001b[A\n",
      "Epoch 342:  90%|██████▎| 60/67 [00:17<00:01,  3.51it/s, loss=2.54, v_num=1, train_loss_step=1.270, val_loss=2.700, train_loss_epoch=1.600]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 342:  91%|██████▎| 61/67 [00:17<00:01,  3.46it/s, loss=2.54, v_num=1, train_loss_step=1.270, val_loss=2.700, train_loss_epoch=1.600]\u001b[A\n",
      "Epoch 342:  93%|██████▍| 62/67 [00:17<00:01,  3.46it/s, loss=2.54, v_num=1, train_loss_step=1.270, val_loss=2.700, train_loss_epoch=1.600]\u001b[A\n",
      "Epoch 342:  94%|██████▌| 63/67 [00:18<00:01,  3.46it/s, loss=2.54, v_num=1, train_loss_step=1.270, val_loss=2.700, train_loss_epoch=1.600]\u001b[A\n",
      "Epoch 342:  96%|██████▋| 64/67 [00:18<00:00,  3.46it/s, loss=2.54, v_num=1, train_loss_step=1.270, val_loss=2.700, train_loss_epoch=1.600]\u001b[A\n",
      "Epoch 342:  97%|██████▊| 65/67 [00:18<00:00,  3.45it/s, loss=2.54, v_num=1, train_loss_step=1.270, val_loss=2.700, train_loss_epoch=1.600]\u001b[A\n",
      "Epoch 342:  99%|██████▉| 66/67 [00:19<00:00,  3.45it/s, loss=2.54, v_num=1, train_loss_step=1.270, val_loss=2.700, train_loss_epoch=1.600]\u001b[A\n",
      "Epoch 342: 100%|███████| 67/67 [00:19<00:00,  3.37it/s, loss=2.54, v_num=1, train_loss_step=1.270, val_loss=2.690, train_loss_epoch=1.600]\u001b[A\n",
      "Epoch 343:  45%|███▏   | 30/67 [00:05<00:07,  5.23it/s, loss=3.06, v_num=1, train_loss_step=0.670, val_loss=2.690, train_loss_epoch=2.280]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 343:  46%|███▏   | 31/67 [00:07<00:09,  3.88it/s, loss=3.06, v_num=1, train_loss_step=0.670, val_loss=2.690, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 343:  48%|███▎   | 32/67 [00:08<00:09,  3.86it/s, loss=3.06, v_num=1, train_loss_step=0.670, val_loss=2.690, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 343:  49%|███▍   | 33/67 [00:08<00:08,  3.84it/s, loss=3.06, v_num=1, train_loss_step=0.670, val_loss=2.690, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 343:  51%|███▌   | 34/67 [00:08<00:08,  3.82it/s, loss=3.06, v_num=1, train_loss_step=0.670, val_loss=2.690, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 343:  52%|███▋   | 35/67 [00:09<00:08,  3.80it/s, loss=3.06, v_num=1, train_loss_step=0.670, val_loss=2.690, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 343:  54%|███▊   | 36/67 [00:09<00:08,  3.78it/s, loss=3.06, v_num=1, train_loss_step=0.670, val_loss=2.690, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 343:  55%|███▊   | 37/67 [00:09<00:07,  3.76it/s, loss=3.06, v_num=1, train_loss_step=0.670, val_loss=2.690, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 343:  57%|███▉   | 38/67 [00:10<00:07,  3.75it/s, loss=3.06, v_num=1, train_loss_step=0.670, val_loss=2.690, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 343:  58%|████   | 39/67 [00:10<00:07,  3.73it/s, loss=3.06, v_num=1, train_loss_step=0.670, val_loss=2.690, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 343:  60%|████▏  | 40/67 [00:10<00:07,  3.72it/s, loss=3.06, v_num=1, train_loss_step=0.670, val_loss=2.690, train_loss_epoch=2.280]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 343:  61%|████▎  | 41/67 [00:11<00:07,  3.64it/s, loss=3.06, v_num=1, train_loss_step=0.670, val_loss=2.690, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 343:  63%|████▍  | 42/67 [00:11<00:06,  3.64it/s, loss=3.06, v_num=1, train_loss_step=0.670, val_loss=2.690, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 343:  64%|████▍  | 43/67 [00:11<00:06,  3.63it/s, loss=3.06, v_num=1, train_loss_step=0.670, val_loss=2.690, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 343:  66%|████▌  | 44/67 [00:12<00:06,  3.62it/s, loss=3.06, v_num=1, train_loss_step=0.670, val_loss=2.690, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 343:  67%|████▋  | 45/67 [00:12<00:06,  3.60it/s, loss=3.06, v_num=1, train_loss_step=0.670, val_loss=2.690, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 343:  69%|████▊  | 46/67 [00:12<00:05,  3.59it/s, loss=3.06, v_num=1, train_loss_step=0.670, val_loss=2.690, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 343:  70%|████▉  | 47/67 [00:13<00:05,  3.57it/s, loss=3.06, v_num=1, train_loss_step=0.670, val_loss=2.690, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 343:  72%|█████  | 48/67 [00:13<00:05,  3.56it/s, loss=3.06, v_num=1, train_loss_step=0.670, val_loss=2.690, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 343:  73%|█████  | 49/67 [00:13<00:05,  3.55it/s, loss=3.06, v_num=1, train_loss_step=0.670, val_loss=2.690, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 343:  75%|█████▏ | 50/67 [00:14<00:04,  3.54it/s, loss=3.06, v_num=1, train_loss_step=0.670, val_loss=2.690, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 343:  76%|█████▎ | 51/67 [00:14<00:04,  3.48it/s, loss=3.06, v_num=1, train_loss_step=0.670, val_loss=2.690, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 343:  78%|█████▍ | 52/67 [00:14<00:04,  3.47it/s, loss=3.06, v_num=1, train_loss_step=0.670, val_loss=2.690, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 343:  79%|█████▌ | 53/67 [00:15<00:04,  3.47it/s, loss=3.06, v_num=1, train_loss_step=0.670, val_loss=2.690, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 343:  81%|█████▋ | 54/67 [00:15<00:03,  3.47it/s, loss=3.06, v_num=1, train_loss_step=0.670, val_loss=2.690, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 343:  82%|█████▋ | 55/67 [00:15<00:03,  3.46it/s, loss=3.06, v_num=1, train_loss_step=0.670, val_loss=2.690, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 343:  84%|█████▊ | 56/67 [00:16<00:03,  3.46it/s, loss=3.06, v_num=1, train_loss_step=0.670, val_loss=2.690, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 343:  85%|█████▉ | 57/67 [00:16<00:02,  3.46it/s, loss=3.06, v_num=1, train_loss_step=0.670, val_loss=2.690, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 343:  87%|██████ | 58/67 [00:16<00:02,  3.45it/s, loss=3.06, v_num=1, train_loss_step=0.670, val_loss=2.690, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 343:  88%|██████▏| 59/67 [00:17<00:02,  3.45it/s, loss=3.06, v_num=1, train_loss_step=0.670, val_loss=2.690, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 343:  90%|██████▎| 60/67 [00:17<00:02,  3.45it/s, loss=3.06, v_num=1, train_loss_step=0.670, val_loss=2.690, train_loss_epoch=2.280]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 343:  91%|██████▎| 61/67 [00:17<00:01,  3.41it/s, loss=3.06, v_num=1, train_loss_step=0.670, val_loss=2.690, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 343:  93%|██████▍| 62/67 [00:18<00:01,  3.40it/s, loss=3.06, v_num=1, train_loss_step=0.670, val_loss=2.690, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 343:  94%|██████▌| 63/67 [00:18<00:01,  3.40it/s, loss=3.06, v_num=1, train_loss_step=0.670, val_loss=2.690, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 343:  96%|██████▋| 64/67 [00:18<00:00,  3.40it/s, loss=3.06, v_num=1, train_loss_step=0.670, val_loss=2.690, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 343:  97%|██████▊| 65/67 [00:19<00:00,  3.40it/s, loss=3.06, v_num=1, train_loss_step=0.670, val_loss=2.690, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 343:  99%|██████▉| 66/67 [00:19<00:00,  3.39it/s, loss=3.06, v_num=1, train_loss_step=0.670, val_loss=2.690, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 343: 100%|███████| 67/67 [00:20<00:00,  3.32it/s, loss=3.06, v_num=1, train_loss_step=0.670, val_loss=2.690, train_loss_epoch=2.280]\u001b[A\n",
      "Epoch 344:  45%|███▏   | 30/67 [00:05<00:06,  5.43it/s, loss=1.88, v_num=1, train_loss_step=0.750, val_loss=2.690, train_loss_epoch=2.620]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 344:  46%|███▏   | 31/67 [00:07<00:08,  4.02it/s, loss=1.88, v_num=1, train_loss_step=0.750, val_loss=2.690, train_loss_epoch=2.620]\u001b[A\n",
      "Epoch 344:  48%|███▎   | 32/67 [00:08<00:08,  4.00it/s, loss=1.88, v_num=1, train_loss_step=0.750, val_loss=2.690, train_loss_epoch=2.620]\u001b[A\n",
      "Epoch 344:  49%|███▍   | 33/67 [00:08<00:08,  3.97it/s, loss=1.88, v_num=1, train_loss_step=0.750, val_loss=2.690, train_loss_epoch=2.620]\u001b[A\n",
      "Epoch 344:  51%|███▌   | 34/67 [00:08<00:08,  3.94it/s, loss=1.88, v_num=1, train_loss_step=0.750, val_loss=2.690, train_loss_epoch=2.620]\u001b[A\n",
      "Epoch 344:  52%|███▋   | 35/67 [00:08<00:08,  3.92it/s, loss=1.88, v_num=1, train_loss_step=0.750, val_loss=2.690, train_loss_epoch=2.620]\u001b[A\n",
      "Epoch 344:  54%|███▊   | 36/67 [00:09<00:07,  3.90it/s, loss=1.88, v_num=1, train_loss_step=0.750, val_loss=2.690, train_loss_epoch=2.620]\u001b[A\n",
      "Epoch 344:  55%|███▊   | 37/67 [00:09<00:07,  3.87it/s, loss=1.88, v_num=1, train_loss_step=0.750, val_loss=2.690, train_loss_epoch=2.620]\u001b[A\n",
      "Epoch 344:  57%|███▉   | 38/67 [00:09<00:07,  3.86it/s, loss=1.88, v_num=1, train_loss_step=0.750, val_loss=2.690, train_loss_epoch=2.620]\u001b[A\n",
      "Epoch 344:  58%|████   | 39/67 [00:10<00:07,  3.84it/s, loss=1.88, v_num=1, train_loss_step=0.750, val_loss=2.690, train_loss_epoch=2.620]\u001b[A\n",
      "Epoch 344:  60%|████▏  | 40/67 [00:10<00:07,  3.82it/s, loss=1.88, v_num=1, train_loss_step=0.750, val_loss=2.690, train_loss_epoch=2.620]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 344:  61%|████▎  | 41/67 [00:10<00:06,  3.75it/s, loss=1.88, v_num=1, train_loss_step=0.750, val_loss=2.690, train_loss_epoch=2.620]\u001b[A\n",
      "Epoch 344:  63%|████▍  | 42/67 [00:11<00:06,  3.74it/s, loss=1.88, v_num=1, train_loss_step=0.750, val_loss=2.690, train_loss_epoch=2.620]\u001b[A\n",
      "Epoch 344:  64%|████▍  | 43/67 [00:11<00:06,  3.72it/s, loss=1.88, v_num=1, train_loss_step=0.750, val_loss=2.690, train_loss_epoch=2.620]\u001b[A\n",
      "Epoch 344:  66%|████▌  | 44/67 [00:11<00:06,  3.72it/s, loss=1.88, v_num=1, train_loss_step=0.750, val_loss=2.690, train_loss_epoch=2.620]\u001b[A\n",
      "Epoch 344:  67%|████▋  | 45/67 [00:12<00:05,  3.71it/s, loss=1.88, v_num=1, train_loss_step=0.750, val_loss=2.690, train_loss_epoch=2.620]\u001b[A\n",
      "Epoch 344:  69%|████▊  | 46/67 [00:12<00:05,  3.69it/s, loss=1.88, v_num=1, train_loss_step=0.750, val_loss=2.690, train_loss_epoch=2.620]\u001b[A\n",
      "Epoch 344:  70%|████▉  | 47/67 [00:12<00:05,  3.68it/s, loss=1.88, v_num=1, train_loss_step=0.750, val_loss=2.690, train_loss_epoch=2.620]\u001b[A\n",
      "Epoch 344:  72%|█████  | 48/67 [00:13<00:05,  3.67it/s, loss=1.88, v_num=1, train_loss_step=0.750, val_loss=2.690, train_loss_epoch=2.620]\u001b[A\n",
      "Epoch 344:  73%|█████  | 49/67 [00:13<00:04,  3.66it/s, loss=1.88, v_num=1, train_loss_step=0.750, val_loss=2.690, train_loss_epoch=2.620]\u001b[A\n",
      "Epoch 344:  75%|█████▏ | 50/67 [00:13<00:04,  3.65it/s, loss=1.88, v_num=1, train_loss_step=0.750, val_loss=2.690, train_loss_epoch=2.620]\u001b[A\n",
      "Epoch 344:  76%|█████▎ | 51/67 [00:14<00:04,  3.59it/s, loss=1.88, v_num=1, train_loss_step=0.750, val_loss=2.690, train_loss_epoch=2.620]\u001b[A\n",
      "Epoch 344:  78%|█████▍ | 52/67 [00:14<00:04,  3.58it/s, loss=1.88, v_num=1, train_loss_step=0.750, val_loss=2.690, train_loss_epoch=2.620]\u001b[A\n",
      "Epoch 344:  79%|█████▌ | 53/67 [00:14<00:03,  3.58it/s, loss=1.88, v_num=1, train_loss_step=0.750, val_loss=2.690, train_loss_epoch=2.620]\u001b[A\n",
      "Epoch 344:  81%|█████▋ | 54/67 [00:15<00:03,  3.57it/s, loss=1.88, v_num=1, train_loss_step=0.750, val_loss=2.690, train_loss_epoch=2.620]\u001b[A\n",
      "Epoch 344:  82%|█████▋ | 55/67 [00:15<00:03,  3.56it/s, loss=1.88, v_num=1, train_loss_step=0.750, val_loss=2.690, train_loss_epoch=2.620]\u001b[A\n",
      "Epoch 344:  84%|█████▊ | 56/67 [00:15<00:03,  3.55it/s, loss=1.88, v_num=1, train_loss_step=0.750, val_loss=2.690, train_loss_epoch=2.620]\u001b[A\n",
      "Epoch 344:  85%|█████▉ | 57/67 [00:16<00:02,  3.54it/s, loss=1.88, v_num=1, train_loss_step=0.750, val_loss=2.690, train_loss_epoch=2.620]\u001b[A\n",
      "Epoch 344:  87%|██████ | 58/67 [00:16<00:02,  3.52it/s, loss=1.88, v_num=1, train_loss_step=0.750, val_loss=2.690, train_loss_epoch=2.620]\u001b[A\n",
      "Epoch 344:  88%|██████▏| 59/67 [00:16<00:02,  3.51it/s, loss=1.88, v_num=1, train_loss_step=0.750, val_loss=2.690, train_loss_epoch=2.620]\u001b[A\n",
      "Epoch 344:  90%|██████▎| 60/67 [00:17<00:01,  3.50it/s, loss=1.88, v_num=1, train_loss_step=0.750, val_loss=2.690, train_loss_epoch=2.620]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 344:  91%|██████▎| 61/67 [00:17<00:01,  3.45it/s, loss=1.88, v_num=1, train_loss_step=0.750, val_loss=2.690, train_loss_epoch=2.620]\u001b[A\n",
      "Epoch 344:  93%|██████▍| 62/67 [00:17<00:01,  3.45it/s, loss=1.88, v_num=1, train_loss_step=0.750, val_loss=2.690, train_loss_epoch=2.620]\u001b[A\n",
      "Epoch 344:  94%|██████▌| 63/67 [00:18<00:01,  3.44it/s, loss=1.88, v_num=1, train_loss_step=0.750, val_loss=2.690, train_loss_epoch=2.620]\u001b[A\n",
      "Epoch 344:  96%|██████▋| 64/67 [00:18<00:00,  3.44it/s, loss=1.88, v_num=1, train_loss_step=0.750, val_loss=2.690, train_loss_epoch=2.620]\u001b[A\n",
      "Epoch 344:  97%|██████▊| 65/67 [00:18<00:00,  3.44it/s, loss=1.88, v_num=1, train_loss_step=0.750, val_loss=2.690, train_loss_epoch=2.620]\u001b[A\n",
      "Epoch 344:  99%|██████▉| 66/67 [00:19<00:00,  3.44it/s, loss=1.88, v_num=1, train_loss_step=0.750, val_loss=2.690, train_loss_epoch=2.620]\u001b[A\n",
      "Epoch 344: 100%|███████| 67/67 [00:19<00:00,  3.37it/s, loss=1.88, v_num=1, train_loss_step=0.750, val_loss=2.690, train_loss_epoch=2.620]\u001b[A\n",
      "Epoch 345:  45%|███▏   | 30/67 [00:06<00:07,  4.91it/s, loss=1.84, v_num=1, train_loss_step=2.460, val_loss=2.690, train_loss_epoch=2.000]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 345:  46%|███▏   | 31/67 [00:08<00:09,  3.66it/s, loss=1.84, v_num=1, train_loss_step=2.460, val_loss=2.690, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 345:  48%|███▎   | 32/67 [00:08<00:09,  3.63it/s, loss=1.84, v_num=1, train_loss_step=2.460, val_loss=2.690, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 345:  49%|███▍   | 33/67 [00:09<00:09,  3.60it/s, loss=1.84, v_num=1, train_loss_step=2.460, val_loss=2.690, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 345:  51%|███▌   | 34/67 [00:09<00:09,  3.58it/s, loss=1.84, v_num=1, train_loss_step=2.460, val_loss=2.690, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 345:  52%|███▋   | 35/67 [00:09<00:08,  3.57it/s, loss=1.84, v_num=1, train_loss_step=2.460, val_loss=2.690, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 345:  54%|███▊   | 36/67 [00:10<00:08,  3.55it/s, loss=1.84, v_num=1, train_loss_step=2.460, val_loss=2.690, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 345:  55%|███▊   | 37/67 [00:10<00:08,  3.53it/s, loss=1.84, v_num=1, train_loss_step=2.460, val_loss=2.690, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 345:  57%|███▉   | 38/67 [00:10<00:08,  3.51it/s, loss=1.84, v_num=1, train_loss_step=2.460, val_loss=2.690, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 345:  58%|████   | 39/67 [00:11<00:08,  3.50it/s, loss=1.84, v_num=1, train_loss_step=2.460, val_loss=2.690, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 345:  60%|████▏  | 40/67 [00:11<00:07,  3.48it/s, loss=1.84, v_num=1, train_loss_step=2.460, val_loss=2.690, train_loss_epoch=2.000]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 345:  61%|████▎  | 41/67 [00:11<00:07,  3.42it/s, loss=1.84, v_num=1, train_loss_step=2.460, val_loss=2.690, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 345:  63%|████▍  | 42/67 [00:12<00:07,  3.41it/s, loss=1.84, v_num=1, train_loss_step=2.460, val_loss=2.690, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 345:  64%|████▍  | 43/67 [00:12<00:07,  3.39it/s, loss=1.84, v_num=1, train_loss_step=2.460, val_loss=2.690, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 345:  66%|████▌  | 44/67 [00:12<00:06,  3.39it/s, loss=1.84, v_num=1, train_loss_step=2.460, val_loss=2.690, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 345:  67%|████▋  | 45/67 [00:13<00:06,  3.38it/s, loss=1.84, v_num=1, train_loss_step=2.460, val_loss=2.690, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 345:  69%|████▊  | 46/67 [00:13<00:06,  3.36it/s, loss=1.84, v_num=1, train_loss_step=2.460, val_loss=2.690, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 345:  70%|████▉  | 47/67 [00:13<00:05,  3.36it/s, loss=1.84, v_num=1, train_loss_step=2.460, val_loss=2.690, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 345:  72%|█████  | 48/67 [00:14<00:05,  3.35it/s, loss=1.84, v_num=1, train_loss_step=2.460, val_loss=2.690, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 345:  73%|█████  | 49/67 [00:14<00:05,  3.34it/s, loss=1.84, v_num=1, train_loss_step=2.460, val_loss=2.690, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 345:  75%|█████▏ | 50/67 [00:14<00:05,  3.33it/s, loss=1.84, v_num=1, train_loss_step=2.460, val_loss=2.690, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 345:  76%|█████▎ | 51/67 [00:15<00:04,  3.29it/s, loss=1.84, v_num=1, train_loss_step=2.460, val_loss=2.690, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 345:  78%|█████▍ | 52/67 [00:15<00:04,  3.28it/s, loss=1.84, v_num=1, train_loss_step=2.460, val_loss=2.690, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 345:  79%|█████▌ | 53/67 [00:16<00:04,  3.28it/s, loss=1.84, v_num=1, train_loss_step=2.460, val_loss=2.690, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 345:  81%|█████▋ | 54/67 [00:16<00:03,  3.27it/s, loss=1.84, v_num=1, train_loss_step=2.460, val_loss=2.690, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 345:  82%|█████▋ | 55/67 [00:16<00:03,  3.27it/s, loss=1.84, v_num=1, train_loss_step=2.460, val_loss=2.690, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 345:  84%|█████▊ | 56/67 [00:17<00:03,  3.26it/s, loss=1.84, v_num=1, train_loss_step=2.460, val_loss=2.690, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 345:  85%|█████▉ | 57/67 [00:17<00:03,  3.26it/s, loss=1.84, v_num=1, train_loss_step=2.460, val_loss=2.690, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 345:  87%|██████ | 58/67 [00:17<00:02,  3.25it/s, loss=1.84, v_num=1, train_loss_step=2.460, val_loss=2.690, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 345:  88%|██████▏| 59/67 [00:18<00:02,  3.25it/s, loss=1.84, v_num=1, train_loss_step=2.460, val_loss=2.690, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 345:  90%|██████▎| 60/67 [00:18<00:02,  3.24it/s, loss=1.84, v_num=1, train_loss_step=2.460, val_loss=2.690, train_loss_epoch=2.000]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 345:  91%|██████▎| 61/67 [00:19<00:01,  3.21it/s, loss=1.84, v_num=1, train_loss_step=2.460, val_loss=2.690, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 345:  93%|██████▍| 62/67 [00:19<00:01,  3.20it/s, loss=1.84, v_num=1, train_loss_step=2.460, val_loss=2.690, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 345:  94%|██████▌| 63/67 [00:19<00:01,  3.20it/s, loss=1.84, v_num=1, train_loss_step=2.460, val_loss=2.690, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 345:  96%|██████▋| 64/67 [00:20<00:00,  3.20it/s, loss=1.84, v_num=1, train_loss_step=2.460, val_loss=2.690, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 345:  97%|██████▊| 65/67 [00:20<00:00,  3.20it/s, loss=1.84, v_num=1, train_loss_step=2.460, val_loss=2.690, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 345:  99%|██████▉| 66/67 [00:20<00:00,  3.20it/s, loss=1.84, v_num=1, train_loss_step=2.460, val_loss=2.690, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 345: 100%|███████| 67/67 [00:21<00:00,  3.13it/s, loss=1.84, v_num=1, train_loss_step=2.460, val_loss=2.690, train_loss_epoch=2.000]\u001b[A\n",
      "Epoch 346:  45%|███▏   | 30/67 [00:05<00:07,  5.16it/s, loss=2.14, v_num=1, train_loss_step=2.940, val_loss=2.690, train_loss_epoch=2.010]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 346:  46%|███▏   | 31/67 [00:07<00:09,  3.89it/s, loss=2.14, v_num=1, train_loss_step=2.940, val_loss=2.690, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 346:  48%|███▎   | 32/67 [00:08<00:09,  3.85it/s, loss=2.14, v_num=1, train_loss_step=2.940, val_loss=2.690, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 346:  49%|███▍   | 33/67 [00:08<00:08,  3.82it/s, loss=2.14, v_num=1, train_loss_step=2.940, val_loss=2.690, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 346:  51%|███▌   | 34/67 [00:08<00:08,  3.79it/s, loss=2.14, v_num=1, train_loss_step=2.940, val_loss=2.690, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 346:  52%|███▋   | 35/67 [00:09<00:08,  3.77it/s, loss=2.14, v_num=1, train_loss_step=2.940, val_loss=2.690, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 346:  54%|███▊   | 36/67 [00:09<00:08,  3.74it/s, loss=2.14, v_num=1, train_loss_step=2.940, val_loss=2.690, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 346:  55%|███▊   | 37/67 [00:09<00:08,  3.71it/s, loss=2.14, v_num=1, train_loss_step=2.940, val_loss=2.690, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 346:  57%|███▉   | 38/67 [00:10<00:07,  3.69it/s, loss=2.14, v_num=1, train_loss_step=2.940, val_loss=2.690, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 346:  58%|████   | 39/67 [00:10<00:07,  3.67it/s, loss=2.14, v_num=1, train_loss_step=2.940, val_loss=2.690, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 346:  60%|████▏  | 40/67 [00:10<00:07,  3.65it/s, loss=2.14, v_num=1, train_loss_step=2.940, val_loss=2.690, train_loss_epoch=2.010]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 346:  61%|████▎  | 41/67 [00:11<00:07,  3.57it/s, loss=2.14, v_num=1, train_loss_step=2.940, val_loss=2.690, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 346:  63%|████▍  | 42/67 [00:11<00:07,  3.56it/s, loss=2.14, v_num=1, train_loss_step=2.940, val_loss=2.690, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 346:  64%|████▍  | 43/67 [00:12<00:06,  3.54it/s, loss=2.14, v_num=1, train_loss_step=2.940, val_loss=2.690, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 346:  66%|████▌  | 44/67 [00:12<00:06,  3.53it/s, loss=2.14, v_num=1, train_loss_step=2.940, val_loss=2.690, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 346:  67%|████▋  | 45/67 [00:12<00:06,  3.52it/s, loss=2.14, v_num=1, train_loss_step=2.940, val_loss=2.690, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 346:  69%|████▊  | 46/67 [00:13<00:05,  3.50it/s, loss=2.14, v_num=1, train_loss_step=2.940, val_loss=2.690, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 346:  70%|████▉  | 47/67 [00:13<00:05,  3.49it/s, loss=2.14, v_num=1, train_loss_step=2.940, val_loss=2.690, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 346:  72%|█████  | 48/67 [00:13<00:05,  3.48it/s, loss=2.14, v_num=1, train_loss_step=2.940, val_loss=2.690, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 346:  73%|█████  | 49/67 [00:14<00:05,  3.47it/s, loss=2.14, v_num=1, train_loss_step=2.940, val_loss=2.690, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 346:  75%|█████▏ | 50/67 [00:14<00:04,  3.46it/s, loss=2.14, v_num=1, train_loss_step=2.940, val_loss=2.690, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 346:  76%|█████▎ | 51/67 [00:14<00:04,  3.41it/s, loss=2.14, v_num=1, train_loss_step=2.940, val_loss=2.690, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 346:  78%|█████▍ | 52/67 [00:15<00:04,  3.40it/s, loss=2.14, v_num=1, train_loss_step=2.940, val_loss=2.690, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 346:  79%|█████▌ | 53/67 [00:15<00:04,  3.39it/s, loss=2.14, v_num=1, train_loss_step=2.940, val_loss=2.690, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 346:  81%|█████▋ | 54/67 [00:15<00:03,  3.38it/s, loss=2.14, v_num=1, train_loss_step=2.940, val_loss=2.690, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 346:  82%|█████▋ | 55/67 [00:16<00:03,  3.37it/s, loss=2.14, v_num=1, train_loss_step=2.940, val_loss=2.690, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 346:  84%|█████▊ | 56/67 [00:16<00:03,  3.37it/s, loss=2.14, v_num=1, train_loss_step=2.940, val_loss=2.690, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 346:  85%|█████▉ | 57/67 [00:16<00:02,  3.36it/s, loss=2.14, v_num=1, train_loss_step=2.940, val_loss=2.690, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 346:  87%|██████ | 58/67 [00:17<00:02,  3.35it/s, loss=2.14, v_num=1, train_loss_step=2.940, val_loss=2.690, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 346:  88%|██████▏| 59/67 [00:17<00:02,  3.34it/s, loss=2.14, v_num=1, train_loss_step=2.940, val_loss=2.690, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 346:  90%|██████▎| 60/67 [00:17<00:02,  3.34it/s, loss=2.14, v_num=1, train_loss_step=2.940, val_loss=2.690, train_loss_epoch=2.010]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 346:  91%|██████▎| 61/67 [00:18<00:01,  3.30it/s, loss=2.14, v_num=1, train_loss_step=2.940, val_loss=2.690, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 346:  93%|██████▍| 62/67 [00:18<00:01,  3.30it/s, loss=2.14, v_num=1, train_loss_step=2.940, val_loss=2.690, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 346:  94%|██████▌| 63/67 [00:19<00:01,  3.30it/s, loss=2.14, v_num=1, train_loss_step=2.940, val_loss=2.690, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 346:  96%|██████▋| 64/67 [00:19<00:00,  3.30it/s, loss=2.14, v_num=1, train_loss_step=2.940, val_loss=2.690, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 346:  97%|██████▊| 65/67 [00:19<00:00,  3.29it/s, loss=2.14, v_num=1, train_loss_step=2.940, val_loss=2.690, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 346:  99%|██████▉| 66/67 [00:20<00:00,  3.29it/s, loss=2.14, v_num=1, train_loss_step=2.940, val_loss=2.690, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 346: 100%|███████| 67/67 [00:20<00:00,  3.21it/s, loss=2.14, v_num=1, train_loss_step=2.940, val_loss=2.690, train_loss_epoch=2.010]\u001b[A\n",
      "Epoch 347:  45%|███▏   | 30/67 [00:05<00:07,  5.12it/s, loss=1.73, v_num=1, train_loss_step=2.130, val_loss=2.690, train_loss_epoch=2.070]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 347:  46%|███▏   | 31/67 [00:08<00:09,  3.86it/s, loss=1.73, v_num=1, train_loss_step=2.130, val_loss=2.690, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 347:  48%|███▎   | 32/67 [00:08<00:09,  3.82it/s, loss=1.73, v_num=1, train_loss_step=2.130, val_loss=2.690, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 347:  49%|███▍   | 33/67 [00:08<00:08,  3.79it/s, loss=1.73, v_num=1, train_loss_step=2.130, val_loss=2.690, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 347:  51%|███▌   | 34/67 [00:09<00:08,  3.76it/s, loss=1.73, v_num=1, train_loss_step=2.130, val_loss=2.690, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 347:  52%|███▋   | 35/67 [00:09<00:08,  3.73it/s, loss=1.73, v_num=1, train_loss_step=2.130, val_loss=2.690, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 347:  54%|███▊   | 36/67 [00:09<00:08,  3.71it/s, loss=1.73, v_num=1, train_loss_step=2.130, val_loss=2.690, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 347:  55%|███▊   | 37/67 [00:10<00:08,  3.68it/s, loss=1.73, v_num=1, train_loss_step=2.130, val_loss=2.690, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 347:  57%|███▉   | 38/67 [00:10<00:07,  3.66it/s, loss=1.73, v_num=1, train_loss_step=2.130, val_loss=2.690, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 347:  58%|████   | 39/67 [00:10<00:07,  3.64it/s, loss=1.73, v_num=1, train_loss_step=2.130, val_loss=2.690, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 347:  60%|████▏  | 40/67 [00:11<00:07,  3.62it/s, loss=1.73, v_num=1, train_loss_step=2.130, val_loss=2.690, train_loss_epoch=2.070]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 347:  61%|████▎  | 41/67 [00:11<00:07,  3.55it/s, loss=1.73, v_num=1, train_loss_step=2.130, val_loss=2.690, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 347:  63%|████▍  | 42/67 [00:11<00:07,  3.54it/s, loss=1.73, v_num=1, train_loss_step=2.130, val_loss=2.690, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 347:  64%|████▍  | 43/67 [00:12<00:06,  3.52it/s, loss=1.73, v_num=1, train_loss_step=2.130, val_loss=2.690, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 347:  66%|████▌  | 44/67 [00:12<00:06,  3.51it/s, loss=1.73, v_num=1, train_loss_step=2.130, val_loss=2.690, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 347:  67%|████▋  | 45/67 [00:12<00:06,  3.50it/s, loss=1.73, v_num=1, train_loss_step=2.130, val_loss=2.690, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 347:  69%|████▊  | 46/67 [00:13<00:06,  3.48it/s, loss=1.73, v_num=1, train_loss_step=2.130, val_loss=2.690, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 347:  70%|████▉  | 47/67 [00:13<00:05,  3.47it/s, loss=1.73, v_num=1, train_loss_step=2.130, val_loss=2.690, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 347:  72%|█████  | 48/67 [00:13<00:05,  3.46it/s, loss=1.73, v_num=1, train_loss_step=2.130, val_loss=2.690, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 347:  73%|█████  | 49/67 [00:14<00:05,  3.45it/s, loss=1.73, v_num=1, train_loss_step=2.130, val_loss=2.690, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 347:  75%|█████▏ | 50/67 [00:14<00:04,  3.44it/s, loss=1.73, v_num=1, train_loss_step=2.130, val_loss=2.690, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 347:  76%|█████▎ | 51/67 [00:15<00:04,  3.38it/s, loss=1.73, v_num=1, train_loss_step=2.130, val_loss=2.690, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 347:  78%|█████▍ | 52/67 [00:15<00:04,  3.37it/s, loss=1.73, v_num=1, train_loss_step=2.130, val_loss=2.690, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 347:  79%|█████▌ | 53/67 [00:15<00:04,  3.36it/s, loss=1.73, v_num=1, train_loss_step=2.130, val_loss=2.690, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 347:  81%|█████▋ | 54/67 [00:16<00:03,  3.36it/s, loss=1.73, v_num=1, train_loss_step=2.130, val_loss=2.690, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 347:  82%|█████▋ | 55/67 [00:16<00:03,  3.35it/s, loss=1.73, v_num=1, train_loss_step=2.130, val_loss=2.690, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 347:  84%|█████▊ | 56/67 [00:16<00:03,  3.34it/s, loss=1.73, v_num=1, train_loss_step=2.130, val_loss=2.690, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 347:  85%|█████▉ | 57/67 [00:17<00:02,  3.34it/s, loss=1.73, v_num=1, train_loss_step=2.130, val_loss=2.690, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 347:  87%|██████ | 58/67 [00:17<00:02,  3.33it/s, loss=1.73, v_num=1, train_loss_step=2.130, val_loss=2.690, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 347:  88%|██████▏| 59/67 [00:17<00:02,  3.32it/s, loss=1.73, v_num=1, train_loss_step=2.130, val_loss=2.690, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 347:  90%|██████▎| 60/67 [00:18<00:02,  3.32it/s, loss=1.73, v_num=1, train_loss_step=2.130, val_loss=2.690, train_loss_epoch=2.070]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 347:  91%|██████▎| 61/67 [00:18<00:01,  3.27it/s, loss=1.73, v_num=1, train_loss_step=2.130, val_loss=2.690, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 347:  93%|██████▍| 62/67 [00:18<00:01,  3.27it/s, loss=1.73, v_num=1, train_loss_step=2.130, val_loss=2.690, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 347:  94%|██████▌| 63/67 [00:19<00:01,  3.26it/s, loss=1.73, v_num=1, train_loss_step=2.130, val_loss=2.690, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 347:  96%|██████▋| 64/67 [00:19<00:00,  3.26it/s, loss=1.73, v_num=1, train_loss_step=2.130, val_loss=2.690, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 347:  97%|██████▊| 65/67 [00:19<00:00,  3.26it/s, loss=1.73, v_num=1, train_loss_step=2.130, val_loss=2.690, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 347:  99%|██████▉| 66/67 [00:20<00:00,  3.25it/s, loss=1.73, v_num=1, train_loss_step=2.130, val_loss=2.690, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 347: 100%|███████| 67/67 [00:21<00:00,  3.18it/s, loss=1.73, v_num=1, train_loss_step=2.130, val_loss=2.690, train_loss_epoch=2.070]\u001b[A\n",
      "Epoch 348:  45%|███▏   | 30/67 [00:05<00:07,  5.11it/s, loss=1.97, v_num=1, train_loss_step=1.680, val_loss=2.690, train_loss_epoch=1.810]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 348:  46%|███▏   | 31/67 [00:08<00:09,  3.83it/s, loss=1.97, v_num=1, train_loss_step=1.680, val_loss=2.690, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 348:  48%|███▎   | 32/67 [00:08<00:09,  3.79it/s, loss=1.97, v_num=1, train_loss_step=1.680, val_loss=2.690, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 348:  49%|███▍   | 33/67 [00:08<00:09,  3.76it/s, loss=1.97, v_num=1, train_loss_step=1.680, val_loss=2.690, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 348:  51%|███▌   | 34/67 [00:09<00:08,  3.72it/s, loss=1.97, v_num=1, train_loss_step=1.680, val_loss=2.690, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 348:  52%|███▋   | 35/67 [00:09<00:08,  3.66it/s, loss=1.97, v_num=1, train_loss_step=1.680, val_loss=2.690, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 348:  54%|███▊   | 36/67 [00:09<00:08,  3.64it/s, loss=1.97, v_num=1, train_loss_step=1.680, val_loss=2.690, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 348:  55%|███▊   | 37/67 [00:10<00:08,  3.62it/s, loss=1.97, v_num=1, train_loss_step=1.680, val_loss=2.690, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 348:  57%|███▉   | 38/67 [00:10<00:08,  3.60it/s, loss=1.97, v_num=1, train_loss_step=1.680, val_loss=2.690, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 348:  58%|████   | 39/67 [00:10<00:07,  3.58it/s, loss=1.97, v_num=1, train_loss_step=1.680, val_loss=2.690, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 348:  60%|████▏  | 40/67 [00:11<00:07,  3.56it/s, loss=1.97, v_num=1, train_loss_step=1.680, val_loss=2.690, train_loss_epoch=1.810]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 348:  61%|████▎  | 41/67 [00:11<00:07,  3.49it/s, loss=1.97, v_num=1, train_loss_step=1.680, val_loss=2.690, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 348:  63%|████▍  | 42/67 [00:12<00:07,  3.48it/s, loss=1.97, v_num=1, train_loss_step=1.680, val_loss=2.690, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 348:  64%|████▍  | 43/67 [00:12<00:06,  3.46it/s, loss=1.97, v_num=1, train_loss_step=1.680, val_loss=2.690, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 348:  66%|████▌  | 44/67 [00:12<00:06,  3.45it/s, loss=1.97, v_num=1, train_loss_step=1.680, val_loss=2.690, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 348:  67%|████▋  | 45/67 [00:13<00:06,  3.44it/s, loss=1.97, v_num=1, train_loss_step=1.680, val_loss=2.690, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 348:  69%|████▊  | 46/67 [00:13<00:06,  3.43it/s, loss=1.97, v_num=1, train_loss_step=1.680, val_loss=2.690, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 348:  70%|████▉  | 47/67 [00:13<00:05,  3.42it/s, loss=1.97, v_num=1, train_loss_step=1.680, val_loss=2.690, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 348:  72%|█████  | 48/67 [00:14<00:05,  3.41it/s, loss=1.97, v_num=1, train_loss_step=1.680, val_loss=2.690, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 348:  73%|█████  | 49/67 [00:14<00:05,  3.40it/s, loss=1.97, v_num=1, train_loss_step=1.680, val_loss=2.690, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 348:  75%|█████▏ | 50/67 [00:14<00:05,  3.39it/s, loss=1.97, v_num=1, train_loss_step=1.680, val_loss=2.690, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 348:  76%|█████▎ | 51/67 [00:15<00:04,  3.33it/s, loss=1.97, v_num=1, train_loss_step=1.680, val_loss=2.690, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 348:  78%|█████▍ | 52/67 [00:15<00:04,  3.32it/s, loss=1.97, v_num=1, train_loss_step=1.680, val_loss=2.690, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 348:  79%|█████▌ | 53/67 [00:16<00:04,  3.31it/s, loss=1.97, v_num=1, train_loss_step=1.680, val_loss=2.690, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 348:  81%|█████▋ | 54/67 [00:16<00:03,  3.30it/s, loss=1.97, v_num=1, train_loss_step=1.680, val_loss=2.690, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 348:  82%|█████▋ | 55/67 [00:16<00:03,  3.30it/s, loss=1.97, v_num=1, train_loss_step=1.680, val_loss=2.690, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 348:  84%|█████▊ | 56/67 [00:17<00:03,  3.29it/s, loss=1.97, v_num=1, train_loss_step=1.680, val_loss=2.690, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 348:  85%|█████▉ | 57/67 [00:17<00:03,  3.29it/s, loss=1.97, v_num=1, train_loss_step=1.680, val_loss=2.690, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 348:  87%|██████ | 58/67 [00:17<00:02,  3.28it/s, loss=1.97, v_num=1, train_loss_step=1.680, val_loss=2.690, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 348:  88%|██████▏| 59/67 [00:17<00:02,  3.28it/s, loss=1.97, v_num=1, train_loss_step=1.680, val_loss=2.690, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 348:  90%|██████▎| 60/67 [00:18<00:02,  3.28it/s, loss=1.97, v_num=1, train_loss_step=1.680, val_loss=2.690, train_loss_epoch=1.810]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 348:  91%|██████▎| 61/67 [00:18<00:01,  3.24it/s, loss=1.97, v_num=1, train_loss_step=1.680, val_loss=2.690, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 348:  93%|██████▍| 62/67 [00:19<00:01,  3.23it/s, loss=1.97, v_num=1, train_loss_step=1.680, val_loss=2.690, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 348:  94%|██████▌| 63/67 [00:19<00:01,  3.23it/s, loss=1.97, v_num=1, train_loss_step=1.680, val_loss=2.690, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 348:  96%|██████▋| 64/67 [00:19<00:00,  3.23it/s, loss=1.97, v_num=1, train_loss_step=1.680, val_loss=2.690, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 348:  97%|██████▊| 65/67 [00:20<00:00,  3.23it/s, loss=1.97, v_num=1, train_loss_step=1.680, val_loss=2.690, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 348:  99%|██████▉| 66/67 [00:20<00:00,  3.22it/s, loss=1.97, v_num=1, train_loss_step=1.680, val_loss=2.690, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 348: 100%|███████| 67/67 [00:21<00:00,  3.15it/s, loss=1.97, v_num=1, train_loss_step=1.680, val_loss=2.690, train_loss_epoch=1.810]\u001b[A\n",
      "Epoch 349:  45%|███▏   | 30/67 [00:05<00:07,  5.15it/s, loss=1.96, v_num=1, train_loss_step=0.817, val_loss=2.690, train_loss_epoch=1.920]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 349:  46%|███▏   | 31/67 [00:08<00:09,  3.86it/s, loss=1.96, v_num=1, train_loss_step=0.817, val_loss=2.690, train_loss_epoch=1.920]\u001b[A\n",
      "Epoch 349:  48%|███▎   | 32/67 [00:08<00:09,  3.83it/s, loss=1.96, v_num=1, train_loss_step=0.817, val_loss=2.690, train_loss_epoch=1.920]\u001b[A\n",
      "Epoch 349:  49%|███▍   | 33/67 [00:08<00:08,  3.80it/s, loss=1.96, v_num=1, train_loss_step=0.817, val_loss=2.690, train_loss_epoch=1.920]\u001b[A\n",
      "Epoch 349:  51%|███▌   | 34/67 [00:09<00:08,  3.77it/s, loss=1.96, v_num=1, train_loss_step=0.817, val_loss=2.690, train_loss_epoch=1.920]\u001b[A\n",
      "Epoch 349:  52%|███▋   | 35/67 [00:09<00:08,  3.74it/s, loss=1.96, v_num=1, train_loss_step=0.817, val_loss=2.690, train_loss_epoch=1.920]\u001b[A\n",
      "Epoch 349:  54%|███▊   | 36/67 [00:09<00:08,  3.72it/s, loss=1.96, v_num=1, train_loss_step=0.817, val_loss=2.690, train_loss_epoch=1.920]\u001b[A\n",
      "Epoch 349:  55%|███▊   | 37/67 [00:10<00:08,  3.69it/s, loss=1.96, v_num=1, train_loss_step=0.817, val_loss=2.690, train_loss_epoch=1.920]\u001b[A\n",
      "Epoch 349:  57%|███▉   | 38/67 [00:10<00:07,  3.67it/s, loss=1.96, v_num=1, train_loss_step=0.817, val_loss=2.690, train_loss_epoch=1.920]\u001b[A\n",
      "Epoch 349:  58%|████   | 39/67 [00:10<00:07,  3.65it/s, loss=1.96, v_num=1, train_loss_step=0.817, val_loss=2.690, train_loss_epoch=1.920]\u001b[A\n",
      "Epoch 349:  60%|████▏  | 40/67 [00:11<00:07,  3.63it/s, loss=1.96, v_num=1, train_loss_step=0.817, val_loss=2.690, train_loss_epoch=1.920]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 349:  61%|████▎  | 41/67 [00:11<00:07,  3.53it/s, loss=1.96, v_num=1, train_loss_step=0.817, val_loss=2.690, train_loss_epoch=1.920]\u001b[A\n",
      "Epoch 349:  63%|████▍  | 42/67 [00:11<00:07,  3.52it/s, loss=1.96, v_num=1, train_loss_step=0.817, val_loss=2.690, train_loss_epoch=1.920]\u001b[A\n",
      "Epoch 349:  64%|████▍  | 43/67 [00:12<00:06,  3.50it/s, loss=1.96, v_num=1, train_loss_step=0.817, val_loss=2.690, train_loss_epoch=1.920]\u001b[A\n",
      "Epoch 349:  66%|████▌  | 44/67 [00:12<00:06,  3.49it/s, loss=1.96, v_num=1, train_loss_step=0.817, val_loss=2.690, train_loss_epoch=1.920]\u001b[A\n",
      "Epoch 349:  67%|████▋  | 45/67 [00:12<00:06,  3.47it/s, loss=1.96, v_num=1, train_loss_step=0.817, val_loss=2.690, train_loss_epoch=1.920]\u001b[A\n",
      "Epoch 349:  69%|████▊  | 46/67 [00:13<00:06,  3.46it/s, loss=1.96, v_num=1, train_loss_step=0.817, val_loss=2.690, train_loss_epoch=1.920]\u001b[A\n",
      "Epoch 349:  70%|████▉  | 47/67 [00:13<00:05,  3.45it/s, loss=1.96, v_num=1, train_loss_step=0.817, val_loss=2.690, train_loss_epoch=1.920]\u001b[A\n",
      "Epoch 349:  72%|█████  | 48/67 [00:13<00:05,  3.44it/s, loss=1.96, v_num=1, train_loss_step=0.817, val_loss=2.690, train_loss_epoch=1.920]\u001b[A\n",
      "Epoch 349:  73%|█████  | 49/67 [00:14<00:05,  3.43it/s, loss=1.96, v_num=1, train_loss_step=0.817, val_loss=2.690, train_loss_epoch=1.920]\u001b[A\n",
      "Epoch 349:  75%|█████▏ | 50/67 [00:14<00:04,  3.42it/s, loss=1.96, v_num=1, train_loss_step=0.817, val_loss=2.690, train_loss_epoch=1.920]\u001b[A\n",
      "Epoch 349:  76%|█████▎ | 51/67 [00:15<00:04,  3.37it/s, loss=1.96, v_num=1, train_loss_step=0.817, val_loss=2.690, train_loss_epoch=1.920]\u001b[A\n",
      "Epoch 349:  78%|█████▍ | 52/67 [00:15<00:04,  3.36it/s, loss=1.96, v_num=1, train_loss_step=0.817, val_loss=2.690, train_loss_epoch=1.920]\u001b[A\n",
      "Epoch 349:  79%|█████▌ | 53/67 [00:15<00:04,  3.35it/s, loss=1.96, v_num=1, train_loss_step=0.817, val_loss=2.690, train_loss_epoch=1.920]\u001b[A\n",
      "Epoch 349:  81%|█████▋ | 54/67 [00:16<00:03,  3.35it/s, loss=1.96, v_num=1, train_loss_step=0.817, val_loss=2.690, train_loss_epoch=1.920]\u001b[A\n",
      "Epoch 349:  82%|█████▋ | 55/67 [00:16<00:03,  3.34it/s, loss=1.96, v_num=1, train_loss_step=0.817, val_loss=2.690, train_loss_epoch=1.920]\u001b[A\n",
      "Epoch 349:  84%|█████▊ | 56/67 [00:16<00:03,  3.34it/s, loss=1.96, v_num=1, train_loss_step=0.817, val_loss=2.690, train_loss_epoch=1.920]\u001b[A\n",
      "Epoch 349:  85%|█████▉ | 57/67 [00:17<00:03,  3.33it/s, loss=1.96, v_num=1, train_loss_step=0.817, val_loss=2.690, train_loss_epoch=1.920]\u001b[A\n",
      "Epoch 349:  87%|██████ | 58/67 [00:17<00:02,  3.32it/s, loss=1.96, v_num=1, train_loss_step=0.817, val_loss=2.690, train_loss_epoch=1.920]\u001b[A\n",
      "Epoch 349:  88%|██████▏| 59/67 [00:17<00:02,  3.32it/s, loss=1.96, v_num=1, train_loss_step=0.817, val_loss=2.690, train_loss_epoch=1.920]\u001b[A\n",
      "Epoch 349:  90%|██████▎| 60/67 [00:18<00:02,  3.31it/s, loss=1.96, v_num=1, train_loss_step=0.817, val_loss=2.690, train_loss_epoch=1.920]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 349:  91%|██████▎| 61/67 [00:18<00:01,  3.27it/s, loss=1.96, v_num=1, train_loss_step=0.817, val_loss=2.690, train_loss_epoch=1.920]\u001b[A\n",
      "Epoch 349:  93%|██████▍| 62/67 [00:18<00:01,  3.27it/s, loss=1.96, v_num=1, train_loss_step=0.817, val_loss=2.690, train_loss_epoch=1.920]\u001b[A\n",
      "Epoch 349:  94%|██████▌| 63/67 [00:19<00:01,  3.26it/s, loss=1.96, v_num=1, train_loss_step=0.817, val_loss=2.690, train_loss_epoch=1.920]\u001b[A\n",
      "Epoch 349:  96%|██████▋| 64/67 [00:19<00:00,  3.26it/s, loss=1.96, v_num=1, train_loss_step=0.817, val_loss=2.690, train_loss_epoch=1.920]\u001b[A\n",
      "Epoch 349:  97%|██████▊| 65/67 [00:19<00:00,  3.26it/s, loss=1.96, v_num=1, train_loss_step=0.817, val_loss=2.690, train_loss_epoch=1.920]\u001b[A\n",
      "Epoch 349:  99%|██████▉| 66/67 [00:20<00:00,  3.25it/s, loss=1.96, v_num=1, train_loss_step=0.817, val_loss=2.690, train_loss_epoch=1.920]\u001b[A\n",
      "Epoch 349: 100%|███████| 67/67 [00:21<00:00,  3.18it/s, loss=1.96, v_num=1, train_loss_step=0.817, val_loss=2.690, train_loss_epoch=1.920]\u001b[A\n",
      "Epoch 350:  45%|███▏   | 30/67 [00:05<00:07,  5.14it/s, loss=1.65, v_num=1, train_loss_step=2.170, val_loss=2.690, train_loss_epoch=1.870]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 350:  46%|███▏   | 31/67 [00:08<00:09,  3.87it/s, loss=1.65, v_num=1, train_loss_step=2.170, val_loss=2.690, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 350:  48%|███▎   | 32/67 [00:08<00:09,  3.84it/s, loss=1.65, v_num=1, train_loss_step=2.170, val_loss=2.690, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 350:  49%|███▍   | 33/67 [00:08<00:08,  3.81it/s, loss=1.65, v_num=1, train_loss_step=2.170, val_loss=2.690, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 350:  51%|███▌   | 34/67 [00:09<00:08,  3.77it/s, loss=1.65, v_num=1, train_loss_step=2.170, val_loss=2.690, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 350:  52%|███▋   | 35/67 [00:09<00:08,  3.74it/s, loss=1.65, v_num=1, train_loss_step=2.170, val_loss=2.690, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 350:  54%|███▊   | 36/67 [00:09<00:08,  3.72it/s, loss=1.65, v_num=1, train_loss_step=2.170, val_loss=2.690, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 350:  55%|███▊   | 37/67 [00:10<00:08,  3.70it/s, loss=1.65, v_num=1, train_loss_step=2.170, val_loss=2.690, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 350:  57%|███▉   | 38/67 [00:10<00:07,  3.68it/s, loss=1.65, v_num=1, train_loss_step=2.170, val_loss=2.690, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 350:  58%|████   | 39/67 [00:10<00:07,  3.66it/s, loss=1.65, v_num=1, train_loss_step=2.170, val_loss=2.690, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 350:  60%|████▏  | 40/67 [00:10<00:07,  3.64it/s, loss=1.65, v_num=1, train_loss_step=2.170, val_loss=2.690, train_loss_epoch=1.870]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 350:  61%|████▎  | 41/67 [00:11<00:07,  3.57it/s, loss=1.65, v_num=1, train_loss_step=2.170, val_loss=2.690, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 350:  63%|████▍  | 42/67 [00:11<00:07,  3.55it/s, loss=1.65, v_num=1, train_loss_step=2.170, val_loss=2.690, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 350:  64%|████▍  | 43/67 [00:12<00:06,  3.53it/s, loss=1.65, v_num=1, train_loss_step=2.170, val_loss=2.690, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 350:  66%|████▌  | 44/67 [00:12<00:06,  3.52it/s, loss=1.65, v_num=1, train_loss_step=2.170, val_loss=2.690, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 350:  67%|████▋  | 45/67 [00:12<00:06,  3.51it/s, loss=1.65, v_num=1, train_loss_step=2.170, val_loss=2.690, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 350:  69%|████▊  | 46/67 [00:13<00:06,  3.49it/s, loss=1.65, v_num=1, train_loss_step=2.170, val_loss=2.690, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 350:  70%|████▉  | 47/67 [00:13<00:05,  3.47it/s, loss=1.65, v_num=1, train_loss_step=2.170, val_loss=2.690, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 350:  72%|█████  | 48/67 [00:13<00:05,  3.46it/s, loss=1.65, v_num=1, train_loss_step=2.170, val_loss=2.690, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 350:  73%|█████  | 49/67 [00:14<00:05,  3.43it/s, loss=1.65, v_num=1, train_loss_step=2.170, val_loss=2.690, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 350:  75%|█████▏ | 50/67 [00:14<00:04,  3.42it/s, loss=1.65, v_num=1, train_loss_step=2.170, val_loss=2.690, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 350:  76%|█████▎ | 51/67 [00:15<00:04,  3.36it/s, loss=1.65, v_num=1, train_loss_step=2.170, val_loss=2.690, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 350:  78%|█████▍ | 52/67 [00:15<00:04,  3.35it/s, loss=1.65, v_num=1, train_loss_step=2.170, val_loss=2.690, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 350:  79%|█████▌ | 53/67 [00:15<00:04,  3.34it/s, loss=1.65, v_num=1, train_loss_step=2.170, val_loss=2.690, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 350:  81%|█████▋ | 54/67 [00:16<00:03,  3.33it/s, loss=1.65, v_num=1, train_loss_step=2.170, val_loss=2.690, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 350:  82%|█████▋ | 55/67 [00:16<00:03,  3.32it/s, loss=1.65, v_num=1, train_loss_step=2.170, val_loss=2.690, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 350:  84%|█████▊ | 56/67 [00:16<00:03,  3.31it/s, loss=1.65, v_num=1, train_loss_step=2.170, val_loss=2.690, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 350:  85%|█████▉ | 57/67 [00:17<00:03,  3.30it/s, loss=1.65, v_num=1, train_loss_step=2.170, val_loss=2.690, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 350:  87%|██████ | 58/67 [00:17<00:02,  3.29it/s, loss=1.65, v_num=1, train_loss_step=2.170, val_loss=2.690, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 350:  88%|██████▏| 59/67 [00:17<00:02,  3.29it/s, loss=1.65, v_num=1, train_loss_step=2.170, val_loss=2.690, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 350:  90%|██████▎| 60/67 [00:18<00:02,  3.28it/s, loss=1.65, v_num=1, train_loss_step=2.170, val_loss=2.690, train_loss_epoch=1.870]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 350:  91%|██████▎| 61/67 [00:18<00:01,  3.24it/s, loss=1.65, v_num=1, train_loss_step=2.170, val_loss=2.690, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 350:  93%|██████▍| 62/67 [00:19<00:01,  3.23it/s, loss=1.65, v_num=1, train_loss_step=2.170, val_loss=2.690, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 350:  94%|██████▌| 63/67 [00:19<00:01,  3.23it/s, loss=1.65, v_num=1, train_loss_step=2.170, val_loss=2.690, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 350:  96%|██████▋| 64/67 [00:19<00:00,  3.22it/s, loss=1.65, v_num=1, train_loss_step=2.170, val_loss=2.690, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 350:  97%|██████▊| 65/67 [00:20<00:00,  3.22it/s, loss=1.65, v_num=1, train_loss_step=2.170, val_loss=2.690, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 350:  99%|██████▉| 66/67 [00:20<00:00,  3.22it/s, loss=1.65, v_num=1, train_loss_step=2.170, val_loss=2.690, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 350: 100%|███████| 67/67 [00:21<00:00,  3.14it/s, loss=1.65, v_num=1, train_loss_step=2.170, val_loss=2.680, train_loss_epoch=1.870]\u001b[A\n",
      "Epoch 351:  45%|███▏   | 30/67 [00:05<00:07,  5.09it/s, loss=1.66, v_num=1, train_loss_step=0.694, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 351:  46%|███▏   | 31/67 [00:08<00:09,  3.78it/s, loss=1.66, v_num=1, train_loss_step=0.694, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 351:  48%|███▎   | 32/67 [00:08<00:09,  3.75it/s, loss=1.66, v_num=1, train_loss_step=0.694, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 351:  49%|███▍   | 33/67 [00:08<00:09,  3.72it/s, loss=1.66, v_num=1, train_loss_step=0.694, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 351:  51%|███▌   | 34/67 [00:09<00:08,  3.70it/s, loss=1.66, v_num=1, train_loss_step=0.694, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 351:  52%|███▋   | 35/67 [00:09<00:08,  3.67it/s, loss=1.66, v_num=1, train_loss_step=0.694, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 351:  54%|███▊   | 36/67 [00:09<00:08,  3.65it/s, loss=1.66, v_num=1, train_loss_step=0.694, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 351:  55%|███▊   | 37/67 [00:10<00:08,  3.63it/s, loss=1.66, v_num=1, train_loss_step=0.694, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 351:  57%|███▉   | 38/67 [00:10<00:08,  3.61it/s, loss=1.66, v_num=1, train_loss_step=0.694, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 351:  58%|████   | 39/67 [00:10<00:07,  3.59it/s, loss=1.66, v_num=1, train_loss_step=0.694, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 351:  60%|████▏  | 40/67 [00:11<00:07,  3.57it/s, loss=1.66, v_num=1, train_loss_step=0.694, val_loss=2.680, train_loss_epoch=1.630]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 351:  61%|████▎  | 41/67 [00:11<00:07,  3.47it/s, loss=1.66, v_num=1, train_loss_step=0.694, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 351:  63%|████▍  | 42/67 [00:12<00:07,  3.46it/s, loss=1.66, v_num=1, train_loss_step=0.694, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 351:  64%|████▍  | 43/67 [00:12<00:06,  3.44it/s, loss=1.66, v_num=1, train_loss_step=0.694, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 351:  66%|████▌  | 44/67 [00:12<00:06,  3.43it/s, loss=1.66, v_num=1, train_loss_step=0.694, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 351:  67%|████▋  | 45/67 [00:13<00:06,  3.41it/s, loss=1.66, v_num=1, train_loss_step=0.694, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 351:  69%|████▊  | 46/67 [00:13<00:06,  3.40it/s, loss=1.66, v_num=1, train_loss_step=0.694, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 351:  70%|████▉  | 47/67 [00:13<00:05,  3.38it/s, loss=1.66, v_num=1, train_loss_step=0.694, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 351:  72%|█████  | 48/67 [00:14<00:05,  3.37it/s, loss=1.66, v_num=1, train_loss_step=0.694, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 351:  73%|█████  | 49/67 [00:14<00:05,  3.36it/s, loss=1.66, v_num=1, train_loss_step=0.694, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 351:  75%|█████▏ | 50/67 [00:14<00:05,  3.35it/s, loss=1.66, v_num=1, train_loss_step=0.694, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 351:  76%|█████▎ | 51/67 [00:15<00:04,  3.30it/s, loss=1.66, v_num=1, train_loss_step=0.694, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 351:  78%|█████▍ | 52/67 [00:15<00:04,  3.29it/s, loss=1.66, v_num=1, train_loss_step=0.694, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 351:  79%|█████▌ | 53/67 [00:16<00:04,  3.28it/s, loss=1.66, v_num=1, train_loss_step=0.694, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 351:  81%|█████▋ | 54/67 [00:16<00:03,  3.27it/s, loss=1.66, v_num=1, train_loss_step=0.694, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 351:  82%|█████▋ | 55/67 [00:16<00:03,  3.27it/s, loss=1.66, v_num=1, train_loss_step=0.694, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 351:  84%|█████▊ | 56/67 [00:17<00:03,  3.26it/s, loss=1.66, v_num=1, train_loss_step=0.694, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 351:  85%|█████▉ | 57/67 [00:17<00:03,  3.25it/s, loss=1.66, v_num=1, train_loss_step=0.694, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 351:  87%|██████ | 58/67 [00:17<00:02,  3.25it/s, loss=1.66, v_num=1, train_loss_step=0.694, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 351:  88%|██████▏| 59/67 [00:18<00:02,  3.24it/s, loss=1.66, v_num=1, train_loss_step=0.694, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 351:  90%|██████▎| 60/67 [00:18<00:02,  3.23it/s, loss=1.66, v_num=1, train_loss_step=0.694, val_loss=2.680, train_loss_epoch=1.630]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 351:  91%|██████▎| 61/67 [00:19<00:01,  3.19it/s, loss=1.66, v_num=1, train_loss_step=0.694, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 351:  93%|██████▍| 62/67 [00:19<00:01,  3.19it/s, loss=1.66, v_num=1, train_loss_step=0.694, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 351:  94%|██████▌| 63/67 [00:19<00:01,  3.18it/s, loss=1.66, v_num=1, train_loss_step=0.694, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 351:  96%|██████▋| 64/67 [00:20<00:00,  3.18it/s, loss=1.66, v_num=1, train_loss_step=0.694, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 351:  97%|██████▊| 65/67 [00:20<00:00,  3.17it/s, loss=1.66, v_num=1, train_loss_step=0.694, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 351:  99%|██████▉| 66/67 [00:20<00:00,  3.17it/s, loss=1.66, v_num=1, train_loss_step=0.694, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 351: 100%|███████| 67/67 [00:21<00:00,  3.10it/s, loss=1.66, v_num=1, train_loss_step=0.694, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 352:  45%|███▏   | 30/67 [00:06<00:07,  4.94it/s, loss=1.75, v_num=1, train_loss_step=0.825, val_loss=2.680, train_loss_epoch=1.710]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 352:  46%|███▏   | 31/67 [00:08<00:09,  3.68it/s, loss=1.75, v_num=1, train_loss_step=0.825, val_loss=2.680, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 352:  48%|███▎   | 32/67 [00:08<00:09,  3.66it/s, loss=1.75, v_num=1, train_loss_step=0.825, val_loss=2.680, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 352:  49%|███▍   | 33/67 [00:09<00:09,  3.63it/s, loss=1.75, v_num=1, train_loss_step=0.825, val_loss=2.680, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 352:  51%|███▌   | 34/67 [00:09<00:09,  3.60it/s, loss=1.75, v_num=1, train_loss_step=0.825, val_loss=2.680, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 352:  52%|███▋   | 35/67 [00:09<00:08,  3.58it/s, loss=1.75, v_num=1, train_loss_step=0.825, val_loss=2.680, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 352:  54%|███▊   | 36/67 [00:10<00:08,  3.56it/s, loss=1.75, v_num=1, train_loss_step=0.825, val_loss=2.680, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 352:  55%|███▊   | 37/67 [00:10<00:08,  3.53it/s, loss=1.75, v_num=1, train_loss_step=0.825, val_loss=2.680, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 352:  57%|███▉   | 38/67 [00:10<00:08,  3.52it/s, loss=1.75, v_num=1, train_loss_step=0.825, val_loss=2.680, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 352:  58%|████   | 39/67 [00:11<00:08,  3.50it/s, loss=1.75, v_num=1, train_loss_step=0.825, val_loss=2.680, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 352:  60%|████▏  | 40/67 [00:11<00:07,  3.47it/s, loss=1.75, v_num=1, train_loss_step=0.825, val_loss=2.680, train_loss_epoch=1.710]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 352:  61%|████▎  | 41/67 [00:12<00:07,  3.38it/s, loss=1.75, v_num=1, train_loss_step=0.825, val_loss=2.680, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 352:  63%|████▍  | 42/67 [00:12<00:07,  3.36it/s, loss=1.75, v_num=1, train_loss_step=0.825, val_loss=2.680, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 352:  64%|████▍  | 43/67 [00:12<00:07,  3.35it/s, loss=1.75, v_num=1, train_loss_step=0.825, val_loss=2.680, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 352:  66%|████▌  | 44/67 [00:13<00:06,  3.34it/s, loss=1.75, v_num=1, train_loss_step=0.825, val_loss=2.680, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 352:  67%|████▋  | 45/67 [00:13<00:06,  3.34it/s, loss=1.75, v_num=1, train_loss_step=0.825, val_loss=2.680, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 352:  69%|████▊  | 46/67 [00:13<00:06,  3.32it/s, loss=1.75, v_num=1, train_loss_step=0.825, val_loss=2.680, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 352:  70%|████▉  | 47/67 [00:14<00:06,  3.32it/s, loss=1.75, v_num=1, train_loss_step=0.825, val_loss=2.680, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 352:  72%|█████  | 48/67 [00:14<00:05,  3.31it/s, loss=1.75, v_num=1, train_loss_step=0.825, val_loss=2.680, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 352:  73%|█████  | 49/67 [00:14<00:05,  3.30it/s, loss=1.75, v_num=1, train_loss_step=0.825, val_loss=2.680, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 352:  75%|█████▏ | 50/67 [00:15<00:05,  3.29it/s, loss=1.75, v_num=1, train_loss_step=0.825, val_loss=2.680, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 352:  76%|█████▎ | 51/67 [00:15<00:04,  3.25it/s, loss=1.75, v_num=1, train_loss_step=0.825, val_loss=2.680, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 352:  78%|█████▍ | 52/67 [00:16<00:04,  3.24it/s, loss=1.75, v_num=1, train_loss_step=0.825, val_loss=2.680, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 352:  79%|█████▌ | 53/67 [00:16<00:04,  3.24it/s, loss=1.75, v_num=1, train_loss_step=0.825, val_loss=2.680, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 352:  81%|█████▋ | 54/67 [00:16<00:04,  3.23it/s, loss=1.75, v_num=1, train_loss_step=0.825, val_loss=2.680, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 352:  82%|█████▋ | 55/67 [00:17<00:03,  3.22it/s, loss=1.75, v_num=1, train_loss_step=0.825, val_loss=2.680, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 352:  84%|█████▊ | 56/67 [00:17<00:03,  3.22it/s, loss=1.75, v_num=1, train_loss_step=0.825, val_loss=2.680, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 352:  85%|█████▉ | 57/67 [00:17<00:03,  3.21it/s, loss=1.75, v_num=1, train_loss_step=0.825, val_loss=2.680, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 352:  87%|██████ | 58/67 [00:18<00:02,  3.21it/s, loss=1.75, v_num=1, train_loss_step=0.825, val_loss=2.680, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 352:  88%|██████▏| 59/67 [00:18<00:02,  3.20it/s, loss=1.75, v_num=1, train_loss_step=0.825, val_loss=2.680, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 352:  90%|██████▎| 60/67 [00:18<00:02,  3.20it/s, loss=1.75, v_num=1, train_loss_step=0.825, val_loss=2.680, train_loss_epoch=1.710]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 352:  91%|██████▎| 61/67 [00:19<00:01,  3.16it/s, loss=1.75, v_num=1, train_loss_step=0.825, val_loss=2.680, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 352:  93%|██████▍| 62/67 [00:19<00:01,  3.16it/s, loss=1.75, v_num=1, train_loss_step=0.825, val_loss=2.680, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 352:  94%|██████▌| 63/67 [00:19<00:01,  3.15it/s, loss=1.75, v_num=1, train_loss_step=0.825, val_loss=2.680, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 352:  96%|██████▋| 64/67 [00:20<00:00,  3.15it/s, loss=1.75, v_num=1, train_loss_step=0.825, val_loss=2.680, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 352:  97%|██████▊| 65/67 [00:20<00:00,  3.15it/s, loss=1.75, v_num=1, train_loss_step=0.825, val_loss=2.680, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 352:  99%|██████▉| 66/67 [00:20<00:00,  3.15it/s, loss=1.75, v_num=1, train_loss_step=0.825, val_loss=2.680, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 352: 100%|███████| 67/67 [00:21<00:00,  3.08it/s, loss=1.75, v_num=1, train_loss_step=0.825, val_loss=2.680, train_loss_epoch=1.710]\u001b[A\n",
      "Epoch 353:  45%|███▌    | 30/67 [00:06<00:07,  4.91it/s, loss=1.7, v_num=1, train_loss_step=3.350, val_loss=2.680, train_loss_epoch=1.790]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 353:  46%|███▋    | 31/67 [00:08<00:09,  3.64it/s, loss=1.7, v_num=1, train_loss_step=3.350, val_loss=2.680, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 353:  48%|███▊    | 32/67 [00:08<00:09,  3.61it/s, loss=1.7, v_num=1, train_loss_step=3.350, val_loss=2.680, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 353:  49%|███▉    | 33/67 [00:09<00:09,  3.58it/s, loss=1.7, v_num=1, train_loss_step=3.350, val_loss=2.680, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 353:  51%|████    | 34/67 [00:09<00:09,  3.55it/s, loss=1.7, v_num=1, train_loss_step=3.350, val_loss=2.680, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 353:  52%|████▏   | 35/67 [00:09<00:09,  3.53it/s, loss=1.7, v_num=1, train_loss_step=3.350, val_loss=2.680, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 353:  54%|████▎   | 36/67 [00:10<00:08,  3.51it/s, loss=1.7, v_num=1, train_loss_step=3.350, val_loss=2.680, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 353:  55%|████▍   | 37/67 [00:10<00:08,  3.49it/s, loss=1.7, v_num=1, train_loss_step=3.350, val_loss=2.680, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 353:  57%|████▌   | 38/67 [00:10<00:08,  3.48it/s, loss=1.7, v_num=1, train_loss_step=3.350, val_loss=2.680, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 353:  58%|████▋   | 39/67 [00:11<00:08,  3.46it/s, loss=1.7, v_num=1, train_loss_step=3.350, val_loss=2.680, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 353:  60%|████▊   | 40/67 [00:11<00:07,  3.44it/s, loss=1.7, v_num=1, train_loss_step=3.350, val_loss=2.680, train_loss_epoch=1.790]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 353:  61%|████▉   | 41/67 [00:12<00:07,  3.38it/s, loss=1.7, v_num=1, train_loss_step=3.350, val_loss=2.680, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 353:  63%|█████   | 42/67 [00:12<00:07,  3.36it/s, loss=1.7, v_num=1, train_loss_step=3.350, val_loss=2.680, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 353:  64%|█████▏  | 43/67 [00:12<00:07,  3.35it/s, loss=1.7, v_num=1, train_loss_step=3.350, val_loss=2.680, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 353:  66%|█████▎  | 44/67 [00:13<00:06,  3.33it/s, loss=1.7, v_num=1, train_loss_step=3.350, val_loss=2.680, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 353:  67%|█████▎  | 45/67 [00:13<00:06,  3.33it/s, loss=1.7, v_num=1, train_loss_step=3.350, val_loss=2.680, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 353:  69%|█████▍  | 46/67 [00:13<00:06,  3.32it/s, loss=1.7, v_num=1, train_loss_step=3.350, val_loss=2.680, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 353:  70%|█████▌  | 47/67 [00:14<00:06,  3.31it/s, loss=1.7, v_num=1, train_loss_step=3.350, val_loss=2.680, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 353:  72%|█████▋  | 48/67 [00:14<00:05,  3.30it/s, loss=1.7, v_num=1, train_loss_step=3.350, val_loss=2.680, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 353:  73%|█████▊  | 49/67 [00:14<00:05,  3.30it/s, loss=1.7, v_num=1, train_loss_step=3.350, val_loss=2.680, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 353:  75%|█████▉  | 50/67 [00:15<00:05,  3.29it/s, loss=1.7, v_num=1, train_loss_step=3.350, val_loss=2.680, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 353:  76%|██████  | 51/67 [00:15<00:04,  3.24it/s, loss=1.7, v_num=1, train_loss_step=3.350, val_loss=2.680, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 353:  78%|██████▏ | 52/67 [00:16<00:04,  3.23it/s, loss=1.7, v_num=1, train_loss_step=3.350, val_loss=2.680, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 353:  79%|██████▎ | 53/67 [00:16<00:04,  3.23it/s, loss=1.7, v_num=1, train_loss_step=3.350, val_loss=2.680, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 353:  81%|██████▍ | 54/67 [00:16<00:04,  3.22it/s, loss=1.7, v_num=1, train_loss_step=3.350, val_loss=2.680, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 353:  82%|██████▌ | 55/67 [00:17<00:03,  3.22it/s, loss=1.7, v_num=1, train_loss_step=3.350, val_loss=2.680, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 353:  84%|██████▋ | 56/67 [00:17<00:03,  3.21it/s, loss=1.7, v_num=1, train_loss_step=3.350, val_loss=2.680, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 353:  85%|██████▊ | 57/67 [00:17<00:03,  3.21it/s, loss=1.7, v_num=1, train_loss_step=3.350, val_loss=2.680, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 353:  87%|██████▉ | 58/67 [00:18<00:02,  3.20it/s, loss=1.7, v_num=1, train_loss_step=3.350, val_loss=2.680, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 353:  88%|███████ | 59/67 [00:18<00:02,  3.20it/s, loss=1.7, v_num=1, train_loss_step=3.350, val_loss=2.680, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 353:  90%|███████▏| 60/67 [00:18<00:02,  3.19it/s, loss=1.7, v_num=1, train_loss_step=3.350, val_loss=2.680, train_loss_epoch=1.790]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 353:  91%|███████▎| 61/67 [00:19<00:01,  3.16it/s, loss=1.7, v_num=1, train_loss_step=3.350, val_loss=2.680, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 353:  93%|███████▍| 62/67 [00:19<00:01,  3.15it/s, loss=1.7, v_num=1, train_loss_step=3.350, val_loss=2.680, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 353:  94%|███████▌| 63/67 [00:19<00:01,  3.15it/s, loss=1.7, v_num=1, train_loss_step=3.350, val_loss=2.680, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 353:  96%|███████▋| 64/67 [00:20<00:00,  3.15it/s, loss=1.7, v_num=1, train_loss_step=3.350, val_loss=2.680, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 353:  97%|███████▊| 65/67 [00:20<00:00,  3.15it/s, loss=1.7, v_num=1, train_loss_step=3.350, val_loss=2.680, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 353:  99%|███████▉| 66/67 [00:20<00:00,  3.15it/s, loss=1.7, v_num=1, train_loss_step=3.350, val_loss=2.680, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 353: 100%|████████| 67/67 [00:21<00:00,  3.08it/s, loss=1.7, v_num=1, train_loss_step=3.350, val_loss=2.680, train_loss_epoch=1.790]\u001b[A\n",
      "Epoch 354:  45%|███▏   | 30/67 [00:05<00:07,  5.02it/s, loss=1.52, v_num=1, train_loss_step=2.740, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 354:  46%|███▏   | 31/67 [00:08<00:09,  3.73it/s, loss=1.52, v_num=1, train_loss_step=2.740, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 354:  48%|███▎   | 32/67 [00:08<00:09,  3.70it/s, loss=1.52, v_num=1, train_loss_step=2.740, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 354:  49%|███▍   | 33/67 [00:08<00:09,  3.68it/s, loss=1.52, v_num=1, train_loss_step=2.740, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 354:  51%|███▌   | 34/67 [00:09<00:09,  3.65it/s, loss=1.52, v_num=1, train_loss_step=2.740, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 354:  52%|███▋   | 35/67 [00:09<00:08,  3.63it/s, loss=1.52, v_num=1, train_loss_step=2.740, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 354:  54%|███▊   | 36/67 [00:09<00:08,  3.61it/s, loss=1.52, v_num=1, train_loss_step=2.740, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 354:  55%|███▊   | 37/67 [00:10<00:08,  3.59it/s, loss=1.52, v_num=1, train_loss_step=2.740, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 354:  57%|███▉   | 38/67 [00:10<00:08,  3.57it/s, loss=1.52, v_num=1, train_loss_step=2.740, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 354:  58%|████   | 39/67 [00:10<00:07,  3.56it/s, loss=1.52, v_num=1, train_loss_step=2.740, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 354:  60%|████▏  | 40/67 [00:11<00:07,  3.54it/s, loss=1.52, v_num=1, train_loss_step=2.740, val_loss=2.680, train_loss_epoch=1.630]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 354:  61%|████▎  | 41/67 [00:11<00:07,  3.46it/s, loss=1.52, v_num=1, train_loss_step=2.740, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 354:  63%|████▍  | 42/67 [00:12<00:07,  3.45it/s, loss=1.52, v_num=1, train_loss_step=2.740, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 354:  64%|████▍  | 43/67 [00:12<00:06,  3.44it/s, loss=1.52, v_num=1, train_loss_step=2.740, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 354:  66%|████▌  | 44/67 [00:12<00:06,  3.43it/s, loss=1.52, v_num=1, train_loss_step=2.740, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 354:  67%|████▋  | 45/67 [00:13<00:06,  3.42it/s, loss=1.52, v_num=1, train_loss_step=2.740, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 354:  69%|████▊  | 46/67 [00:13<00:06,  3.41it/s, loss=1.52, v_num=1, train_loss_step=2.740, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 354:  70%|████▉  | 47/67 [00:13<00:05,  3.40it/s, loss=1.52, v_num=1, train_loss_step=2.740, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 354:  72%|█████  | 48/67 [00:14<00:05,  3.39it/s, loss=1.52, v_num=1, train_loss_step=2.740, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 354:  73%|█████  | 49/67 [00:14<00:05,  3.38it/s, loss=1.52, v_num=1, train_loss_step=2.740, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 354:  75%|█████▏ | 50/67 [00:14<00:05,  3.38it/s, loss=1.52, v_num=1, train_loss_step=2.740, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 354:  76%|█████▎ | 51/67 [00:15<00:04,  3.33it/s, loss=1.52, v_num=1, train_loss_step=2.740, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 354:  78%|█████▍ | 52/67 [00:15<00:04,  3.32it/s, loss=1.52, v_num=1, train_loss_step=2.740, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 354:  79%|█████▌ | 53/67 [00:15<00:04,  3.32it/s, loss=1.52, v_num=1, train_loss_step=2.740, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 354:  81%|█████▋ | 54/67 [00:16<00:03,  3.31it/s, loss=1.52, v_num=1, train_loss_step=2.740, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 354:  82%|█████▋ | 55/67 [00:16<00:03,  3.30it/s, loss=1.52, v_num=1, train_loss_step=2.740, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 354:  84%|█████▊ | 56/67 [00:16<00:03,  3.30it/s, loss=1.52, v_num=1, train_loss_step=2.740, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 354:  85%|█████▉ | 57/67 [00:17<00:03,  3.29it/s, loss=1.52, v_num=1, train_loss_step=2.740, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 354:  87%|██████ | 58/67 [00:17<00:02,  3.29it/s, loss=1.52, v_num=1, train_loss_step=2.740, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 354:  88%|██████▏| 59/67 [00:17<00:02,  3.28it/s, loss=1.52, v_num=1, train_loss_step=2.740, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 354:  90%|██████▎| 60/67 [00:18<00:02,  3.28it/s, loss=1.52, v_num=1, train_loss_step=2.740, val_loss=2.680, train_loss_epoch=1.630]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 354:  91%|██████▎| 61/67 [00:18<00:01,  3.24it/s, loss=1.52, v_num=1, train_loss_step=2.740, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 354:  93%|██████▍| 62/67 [00:19<00:01,  3.24it/s, loss=1.52, v_num=1, train_loss_step=2.740, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 354:  94%|██████▌| 63/67 [00:19<00:01,  3.24it/s, loss=1.52, v_num=1, train_loss_step=2.740, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 354:  96%|██████▋| 64/67 [00:19<00:00,  3.23it/s, loss=1.52, v_num=1, train_loss_step=2.740, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 354:  97%|██████▊| 65/67 [00:20<00:00,  3.23it/s, loss=1.52, v_num=1, train_loss_step=2.740, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 354:  99%|██████▉| 66/67 [00:20<00:00,  3.23it/s, loss=1.52, v_num=1, train_loss_step=2.740, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 354: 100%|███████| 67/67 [00:21<00:00,  3.16it/s, loss=1.52, v_num=1, train_loss_step=2.740, val_loss=2.680, train_loss_epoch=1.630]\u001b[A\n",
      "Epoch 355:  45%|███▏   | 30/67 [00:05<00:07,  5.17it/s, loss=1.93, v_num=1, train_loss_step=3.240, val_loss=2.680, train_loss_epoch=1.480]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 355:  46%|███▏   | 31/67 [00:07<00:09,  3.90it/s, loss=1.93, v_num=1, train_loss_step=3.240, val_loss=2.680, train_loss_epoch=1.480]\u001b[A\n",
      "Epoch 355:  48%|███▎   | 32/67 [00:08<00:09,  3.87it/s, loss=1.93, v_num=1, train_loss_step=3.240, val_loss=2.680, train_loss_epoch=1.480]\u001b[A\n",
      "Epoch 355:  49%|███▍   | 33/67 [00:08<00:08,  3.84it/s, loss=1.93, v_num=1, train_loss_step=3.240, val_loss=2.680, train_loss_epoch=1.480]\u001b[A\n",
      "Epoch 355:  51%|███▌   | 34/67 [00:08<00:08,  3.80it/s, loss=1.93, v_num=1, train_loss_step=3.240, val_loss=2.680, train_loss_epoch=1.480]\u001b[A\n",
      "Epoch 355:  52%|███▋   | 35/67 [00:09<00:08,  3.78it/s, loss=1.93, v_num=1, train_loss_step=3.240, val_loss=2.680, train_loss_epoch=1.480]\u001b[A\n",
      "Epoch 355:  54%|███▊   | 36/67 [00:09<00:08,  3.75it/s, loss=1.93, v_num=1, train_loss_step=3.240, val_loss=2.680, train_loss_epoch=1.480]\u001b[A\n",
      "Epoch 355:  55%|███▊   | 37/67 [00:09<00:08,  3.72it/s, loss=1.93, v_num=1, train_loss_step=3.240, val_loss=2.680, train_loss_epoch=1.480]\u001b[A\n",
      "Epoch 355:  57%|███▉   | 38/67 [00:10<00:07,  3.70it/s, loss=1.93, v_num=1, train_loss_step=3.240, val_loss=2.680, train_loss_epoch=1.480]\u001b[A\n",
      "Epoch 355:  58%|████   | 39/67 [00:10<00:07,  3.68it/s, loss=1.93, v_num=1, train_loss_step=3.240, val_loss=2.680, train_loss_epoch=1.480]\u001b[A\n",
      "Epoch 355:  60%|████▏  | 40/67 [00:10<00:07,  3.65it/s, loss=1.93, v_num=1, train_loss_step=3.240, val_loss=2.680, train_loss_epoch=1.480]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 355:  61%|████▎  | 41/67 [00:11<00:07,  3.58it/s, loss=1.93, v_num=1, train_loss_step=3.240, val_loss=2.680, train_loss_epoch=1.480]\u001b[A\n",
      "Epoch 355:  63%|████▍  | 42/67 [00:11<00:07,  3.56it/s, loss=1.93, v_num=1, train_loss_step=3.240, val_loss=2.680, train_loss_epoch=1.480]\u001b[A\n",
      "Epoch 355:  64%|████▍  | 43/67 [00:12<00:06,  3.55it/s, loss=1.93, v_num=1, train_loss_step=3.240, val_loss=2.680, train_loss_epoch=1.480]\u001b[A\n",
      "Epoch 355:  66%|████▌  | 44/67 [00:12<00:06,  3.53it/s, loss=1.93, v_num=1, train_loss_step=3.240, val_loss=2.680, train_loss_epoch=1.480]\u001b[A\n",
      "Epoch 355:  67%|████▋  | 45/67 [00:12<00:06,  3.52it/s, loss=1.93, v_num=1, train_loss_step=3.240, val_loss=2.680, train_loss_epoch=1.480]\u001b[A\n",
      "Epoch 355:  69%|████▊  | 46/67 [00:13<00:05,  3.51it/s, loss=1.93, v_num=1, train_loss_step=3.240, val_loss=2.680, train_loss_epoch=1.480]\u001b[A\n",
      "Epoch 355:  70%|████▉  | 47/67 [00:13<00:05,  3.50it/s, loss=1.93, v_num=1, train_loss_step=3.240, val_loss=2.680, train_loss_epoch=1.480]\u001b[A\n",
      "Epoch 355:  72%|█████  | 48/67 [00:13<00:05,  3.48it/s, loss=1.93, v_num=1, train_loss_step=3.240, val_loss=2.680, train_loss_epoch=1.480]\u001b[A\n",
      "Epoch 355:  73%|█████  | 49/67 [00:14<00:05,  3.47it/s, loss=1.93, v_num=1, train_loss_step=3.240, val_loss=2.680, train_loss_epoch=1.480]\u001b[A\n",
      "Epoch 355:  75%|█████▏ | 50/67 [00:14<00:04,  3.46it/s, loss=1.93, v_num=1, train_loss_step=3.240, val_loss=2.680, train_loss_epoch=1.480]\u001b[A\n",
      "Epoch 355:  76%|█████▎ | 51/67 [00:14<00:04,  3.40it/s, loss=1.93, v_num=1, train_loss_step=3.240, val_loss=2.680, train_loss_epoch=1.480]\u001b[A\n",
      "Epoch 355:  78%|█████▍ | 52/67 [00:15<00:04,  3.39it/s, loss=1.93, v_num=1, train_loss_step=3.240, val_loss=2.680, train_loss_epoch=1.480]\u001b[A\n",
      "Epoch 355:  79%|█████▌ | 53/67 [00:15<00:04,  3.38it/s, loss=1.93, v_num=1, train_loss_step=3.240, val_loss=2.680, train_loss_epoch=1.480]\u001b[A\n",
      "Epoch 355:  81%|█████▋ | 54/67 [00:15<00:03,  3.38it/s, loss=1.93, v_num=1, train_loss_step=3.240, val_loss=2.680, train_loss_epoch=1.480]\u001b[A\n",
      "Epoch 355:  82%|█████▋ | 55/67 [00:16<00:03,  3.37it/s, loss=1.93, v_num=1, train_loss_step=3.240, val_loss=2.680, train_loss_epoch=1.480]\u001b[A\n",
      "Epoch 355:  84%|█████▊ | 56/67 [00:16<00:03,  3.36it/s, loss=1.93, v_num=1, train_loss_step=3.240, val_loss=2.680, train_loss_epoch=1.480]\u001b[A\n",
      "Epoch 355:  85%|█████▉ | 57/67 [00:16<00:02,  3.35it/s, loss=1.93, v_num=1, train_loss_step=3.240, val_loss=2.680, train_loss_epoch=1.480]\u001b[A\n",
      "Epoch 355:  87%|██████ | 58/67 [00:17<00:02,  3.34it/s, loss=1.93, v_num=1, train_loss_step=3.240, val_loss=2.680, train_loss_epoch=1.480]\u001b[A\n",
      "Epoch 355:  88%|██████▏| 59/67 [00:17<00:02,  3.34it/s, loss=1.93, v_num=1, train_loss_step=3.240, val_loss=2.680, train_loss_epoch=1.480]\u001b[A\n",
      "Epoch 355:  90%|██████▎| 60/67 [00:17<00:02,  3.33it/s, loss=1.93, v_num=1, train_loss_step=3.240, val_loss=2.680, train_loss_epoch=1.480]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 355:  91%|██████▎| 61/67 [00:18<00:01,  3.30it/s, loss=1.93, v_num=1, train_loss_step=3.240, val_loss=2.680, train_loss_epoch=1.480]\u001b[A\n",
      "Epoch 355:  93%|██████▍| 62/67 [00:18<00:01,  3.29it/s, loss=1.93, v_num=1, train_loss_step=3.240, val_loss=2.680, train_loss_epoch=1.480]\u001b[A\n",
      "Epoch 355:  94%|██████▌| 63/67 [00:19<00:01,  3.29it/s, loss=1.93, v_num=1, train_loss_step=3.240, val_loss=2.680, train_loss_epoch=1.480]\u001b[A\n",
      "Epoch 355:  96%|██████▋| 64/67 [00:19<00:00,  3.28it/s, loss=1.93, v_num=1, train_loss_step=3.240, val_loss=2.680, train_loss_epoch=1.480]\u001b[A\n",
      "Epoch 355:  97%|██████▊| 65/67 [00:19<00:00,  3.28it/s, loss=1.93, v_num=1, train_loss_step=3.240, val_loss=2.680, train_loss_epoch=1.480]\u001b[A\n",
      "Epoch 355:  99%|██████▉| 66/67 [00:20<00:00,  3.28it/s, loss=1.93, v_num=1, train_loss_step=3.240, val_loss=2.680, train_loss_epoch=1.480]\u001b[A\n",
      "Epoch 355: 100%|███████| 67/67 [00:20<00:00,  3.21it/s, loss=1.93, v_num=1, train_loss_step=3.240, val_loss=2.680, train_loss_epoch=1.480]\u001b[A\n",
      "Epoch 356:  45%|███▏   | 30/67 [00:05<00:07,  5.28it/s, loss=2.01, v_num=1, train_loss_step=0.616, val_loss=2.680, train_loss_epoch=2.110]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                     | 0/37 [00:00<?, ?it/s]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 356:  46%|███▏   | 31/67 [00:07<00:09,  3.98it/s, loss=2.01, v_num=1, train_loss_step=0.616, val_loss=2.680, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 356:  48%|███▎   | 32/67 [00:08<00:08,  3.93it/s, loss=2.01, v_num=1, train_loss_step=0.616, val_loss=2.680, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 356:  49%|███▍   | 33/67 [00:08<00:08,  3.89it/s, loss=2.01, v_num=1, train_loss_step=0.616, val_loss=2.680, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 356:  51%|███▌   | 34/67 [00:08<00:08,  3.86it/s, loss=2.01, v_num=1, train_loss_step=0.616, val_loss=2.680, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 356:  52%|███▋   | 35/67 [00:09<00:08,  3.83it/s, loss=2.01, v_num=1, train_loss_step=0.616, val_loss=2.680, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 356:  54%|███▊   | 36/67 [00:09<00:08,  3.80it/s, loss=2.01, v_num=1, train_loss_step=0.616, val_loss=2.680, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 356:  55%|███▊   | 37/67 [00:09<00:07,  3.77it/s, loss=2.01, v_num=1, train_loss_step=0.616, val_loss=2.680, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 356:  57%|███▉   | 38/67 [00:10<00:07,  3.74it/s, loss=2.01, v_num=1, train_loss_step=0.616, val_loss=2.680, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 356:  58%|████   | 39/67 [00:10<00:07,  3.72it/s, loss=2.01, v_num=1, train_loss_step=0.616, val_loss=2.680, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 356:  60%|████▏  | 40/67 [00:10<00:07,  3.70it/s, loss=2.01, v_num=1, train_loss_step=0.616, val_loss=2.680, train_loss_epoch=2.110]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 356:  61%|████▎  | 41/67 [00:11<00:07,  3.62it/s, loss=2.01, v_num=1, train_loss_step=0.616, val_loss=2.680, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 356:  63%|████▍  | 42/67 [00:11<00:06,  3.60it/s, loss=2.01, v_num=1, train_loss_step=0.616, val_loss=2.680, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 356:  64%|████▍  | 43/67 [00:11<00:06,  3.59it/s, loss=2.01, v_num=1, train_loss_step=0.616, val_loss=2.680, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 356:  66%|████▌  | 44/67 [00:12<00:06,  3.57it/s, loss=2.01, v_num=1, train_loss_step=0.616, val_loss=2.680, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 356:  67%|████▋  | 45/67 [00:12<00:06,  3.56it/s, loss=2.01, v_num=1, train_loss_step=0.616, val_loss=2.680, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 356:  69%|████▊  | 46/67 [00:12<00:05,  3.54it/s, loss=2.01, v_num=1, train_loss_step=0.616, val_loss=2.680, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 356:  70%|████▉  | 47/67 [00:13<00:05,  3.53it/s, loss=2.01, v_num=1, train_loss_step=0.616, val_loss=2.680, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 356:  72%|█████  | 48/67 [00:13<00:05,  3.52it/s, loss=2.01, v_num=1, train_loss_step=0.616, val_loss=2.680, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 356:  73%|█████  | 49/67 [00:13<00:05,  3.50it/s, loss=2.01, v_num=1, train_loss_step=0.616, val_loss=2.680, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 356:  75%|█████▏ | 50/67 [00:14<00:04,  3.49it/s, loss=2.01, v_num=1, train_loss_step=0.616, val_loss=2.680, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 356:  76%|█████▎ | 51/67 [00:14<00:04,  3.42it/s, loss=2.01, v_num=1, train_loss_step=0.616, val_loss=2.680, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 356:  78%|█████▍ | 52/67 [00:15<00:04,  3.41it/s, loss=2.01, v_num=1, train_loss_step=0.616, val_loss=2.680, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 356:  79%|█████▌ | 53/67 [00:15<00:04,  3.40it/s, loss=2.01, v_num=1, train_loss_step=0.616, val_loss=2.680, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 356:  81%|█████▋ | 54/67 [00:15<00:03,  3.40it/s, loss=2.01, v_num=1, train_loss_step=0.616, val_loss=2.680, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 356:  82%|█████▋ | 55/67 [00:16<00:03,  3.39it/s, loss=2.01, v_num=1, train_loss_step=0.616, val_loss=2.680, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 356:  84%|█████▊ | 56/67 [00:16<00:03,  3.38it/s, loss=2.01, v_num=1, train_loss_step=0.616, val_loss=2.680, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 356:  85%|█████▉ | 57/67 [00:16<00:02,  3.37it/s, loss=2.01, v_num=1, train_loss_step=0.616, val_loss=2.680, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 356:  87%|██████ | 58/67 [00:17<00:02,  3.37it/s, loss=2.01, v_num=1, train_loss_step=0.616, val_loss=2.680, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 356:  88%|██████▏| 59/67 [00:17<00:02,  3.36it/s, loss=2.01, v_num=1, train_loss_step=0.616, val_loss=2.680, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 356:  90%|██████▎| 60/67 [00:17<00:02,  3.35it/s, loss=2.01, v_num=1, train_loss_step=0.616, val_loss=2.680, train_loss_epoch=2.110]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 356:  91%|██████▎| 61/67 [00:18<00:01,  3.30it/s, loss=2.01, v_num=1, train_loss_step=0.616, val_loss=2.680, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 356:  93%|██████▍| 62/67 [00:18<00:01,  3.30it/s, loss=2.01, v_num=1, train_loss_step=0.616, val_loss=2.680, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 356:  94%|██████▌| 63/67 [00:19<00:01,  3.29it/s, loss=2.01, v_num=1, train_loss_step=0.616, val_loss=2.680, train_loss_epoch=2.110]\u001b[Awtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 356:  96%|██████▋| 64/67 [00:19<00:00,  3.29it/s, loss=2.01, v_num=1, train_loss_step=0.616, val_loss=2.680, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 356:  97%|██████▊| 65/67 [00:19<00:00,  3.29it/s, loss=2.01, v_num=1, train_loss_step=0.616, val_loss=2.680, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 356:  99%|██████▉| 66/67 [00:20<00:00,  3.28it/s, loss=2.01, v_num=1, train_loss_step=0.616, val_loss=2.680, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 356: 100%|███████| 67/67 [00:20<00:00,  3.21it/s, loss=2.01, v_num=1, train_loss_step=0.616, val_loss=nan.0, train_loss_epoch=2.110]\u001b[A\n",
      "Epoch 356: 100%|███████| 67/67 [00:20<00:00,  3.21it/s, loss=2.01, v_num=1, train_loss_step=0.616, val_loss=nan.0, train_loss_epoch=1.830]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# fit network\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/cristian/Extreme SSD/Investigacion/DATATHONES/entel-2022/DATATHON-ENTEL-2022---Reto2/results/models/tft/MODEL_tft-tweedie-loss-epoch_355-val_loss_2.679.ckpt\n",
      "best_model_name =  MODEL_tft-tweedie-loss-epoch_355-val_loss_2.679.ckpt\n"
     ]
    }
   ],
   "source": [
    "# load the best model according to the validation loss\n",
    "# (given that we use early stopping, this is not necessarily the last epoch)\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "print(best_model_path)\n",
    "best_model_name = best_model_path.split('/')[-1]\n",
    "print('best_model_name = ',best_model_name)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# raw predictions are a dictionary from which all kind of information including quantiles can be extracted\n",
    "raw_predictions, x = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)\n",
    "print('hola')\n",
    "\n",
    "for idx in range(5):  # plot 10 examples\n",
    "    best_tft.plot_prediction(x, raw_predictions, idx=idx, add_loss_to_title=True);\n",
    "    \n",
    "predictions, x = best_tft.predict(val_dataloader, return_x=True)\n",
    "predictions_vs_actuals = best_tft.calculate_prediction_actual_by_variable(x, predictions)\n",
    "best_tft.plot_prediction_actual_by_variable(predictions_vs_actuals);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse_val =  0.014048364944756031\n"
     ]
    }
   ],
   "source": [
    "# calcualte root mean squared error on validation set\n",
    "actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
    "val_predictions = best_tft.predict(val_dataloader)\n",
    "criterion = nn.MSELoss()\n",
    "rmse_val = torch.sqrt(criterion(actuals,val_predictions)).item()\n",
    "print('rmse_val = ',rmse_val)\n",
    "#rmse_val =  4.774808883666992"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='ratio'>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.boxplot(data=df_train,x='ratio')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in range(1):\n",
    "    print(actuals[i],val_predictions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_prediction_length: 10\n",
      "max_encoder_length   : 40\n"
     ]
    }
   ],
   "source": [
    "print('max_prediction_length:',max_prediction_length)\n",
    "print('max_encoder_length   :',max_encoder_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 49\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Z_MODELO</th>\n",
       "      <th>Z_PUNTO_VENTA</th>\n",
       "      <th>Z_GAMA</th>\n",
       "      <th>ratio</th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>days_from_payday</th>\n",
       "      <th>log_ratio</th>\n",
       "      <th>mean_sales_by_Z_MODELO</th>\n",
       "      <th>mean_sales_by_Z_PUNTO_VENTA</th>\n",
       "      <th>mean_sales_by_Z_GAMA</th>\n",
       "      <th>mean_sales_by_Z_MODELO_Z_GAMA</th>\n",
       "      <th>mean_sales_by_Z_PUNTO_VENTA_Z_GAMA</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>month</th>\n",
       "      <th>dayofyear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>-18.421875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029221</td>\n",
       "      <td>0.002880</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034363</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>-18.421875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024139</td>\n",
       "      <td>0.002939</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037018</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>-18.421875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026718</td>\n",
       "      <td>0.003031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043304</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>-18.421875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028458</td>\n",
       "      <td>0.002882</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039307</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>-18.421875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029739</td>\n",
       "      <td>0.002974</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045258</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358645</th>\n",
       "      <td>MOD_99</td>\n",
       "      <td>PVENT_99</td>\n",
       "      <td>GAM_3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "      <td>-18.421875</td>\n",
       "      <td>0.007484</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.002441</td>\n",
       "      <td>0.007484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358646</th>\n",
       "      <td>MOD_99</td>\n",
       "      <td>PVENT_99</td>\n",
       "      <td>GAM_3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>46</td>\n",
       "      <td>11</td>\n",
       "      <td>-18.421875</td>\n",
       "      <td>0.007896</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.007896</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358647</th>\n",
       "      <td>MOD_99</td>\n",
       "      <td>PVENT_99</td>\n",
       "      <td>GAM_3</td>\n",
       "      <td>0.005993</td>\n",
       "      <td>47</td>\n",
       "      <td>4</td>\n",
       "      <td>-5.117188</td>\n",
       "      <td>0.006664</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>0.002316</td>\n",
       "      <td>0.006664</td>\n",
       "      <td>0.000545</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358648</th>\n",
       "      <td>MOD_99</td>\n",
       "      <td>PVENT_99</td>\n",
       "      <td>GAM_3</td>\n",
       "      <td>0.003273</td>\n",
       "      <td>48</td>\n",
       "      <td>12</td>\n",
       "      <td>-5.722656</td>\n",
       "      <td>0.010201</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.002502</td>\n",
       "      <td>0.010201</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358649</th>\n",
       "      <td>MOD_99</td>\n",
       "      <td>PVENT_99</td>\n",
       "      <td>GAM_3</td>\n",
       "      <td>0.003542</td>\n",
       "      <td>49</td>\n",
       "      <td>5</td>\n",
       "      <td>-5.644531</td>\n",
       "      <td>0.011818</td>\n",
       "      <td>0.000842</td>\n",
       "      <td>0.002140</td>\n",
       "      <td>0.011818</td>\n",
       "      <td>0.000644</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1886920 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Z_MODELO Z_PUNTO_VENTA Z_GAMA     ratio  date_block_num  \\\n",
       "10         MOD_1       PVENT_1  GAM_1  0.000000              10   \n",
       "11         MOD_1       PVENT_1  GAM_1  0.000000              11   \n",
       "12         MOD_1       PVENT_1  GAM_1  0.000000              12   \n",
       "13         MOD_1       PVENT_1  GAM_1  0.000000              13   \n",
       "14         MOD_1       PVENT_1  GAM_1  0.000000              14   \n",
       "...          ...           ...    ...       ...             ...   \n",
       "2358645   MOD_99      PVENT_99  GAM_3  0.000000              45   \n",
       "2358646   MOD_99      PVENT_99  GAM_3  0.000000              46   \n",
       "2358647   MOD_99      PVENT_99  GAM_3  0.005993              47   \n",
       "2358648   MOD_99      PVENT_99  GAM_3  0.003273              48   \n",
       "2358649   MOD_99      PVENT_99  GAM_3  0.003542              49   \n",
       "\n",
       "         days_from_payday  log_ratio  mean_sales_by_Z_MODELO  \\\n",
       "10                      5 -18.421875                0.000000   \n",
       "11                     13 -18.421875                0.000000   \n",
       "12                      6 -18.421875                0.000000   \n",
       "13                     15 -18.421875                0.000000   \n",
       "14                      8 -18.421875                0.000000   \n",
       "...                   ...        ...                     ...   \n",
       "2358645                 3 -18.421875                0.007484   \n",
       "2358646                11 -18.421875                0.007896   \n",
       "2358647                 4  -5.117188                0.006664   \n",
       "2358648                12  -5.722656                0.010201   \n",
       "2358649                 5  -5.644531                0.011818   \n",
       "\n",
       "         mean_sales_by_Z_PUNTO_VENTA  mean_sales_by_Z_GAMA  \\\n",
       "10                          0.029221              0.002880   \n",
       "11                          0.024139              0.002939   \n",
       "12                          0.026718              0.003031   \n",
       "13                          0.028458              0.002882   \n",
       "14                          0.029739              0.002974   \n",
       "...                              ...                   ...   \n",
       "2358645                     0.000578              0.002441   \n",
       "2358646                     0.000306              0.002632   \n",
       "2358647                     0.000737              0.002316   \n",
       "2358648                     0.000510              0.002502   \n",
       "2358649                     0.000842              0.002140   \n",
       "\n",
       "         mean_sales_by_Z_MODELO_Z_GAMA  mean_sales_by_Z_PUNTO_VENTA_Z_GAMA  \\\n",
       "10                            0.000000                            0.034363   \n",
       "11                            0.000000                            0.037018   \n",
       "12                            0.000000                            0.043304   \n",
       "13                            0.000000                            0.039307   \n",
       "14                            0.000000                            0.045258   \n",
       "...                                ...                                 ...   \n",
       "2358645                       0.007484                            0.000000   \n",
       "2358646                       0.007896                            0.000509   \n",
       "2358647                       0.006664                            0.000545   \n",
       "2358648                       0.010201                            0.000744   \n",
       "2358649                       0.011818                            0.000644   \n",
       "\n",
       "        dayofweek month dayofyear  \n",
       "10              0     7       207  \n",
       "11              0     8       214  \n",
       "12              0     8       221  \n",
       "13              0     8       228  \n",
       "14              0     8       235  \n",
       "...           ...   ...       ...  \n",
       "2358645         0     3        87  \n",
       "2358646         0     4        94  \n",
       "2358647         0     4       101  \n",
       "2358648         0     4       108  \n",
       "2358649         0     4       115  \n",
       "\n",
       "[1886920 rows x 15 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select last 30 days from data (max_encoder_length is 24)\n",
    "encoder_data = df_train[lambda x: x.date_block_num > x.date_block_num.max() - max_encoder_length]\n",
    "\n",
    "print(encoder_data['date_block_num'].min(),encoder_data['date_block_num'].max())\n",
    "#print(encoder_data['DATE'].min(),encoder_data['DATE'].max())\n",
    "encoder_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "last_data = df_train[df_train['date_block_num'].isin([idx  -  max_prediction_length for idx in df_test['date_block_num'].unique()])]\n",
    "last_data['date_block_num'] = last_data['date_block_num'] + max_prediction_length\n",
    "\n",
    "decoder_data = pd.merge(df_test[[col for col in df_test.columns if 'ratio' not in col]], \n",
    "        last_data[['date_block_num','Z_MODELO','Z_PUNTO_VENTA','Z_GAMA',\"ratio\"]+statistics_columns],\n",
    "        on = ['date_block_num', 'Z_MODELO','Z_PUNTO_VENTA','Z_GAMA',],\n",
    "                        how='left'\n",
    "        )\n",
    "\n",
    "\n",
    "encoder_data.replace([np.inf, -np.inf, np.nan],0,inplace=True)\n",
    "decoder_data.replace([np.inf, -np.inf, np.nan],0,inplace=True)\n",
    "\n",
    "\n",
    "# combine encoder and decoder data\n",
    "new_prediction_data = pd.concat([encoder_data, decoder_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>Z_MODELO</th>\n",
       "      <th>Z_PUNTO_VENTA</th>\n",
       "      <th>Z_GAMA</th>\n",
       "      <th>ratio</th>\n",
       "      <th>log_ratio</th>\n",
       "      <th>mean_sales_by_Z_MODELO</th>\n",
       "      <th>mean_sales_by_Z_PUNTO_VENTA</th>\n",
       "      <th>mean_sales_by_Z_GAMA</th>\n",
       "      <th>mean_sales_by_Z_MODELO_Z_GAMA</th>\n",
       "      <th>mean_sales_by_Z_PUNTO_VENTA_Z_GAMA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>50</td>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0.361084</td>\n",
       "      <td>-1.018555</td>\n",
       "      <td>0.010727</td>\n",
       "      <td>0.033447</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>0.010727</td>\n",
       "      <td>0.048187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>51</td>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0.548340</td>\n",
       "      <td>-0.600586</td>\n",
       "      <td>0.015358</td>\n",
       "      <td>0.030426</td>\n",
       "      <td>0.002720</td>\n",
       "      <td>0.015358</td>\n",
       "      <td>0.047791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>52</td>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0.884277</td>\n",
       "      <td>-0.122864</td>\n",
       "      <td>0.018402</td>\n",
       "      <td>0.036530</td>\n",
       "      <td>0.002678</td>\n",
       "      <td>0.018402</td>\n",
       "      <td>0.054840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>53</td>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>1.017578</td>\n",
       "      <td>0.017349</td>\n",
       "      <td>0.021439</td>\n",
       "      <td>0.029984</td>\n",
       "      <td>0.002529</td>\n",
       "      <td>0.021439</td>\n",
       "      <td>0.040741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>54</td>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>1.449219</td>\n",
       "      <td>0.371094</td>\n",
       "      <td>0.029770</td>\n",
       "      <td>0.035919</td>\n",
       "      <td>0.002375</td>\n",
       "      <td>0.029770</td>\n",
       "      <td>0.046478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358645</th>\n",
       "      <td>55</td>\n",
       "      <td>MOD_99</td>\n",
       "      <td>PVENT_99</td>\n",
       "      <td>GAM_3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-18.421875</td>\n",
       "      <td>0.007484</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.002441</td>\n",
       "      <td>0.007484</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358646</th>\n",
       "      <td>56</td>\n",
       "      <td>MOD_99</td>\n",
       "      <td>PVENT_99</td>\n",
       "      <td>GAM_3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-18.421875</td>\n",
       "      <td>0.007896</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.007896</td>\n",
       "      <td>0.000509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358647</th>\n",
       "      <td>57</td>\n",
       "      <td>MOD_99</td>\n",
       "      <td>PVENT_99</td>\n",
       "      <td>GAM_3</td>\n",
       "      <td>0.005993</td>\n",
       "      <td>-5.117188</td>\n",
       "      <td>0.006664</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>0.002316</td>\n",
       "      <td>0.006664</td>\n",
       "      <td>0.000545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358648</th>\n",
       "      <td>58</td>\n",
       "      <td>MOD_99</td>\n",
       "      <td>PVENT_99</td>\n",
       "      <td>GAM_3</td>\n",
       "      <td>0.003273</td>\n",
       "      <td>-5.722656</td>\n",
       "      <td>0.010201</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.002502</td>\n",
       "      <td>0.010201</td>\n",
       "      <td>0.000744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358649</th>\n",
       "      <td>59</td>\n",
       "      <td>MOD_99</td>\n",
       "      <td>PVENT_99</td>\n",
       "      <td>GAM_3</td>\n",
       "      <td>0.003542</td>\n",
       "      <td>-5.644531</td>\n",
       "      <td>0.011818</td>\n",
       "      <td>0.000842</td>\n",
       "      <td>0.002140</td>\n",
       "      <td>0.011818</td>\n",
       "      <td>0.000644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>471730 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date_block_num Z_MODELO Z_PUNTO_VENTA Z_GAMA     ratio  log_ratio  \\\n",
       "40                   50    MOD_1       PVENT_1  GAM_1  0.361084  -1.018555   \n",
       "41                   51    MOD_1       PVENT_1  GAM_1  0.548340  -0.600586   \n",
       "42                   52    MOD_1       PVENT_1  GAM_1  0.884277  -0.122864   \n",
       "43                   53    MOD_1       PVENT_1  GAM_1  1.017578   0.017349   \n",
       "44                   54    MOD_1       PVENT_1  GAM_1  1.449219   0.371094   \n",
       "...                 ...      ...           ...    ...       ...        ...   \n",
       "2358645              55   MOD_99      PVENT_99  GAM_3  0.000000 -18.421875   \n",
       "2358646              56   MOD_99      PVENT_99  GAM_3  0.000000 -18.421875   \n",
       "2358647              57   MOD_99      PVENT_99  GAM_3  0.005993  -5.117188   \n",
       "2358648              58   MOD_99      PVENT_99  GAM_3  0.003273  -5.722656   \n",
       "2358649              59   MOD_99      PVENT_99  GAM_3  0.003542  -5.644531   \n",
       "\n",
       "         mean_sales_by_Z_MODELO  mean_sales_by_Z_PUNTO_VENTA  \\\n",
       "40                     0.010727                     0.033447   \n",
       "41                     0.015358                     0.030426   \n",
       "42                     0.018402                     0.036530   \n",
       "43                     0.021439                     0.029984   \n",
       "44                     0.029770                     0.035919   \n",
       "...                         ...                          ...   \n",
       "2358645                0.007484                     0.000578   \n",
       "2358646                0.007896                     0.000306   \n",
       "2358647                0.006664                     0.000737   \n",
       "2358648                0.010201                     0.000510   \n",
       "2358649                0.011818                     0.000842   \n",
       "\n",
       "         mean_sales_by_Z_GAMA  mean_sales_by_Z_MODELO_Z_GAMA  \\\n",
       "40                   0.002653                       0.010727   \n",
       "41                   0.002720                       0.015358   \n",
       "42                   0.002678                       0.018402   \n",
       "43                   0.002529                       0.021439   \n",
       "44                   0.002375                       0.029770   \n",
       "...                       ...                            ...   \n",
       "2358645              0.002441                       0.007484   \n",
       "2358646              0.002632                       0.007896   \n",
       "2358647              0.002316                       0.006664   \n",
       "2358648              0.002502                       0.010201   \n",
       "2358649              0.002140                       0.011818   \n",
       "\n",
       "         mean_sales_by_Z_PUNTO_VENTA_Z_GAMA  \n",
       "40                                 0.048187  \n",
       "41                                 0.047791  \n",
       "42                                 0.054840  \n",
       "43                                 0.040741  \n",
       "44                                 0.046478  \n",
       "...                                     ...  \n",
       "2358645                            0.000000  \n",
       "2358646                            0.000509  \n",
       "2358647                            0.000545  \n",
       "2358648                            0.000744  \n",
       "2358649                            0.000644  \n",
       "\n",
       "[471730 rows x 11 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_data[['date_block_num','Z_MODELO','Z_PUNTO_VENTA','Z_GAMA',\"ratio\"]+statistics_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "aux = encoder_data['ratio'].values\n",
    "\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "plt.plot(aux,'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "aux = decoder_data['ratio'].values\n",
    "\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "plt.plot(aux,'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "new_raw_predictions, new_x = best_tft.predict(new_prediction_data, mode=\"raw\", return_x=True)\n",
    "\n",
    "for idx in range(10):  # plot 10 examples\n",
    "    best_tft.plot_prediction(new_x, new_raw_predictions, idx=idx, show_future_observed=False);\n",
    "    \n",
    "interpretation = best_tft.interpret_output(new_raw_predictions, reduction=\"sum\")\n",
    "best_tft.plot_interpretation(interpretation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_raw_predictions = best_tft.predict(new_prediction_data, mode=\"prediction\", return_x=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(new_raw_predictions.numpy()).T\n",
    "predictions['date_block_num'] = sorted(df_test['date_block_num'].unique())\n",
    "predictions = pd.melt(predictions, id_vars=['date_block_num'])\n",
    "predictions = predictions.sort_values(['date_block_num', 'variable']).reset_index(drop=True)\n",
    "df_test[['date_block_num','Z_MODELO','Z_PUNTO_VENTA','Z_GAMA']].sort_values(['date_block_num', 'Z_MODELO','Z_PUNTO_VENTA','Z_GAMA']).reset_index(drop=True)\n",
    "df_test2 = df_test.join(predictions['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>Z_MODELO</th>\n",
       "      <th>Z_PUNTO_VENTA</th>\n",
       "      <th>Z_GAMA</th>\n",
       "      <th>ratio</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>month</th>\n",
       "      <th>dayofyear</th>\n",
       "      <th>days_from_payday</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>122</td>\n",
       "      <td>13</td>\n",
       "      <td>0.344408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51</td>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>129</td>\n",
       "      <td>6</td>\n",
       "      <td>0.060787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52</td>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>136</td>\n",
       "      <td>15</td>\n",
       "      <td>0.003178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>143</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.000798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54</td>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   date_block_num Z_MODELO Z_PUNTO_VENTA Z_GAMA  ratio dayofweek month  \\\n",
       "0              50    MOD_1       PVENT_1  GAM_1    0.0         0     5   \n",
       "1              51    MOD_1       PVENT_1  GAM_1    0.0         0     5   \n",
       "2              52    MOD_1       PVENT_1  GAM_1    0.0         0     5   \n",
       "3              53    MOD_1       PVENT_1  GAM_1    0.0         0     5   \n",
       "4              54    MOD_1       PVENT_1  GAM_1    0.0         0     5   \n",
       "\n",
       "  dayofyear  days_from_payday     value  \n",
       "0       122                13  0.344408  \n",
       "1       129                 6  0.060787  \n",
       "2       136                15  0.003178  \n",
       "3       143                 8 -0.000798  \n",
       "4       150                 1  0.004788  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "reverse_mapping_file = '../../utils/reverse_dict_mapping_list.txt'\n",
    "\n",
    "with open(reverse_mapping_file, 'rb') as f:\n",
    "    reverse_mapping = pickle.load( f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reverse_mapping#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#descriptive_columns = ['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA']\n",
    "descriptive_columns = ['Z_MARCA', 'Z_GAMA', 'Z_MODELO',\n",
    "                       'Z_DEPARTAMENTO', 'Z_PUNTO_VENTA']\n",
    "i=0\n",
    "for column in descriptive_columns:\n",
    "    if column in df_test2.columns:\n",
    "        df_test2[column] = df_test2[column].map(reverse_mapping[i])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>Z_MODELO</th>\n",
       "      <th>Z_PUNTO_VENTA</th>\n",
       "      <th>Z_GAMA</th>\n",
       "      <th>ratio</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>month</th>\n",
       "      <th>dayofyear</th>\n",
       "      <th>days_from_payday</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "      <td>da45328ba820604eb99694768f2a430cd933d161601dcb...</td>\n",
       "      <td>76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>122</td>\n",
       "      <td>13</td>\n",
       "      <td>0.344408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "      <td>da45328ba820604eb99694768f2a430cd933d161601dcb...</td>\n",
       "      <td>76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>129</td>\n",
       "      <td>6</td>\n",
       "      <td>0.060787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "      <td>da45328ba820604eb99694768f2a430cd933d161601dcb...</td>\n",
       "      <td>76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>136</td>\n",
       "      <td>15</td>\n",
       "      <td>0.003178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "      <td>da45328ba820604eb99694768f2a430cd933d161601dcb...</td>\n",
       "      <td>76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>143</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.000798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "      <td>da45328ba820604eb99694768f2a430cd933d161601dcb...</td>\n",
       "      <td>76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   date_block_num                                           Z_MODELO  \\\n",
       "0              50  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...   \n",
       "1              51  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...   \n",
       "2              52  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...   \n",
       "3              53  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...   \n",
       "4              54  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...   \n",
       "\n",
       "                                       Z_PUNTO_VENTA  \\\n",
       "0  da45328ba820604eb99694768f2a430cd933d161601dcb...   \n",
       "1  da45328ba820604eb99694768f2a430cd933d161601dcb...   \n",
       "2  da45328ba820604eb99694768f2a430cd933d161601dcb...   \n",
       "3  da45328ba820604eb99694768f2a430cd933d161601dcb...   \n",
       "4  da45328ba820604eb99694768f2a430cd933d161601dcb...   \n",
       "\n",
       "                                              Z_GAMA  ratio dayofweek month  \\\n",
       "0  76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...    0.0         0     5   \n",
       "1  76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...    0.0         0     5   \n",
       "2  76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...    0.0         0     5   \n",
       "3  76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...    0.0         0     5   \n",
       "4  76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...    0.0         0     5   \n",
       "\n",
       "  dayofyear  days_from_payday     value  \n",
       "0       122                13  0.344408  \n",
       "1       129                 6  0.060787  \n",
       "2       136                15  0.003178  \n",
       "3       143                 8 -0.000798  \n",
       "4       150                 1  0.004788  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inv_dict_dates = {v: k for k, v in dict_dates.items()}\n",
    "df_test2['Z_WEEK'] = df_test2['date_block_num'].map(inv_dict_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA','Z_WEEK']:\n",
    "    df_test2[column] = df_test2[column].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test2['ID'] = df_test2['Z_MODELO'] + '|' + df_test2['Z_PUNTO_VENTA'] + '|' + df_test2['Z_GAMA'] + '|' + df_test2['Z_WEEK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test2['ratio'] = np.maximum(df_test2['value'],0)\n",
    "submission = df_test2[['Z_WEEK','ID','ratio']]#.groupby('ID').sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>Z_MODELO</th>\n",
       "      <th>Z_PUNTO_VENTA</th>\n",
       "      <th>Z_GAMA</th>\n",
       "      <th>ratio</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>month</th>\n",
       "      <th>dayofyear</th>\n",
       "      <th>days_from_payday</th>\n",
       "      <th>value</th>\n",
       "      <th>Z_WEEK</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "      <td>da45328ba820604eb99694768f2a430cd933d161601dcb...</td>\n",
       "      <td>76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...</td>\n",
       "      <td>0.344408</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>122</td>\n",
       "      <td>13</td>\n",
       "      <td>0.344408</td>\n",
       "      <td>SEMANA_51</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "      <td>da45328ba820604eb99694768f2a430cd933d161601dcb...</td>\n",
       "      <td>76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...</td>\n",
       "      <td>0.060787</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>129</td>\n",
       "      <td>6</td>\n",
       "      <td>0.060787</td>\n",
       "      <td>SEMANA_52</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "      <td>da45328ba820604eb99694768f2a430cd933d161601dcb...</td>\n",
       "      <td>76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...</td>\n",
       "      <td>0.003178</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>136</td>\n",
       "      <td>15</td>\n",
       "      <td>0.003178</td>\n",
       "      <td>SEMANA_53</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "      <td>da45328ba820604eb99694768f2a430cd933d161601dcb...</td>\n",
       "      <td>76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>143</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.000798</td>\n",
       "      <td>SEMANA_54</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "      <td>da45328ba820604eb99694768f2a430cd933d161601dcb...</td>\n",
       "      <td>76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...</td>\n",
       "      <td>0.004788</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004788</td>\n",
       "      <td>SEMANA_55</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>55</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "      <td>da45328ba820604eb99694768f2a430cd933d161601dcb...</td>\n",
       "      <td>76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>157</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>SEMANA_56</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>56</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "      <td>da45328ba820604eb99694768f2a430cd933d161601dcb...</td>\n",
       "      <td>76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...</td>\n",
       "      <td>0.002987</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>164</td>\n",
       "      <td>2</td>\n",
       "      <td>0.002987</td>\n",
       "      <td>SEMANA_57</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>57</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "      <td>da45328ba820604eb99694768f2a430cd933d161601dcb...</td>\n",
       "      <td>76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>171</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>SEMANA_58</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>58</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "      <td>da45328ba820604eb99694768f2a430cd933d161601dcb...</td>\n",
       "      <td>76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...</td>\n",
       "      <td>0.000980</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>178</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000980</td>\n",
       "      <td>SEMANA_59</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>59</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "      <td>da45328ba820604eb99694768f2a430cd933d161601dcb...</td>\n",
       "      <td>76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>185</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>SEMANA_60</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   date_block_num                                           Z_MODELO  \\\n",
       "0              50  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...   \n",
       "1              51  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...   \n",
       "2              52  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...   \n",
       "3              53  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...   \n",
       "4              54  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...   \n",
       "5              55  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...   \n",
       "6              56  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...   \n",
       "7              57  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...   \n",
       "8              58  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...   \n",
       "9              59  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...   \n",
       "\n",
       "                                       Z_PUNTO_VENTA  \\\n",
       "0  da45328ba820604eb99694768f2a430cd933d161601dcb...   \n",
       "1  da45328ba820604eb99694768f2a430cd933d161601dcb...   \n",
       "2  da45328ba820604eb99694768f2a430cd933d161601dcb...   \n",
       "3  da45328ba820604eb99694768f2a430cd933d161601dcb...   \n",
       "4  da45328ba820604eb99694768f2a430cd933d161601dcb...   \n",
       "5  da45328ba820604eb99694768f2a430cd933d161601dcb...   \n",
       "6  da45328ba820604eb99694768f2a430cd933d161601dcb...   \n",
       "7  da45328ba820604eb99694768f2a430cd933d161601dcb...   \n",
       "8  da45328ba820604eb99694768f2a430cd933d161601dcb...   \n",
       "9  da45328ba820604eb99694768f2a430cd933d161601dcb...   \n",
       "\n",
       "                                              Z_GAMA     ratio dayofweek  \\\n",
       "0  76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...  0.344408         0   \n",
       "1  76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...  0.060787         0   \n",
       "2  76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...  0.003178         0   \n",
       "3  76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...  0.000000         0   \n",
       "4  76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...  0.004788         0   \n",
       "5  76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...  0.000444         0   \n",
       "6  76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...  0.002987         0   \n",
       "7  76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...  0.000469         0   \n",
       "8  76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...  0.000980         0   \n",
       "9  76df0c6db32d4e04e0ef6a3a6a1e1686677e34308d9435...  0.000627         0   \n",
       "\n",
       "  month dayofyear  days_from_payday     value     Z_WEEK  \\\n",
       "0     5       122                13  0.344408  SEMANA_51   \n",
       "1     5       129                 6  0.060787  SEMANA_52   \n",
       "2     5       136                15  0.003178  SEMANA_53   \n",
       "3     5       143                 8 -0.000798  SEMANA_54   \n",
       "4     5       150                 1  0.004788  SEMANA_55   \n",
       "5     6       157                 9  0.000444  SEMANA_56   \n",
       "6     6       164                 2  0.002987  SEMANA_57   \n",
       "7     6       171                10  0.000469  SEMANA_58   \n",
       "8     6       178                 3  0.000980  SEMANA_59   \n",
       "9     7       185                11  0.000627  SEMANA_60   \n",
       "\n",
       "                                                  ID  \n",
       "0  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...  \n",
       "1  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...  \n",
       "2  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...  \n",
       "3  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...  \n",
       "4  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...  \n",
       "5  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...  \n",
       "6  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...  \n",
       "7  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...  \n",
       "8  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...  \n",
       "9  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "aux = df_test2['ratio'].values\n",
    "\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "plt.plot(aux,'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "aux = df_train['ratio'].values\n",
    "\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "plt.plot(aux,'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "prediction = submission['ratio'].values\n",
    "df_baseline = pd.read_csv('../../results/Submission_ratios_01.csv')\n",
    "real = df_baseline['ratios']\n",
    "\n",
    "\n",
    "y_actual = df_baseline['Demanda']\n",
    "y_predicted = submission['Demanda'].values\n",
    "\n",
    "rms = mean_squared_error(y_actual, y_predicted, squared=False)\n",
    "rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "fig = plt.figure(figsize=(30,5),facecolor='white')\n",
    "\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(prediction,'bo')\n",
    "plt.title('prediction')\n",
    "\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(real,'go')\n",
    "plt.title('real')\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(prediction,'bo',alpha=0.7,label='prediction')\n",
    "plt.plot(real,'go',alpha=0.1,label='real')\n",
    "plt.title('real')\n",
    "\n",
    "plt.suptitle(best_model_name+'_MAX_EPOCHS_'+str(MAX_EPOCHS)+'_val_'+str(np.round(rmse_val,2))+'_refencia_best_pred_rmse_'+str(np.round(rms,2))+'_PREDICTIONS')\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(\n",
    "            '../../results/models/tft/'+best_model_name+'_MAX_EPOCHS_'+str(MAX_EPOCHS)+'_val_'+str(np.round(rmse_val,2))+'_refencia_best_pred_rmse_'+str(np.round(rms,2))+'_PREDICTIONS.png') "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_actual = df_baseline['Demanda']\n",
    "y_predicted = submission['Demanda'].values\n",
    "\n",
    "rms = mean_squared_error(y_actual, y_predicted, squared=False)\n",
    "rms"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#rmse_val = 3\n",
    "submission['Demanda_real'] = df_baseline['Demanda']\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "tendencia_semanal = submission[['Z_WEEK','Demanda','Demanda_real']].groupby(['Z_WEEK']).sum().reset_index()\n",
    "\n",
    "graph = pd.melt(tendencia_semanal,id_vars=['Z_WEEK'],value_vars=['Demanda','Demanda_real'],)\n",
    "\n",
    "#fig = px.line(graph,x='Z_WEEK',y='value',color='variable')\n",
    "#fig.show()\n",
    "import seaborn as sns\n",
    "sns.set(style='white')\n",
    "sns_fig = sns.catplot(x=\"Z_WEEK\", y=\"value\", hue=\"variable\", kind=\"point\", data=graph, height=4.27, aspect=25.7/8.27,facecolor='w')\n",
    "plt.title(best_model_name+'_MAX_EPOCHS_'+str(MAX_EPOCHS)+'_val_'+str(np.round(rmse_val,2))+'_refencia_best_pred_rmse_'+str(np.round(rms,2))+'_DEMANDA_SEMANAL_TOTAL')\n",
    "fig = sns_fig.figure\n",
    "fig.savefig(\n",
    "            '../../results/models/tft/'+best_model_name+'_MAX_EPOCHS_'+str(MAX_EPOCHS)+'_val_'+str(np.round(rmse_val,2))+'_refencia_best_pred_rmse_'+str(np.round(rms,2))+'_DEMANDA_SEMANAL_TOTAL.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submission['Demanda'] = 0.9#submission['value']\n",
    "import numpy as np\n",
    "# +'_refencia_best_pred_rmse_'+str(np.round(rms,2))+\n",
    "submission[['ID', 'ratio']].to_csv('../../results/models/tft/'+best_model_name+'_MAX_EPOCHS_'+str(MAX_EPOCHS)+'_val_'+str(np.round(rmse_val,2))+'_RATIOS.csv', index = False, sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Z_WEEK</th>\n",
       "      <th>ID</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SEMANA_51</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "      <td>0.344408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Z_WEEK                                                 ID     ratio\n",
       "0  SEMANA_51  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...  0.344408"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Z_WEEK</th>\n",
       "      <th>ID</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SEMANA_51</td>\n",
       "      <td>b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...</td>\n",
       "      <td>0.344408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Z_WEEK                                                 ID     ratio\n",
       "0  SEMANA_51  b48f98af5dc143cab1e64b72394fc2a31c8f2f53e20101...  0.344408"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "bb936826e9998cf8c0ce19ca18d08375e9e9e4488ea4df72de7138dd75138a24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
