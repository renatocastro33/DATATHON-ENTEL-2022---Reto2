{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from time import sleep\n",
    "\n",
    "def get_distance_from_paydays(date):\n",
    "    end_of_month = date.daysinmonth\n",
    "    distance_to_1st = 0 if date.day >=15 else 15 - date.day\n",
    "    distance_to15th = 0 if date.day < 15 else end_of_month - date.day\n",
    "    return distance_to_1st + distance_to15th\n",
    "\n",
    "def std(x): return np.std(x)\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            if str(col_type) == numerics:\n",
    "                c_min = df[col].min()\n",
    "                c_max = df[col].max()\n",
    "\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            if str(col_type)[:5] == 'float':\n",
    "                c_min = df[col].min()\n",
    "                c_max = df[col].max()\n",
    "\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "## Simple \"Memory profilers\" to see memory usage\n",
    "def get_memory_usage():\n",
    "    return np.round(psutil.Process(os.getpid()).memory_info()[0]/2.**30, 2) \n",
    "        \n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f%s%s\" % (num, 'Yi', suffix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 107.97 MB\n",
      "Memory usage after optimization is: 33.78 MB\n",
      "Decreased by 68.7%\n",
      "Memory usage of dataframe is 21.59 MB\n",
      "Memory usage after optimization is: 4.08 MB\n",
      "Decreased by 81.1%\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    df_train = pd.read_csv('../../dataset/train/train_converted.csv')\n",
    "    df_test  = pd.read_csv('../../dataset/test/test_converted.csv')\n",
    "    df_train = df_train[['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA','Z_WEEK','Z_WEEK_DATE','Demanda']].groupby(['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA','Z_WEEK','Z_WEEK_DATE']).sum().reset_index()\n",
    "    df_test = df_test[['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA','Z_WEEK','Z_WEEK_DATE','Demanda']].groupby(['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA','Z_WEEK','Z_WEEK_DATE']).sum().reset_index()\n",
    "\n",
    "else:\n",
    "\n",
    "    df_train = pd.read_pickle('../../dataset/train/train_converted_fill.pkl')\n",
    "    df_test  = pd.read_pickle('../../dataset/test/test_converted_fill.pkl')\n",
    "\n",
    "df_train = reduce_mem_usage(df_train)\n",
    "df_test = reduce_mem_usage(df_test)\n",
    "\n",
    "\n",
    "df_train.replace(['',np.inf, -np.inf, np.nan],0,inplace=True)\n",
    "df_test.replace(['',np.inf, -np.inf, np.nan],0,inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating date_block_num ...\n",
      "(2358650, 6) (471730, 6)\n",
      "Creating date_block_num completed!\n",
      "Preprocessing TRAINING DATASET ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:   0%|\u001b[32m                                                  \u001b[0m| 0/7 [00:00<?, ?it/s]\u001b[0m\n",
      "i:  14%|\u001b[32m██████                                    \u001b[0m| 1/7 [00:02<00:12,  2.01s/it]\u001b[0m\u001b[A\n",
      "j:   0%|\u001b[31m                                                  \u001b[0m| 0/1 [00:01<?, ?it/s]\u001b[0m\u001b[A\n",
      "j:   0%|\u001b[31m                                                  \u001b[0m| 0/1 [00:00<?, ?it/s]\u001b[0m\u001b[A\n",
      "j: 100%|\u001b[31m██████████████████████████████████████████\u001b[0m| 1/1 [00:00<00:00,  2.11it/s]\u001b[0m\u001b[A\n",
      "i: 100%|\u001b[32m██████████████████████████████████████████\u001b[0m| 7/7 [00:07<00:00,  1.09s/it]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing TRAINING DATASET COMPLETED!\n",
      "Preprocessing TESTING DATASET ...\n",
      "Preprocessing TESTING DATASET COMPLETED!\n"
     ]
    }
   ],
   "source": [
    "print('Creating date_block_num ...')\n",
    "N_submission = df_test.shape[0]\n",
    "N_sales      = df_train.shape[0]\n",
    "\n",
    "print(df_train.shape,df_test.shape)\n",
    "\n",
    "dates = (set(df_train['Z_WEEK'].unique()) | set(df_test['Z_WEEK'].unique()))#df_auxiliar['Z_WEEK'].unique()\n",
    "dates = sorted(dates)\n",
    "\n",
    "dict_dates = {}\n",
    "for idx,date in enumerate(dates):\n",
    "    dict_dates[date] =idx\n",
    "    \n",
    "    \n",
    "df_train['date_block_num'] = df_train['Z_WEEK'].map(dict_dates)\n",
    "df_test['date_block_num'] = df_test['Z_WEEK'].map(dict_dates)\n",
    "\n",
    "df_train.replace(['',np.inf, -np.inf, np.nan],0,inplace=True)\n",
    "df_test.replace(['',np.inf, -np.inf, np.nan],0,inplace=True)\n",
    "\n",
    "print('Creating date_block_num completed!')\n",
    "\n",
    "\n",
    "print('Preprocessing TRAINING DATASET ...')\n",
    "\n",
    "\n",
    "df_train['Z_WEEK_DATE'] = pd.to_datetime(df_train['Z_WEEK_DATE'])\n",
    "df_train['days_from_payday'] = df_train['Z_WEEK_DATE'].apply(get_distance_from_paydays)\n",
    "\n",
    "\n",
    "statistics_columns = [ ]\n",
    "\n",
    "df_train[\"log_Demanda\"] = np.log(df_train['Demanda'] + 1e-8)\n",
    "#df_test[\"log_Demanda\"] = np.log(1e-8)\n",
    "\n",
    "statistics_columns.append('log_Demanda')\n",
    "\n",
    "#'''\n",
    "bar1 = tqdm([\n",
    "    ['Z_MODELO'],\n",
    "    ['Z_PUNTO_VENTA'],\n",
    "    ['Z_GAMA'],\n",
    "    ['Z_MODELO','Z_PUNTO_VENTA'],\n",
    "    ['Z_MODELO','Z_GAMA'],\n",
    "    ['Z_PUNTO_VENTA','Z_GAMA'],\n",
    "    ['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA']], position=0, desc=\"i\",colour='green', ncols=80)\n",
    "time.sleep(1)\n",
    "\n",
    "bar2 = tqdm(['mean'], position=1, desc=\"j\", colour='red', ncols=80, leave=False) #'std','max','min','sum'\n",
    "time.sleep(1)\n",
    "\n",
    "unique_columns = [ ]\n",
    "        \n",
    "for column_names in bar1:\n",
    "    bar1.update()\n",
    "    bar2.refresh()  #force print final state\n",
    "    time.sleep(0.1)\n",
    "    bar2.reset()  #reuse bar\n",
    "    for statistic in bar2:\n",
    "        \n",
    "        new_column_name = statistic+'_sales_by_'+'_'.join(column_names)\n",
    "        #df_train[new_column_name] = df_train.groupby([\"Z_WEEK_DATE\"]+column_names, observed=True).Demanda.transform(statistic)\n",
    "        if statistic == 'mean':\n",
    "            df_agg = df_train.groupby([\"Z_WEEK\"]+column_names, observed=True).Demanda.mean()\n",
    "        if statistic == 'std':\n",
    "            df_agg = df_train.groupby([\"Z_WEEK\"]+column_names, observed=True).Demanda.std(ddof=0)\n",
    "        if statistic == 'max':\n",
    "            df_agg = df_train.groupby([\"Z_WEEK\"]+column_names, observed=True).Demanda.max()\n",
    "        if statistic == 'min':\n",
    "            df_agg = df_train.groupby([\"Z_WEEK\"]+column_names, observed=True).Demanda.min()\n",
    "        if statistic == 'sum':\n",
    "            df_agg = df_train.groupby([\"Z_WEEK\"]+column_names, observed=True).Demanda.sum()        \n",
    "        if df_agg.shape[0] >= df_train.shape[0]*0.7:\n",
    "            unique_columns.append([[\"Z_WEEK\"]+column_names,new_column_name])\n",
    "            continue\n",
    "        \n",
    "        df_agg = df_agg.reset_index()\n",
    "        df_agg.columns = df_agg.columns.str.replace('Demanda', new_column_name)\n",
    "        \n",
    "        df_train = df_train.merge(df_agg,on=[\"Z_WEEK\"]+column_names,how='left')\n",
    "        statistics_columns.append(new_column_name)\n",
    "        bar2.update()\n",
    "        time.sleep(0.05)\n",
    "#'''\n",
    "df_train['dayofweek'] = df_train['Z_WEEK_DATE'].dt.dayofweek.astype('str').astype('category')\n",
    "df_train['month'] = df_train['Z_WEEK_DATE'].dt.month.astype('str').astype('category')\n",
    "df_train['dayofyear'] = df_train['Z_WEEK_DATE'].dt.dayofyear.astype('str').astype('category')\n",
    "\n",
    "df_train.drop(columns=['Z_WEEK_DATE'],inplace=True)\n",
    "df_train.drop(columns=['Z_WEEK'],inplace=True)\n",
    "\n",
    "print('Preprocessing TRAINING DATASET COMPLETED!')\n",
    "print('Preprocessing TESTING DATASET ...')\n",
    "\n",
    "\n",
    "df_test['Z_WEEK_DATE'] = pd.to_datetime(df_test['Z_WEEK_DATE'])\n",
    "df_test['days_from_payday'] = df_test['Z_WEEK_DATE'].apply(get_distance_from_paydays)\n",
    "\n",
    "inv_dict_dates = {v: k for k, v in dict_dates.items()}\n",
    "#df_test['Z_WEEK'] = df_test['date_block_num'].map(inv_dict_dates)\n",
    "df_test = df_test[['date_block_num','Z_MODELO','Z_PUNTO_VENTA','Z_GAMA',\"Demanda\",\"Z_WEEK_DATE\"]]\n",
    "\n",
    "df_test['dayofweek'] = df_test['Z_WEEK_DATE'].dt.dayofweek.astype('str').astype('category')\n",
    "df_test['month'] = df_test['Z_WEEK_DATE'].dt.month.astype('str').astype('category')\n",
    "df_test['dayofyear'] = df_test['Z_WEEK_DATE'].dt.dayofyear.astype('str').astype('category')\n",
    "\n",
    "\n",
    "\n",
    "df_test['days_from_payday'] = df_test['Z_WEEK_DATE'].apply(get_distance_from_paydays)\n",
    "df_test.drop(columns=['Z_WEEK_DATE'],inplace=True)\n",
    "\n",
    "print('Preprocessing TESTING DATASET COMPLETED!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 182.24 MB\n",
      "Memory usage after optimization is: 101.26 MB\n",
      "Decreased by 44.4%\n"
     ]
    }
   ],
   "source": [
    "df_train = reduce_mem_usage(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 5.43 MB\n",
      "Memory usage after optimization is: 5.43 MB\n",
      "Decreased by 0.0%\n"
     ]
    }
   ],
   "source": [
    "df_test = reduce_mem_usage(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%timeit df_train.to_pickle('../../dataset/train/train_converted_fill_process.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%timeit df_test.to_pickle('../../dataset/test/test_converted_fill_process.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2358650, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Z_MODELO</th>\n",
       "      <th>Z_PUNTO_VENTA</th>\n",
       "      <th>Z_GAMA</th>\n",
       "      <th>Demanda</th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>days_from_payday</th>\n",
       "      <th>log_Demanda</th>\n",
       "      <th>mean_sales_by_Z_MODELO</th>\n",
       "      <th>mean_sales_by_Z_PUNTO_VENTA</th>\n",
       "      <th>mean_sales_by_Z_GAMA</th>\n",
       "      <th>mean_sales_by_Z_MODELO_Z_GAMA</th>\n",
       "      <th>mean_sales_by_Z_PUNTO_VENTA_Z_GAMA</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>month</th>\n",
       "      <th>dayofyear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>-18.421875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.757812</td>\n",
       "      <td>1.141602</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.359375</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>-18.421875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.703125</td>\n",
       "      <td>1.191406</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.218750</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Z_MODELO Z_PUNTO_VENTA Z_GAMA  Demanda date_block_num  days_from_payday  \\\n",
       "0    MOD_1       PVENT_1  GAM_1        0              0                14   \n",
       "1    MOD_1       PVENT_1  GAM_1        0              1                 7   \n",
       "\n",
       "   log_Demanda  mean_sales_by_Z_MODELO  mean_sales_by_Z_PUNTO_VENTA  \\\n",
       "0   -18.421875                     0.0                    10.757812   \n",
       "1   -18.421875                     0.0                    12.703125   \n",
       "\n",
       "   mean_sales_by_Z_GAMA  mean_sales_by_Z_MODELO_Z_GAMA  \\\n",
       "0              1.141602                            0.0   \n",
       "1              1.191406                            0.0   \n",
       "\n",
       "   mean_sales_by_Z_PUNTO_VENTA_Z_GAMA dayofweek month dayofyear  \n",
       "0                           21.359375         0     5       137  \n",
       "1                           26.218750         0     5       144  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "#print(list(df_train.columns))\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has\n",
      "                not been set for this class (SMAPE). The property determines if `update` by\n",
      "                default needs access to the full metric state. If this is not the case, significant speedups can be\n",
      "                achieved and we recommend setting this to `False`.\n",
      "                We provide an checking function\n",
      "                `from torchmetrics.utilities import check_forward_full_state_property`\n",
      "                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,\n",
      "                default for now) or if `full_state_update=False` can be used safely.\n",
      "                \n",
      "  warnings.warn(*args, **kwargs)\n",
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "import pytorch_forecasting\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer,EncoderNormalizer\n",
    "\n",
    "from pytorch_forecasting.metrics import SMAPE, PoissonLoss, QuantileLoss\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "# configure network and trainer\n",
    "pl.seed_everything(42)\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_steps = df_test['date_block_num'].nunique()\n",
    "prediction_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2358650 entries, 0 to 2358649\n",
      "Data columns (total 15 columns):\n",
      " #   Column                              Dtype   \n",
      "---  ------                              -----   \n",
      " 0   Z_MODELO                            category\n",
      " 1   Z_PUNTO_VENTA                       category\n",
      " 2   Z_GAMA                              category\n",
      " 3   Demanda                             int64   \n",
      " 4   date_block_num                      category\n",
      " 5   days_from_payday                    int64   \n",
      " 6   log_Demanda                         float16 \n",
      " 7   mean_sales_by_Z_MODELO              float16 \n",
      " 8   mean_sales_by_Z_PUNTO_VENTA         float16 \n",
      " 9   mean_sales_by_Z_GAMA                float16 \n",
      " 10  mean_sales_by_Z_MODELO_Z_GAMA       float16 \n",
      " 11  mean_sales_by_Z_PUNTO_VENTA_Z_GAMA  float16 \n",
      " 12  dayofweek                           category\n",
      " 13  month                               category\n",
      " 14  dayofyear                           category\n",
      "dtypes: category(7), float16(6), int64(2)\n",
      "memory usage: 101.3 MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['date_block_num'] = df_train['date_block_num'].astype(int)\n",
    "df_test['date_block_num'] = df_test['date_block_num'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Demanda'] = df_train['Demanda'].astype(np.float16)\n",
    "df_test['Demanda'] = df_test['Demanda'].astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z_MODELO train (0, 15)\n",
      "Z_MODELO test  (0, 9)\n",
      "Z_PUNTO_VENTA train (0, 15)\n",
      "Z_PUNTO_VENTA test  (0, 9)\n",
      "Z_GAMA train (0, 15)\n",
      "Z_GAMA test  (0, 9)\n",
      "Demanda train (1, 15)\n",
      "Demanda test  (0, 9)\n",
      "date_block_num train (0, 15)\n",
      "date_block_num test  (0, 9)\n",
      "days_from_payday train (0, 15)\n",
      "days_from_payday test  (0, 9)\n",
      "log_Demanda train (0, 15)\n",
      "mean_sales_by_Z_MODELO train (0, 15)\n",
      "mean_sales_by_Z_PUNTO_VENTA train (0, 15)\n",
      "mean_sales_by_Z_GAMA train (0, 15)\n",
      "mean_sales_by_Z_MODELO_Z_GAMA train (0, 15)\n",
      "mean_sales_by_Z_PUNTO_VENTA_Z_GAMA train (0, 15)\n",
      "dayofweek train (0, 15)\n",
      "dayofweek test  (0, 9)\n",
      "month train (0, 15)\n",
      "month test  (0, 9)\n",
      "dayofyear train (0, 15)\n",
      "dayofyear test  (0, 9)\n"
     ]
    }
   ],
   "source": [
    "for column in df_train.columns:\n",
    "    print(column,'train',df_train[df_train[column]==358].shape)\n",
    "    if column in df_test.columns:\n",
    "        print(column,'test ',df_test[df_test[column]==358].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Z_MODELO</th>\n",
       "      <th>Z_PUNTO_VENTA</th>\n",
       "      <th>Z_GAMA</th>\n",
       "      <th>Demanda</th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>days_from_payday</th>\n",
       "      <th>log_Demanda</th>\n",
       "      <th>mean_sales_by_Z_MODELO</th>\n",
       "      <th>mean_sales_by_Z_PUNTO_VENTA</th>\n",
       "      <th>mean_sales_by_Z_GAMA</th>\n",
       "      <th>mean_sales_by_Z_MODELO_Z_GAMA</th>\n",
       "      <th>mean_sales_by_Z_PUNTO_VENTA_Z_GAMA</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>month</th>\n",
       "      <th>dayofyear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>-18.421875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.757812</td>\n",
       "      <td>1.141602</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.359375</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>-18.421875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.703125</td>\n",
       "      <td>1.191406</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.218750</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Z_MODELO Z_PUNTO_VENTA Z_GAMA  Demanda  date_block_num  days_from_payday  \\\n",
       "0    MOD_1       PVENT_1  GAM_1      0.0               0                14   \n",
       "1    MOD_1       PVENT_1  GAM_1      0.0               1                 7   \n",
       "\n",
       "   log_Demanda  mean_sales_by_Z_MODELO  mean_sales_by_Z_PUNTO_VENTA  \\\n",
       "0   -18.421875                     0.0                    10.757812   \n",
       "1   -18.421875                     0.0                    12.703125   \n",
       "\n",
       "   mean_sales_by_Z_GAMA  mean_sales_by_Z_MODELO_Z_GAMA  \\\n",
       "0              1.141602                            0.0   \n",
       "1              1.191406                            0.0   \n",
       "\n",
       "   mean_sales_by_Z_PUNTO_VENTA_Z_GAMA dayofweek month dayofyear  \n",
       "0                           21.359375         0     5       137  \n",
       "1                           26.218750         0     5       144  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'day_of_week', 'day', 'is_month_end', 'day_of_year',  'is_quarter_start', 'year', 'month', 'is_year_start', 'is_month_start', \n",
    "# 'I103','S103', 'C101','I100' , 'C100', 'ID', 'I102','S102',, 'S101', 'S100', 'item_id', 'date_block_num', 'I101'\n",
    "max_prediction_length = prediction_steps\n",
    "\n",
    "max_encoder_length = 40\n",
    "\n",
    "training_cutoff = df_train['date_block_num'].max() - max_prediction_length\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    df_train[lambda x: x['date_block_num'] <= training_cutoff],\n",
    "    time_idx='date_block_num',\n",
    "    target=\"Demanda\",\n",
    "    group_ids=['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA'],\n",
    "    min_encoder_length= max_encoder_length // 2,   \n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "        \n",
    "    static_categoricals=['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA'],\n",
    "    \n",
    "    time_varying_known_categoricals=[\n",
    "                                     \"month\", \n",
    "                                     \"dayofweek\",\n",
    "                                     \"dayofyear\"],\n",
    "    \n",
    "    time_varying_known_reals=[\"date_block_num\",'days_from_payday'],\n",
    "    time_varying_unknown_categoricals=['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA'],  \n",
    "    time_varying_unknown_reals= statistics_columns+['date_block_num'],#'Demanda',  statistics_columns+['Demanda'],#'date_block_num'],\n",
    "       \n",
    "    #target_normalizer=GroupNormalizer(\n",
    "    #    groups=['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA'], transformation=\"softplus\"\n",
    "    #),  # use softplus and normalize by group    \n",
    "    \n",
    "    categorical_encoders={                          \n",
    "                          \"Z_GAMA\":  pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "                          \"Z_MODELO\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "                          \"Z_PUNTO_VENTA\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "                          \"dayofweek\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "                          \"month\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "                          \"dayofyear\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "                          \"date_block_num\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "                         },\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = TimeSeriesDataSet.from_dataset(training, df_train, predict=True, stop_randomization=True)\n",
    "\n",
    "batch_size = 128\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=3)\n",
    "\n",
    "val_dataloader   = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting.metrics import MultiHorizonMetric\n",
    "from typing import Dict, List, Tuple, Union\n",
    "from pytorch_forecasting.metrics import TweedieLoss,NegativeBinomialDistributionLoss,BetaDistributionLoss\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "'''\n",
    "def tweedieloss(y_true, y_pred):\n",
    "    p=1.5\n",
    "    a = K.pow(y_true, 2-p)/((1-p) * (2-p))\n",
    "    b = K.pow(y_pred, 1-p)/(1-p)\n",
    "    c = K.pow(y_pred, 2-p)/(2-p)\n",
    "    dev = 2 * (a -y_true *b  +c)\n",
    "    return K.mean(dev)\n",
    "'''\n",
    "class new_tweedieloss(MultiHorizonMetric):\n",
    "    def __init__(self, reduction=\"none\", **kwargs):\n",
    "        super().__init__(reduction=reduction, **kwargs)\n",
    "    def loss(self, y_pred: Dict[str, torch.Tensor], target):\n",
    "        p = 1.5#torch.tensor([1.01], dtype=torch.float16)\n",
    "        eps = 1e-10\n",
    "        \n",
    "        factor = 1\n",
    "        \n",
    "        if y_pred.sum() <= eps:\n",
    "            print(\"wtahsd!!\")\n",
    "            factor = 1e19\n",
    "            # y_pred = np.random.rand(len(y_pred))\n",
    "        else:\n",
    "            # y_pred = np.where(y_pred<0, eps, y_pred)  #Filter 0 and negative values \n",
    "            y_pred = torch.abs(y_pred)\n",
    "        #.requires_grad_(True)\n",
    "        preds = self.to_prediction(y_pred) + eps\n",
    "        #'''\n",
    "        \n",
    "        a = target*(torch.pow(preds,1-p))/(1-p)\n",
    "        b = torch.pow(preds,2-p)/(2-p)\n",
    "        tweddie = torch.mean((-a+b)/factor)\n",
    "        '''\n",
    "        a = torch.pow(target, 2-p)/((1-p) * (2-p))\n",
    "        b = torch.pow(preds, 1-p)/(1-p)\n",
    "        c = torch.pow(preds, 2-p)/(2-p)\n",
    "        tweddie = -2 * (a -target *b  +c)\n",
    "        '''\n",
    "        return tweddie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Quadro RTX 5000\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 40.2k\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer = pl.Trainer(gpus=1,gradient_clip_val=0.1)\n",
    "trainer.enforce_positive_output=True\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "\n",
    "    # not meaningful for finding the learning rate but otherwise very important\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=16,  # most important hyperparameter apart from learning rate\n",
    "    # number of attention heads. Set to up to 4 for large datasets\n",
    "    attention_head_size=1,\n",
    "    dropout=0.1,  # between 0.1 and 0.3 are good values\n",
    "    hidden_continuous_size=8,  # set to <= hidden_size\n",
    "    output_size=1,  # 7 quantiles by default\n",
    "    loss = new_tweedieloss(),#.to(device),    \n",
    "    # reduce learning rate if no improvement in validation loss after x epochs\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Finding best initial lr:  75%|███████▌  | 75/100 [00:13<00:04,  6.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wtahsd!!\n",
      "wtahsd!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding best initial lr: 100%|██████████| 100/100 [00:17<00:00,  5.47it/s]`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Finding best initial lr: 100%|██████████| 100/100 [00:17<00:00,  5.71it/s]\n",
      "Restoring states from the checkpoint path at /notebooks/entel-2022/DATATHON-ENTEL-2022---Reto2/notebooks/cristian/.lr_find_953a17e4-0eaf-4253-b63a-1905d672b371.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suggested learning rate: 5.248074602497725e-07\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# find optimal learning rate\n",
    "res = trainer.tuner.lr_find(\n",
    "    \n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    "    max_lr=0.1,\n",
    "    min_lr=1e-7,\n",
    ")\n",
    "\n",
    "print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "fig = res.plot(show=True, suggest=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.248074602497725e-07"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.suggestion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 39.8k\n"
     ]
    }
   ],
   "source": [
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate= res.suggestion(),\n",
    "    hidden_size=16,\n",
    "    attention_head_size=4,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    output_size=1, \n",
    "    loss = new_tweedieloss(),\n",
    "    log_interval=10,  \n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "holas\n"
     ]
    }
   ],
   "source": [
    "#Early Stopping \n",
    "MIN_DELTA  = 1e-7\n",
    "PATIENCE   = 30\n",
    "\n",
    "#PL Trainer\n",
    "MAX_EPOCHS = 5000\n",
    "\n",
    "GPUS = 1\n",
    "\n",
    "\n",
    "\n",
    "# configure network and trainer\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=MIN_DELTA, patience=PATIENCE, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor(logging_interval='epoch')  # log the learning rate\n",
    "logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='../../results/models/tft/',\n",
    "    filename='MODEL_tft-tweedie-loss-epoch_{epoch:02d}-val_loss_{val_loss:.3f}',\n",
    "    auto_insert_metric_name=False,\n",
    "    \n",
    " )\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    gpus=GPUS,\n",
    "    #weights_summary=\"top\",\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=30,#oment in for training, running valiation every 30 batches\n",
    "    #fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "    callbacks=[lr_logger, early_stop_callback,checkpoint_callback],\n",
    "    #log_every_n_steps=10,\n",
    "    logger=logger,\n",
    ")\n",
    "trainer.enforce_positive_output=True\n",
    "\n",
    "\n",
    "print('holas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | new_tweedieloss                 | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 13.5 K\n",
      "3  | prescalers                         | ModuleDict                      | 192   \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 2.3 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 8.0 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 2.2 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K \n",
      "11 | lstm_encoder                       | LSTM                            | 2.2 K \n",
      "12 | lstm_decoder                       | LSTM                            | 2.2 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 676   \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 576   \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 576   \n",
      "20 | output_layer                       | Linear                          | 17    \n",
      "----------------------------------------------------------------------------------------\n",
      "39.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "39.8 K    Total params\n",
      "0.159     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  45%|████▍     | 30/67 [00:05<00:06,  5.30it/s, loss=643, v_num=23, train_loss_step=912.0]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  46%|████▋     | 31/67 [00:07<00:08,  4.09it/s, loss=643, v_num=23, train_loss_step=912.0]\n",
      "Epoch 0:  48%|████▊     | 32/67 [00:07<00:08,  4.06it/s, loss=643, v_num=23, train_loss_step=912.0]\n",
      "Epoch 0:  49%|████▉     | 33/67 [00:08<00:08,  4.04it/s, loss=643, v_num=23, train_loss_step=912.0]\n",
      "Epoch 0:  51%|█████     | 34/67 [00:08<00:08,  4.02it/s, loss=643, v_num=23, train_loss_step=912.0]\n",
      "Epoch 0:  52%|█████▏    | 35/67 [00:08<00:08,  3.99it/s, loss=643, v_num=23, train_loss_step=912.0]\n",
      "Epoch 0:  54%|█████▎    | 36/67 [00:09<00:07,  3.98it/s, loss=643, v_num=23, train_loss_step=912.0]\n",
      "Epoch 0:  55%|█████▌    | 37/67 [00:09<00:07,  3.90it/s, loss=643, v_num=23, train_loss_step=912.0]\n",
      "Epoch 0:  57%|█████▋    | 38/67 [00:09<00:07,  3.88it/s, loss=643, v_num=23, train_loss_step=912.0]\n",
      "Epoch 0:  58%|█████▊    | 39/67 [00:10<00:07,  3.87it/s, loss=643, v_num=23, train_loss_step=912.0]\n",
      "Epoch 0:  60%|█████▉    | 40/67 [00:10<00:07,  3.78it/s, loss=643, v_num=23, train_loss_step=912.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 0:  61%|██████    | 41/67 [00:10<00:06,  3.73it/s, loss=643, v_num=23, train_loss_step=912.0]\n",
      "Epoch 0:  63%|██████▎   | 42/67 [00:11<00:06,  3.72it/s, loss=643, v_num=23, train_loss_step=912.0]\n",
      "Epoch 0:  64%|██████▍   | 43/67 [00:12<00:06,  3.57it/s, loss=643, v_num=23, train_loss_step=912.0]\n",
      "Epoch 0:  66%|██████▌   | 44/67 [00:12<00:06,  3.57it/s, loss=643, v_num=23, train_loss_step=912.0]\n",
      "Epoch 0:  67%|██████▋   | 45/67 [00:12<00:06,  3.57it/s, loss=643, v_num=23, train_loss_step=912.0]\n",
      "Epoch 0:  69%|██████▊   | 46/67 [00:13<00:05,  3.51it/s, loss=643, v_num=23, train_loss_step=912.0]\n",
      "Epoch 0:  70%|███████   | 47/67 [00:13<00:05,  3.50it/s, loss=643, v_num=23, train_loss_step=912.0]\n",
      "Epoch 0:  72%|███████▏  | 48/67 [00:13<00:05,  3.50it/s, loss=643, v_num=23, train_loss_step=912.0]\n",
      "Epoch 0:  73%|███████▎  | 49/67 [00:14<00:05,  3.44it/s, loss=643, v_num=23, train_loss_step=912.0]\n",
      "Epoch 0:  75%|███████▍  | 50/67 [00:14<00:04,  3.44it/s, loss=643, v_num=23, train_loss_step=912.0]\n",
      "Epoch 0:  76%|███████▌  | 51/67 [00:14<00:04,  3.41it/s, loss=643, v_num=23, train_loss_step=912.0]\n",
      "Epoch 0:  78%|███████▊  | 52/67 [00:15<00:04,  3.39it/s, loss=643, v_num=23, train_loss_step=912.0]\n",
      "Epoch 0:  79%|███████▉  | 53/67 [00:15<00:04,  3.39it/s, loss=643, v_num=23, train_loss_step=912.0]\n",
      "Epoch 0:  81%|████████  | 54/67 [00:15<00:03,  3.39it/s, loss=643, v_num=23, train_loss_step=912.0]\n",
      "Epoch 0:  82%|████████▏ | 55/67 [00:16<00:03,  3.35it/s, loss=643, v_num=23, train_loss_step=912.0]\n",
      "Epoch 0:  84%|████████▎ | 56/67 [00:16<00:03,  3.35it/s, loss=643, v_num=23, train_loss_step=912.0]\n",
      "Epoch 0:  85%|████████▌ | 57/67 [00:16<00:02,  3.36it/s, loss=643, v_num=23, train_loss_step=912.0]\n",
      "Epoch 0:  87%|████████▋ | 58/67 [00:17<00:02,  3.31it/s, loss=643, v_num=23, train_loss_step=912.0]\n",
      "Epoch 0:  88%|████████▊ | 59/67 [00:17<00:02,  3.31it/s, loss=643, v_num=23, train_loss_step=912.0]\n",
      "Epoch 0:  90%|████████▉ | 60/67 [00:18<00:02,  3.32it/s, loss=643, v_num=23, train_loss_step=912.0]\n",
      "Epoch 0:  91%|█████████ | 61/67 [00:18<00:01,  3.25it/s, loss=643, v_num=23, train_loss_step=912.0]\n",
      "Epoch 0:  93%|█████████▎| 62/67 [00:19<00:01,  3.25it/s, loss=643, v_num=23, train_loss_step=912.0]\n",
      "Epoch 0:  94%|█████████▍| 63/67 [00:19<00:01,  3.26it/s, loss=643, v_num=23, train_loss_step=912.0]\n",
      "Epoch 0:  96%|█████████▌| 64/67 [00:19<00:00,  3.21it/s, loss=643, v_num=23, train_loss_step=912.0]\n",
      "Epoch 0:  97%|█████████▋| 65/67 [00:20<00:00,  3.21it/s, loss=643, v_num=23, train_loss_step=912.0]\n",
      "Epoch 0:  99%|█████████▊| 66/67 [00:20<00:00,  3.22it/s, loss=643, v_num=23, train_loss_step=912.0]\n",
      "Epoch 0: 100%|██████████| 67/67 [00:21<00:00,  3.15it/s, loss=643, v_num=23, train_loss_step=912.0, val_loss=901.0]\n",
      "Epoch 1:  45%|████▍     | 30/67 [00:05<00:07,  5.13it/s, loss=994, v_num=23, train_loss_step=555.0, val_loss=901.0, train_loss_epoch=622.0]       \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  46%|████▋     | 31/67 [00:07<00:09,  3.99it/s, loss=994, v_num=23, train_loss_step=555.0, val_loss=901.0, train_loss_epoch=622.0]\n",
      "Epoch 1:  48%|████▊     | 32/67 [00:08<00:08,  3.97it/s, loss=994, v_num=23, train_loss_step=555.0, val_loss=901.0, train_loss_epoch=622.0]\n",
      "Epoch 1:  49%|████▉     | 33/67 [00:08<00:08,  3.78it/s, loss=994, v_num=23, train_loss_step=555.0, val_loss=901.0, train_loss_epoch=622.0]\n",
      "Epoch 1:  51%|█████     | 34/67 [00:09<00:08,  3.75it/s, loss=994, v_num=23, train_loss_step=555.0, val_loss=901.0, train_loss_epoch=622.0]\n",
      "Epoch 1:  52%|█████▏    | 35/67 [00:09<00:08,  3.75it/s, loss=994, v_num=23, train_loss_step=555.0, val_loss=901.0, train_loss_epoch=622.0]\n",
      "Epoch 1:  54%|█████▎    | 36/67 [00:09<00:08,  3.74it/s, loss=994, v_num=23, train_loss_step=555.0, val_loss=901.0, train_loss_epoch=622.0]\n",
      "Epoch 1:  55%|█████▌    | 37/67 [00:10<00:08,  3.65it/s, loss=994, v_num=23, train_loss_step=555.0, val_loss=901.0, train_loss_epoch=622.0]\n",
      "Epoch 1:  57%|█████▋    | 38/67 [00:10<00:07,  3.65it/s, loss=994, v_num=23, train_loss_step=555.0, val_loss=901.0, train_loss_epoch=622.0]\n",
      "Epoch 1:  58%|█████▊    | 39/67 [00:10<00:07,  3.64it/s, loss=994, v_num=23, train_loss_step=555.0, val_loss=901.0, train_loss_epoch=622.0]\n",
      "Epoch 1:  60%|█████▉    | 40/67 [00:11<00:07,  3.57it/s, loss=994, v_num=23, train_loss_step=555.0, val_loss=901.0, train_loss_epoch=622.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 1:  61%|██████    | 41/67 [00:11<00:07,  3.54it/s, loss=994, v_num=23, train_loss_step=555.0, val_loss=901.0, train_loss_epoch=622.0]\n",
      "Epoch 1:  63%|██████▎   | 42/67 [00:11<00:07,  3.53it/s, loss=994, v_num=23, train_loss_step=555.0, val_loss=901.0, train_loss_epoch=622.0]\n",
      "Epoch 1:  64%|██████▍   | 43/67 [00:12<00:06,  3.50it/s, loss=994, v_num=23, train_loss_step=555.0, val_loss=901.0, train_loss_epoch=622.0]\n",
      "Epoch 1:  66%|██████▌   | 44/67 [00:12<00:06,  3.50it/s, loss=994, v_num=23, train_loss_step=555.0, val_loss=901.0, train_loss_epoch=622.0]\n",
      "Epoch 1:  67%|██████▋   | 45/67 [00:12<00:06,  3.50it/s, loss=994, v_num=23, train_loss_step=555.0, val_loss=901.0, train_loss_epoch=622.0]\n",
      "Epoch 1:  69%|██████▊   | 46/67 [00:13<00:06,  3.44it/s, loss=994, v_num=23, train_loss_step=555.0, val_loss=901.0, train_loss_epoch=622.0]\n",
      "Epoch 1:  70%|███████   | 47/67 [00:13<00:05,  3.44it/s, loss=994, v_num=23, train_loss_step=555.0, val_loss=901.0, train_loss_epoch=622.0]\n",
      "Epoch 1:  72%|███████▏  | 48/67 [00:13<00:05,  3.44it/s, loss=994, v_num=23, train_loss_step=555.0, val_loss=901.0, train_loss_epoch=622.0]\n",
      "Epoch 1:  73%|███████▎  | 49/67 [00:14<00:05,  3.39it/s, loss=994, v_num=23, train_loss_step=555.0, val_loss=901.0, train_loss_epoch=622.0]\n",
      "Epoch 1:  75%|███████▍  | 50/67 [00:14<00:05,  3.39it/s, loss=994, v_num=23, train_loss_step=555.0, val_loss=901.0, train_loss_epoch=622.0]\n",
      "Epoch 1:  76%|███████▌  | 51/67 [00:15<00:04,  3.36it/s, loss=994, v_num=23, train_loss_step=555.0, val_loss=901.0, train_loss_epoch=622.0]\n",
      "Epoch 1:  78%|███████▊  | 52/67 [00:15<00:04,  3.34it/s, loss=994, v_num=23, train_loss_step=555.0, val_loss=901.0, train_loss_epoch=622.0]\n",
      "Epoch 1:  79%|███████▉  | 53/67 [00:15<00:04,  3.34it/s, loss=994, v_num=23, train_loss_step=555.0, val_loss=901.0, train_loss_epoch=622.0]\n",
      "Epoch 1:  81%|████████  | 54/67 [00:16<00:03,  3.35it/s, loss=994, v_num=23, train_loss_step=555.0, val_loss=901.0, train_loss_epoch=622.0]\n",
      "Epoch 1:  82%|████████▏ | 55/67 [00:16<00:03,  3.26it/s, loss=994, v_num=23, train_loss_step=555.0, val_loss=901.0, train_loss_epoch=622.0]\n",
      "Epoch 1:  84%|████████▎ | 56/67 [00:17<00:03,  3.26it/s, loss=994, v_num=23, train_loss_step=555.0, val_loss=901.0, train_loss_epoch=622.0]\n",
      "Epoch 1:  85%|████████▌ | 57/67 [00:17<00:03,  3.26it/s, loss=994, v_num=23, train_loss_step=555.0, val_loss=901.0, train_loss_epoch=622.0]\n",
      "Epoch 1:  87%|████████▋ | 58/67 [00:18<00:02,  3.22it/s, loss=994, v_num=23, train_loss_step=555.0, val_loss=901.0, train_loss_epoch=622.0]\n",
      "Epoch 1:  88%|████████▊ | 59/67 [00:18<00:02,  3.23it/s, loss=994, v_num=23, train_loss_step=555.0, val_loss=901.0, train_loss_epoch=622.0]\n",
      "Epoch 1:  90%|████████▉ | 60/67 [00:18<00:02,  3.23it/s, loss=994, v_num=23, train_loss_step=555.0, val_loss=901.0, train_loss_epoch=622.0]\n",
      "Epoch 1:  91%|█████████ | 61/67 [00:19<00:01,  3.12it/s, loss=994, v_num=23, train_loss_step=555.0, val_loss=901.0, train_loss_epoch=622.0]\n",
      "Epoch 1:  93%|█████████▎| 62/67 [00:19<00:01,  3.13it/s, loss=994, v_num=23, train_loss_step=555.0, val_loss=901.0, train_loss_epoch=622.0]\n",
      "Epoch 1:  94%|█████████▍| 63/67 [00:20<00:01,  3.13it/s, loss=994, v_num=23, train_loss_step=555.0, val_loss=901.0, train_loss_epoch=622.0]\n",
      "Epoch 1:  96%|█████████▌| 64/67 [00:20<00:00,  3.14it/s, loss=994, v_num=23, train_loss_step=555.0, val_loss=901.0, train_loss_epoch=622.0]\n",
      "Epoch 1:  97%|█████████▋| 65/67 [00:20<00:00,  3.14it/s, loss=994, v_num=23, train_loss_step=555.0, val_loss=901.0, train_loss_epoch=622.0]\n",
      "Epoch 1:  99%|█████████▊| 66/67 [00:20<00:00,  3.15it/s, loss=994, v_num=23, train_loss_step=555.0, val_loss=901.0, train_loss_epoch=622.0]\n",
      "Epoch 1: 100%|██████████| 67/67 [00:21<00:00,  3.09it/s, loss=994, v_num=23, train_loss_step=555.0, val_loss=901.0, train_loss_epoch=622.0]\n",
      "Epoch 2:  45%|████▍     | 30/67 [00:05<00:07,  5.08it/s, loss=906, v_num=23, train_loss_step=731.0, val_loss=901.0, train_loss_epoch=930.0]       \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  46%|████▋     | 31/67 [00:07<00:09,  3.92it/s, loss=906, v_num=23, train_loss_step=731.0, val_loss=901.0, train_loss_epoch=930.0]\n",
      "Epoch 2:  48%|████▊     | 32/67 [00:08<00:08,  3.90it/s, loss=906, v_num=23, train_loss_step=731.0, val_loss=901.0, train_loss_epoch=930.0]\n",
      "Epoch 2:  49%|████▉     | 33/67 [00:08<00:08,  3.88it/s, loss=906, v_num=23, train_loss_step=731.0, val_loss=901.0, train_loss_epoch=930.0]\n",
      "Epoch 2:  51%|█████     | 34/67 [00:08<00:08,  3.85it/s, loss=906, v_num=23, train_loss_step=731.0, val_loss=901.0, train_loss_epoch=930.0]\n",
      "Epoch 2:  52%|█████▏    | 35/67 [00:09<00:08,  3.85it/s, loss=906, v_num=23, train_loss_step=731.0, val_loss=901.0, train_loss_epoch=930.0]\n",
      "Epoch 2:  54%|█████▎    | 36/67 [00:09<00:08,  3.84it/s, loss=906, v_num=23, train_loss_step=731.0, val_loss=901.0, train_loss_epoch=930.0]\n",
      "Epoch 2:  55%|█████▌    | 37/67 [00:09<00:08,  3.74it/s, loss=906, v_num=23, train_loss_step=731.0, val_loss=901.0, train_loss_epoch=930.0]\n",
      "Epoch 2:  57%|█████▋    | 38/67 [00:10<00:07,  3.74it/s, loss=906, v_num=23, train_loss_step=731.0, val_loss=901.0, train_loss_epoch=930.0]\n",
      "Epoch 2:  58%|█████▊    | 39/67 [00:10<00:07,  3.73it/s, loss=906, v_num=23, train_loss_step=731.0, val_loss=901.0, train_loss_epoch=930.0]\n",
      "Epoch 2:  60%|█████▉    | 40/67 [00:11<00:07,  3.63it/s, loss=906, v_num=23, train_loss_step=731.0, val_loss=901.0, train_loss_epoch=930.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 2:  61%|██████    | 41/67 [00:11<00:07,  3.59it/s, loss=906, v_num=23, train_loss_step=731.0, val_loss=901.0, train_loss_epoch=930.0]\n",
      "Epoch 2:  63%|██████▎   | 42/67 [00:11<00:06,  3.59it/s, loss=906, v_num=23, train_loss_step=731.0, val_loss=901.0, train_loss_epoch=930.0]\n",
      "Epoch 2:  64%|██████▍   | 43/67 [00:12<00:06,  3.56it/s, loss=906, v_num=23, train_loss_step=731.0, val_loss=901.0, train_loss_epoch=930.0]\n",
      "Epoch 2:  66%|██████▌   | 44/67 [00:12<00:06,  3.56it/s, loss=906, v_num=23, train_loss_step=731.0, val_loss=901.0, train_loss_epoch=930.0]\n",
      "Epoch 2:  67%|██████▋   | 45/67 [00:12<00:06,  3.56it/s, loss=906, v_num=23, train_loss_step=731.0, val_loss=901.0, train_loss_epoch=930.0]\n",
      "Epoch 2:  69%|██████▊   | 46/67 [00:13<00:06,  3.40it/s, loss=906, v_num=23, train_loss_step=731.0, val_loss=901.0, train_loss_epoch=930.0]\n",
      "Epoch 2:  70%|███████   | 47/67 [00:13<00:05,  3.41it/s, loss=906, v_num=23, train_loss_step=731.0, val_loss=901.0, train_loss_epoch=930.0]\n",
      "Epoch 2:  72%|███████▏  | 48/67 [00:14<00:05,  3.41it/s, loss=906, v_num=23, train_loss_step=731.0, val_loss=901.0, train_loss_epoch=930.0]\n",
      "Epoch 2:  73%|███████▎  | 49/67 [00:14<00:05,  3.34it/s, loss=906, v_num=23, train_loss_step=731.0, val_loss=901.0, train_loss_epoch=930.0]\n",
      "Epoch 2:  75%|███████▍  | 50/67 [00:14<00:05,  3.35it/s, loss=906, v_num=23, train_loss_step=731.0, val_loss=901.0, train_loss_epoch=930.0]\n",
      "Epoch 2:  76%|███████▌  | 51/67 [00:15<00:04,  3.32it/s, loss=906, v_num=23, train_loss_step=731.0, val_loss=901.0, train_loss_epoch=930.0]\n",
      "Epoch 2:  78%|███████▊  | 52/67 [00:15<00:04,  3.30it/s, loss=906, v_num=23, train_loss_step=731.0, val_loss=901.0, train_loss_epoch=930.0]\n",
      "Epoch 2:  79%|███████▉  | 53/67 [00:16<00:04,  3.31it/s, loss=906, v_num=23, train_loss_step=731.0, val_loss=901.0, train_loss_epoch=930.0]\n",
      "Epoch 2:  81%|████████  | 54/67 [00:16<00:03,  3.31it/s, loss=906, v_num=23, train_loss_step=731.0, val_loss=901.0, train_loss_epoch=930.0]\n",
      "Epoch 2:  82%|████████▏ | 55/67 [00:16<00:03,  3.27it/s, loss=906, v_num=23, train_loss_step=731.0, val_loss=901.0, train_loss_epoch=930.0]\n",
      "Epoch 2:  84%|████████▎ | 56/67 [00:17<00:03,  3.28it/s, loss=906, v_num=23, train_loss_step=731.0, val_loss=901.0, train_loss_epoch=930.0]\n",
      "Epoch 2:  85%|████████▌ | 57/67 [00:17<00:03,  3.28it/s, loss=906, v_num=23, train_loss_step=731.0, val_loss=901.0, train_loss_epoch=930.0]\n",
      "Epoch 2:  87%|████████▋ | 58/67 [00:17<00:02,  3.24it/s, loss=906, v_num=23, train_loss_step=731.0, val_loss=901.0, train_loss_epoch=930.0]\n",
      "Epoch 2:  88%|████████▊ | 59/67 [00:18<00:02,  3.24it/s, loss=906, v_num=23, train_loss_step=731.0, val_loss=901.0, train_loss_epoch=930.0]\n",
      "Epoch 2:  90%|████████▉ | 60/67 [00:18<00:02,  3.25it/s, loss=906, v_num=23, train_loss_step=731.0, val_loss=901.0, train_loss_epoch=930.0]\n",
      "Epoch 2:  91%|█████████ | 61/67 [00:19<00:01,  3.19it/s, loss=906, v_num=23, train_loss_step=731.0, val_loss=901.0, train_loss_epoch=930.0]\n",
      "Epoch 2:  93%|█████████▎| 62/67 [00:19<00:01,  3.19it/s, loss=906, v_num=23, train_loss_step=731.0, val_loss=901.0, train_loss_epoch=930.0]\n",
      "Epoch 2:  94%|█████████▍| 63/67 [00:19<00:01,  3.20it/s, loss=906, v_num=23, train_loss_step=731.0, val_loss=901.0, train_loss_epoch=930.0]\n",
      "Epoch 2:  96%|█████████▌| 64/67 [00:20<00:00,  3.18it/s, loss=906, v_num=23, train_loss_step=731.0, val_loss=901.0, train_loss_epoch=930.0]\n",
      "Epoch 2:  97%|█████████▋| 65/67 [00:20<00:00,  3.18it/s, loss=906, v_num=23, train_loss_step=731.0, val_loss=901.0, train_loss_epoch=930.0]\n",
      "Epoch 2:  99%|█████████▊| 66/67 [00:20<00:00,  3.19it/s, loss=906, v_num=23, train_loss_step=731.0, val_loss=901.0, train_loss_epoch=930.0]\n",
      "Epoch 2: 100%|██████████| 67/67 [00:21<00:00,  3.13it/s, loss=906, v_num=23, train_loss_step=731.0, val_loss=901.0, train_loss_epoch=930.0]\n",
      "Epoch 3:  45%|████▍     | 30/67 [00:06<00:07,  4.89it/s, loss=774, v_num=23, train_loss_step=927.0, val_loss=901.0, train_loss_epoch=896.0]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  46%|████▋     | 31/67 [00:08<00:09,  3.82it/s, loss=774, v_num=23, train_loss_step=927.0, val_loss=901.0, train_loss_epoch=896.0]\n",
      "Epoch 3:  48%|████▊     | 32/67 [00:08<00:09,  3.80it/s, loss=774, v_num=23, train_loss_step=927.0, val_loss=901.0, train_loss_epoch=896.0]\n",
      "Epoch 3:  49%|████▉     | 33/67 [00:08<00:08,  3.78it/s, loss=774, v_num=23, train_loss_step=927.0, val_loss=901.0, train_loss_epoch=896.0]\n",
      "Epoch 3:  51%|█████     | 34/67 [00:09<00:08,  3.75it/s, loss=774, v_num=23, train_loss_step=927.0, val_loss=901.0, train_loss_epoch=896.0]\n",
      "Epoch 3:  52%|█████▏    | 35/67 [00:09<00:08,  3.75it/s, loss=774, v_num=23, train_loss_step=927.0, val_loss=901.0, train_loss_epoch=896.0]\n",
      "Epoch 3:  54%|█████▎    | 36/67 [00:09<00:08,  3.74it/s, loss=774, v_num=23, train_loss_step=927.0, val_loss=901.0, train_loss_epoch=896.0]\n",
      "Epoch 3:  55%|█████▌    | 37/67 [00:10<00:08,  3.55it/s, loss=774, v_num=23, train_loss_step=927.0, val_loss=901.0, train_loss_epoch=896.0]\n",
      "Epoch 3:  57%|█████▋    | 38/67 [00:10<00:08,  3.54it/s, loss=774, v_num=23, train_loss_step=927.0, val_loss=901.0, train_loss_epoch=896.0]\n",
      "Epoch 3:  58%|█████▊    | 39/67 [00:11<00:07,  3.55it/s, loss=774, v_num=23, train_loss_step=927.0, val_loss=901.0, train_loss_epoch=896.0]\n",
      "Epoch 3:  60%|█████▉    | 40/67 [00:11<00:07,  3.49it/s, loss=774, v_num=23, train_loss_step=927.0, val_loss=901.0, train_loss_epoch=896.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 3:  61%|██████    | 41/67 [00:11<00:07,  3.45it/s, loss=774, v_num=23, train_loss_step=927.0, val_loss=901.0, train_loss_epoch=896.0]\n",
      "Epoch 3:  63%|██████▎   | 42/67 [00:12<00:07,  3.45it/s, loss=774, v_num=23, train_loss_step=927.0, val_loss=901.0, train_loss_epoch=896.0]\n",
      "Epoch 3:  64%|██████▍   | 43/67 [00:12<00:06,  3.43it/s, loss=774, v_num=23, train_loss_step=927.0, val_loss=901.0, train_loss_epoch=896.0]\n",
      "Epoch 3:  66%|██████▌   | 44/67 [00:12<00:06,  3.43it/s, loss=774, v_num=23, train_loss_step=927.0, val_loss=901.0, train_loss_epoch=896.0]\n",
      "Epoch 3:  67%|██████▋   | 45/67 [00:13<00:06,  3.43it/s, loss=774, v_num=23, train_loss_step=927.0, val_loss=901.0, train_loss_epoch=896.0]\n",
      "Epoch 3:  69%|██████▊   | 46/67 [00:13<00:06,  3.39it/s, loss=774, v_num=23, train_loss_step=927.0, val_loss=901.0, train_loss_epoch=896.0]\n",
      "Epoch 3:  70%|███████   | 47/67 [00:13<00:05,  3.38it/s, loss=774, v_num=23, train_loss_step=927.0, val_loss=901.0, train_loss_epoch=896.0]\n",
      "Epoch 3:  72%|███████▏  | 48/67 [00:14<00:05,  3.39it/s, loss=774, v_num=23, train_loss_step=927.0, val_loss=901.0, train_loss_epoch=896.0]\n",
      "Epoch 3:  73%|███████▎  | 49/67 [00:14<00:05,  3.35it/s, loss=774, v_num=23, train_loss_step=927.0, val_loss=901.0, train_loss_epoch=896.0]\n",
      "Epoch 3:  75%|███████▍  | 50/67 [00:14<00:05,  3.35it/s, loss=774, v_num=23, train_loss_step=927.0, val_loss=901.0, train_loss_epoch=896.0]\n",
      "Epoch 3:  76%|███████▌  | 51/67 [00:15<00:04,  3.32it/s, loss=774, v_num=23, train_loss_step=927.0, val_loss=901.0, train_loss_epoch=896.0]\n",
      "Epoch 3:  78%|███████▊  | 52/67 [00:15<00:04,  3.32it/s, loss=774, v_num=23, train_loss_step=927.0, val_loss=901.0, train_loss_epoch=896.0]\n",
      "Epoch 3:  79%|███████▉  | 53/67 [00:15<00:04,  3.32it/s, loss=774, v_num=23, train_loss_step=927.0, val_loss=901.0, train_loss_epoch=896.0]\n",
      "Epoch 3:  81%|████████  | 54/67 [00:16<00:03,  3.32it/s, loss=774, v_num=23, train_loss_step=927.0, val_loss=901.0, train_loss_epoch=896.0]\n",
      "Epoch 3:  82%|████████▏ | 55/67 [00:16<00:03,  3.28it/s, loss=774, v_num=23, train_loss_step=927.0, val_loss=901.0, train_loss_epoch=896.0]\n",
      "Epoch 3:  84%|████████▎ | 56/67 [00:17<00:03,  3.28it/s, loss=774, v_num=23, train_loss_step=927.0, val_loss=901.0, train_loss_epoch=896.0]\n",
      "Epoch 3:  85%|████████▌ | 57/67 [00:17<00:03,  3.29it/s, loss=774, v_num=23, train_loss_step=927.0, val_loss=901.0, train_loss_epoch=896.0]\n",
      "Epoch 3:  87%|████████▋ | 58/67 [00:17<00:02,  3.24it/s, loss=774, v_num=23, train_loss_step=927.0, val_loss=901.0, train_loss_epoch=896.0]\n",
      "Epoch 3:  88%|████████▊ | 59/67 [00:18<00:02,  3.24it/s, loss=774, v_num=23, train_loss_step=927.0, val_loss=901.0, train_loss_epoch=896.0]\n",
      "Epoch 3:  90%|████████▉ | 60/67 [00:18<00:02,  3.25it/s, loss=774, v_num=23, train_loss_step=927.0, val_loss=901.0, train_loss_epoch=896.0]\n",
      "Epoch 3:  91%|█████████ | 61/67 [00:19<00:01,  3.20it/s, loss=774, v_num=23, train_loss_step=927.0, val_loss=901.0, train_loss_epoch=896.0]\n",
      "Epoch 3:  93%|█████████▎| 62/67 [00:19<00:01,  3.20it/s, loss=774, v_num=23, train_loss_step=927.0, val_loss=901.0, train_loss_epoch=896.0]\n",
      "Epoch 3:  94%|█████████▍| 63/67 [00:19<00:01,  3.21it/s, loss=774, v_num=23, train_loss_step=927.0, val_loss=901.0, train_loss_epoch=896.0]\n",
      "Epoch 3:  96%|█████████▌| 64/67 [00:20<00:00,  3.15it/s, loss=774, v_num=23, train_loss_step=927.0, val_loss=901.0, train_loss_epoch=896.0]\n",
      "Epoch 3:  97%|█████████▋| 65/67 [00:20<00:00,  3.16it/s, loss=774, v_num=23, train_loss_step=927.0, val_loss=901.0, train_loss_epoch=896.0]\n",
      "Epoch 3:  99%|█████████▊| 66/67 [00:20<00:00,  3.17it/s, loss=774, v_num=23, train_loss_step=927.0, val_loss=901.0, train_loss_epoch=896.0]\n",
      "Epoch 3: 100%|██████████| 67/67 [00:21<00:00,  3.06it/s, loss=774, v_num=23, train_loss_step=927.0, val_loss=901.0, train_loss_epoch=896.0]\n",
      "Epoch 4:  45%|████▍     | 30/67 [00:05<00:07,  5.10it/s, loss=973, v_num=23, train_loss_step=165.0, val_loss=901.0, train_loss_epoch=701.0]       \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  46%|████▋     | 31/67 [00:07<00:09,  3.95it/s, loss=973, v_num=23, train_loss_step=165.0, val_loss=901.0, train_loss_epoch=701.0]\n",
      "Epoch 4:  48%|████▊     | 32/67 [00:08<00:08,  3.93it/s, loss=973, v_num=23, train_loss_step=165.0, val_loss=901.0, train_loss_epoch=701.0]\n",
      "Epoch 4:  49%|████▉     | 33/67 [00:08<00:08,  3.91it/s, loss=973, v_num=23, train_loss_step=165.0, val_loss=901.0, train_loss_epoch=701.0]\n",
      "Epoch 4:  51%|█████     | 34/67 [00:08<00:08,  3.89it/s, loss=973, v_num=23, train_loss_step=165.0, val_loss=901.0, train_loss_epoch=701.0]\n",
      "Epoch 4:  52%|█████▏    | 35/67 [00:09<00:08,  3.86it/s, loss=973, v_num=23, train_loss_step=165.0, val_loss=901.0, train_loss_epoch=701.0]\n",
      "Epoch 4:  54%|█████▎    | 36/67 [00:09<00:08,  3.85it/s, loss=973, v_num=23, train_loss_step=165.0, val_loss=901.0, train_loss_epoch=701.0]\n",
      "Epoch 4:  55%|█████▌    | 37/67 [00:09<00:07,  3.77it/s, loss=973, v_num=23, train_loss_step=165.0, val_loss=901.0, train_loss_epoch=701.0]\n",
      "Epoch 4:  57%|█████▋    | 38/67 [00:10<00:07,  3.76it/s, loss=973, v_num=23, train_loss_step=165.0, val_loss=901.0, train_loss_epoch=701.0]\n",
      "Epoch 4:  58%|█████▊    | 39/67 [00:10<00:07,  3.76it/s, loss=973, v_num=23, train_loss_step=165.0, val_loss=901.0, train_loss_epoch=701.0]\n",
      "Epoch 4:  60%|█████▉    | 40/67 [00:10<00:07,  3.69it/s, loss=973, v_num=23, train_loss_step=165.0, val_loss=901.0, train_loss_epoch=701.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 4:  61%|██████    | 41/67 [00:11<00:07,  3.64it/s, loss=973, v_num=23, train_loss_step=165.0, val_loss=901.0, train_loss_epoch=701.0]\n",
      "Epoch 4:  63%|██████▎   | 42/67 [00:11<00:06,  3.64it/s, loss=973, v_num=23, train_loss_step=165.0, val_loss=901.0, train_loss_epoch=701.0]\n",
      "Epoch 4:  64%|██████▍   | 43/67 [00:11<00:06,  3.60it/s, loss=973, v_num=23, train_loss_step=165.0, val_loss=901.0, train_loss_epoch=701.0]\n",
      "Epoch 4:  66%|██████▌   | 44/67 [00:12<00:06,  3.60it/s, loss=973, v_num=23, train_loss_step=165.0, val_loss=901.0, train_loss_epoch=701.0]\n",
      "Epoch 4:  67%|██████▋   | 45/67 [00:12<00:06,  3.60it/s, loss=973, v_num=23, train_loss_step=165.0, val_loss=901.0, train_loss_epoch=701.0]\n",
      "Epoch 4:  69%|██████▊   | 46/67 [00:12<00:05,  3.54it/s, loss=973, v_num=23, train_loss_step=165.0, val_loss=901.0, train_loss_epoch=701.0]\n",
      "Epoch 4:  70%|███████   | 47/67 [00:13<00:05,  3.54it/s, loss=973, v_num=23, train_loss_step=165.0, val_loss=901.0, train_loss_epoch=701.0]\n",
      "Epoch 4:  72%|███████▏  | 48/67 [00:13<00:05,  3.54it/s, loss=973, v_num=23, train_loss_step=165.0, val_loss=901.0, train_loss_epoch=701.0]\n",
      "Epoch 4:  73%|███████▎  | 49/67 [00:14<00:05,  3.49it/s, loss=973, v_num=23, train_loss_step=165.0, val_loss=901.0, train_loss_epoch=701.0]\n",
      "Epoch 4:  75%|███████▍  | 50/67 [00:14<00:04,  3.49it/s, loss=973, v_num=23, train_loss_step=165.0, val_loss=901.0, train_loss_epoch=701.0]\n",
      "Epoch 4:  76%|███████▌  | 51/67 [00:14<00:04,  3.45it/s, loss=973, v_num=23, train_loss_step=165.0, val_loss=901.0, train_loss_epoch=701.0]\n",
      "Epoch 4:  78%|███████▊  | 52/67 [00:15<00:04,  3.44it/s, loss=973, v_num=23, train_loss_step=165.0, val_loss=901.0, train_loss_epoch=701.0]\n",
      "Epoch 4:  79%|███████▉  | 53/67 [00:15<00:04,  3.44it/s, loss=973, v_num=23, train_loss_step=165.0, val_loss=901.0, train_loss_epoch=701.0]\n",
      "Epoch 4:  81%|████████  | 54/67 [00:16<00:03,  3.35it/s, loss=973, v_num=23, train_loss_step=165.0, val_loss=901.0, train_loss_epoch=701.0]\n",
      "Epoch 4:  82%|████████▏ | 55/67 [00:16<00:03,  3.31it/s, loss=973, v_num=23, train_loss_step=165.0, val_loss=901.0, train_loss_epoch=701.0]\n",
      "Epoch 4:  84%|████████▎ | 56/67 [00:16<00:03,  3.31it/s, loss=973, v_num=23, train_loss_step=165.0, val_loss=901.0, train_loss_epoch=701.0]\n",
      "Epoch 4:  85%|████████▌ | 57/67 [00:17<00:03,  3.31it/s, loss=973, v_num=23, train_loss_step=165.0, val_loss=901.0, train_loss_epoch=701.0]\n",
      "Epoch 4:  87%|████████▋ | 58/67 [00:17<00:02,  3.28it/s, loss=973, v_num=23, train_loss_step=165.0, val_loss=901.0, train_loss_epoch=701.0]\n",
      "Epoch 4:  88%|████████▊ | 59/67 [00:17<00:02,  3.28it/s, loss=973, v_num=23, train_loss_step=165.0, val_loss=901.0, train_loss_epoch=701.0]\n",
      "Epoch 4:  90%|████████▉ | 60/67 [00:18<00:02,  3.28it/s, loss=973, v_num=23, train_loss_step=165.0, val_loss=901.0, train_loss_epoch=701.0]\n",
      "Epoch 4:  91%|█████████ | 61/67 [00:18<00:01,  3.23it/s, loss=973, v_num=23, train_loss_step=165.0, val_loss=901.0, train_loss_epoch=701.0]\n",
      "Epoch 4:  93%|█████████▎| 62/67 [00:19<00:01,  3.23it/s, loss=973, v_num=23, train_loss_step=165.0, val_loss=901.0, train_loss_epoch=701.0]\n",
      "Epoch 4:  94%|█████████▍| 63/67 [00:19<00:01,  3.23it/s, loss=973, v_num=23, train_loss_step=165.0, val_loss=901.0, train_loss_epoch=701.0]\n",
      "Epoch 4:  96%|█████████▌| 64/67 [00:19<00:00,  3.23it/s, loss=973, v_num=23, train_loss_step=165.0, val_loss=901.0, train_loss_epoch=701.0]\n",
      "Epoch 4:  97%|█████████▋| 65/67 [00:20<00:00,  3.23it/s, loss=973, v_num=23, train_loss_step=165.0, val_loss=901.0, train_loss_epoch=701.0]\n",
      "Epoch 4:  99%|█████████▊| 66/67 [00:20<00:00,  3.24it/s, loss=973, v_num=23, train_loss_step=165.0, val_loss=901.0, train_loss_epoch=701.0]\n",
      "Epoch 4: 100%|██████████| 67/67 [00:21<00:00,  3.17it/s, loss=973, v_num=23, train_loss_step=165.0, val_loss=901.0, train_loss_epoch=701.0]\n",
      "Epoch 5:  45%|████▍     | 30/67 [00:05<00:07,  5.12it/s, loss=737, v_num=23, train_loss_step=560.0, val_loss=901.0, train_loss_epoch=866.0]     \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5:  46%|████▋     | 31/67 [00:07<00:09,  3.95it/s, loss=737, v_num=23, train_loss_step=560.0, val_loss=901.0, train_loss_epoch=866.0]\n",
      "Epoch 5:  48%|████▊     | 32/67 [00:08<00:08,  3.92it/s, loss=737, v_num=23, train_loss_step=560.0, val_loss=901.0, train_loss_epoch=866.0]\n",
      "Epoch 5:  49%|████▉     | 33/67 [00:08<00:08,  3.91it/s, loss=737, v_num=23, train_loss_step=560.0, val_loss=901.0, train_loss_epoch=866.0]\n",
      "Epoch 5:  51%|█████     | 34/67 [00:08<00:08,  3.89it/s, loss=737, v_num=23, train_loss_step=560.0, val_loss=901.0, train_loss_epoch=866.0]\n",
      "Epoch 5:  52%|█████▏    | 35/67 [00:09<00:08,  3.87it/s, loss=737, v_num=23, train_loss_step=560.0, val_loss=901.0, train_loss_epoch=866.0]\n",
      "Epoch 5:  54%|█████▎    | 36/67 [00:09<00:08,  3.85it/s, loss=737, v_num=23, train_loss_step=560.0, val_loss=901.0, train_loss_epoch=866.0]\n",
      "Epoch 5:  55%|█████▌    | 37/67 [00:09<00:07,  3.76it/s, loss=737, v_num=23, train_loss_step=560.0, val_loss=901.0, train_loss_epoch=866.0]\n",
      "Epoch 5:  57%|█████▋    | 38/67 [00:10<00:07,  3.75it/s, loss=737, v_num=23, train_loss_step=560.0, val_loss=901.0, train_loss_epoch=866.0]\n",
      "Epoch 5:  58%|█████▊    | 39/67 [00:10<00:07,  3.75it/s, loss=737, v_num=23, train_loss_step=560.0, val_loss=901.0, train_loss_epoch=866.0]\n",
      "Epoch 5:  60%|█████▉    | 40/67 [00:10<00:07,  3.66it/s, loss=737, v_num=23, train_loss_step=560.0, val_loss=901.0, train_loss_epoch=866.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 5:  61%|██████    | 41/67 [00:11<00:07,  3.62it/s, loss=737, v_num=23, train_loss_step=560.0, val_loss=901.0, train_loss_epoch=866.0]\n",
      "Epoch 5:  63%|██████▎   | 42/67 [00:11<00:06,  3.61it/s, loss=737, v_num=23, train_loss_step=560.0, val_loss=901.0, train_loss_epoch=866.0]\n",
      "Epoch 5:  64%|██████▍   | 43/67 [00:11<00:06,  3.59it/s, loss=737, v_num=23, train_loss_step=560.0, val_loss=901.0, train_loss_epoch=866.0]\n",
      "Epoch 5:  66%|██████▌   | 44/67 [00:12<00:06,  3.59it/s, loss=737, v_num=23, train_loss_step=560.0, val_loss=901.0, train_loss_epoch=866.0]\n",
      "Epoch 5:  67%|██████▋   | 45/67 [00:12<00:06,  3.58it/s, loss=737, v_num=23, train_loss_step=560.0, val_loss=901.0, train_loss_epoch=866.0]\n",
      "Epoch 5:  69%|██████▊   | 46/67 [00:13<00:06,  3.41it/s, loss=737, v_num=23, train_loss_step=560.0, val_loss=901.0, train_loss_epoch=866.0]\n",
      "Epoch 5:  70%|███████   | 47/67 [00:13<00:05,  3.41it/s, loss=737, v_num=23, train_loss_step=560.0, val_loss=901.0, train_loss_epoch=866.0]\n",
      "Epoch 5:  72%|███████▏  | 48/67 [00:14<00:05,  3.41it/s, loss=737, v_num=23, train_loss_step=560.0, val_loss=901.0, train_loss_epoch=866.0]\n",
      "Epoch 5:  73%|███████▎  | 49/67 [00:14<00:05,  3.36it/s, loss=737, v_num=23, train_loss_step=560.0, val_loss=901.0, train_loss_epoch=866.0]\n",
      "Epoch 5:  75%|███████▍  | 50/67 [00:14<00:05,  3.36it/s, loss=737, v_num=23, train_loss_step=560.0, val_loss=901.0, train_loss_epoch=866.0]\n",
      "Epoch 5:  76%|███████▌  | 51/67 [00:15<00:04,  3.33it/s, loss=737, v_num=23, train_loss_step=560.0, val_loss=901.0, train_loss_epoch=866.0]\n",
      "Epoch 5:  78%|███████▊  | 52/67 [00:15<00:04,  3.30it/s, loss=737, v_num=23, train_loss_step=560.0, val_loss=901.0, train_loss_epoch=866.0]\n",
      "Epoch 5:  79%|███████▉  | 53/67 [00:16<00:04,  3.30it/s, loss=737, v_num=23, train_loss_step=560.0, val_loss=901.0, train_loss_epoch=866.0]\n",
      "Epoch 5:  81%|████████  | 54/67 [00:16<00:03,  3.31it/s, loss=737, v_num=23, train_loss_step=560.0, val_loss=901.0, train_loss_epoch=866.0]\n",
      "Epoch 5:  82%|████████▏ | 55/67 [00:16<00:03,  3.25it/s, loss=737, v_num=23, train_loss_step=560.0, val_loss=901.0, train_loss_epoch=866.0]\n",
      "Epoch 5:  84%|████████▎ | 56/67 [00:17<00:03,  3.25it/s, loss=737, v_num=23, train_loss_step=560.0, val_loss=901.0, train_loss_epoch=866.0]\n",
      "Epoch 5:  85%|████████▌ | 57/67 [00:17<00:03,  3.26it/s, loss=737, v_num=23, train_loss_step=560.0, val_loss=901.0, train_loss_epoch=866.0]\n",
      "Epoch 5:  87%|████████▋ | 58/67 [00:18<00:02,  3.22it/s, loss=737, v_num=23, train_loss_step=560.0, val_loss=901.0, train_loss_epoch=866.0]\n",
      "Epoch 5:  88%|████████▊ | 59/67 [00:18<00:02,  3.23it/s, loss=737, v_num=23, train_loss_step=560.0, val_loss=901.0, train_loss_epoch=866.0]\n",
      "Epoch 5:  90%|████████▉ | 60/67 [00:18<00:02,  3.23it/s, loss=737, v_num=23, train_loss_step=560.0, val_loss=901.0, train_loss_epoch=866.0]\n",
      "Epoch 5:  91%|█████████ | 61/67 [00:19<00:01,  3.18it/s, loss=737, v_num=23, train_loss_step=560.0, val_loss=901.0, train_loss_epoch=866.0]\n",
      "Epoch 5:  93%|█████████▎| 62/67 [00:19<00:01,  3.18it/s, loss=737, v_num=23, train_loss_step=560.0, val_loss=901.0, train_loss_epoch=866.0]\n",
      "Epoch 5:  94%|█████████▍| 63/67 [00:19<00:01,  3.19it/s, loss=737, v_num=23, train_loss_step=560.0, val_loss=901.0, train_loss_epoch=866.0]\n",
      "Epoch 5:  96%|█████████▌| 64/67 [00:20<00:00,  3.18it/s, loss=737, v_num=23, train_loss_step=560.0, val_loss=901.0, train_loss_epoch=866.0]\n",
      "Epoch 5:  97%|█████████▋| 65/67 [00:20<00:00,  3.18it/s, loss=737, v_num=23, train_loss_step=560.0, val_loss=901.0, train_loss_epoch=866.0]\n",
      "Epoch 5:  99%|█████████▊| 66/67 [00:20<00:00,  3.19it/s, loss=737, v_num=23, train_loss_step=560.0, val_loss=901.0, train_loss_epoch=866.0]\n",
      "Epoch 5: 100%|██████████| 67/67 [00:21<00:00,  3.12it/s, loss=737, v_num=23, train_loss_step=560.0, val_loss=901.0, train_loss_epoch=866.0]\n",
      "Epoch 6:  45%|████▍     | 30/67 [00:05<00:07,  5.05it/s, loss=845, v_num=23, train_loss_step=971.0, val_loss=901.0, train_loss_epoch=709.0]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6:  46%|████▋     | 31/67 [00:07<00:09,  3.89it/s, loss=845, v_num=23, train_loss_step=971.0, val_loss=901.0, train_loss_epoch=709.0]\n",
      "Epoch 6:  48%|████▊     | 32/67 [00:08<00:09,  3.87it/s, loss=845, v_num=23, train_loss_step=971.0, val_loss=901.0, train_loss_epoch=709.0]\n",
      "Epoch 6:  49%|████▉     | 33/67 [00:08<00:08,  3.85it/s, loss=845, v_num=23, train_loss_step=971.0, val_loss=901.0, train_loss_epoch=709.0]\n",
      "Epoch 6:  51%|█████     | 34/67 [00:08<00:08,  3.83it/s, loss=845, v_num=23, train_loss_step=971.0, val_loss=901.0, train_loss_epoch=709.0]\n",
      "Epoch 6:  52%|█████▏    | 35/67 [00:09<00:08,  3.82it/s, loss=845, v_num=23, train_loss_step=971.0, val_loss=901.0, train_loss_epoch=709.0]\n",
      "Epoch 6:  54%|█████▎    | 36/67 [00:09<00:08,  3.81it/s, loss=845, v_num=23, train_loss_step=971.0, val_loss=901.0, train_loss_epoch=709.0]\n",
      "Epoch 6:  55%|█████▌    | 37/67 [00:10<00:08,  3.54it/s, loss=845, v_num=23, train_loss_step=971.0, val_loss=901.0, train_loss_epoch=709.0]\n",
      "Epoch 6:  57%|█████▋    | 38/67 [00:10<00:08,  3.54it/s, loss=845, v_num=23, train_loss_step=971.0, val_loss=901.0, train_loss_epoch=709.0]\n",
      "Epoch 6:  58%|█████▊    | 39/67 [00:10<00:07,  3.55it/s, loss=845, v_num=23, train_loss_step=971.0, val_loss=901.0, train_loss_epoch=709.0]\n",
      "Epoch 6:  60%|█████▉    | 40/67 [00:11<00:07,  3.47it/s, loss=845, v_num=23, train_loss_step=971.0, val_loss=901.0, train_loss_epoch=709.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 6:  61%|██████    | 41/67 [00:11<00:07,  3.44it/s, loss=845, v_num=23, train_loss_step=971.0, val_loss=901.0, train_loss_epoch=709.0]\n",
      "Epoch 6:  63%|██████▎   | 42/67 [00:12<00:07,  3.44it/s, loss=845, v_num=23, train_loss_step=971.0, val_loss=901.0, train_loss_epoch=709.0]\n",
      "Epoch 6:  64%|██████▍   | 43/67 [00:12<00:07,  3.41it/s, loss=845, v_num=23, train_loss_step=971.0, val_loss=901.0, train_loss_epoch=709.0]\n",
      "Epoch 6:  66%|██████▌   | 44/67 [00:12<00:06,  3.41it/s, loss=845, v_num=23, train_loss_step=971.0, val_loss=901.0, train_loss_epoch=709.0]\n",
      "Epoch 6:  67%|██████▋   | 45/67 [00:13<00:06,  3.42it/s, loss=845, v_num=23, train_loss_step=971.0, val_loss=901.0, train_loss_epoch=709.0]\n",
      "Epoch 6:  69%|██████▊   | 46/67 [00:13<00:06,  3.36it/s, loss=845, v_num=23, train_loss_step=971.0, val_loss=901.0, train_loss_epoch=709.0]\n",
      "Epoch 6:  70%|███████   | 47/67 [00:13<00:05,  3.36it/s, loss=845, v_num=23, train_loss_step=971.0, val_loss=901.0, train_loss_epoch=709.0]\n",
      "Epoch 6:  72%|███████▏  | 48/67 [00:14<00:05,  3.36it/s, loss=845, v_num=23, train_loss_step=971.0, val_loss=901.0, train_loss_epoch=709.0]\n",
      "Epoch 6:  73%|███████▎  | 49/67 [00:14<00:05,  3.32it/s, loss=845, v_num=23, train_loss_step=971.0, val_loss=901.0, train_loss_epoch=709.0]\n",
      "Epoch 6:  75%|███████▍  | 50/67 [00:15<00:05,  3.32it/s, loss=845, v_num=23, train_loss_step=971.0, val_loss=901.0, train_loss_epoch=709.0]\n",
      "Epoch 6:  76%|███████▌  | 51/67 [00:15<00:04,  3.30it/s, loss=845, v_num=23, train_loss_step=971.0, val_loss=901.0, train_loss_epoch=709.0]\n",
      "Epoch 6:  78%|███████▊  | 52/67 [00:15<00:04,  3.28it/s, loss=845, v_num=23, train_loss_step=971.0, val_loss=901.0, train_loss_epoch=709.0]\n",
      "Epoch 6:  79%|███████▉  | 53/67 [00:16<00:04,  3.28it/s, loss=845, v_num=23, train_loss_step=971.0, val_loss=901.0, train_loss_epoch=709.0]\n",
      "Epoch 6:  81%|████████  | 54/67 [00:16<00:03,  3.29it/s, loss=845, v_num=23, train_loss_step=971.0, val_loss=901.0, train_loss_epoch=709.0]\n",
      "Epoch 6:  82%|████████▏ | 55/67 [00:16<00:03,  3.25it/s, loss=845, v_num=23, train_loss_step=971.0, val_loss=901.0, train_loss_epoch=709.0]\n",
      "Epoch 6:  84%|████████▎ | 56/67 [00:17<00:03,  3.25it/s, loss=845, v_num=23, train_loss_step=971.0, val_loss=901.0, train_loss_epoch=709.0]\n",
      "Epoch 6:  85%|████████▌ | 57/67 [00:17<00:03,  3.26it/s, loss=845, v_num=23, train_loss_step=971.0, val_loss=901.0, train_loss_epoch=709.0]\n",
      "Epoch 6:  87%|████████▋ | 58/67 [00:18<00:02,  3.22it/s, loss=845, v_num=23, train_loss_step=971.0, val_loss=901.0, train_loss_epoch=709.0]\n",
      "Epoch 6:  88%|████████▊ | 59/67 [00:18<00:02,  3.23it/s, loss=845, v_num=23, train_loss_step=971.0, val_loss=901.0, train_loss_epoch=709.0]\n",
      "Epoch 6:  90%|████████▉ | 60/67 [00:18<00:02,  3.23it/s, loss=845, v_num=23, train_loss_step=971.0, val_loss=901.0, train_loss_epoch=709.0]\n",
      "Epoch 6:  91%|█████████ | 61/67 [00:19<00:01,  3.17it/s, loss=845, v_num=23, train_loss_step=971.0, val_loss=901.0, train_loss_epoch=709.0]\n",
      "Epoch 6:  93%|█████████▎| 62/67 [00:19<00:01,  3.18it/s, loss=845, v_num=23, train_loss_step=971.0, val_loss=901.0, train_loss_epoch=709.0]\n",
      "Epoch 6:  94%|█████████▍| 63/67 [00:19<00:01,  3.18it/s, loss=845, v_num=23, train_loss_step=971.0, val_loss=901.0, train_loss_epoch=709.0]\n",
      "Epoch 6:  96%|█████████▌| 64/67 [00:20<00:00,  3.16it/s, loss=845, v_num=23, train_loss_step=971.0, val_loss=901.0, train_loss_epoch=709.0]\n",
      "Epoch 6:  97%|█████████▋| 65/67 [00:20<00:00,  3.16it/s, loss=845, v_num=23, train_loss_step=971.0, val_loss=901.0, train_loss_epoch=709.0]\n",
      "Epoch 6:  99%|█████████▊| 66/67 [00:20<00:00,  3.17it/s, loss=845, v_num=23, train_loss_step=971.0, val_loss=901.0, train_loss_epoch=709.0]\n",
      "Epoch 6: 100%|██████████| 67/67 [00:22<00:00,  3.04it/s, loss=845, v_num=23, train_loss_step=971.0, val_loss=901.0, train_loss_epoch=709.0]\n",
      "Epoch 7:  45%|████▍     | 30/67 [00:06<00:07,  4.99it/s, loss=782, v_num=23, train_loss_step=623.0, val_loss=901.0, train_loss_epoch=730.0]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7:  46%|████▋     | 31/67 [00:08<00:09,  3.87it/s, loss=782, v_num=23, train_loss_step=623.0, val_loss=901.0, train_loss_epoch=730.0]\n",
      "Epoch 7:  48%|████▊     | 32/67 [00:08<00:09,  3.86it/s, loss=782, v_num=23, train_loss_step=623.0, val_loss=901.0, train_loss_epoch=730.0]\n",
      "Epoch 7:  49%|████▉     | 33/67 [00:08<00:08,  3.84it/s, loss=782, v_num=23, train_loss_step=623.0, val_loss=901.0, train_loss_epoch=730.0]\n",
      "Epoch 7:  51%|█████     | 34/67 [00:08<00:08,  3.82it/s, loss=782, v_num=23, train_loss_step=623.0, val_loss=901.0, train_loss_epoch=730.0]\n",
      "Epoch 7:  52%|█████▏    | 35/67 [00:09<00:08,  3.81it/s, loss=782, v_num=23, train_loss_step=623.0, val_loss=901.0, train_loss_epoch=730.0]\n",
      "Epoch 7:  54%|█████▎    | 36/67 [00:09<00:08,  3.80it/s, loss=782, v_num=23, train_loss_step=623.0, val_loss=901.0, train_loss_epoch=730.0]\n",
      "Epoch 7:  55%|█████▌    | 37/67 [00:09<00:08,  3.75it/s, loss=782, v_num=23, train_loss_step=623.0, val_loss=901.0, train_loss_epoch=730.0]\n",
      "Epoch 7:  57%|█████▋    | 38/67 [00:10<00:07,  3.74it/s, loss=782, v_num=23, train_loss_step=623.0, val_loss=901.0, train_loss_epoch=730.0]\n",
      "Epoch 7:  58%|█████▊    | 39/67 [00:10<00:07,  3.73it/s, loss=782, v_num=23, train_loss_step=623.0, val_loss=901.0, train_loss_epoch=730.0]\n",
      "Epoch 7:  60%|█████▉    | 40/67 [00:10<00:07,  3.65it/s, loss=782, v_num=23, train_loss_step=623.0, val_loss=901.0, train_loss_epoch=730.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 7:  61%|██████    | 41/67 [00:11<00:07,  3.60it/s, loss=782, v_num=23, train_loss_step=623.0, val_loss=901.0, train_loss_epoch=730.0]\n",
      "Epoch 7:  63%|██████▎   | 42/67 [00:11<00:06,  3.59it/s, loss=782, v_num=23, train_loss_step=623.0, val_loss=901.0, train_loss_epoch=730.0]\n",
      "Epoch 7:  64%|██████▍   | 43/67 [00:12<00:06,  3.57it/s, loss=782, v_num=23, train_loss_step=623.0, val_loss=901.0, train_loss_epoch=730.0]\n",
      "Epoch 7:  66%|██████▌   | 44/67 [00:12<00:06,  3.57it/s, loss=782, v_num=23, train_loss_step=623.0, val_loss=901.0, train_loss_epoch=730.0]\n",
      "Epoch 7:  67%|██████▋   | 45/67 [00:12<00:06,  3.57it/s, loss=782, v_num=23, train_loss_step=623.0, val_loss=901.0, train_loss_epoch=730.0]\n",
      "Epoch 7:  69%|██████▊   | 46/67 [00:13<00:05,  3.51it/s, loss=782, v_num=23, train_loss_step=623.0, val_loss=901.0, train_loss_epoch=730.0]\n",
      "Epoch 7:  70%|███████   | 47/67 [00:13<00:05,  3.51it/s, loss=782, v_num=23, train_loss_step=623.0, val_loss=901.0, train_loss_epoch=730.0]\n",
      "Epoch 7:  72%|███████▏  | 48/67 [00:13<00:05,  3.50it/s, loss=782, v_num=23, train_loss_step=623.0, val_loss=901.0, train_loss_epoch=730.0]\n",
      "Epoch 7:  73%|███████▎  | 49/67 [00:14<00:05,  3.46it/s, loss=782, v_num=23, train_loss_step=623.0, val_loss=901.0, train_loss_epoch=730.0]\n",
      "Epoch 7:  75%|███████▍  | 50/67 [00:14<00:04,  3.46it/s, loss=782, v_num=23, train_loss_step=623.0, val_loss=901.0, train_loss_epoch=730.0]\n",
      "Epoch 7:  76%|███████▌  | 51/67 [00:14<00:04,  3.43it/s, loss=782, v_num=23, train_loss_step=623.0, val_loss=901.0, train_loss_epoch=730.0]\n",
      "Epoch 7:  78%|███████▊  | 52/67 [00:15<00:04,  3.42it/s, loss=782, v_num=23, train_loss_step=623.0, val_loss=901.0, train_loss_epoch=730.0]\n",
      "Epoch 7:  79%|███████▉  | 53/67 [00:15<00:04,  3.42it/s, loss=782, v_num=23, train_loss_step=623.0, val_loss=901.0, train_loss_epoch=730.0]\n",
      "Epoch 7:  81%|████████  | 54/67 [00:15<00:03,  3.42it/s, loss=782, v_num=23, train_loss_step=623.0, val_loss=901.0, train_loss_epoch=730.0]\n",
      "Epoch 7:  82%|████████▏ | 55/67 [00:16<00:03,  3.38it/s, loss=782, v_num=23, train_loss_step=623.0, val_loss=901.0, train_loss_epoch=730.0]\n",
      "Epoch 7:  84%|████████▎ | 56/67 [00:16<00:03,  3.38it/s, loss=782, v_num=23, train_loss_step=623.0, val_loss=901.0, train_loss_epoch=730.0]\n",
      "Epoch 7:  85%|████████▌ | 57/67 [00:16<00:02,  3.38it/s, loss=782, v_num=23, train_loss_step=623.0, val_loss=901.0, train_loss_epoch=730.0]\n",
      "Epoch 7:  87%|████████▋ | 58/67 [00:17<00:02,  3.35it/s, loss=782, v_num=23, train_loss_step=623.0, val_loss=901.0, train_loss_epoch=730.0]\n",
      "Epoch 7:  88%|████████▊ | 59/67 [00:17<00:02,  3.35it/s, loss=782, v_num=23, train_loss_step=623.0, val_loss=901.0, train_loss_epoch=730.0]\n",
      "Epoch 7:  90%|████████▉ | 60/67 [00:17<00:02,  3.35it/s, loss=782, v_num=23, train_loss_step=623.0, val_loss=901.0, train_loss_epoch=730.0]\n",
      "Epoch 7:  91%|█████████ | 61/67 [00:19<00:01,  3.20it/s, loss=782, v_num=23, train_loss_step=623.0, val_loss=901.0, train_loss_epoch=730.0]\n",
      "Epoch 7:  93%|█████████▎| 62/67 [00:19<00:01,  3.21it/s, loss=782, v_num=23, train_loss_step=623.0, val_loss=901.0, train_loss_epoch=730.0]\n",
      "Epoch 7:  94%|█████████▍| 63/67 [00:19<00:01,  3.21it/s, loss=782, v_num=23, train_loss_step=623.0, val_loss=901.0, train_loss_epoch=730.0]\n",
      "Epoch 7:  96%|█████████▌| 64/67 [00:19<00:00,  3.20it/s, loss=782, v_num=23, train_loss_step=623.0, val_loss=901.0, train_loss_epoch=730.0]\n",
      "Epoch 7:  97%|█████████▋| 65/67 [00:20<00:00,  3.21it/s, loss=782, v_num=23, train_loss_step=623.0, val_loss=901.0, train_loss_epoch=730.0]\n",
      "Epoch 7:  99%|█████████▊| 66/67 [00:20<00:00,  3.21it/s, loss=782, v_num=23, train_loss_step=623.0, val_loss=901.0, train_loss_epoch=730.0]\n",
      "Epoch 7: 100%|██████████| 67/67 [00:21<00:00,  3.15it/s, loss=782, v_num=23, train_loss_step=623.0, val_loss=901.0, train_loss_epoch=730.0]\n",
      "Epoch 8:  45%|████▍     | 30/67 [00:05<00:07,  5.09it/s, loss=853, v_num=23, train_loss_step=3.2e+3, val_loss=901.0, train_loss_epoch=795.0] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8:  46%|████▋     | 31/67 [00:07<00:09,  3.93it/s, loss=853, v_num=23, train_loss_step=3.2e+3, val_loss=901.0, train_loss_epoch=795.0]\n",
      "Epoch 8:  48%|████▊     | 32/67 [00:08<00:08,  3.91it/s, loss=853, v_num=23, train_loss_step=3.2e+3, val_loss=901.0, train_loss_epoch=795.0]\n",
      "Epoch 8:  49%|████▉     | 33/67 [00:08<00:08,  3.89it/s, loss=853, v_num=23, train_loss_step=3.2e+3, val_loss=901.0, train_loss_epoch=795.0]\n",
      "Epoch 8:  51%|█████     | 34/67 [00:08<00:08,  3.86it/s, loss=853, v_num=23, train_loss_step=3.2e+3, val_loss=901.0, train_loss_epoch=795.0]\n",
      "Epoch 8:  52%|█████▏    | 35/67 [00:09<00:08,  3.85it/s, loss=853, v_num=23, train_loss_step=3.2e+3, val_loss=901.0, train_loss_epoch=795.0]\n",
      "Epoch 8:  54%|█████▎    | 36/67 [00:09<00:08,  3.84it/s, loss=853, v_num=23, train_loss_step=3.2e+3, val_loss=901.0, train_loss_epoch=795.0]\n",
      "Epoch 8:  55%|█████▌    | 37/67 [00:09<00:07,  3.77it/s, loss=853, v_num=23, train_loss_step=3.2e+3, val_loss=901.0, train_loss_epoch=795.0]\n",
      "Epoch 8:  57%|█████▋    | 38/67 [00:10<00:07,  3.77it/s, loss=853, v_num=23, train_loss_step=3.2e+3, val_loss=901.0, train_loss_epoch=795.0]\n",
      "Epoch 8:  58%|█████▊    | 39/67 [00:10<00:07,  3.76it/s, loss=853, v_num=23, train_loss_step=3.2e+3, val_loss=901.0, train_loss_epoch=795.0]\n",
      "Epoch 8:  60%|█████▉    | 40/67 [00:10<00:07,  3.69it/s, loss=853, v_num=23, train_loss_step=3.2e+3, val_loss=901.0, train_loss_epoch=795.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 8:  61%|██████    | 41/67 [00:11<00:07,  3.64it/s, loss=853, v_num=23, train_loss_step=3.2e+3, val_loss=901.0, train_loss_epoch=795.0]\n",
      "Epoch 8:  63%|██████▎   | 42/67 [00:11<00:06,  3.64it/s, loss=853, v_num=23, train_loss_step=3.2e+3, val_loss=901.0, train_loss_epoch=795.0]\n",
      "Epoch 8:  64%|██████▍   | 43/67 [00:11<00:06,  3.61it/s, loss=853, v_num=23, train_loss_step=3.2e+3, val_loss=901.0, train_loss_epoch=795.0]\n",
      "Epoch 8:  66%|██████▌   | 44/67 [00:12<00:06,  3.60it/s, loss=853, v_num=23, train_loss_step=3.2e+3, val_loss=901.0, train_loss_epoch=795.0]\n",
      "Epoch 8:  67%|██████▋   | 45/67 [00:12<00:06,  3.60it/s, loss=853, v_num=23, train_loss_step=3.2e+3, val_loss=901.0, train_loss_epoch=795.0]\n",
      "Epoch 8:  69%|██████▊   | 46/67 [00:12<00:05,  3.54it/s, loss=853, v_num=23, train_loss_step=3.2e+3, val_loss=901.0, train_loss_epoch=795.0]\n",
      "Epoch 8:  70%|███████   | 47/67 [00:13<00:05,  3.54it/s, loss=853, v_num=23, train_loss_step=3.2e+3, val_loss=901.0, train_loss_epoch=795.0]\n",
      "Epoch 8:  72%|███████▏  | 48/67 [00:13<00:05,  3.54it/s, loss=853, v_num=23, train_loss_step=3.2e+3, val_loss=901.0, train_loss_epoch=795.0]\n",
      "Epoch 8:  73%|███████▎  | 49/67 [00:14<00:05,  3.49it/s, loss=853, v_num=23, train_loss_step=3.2e+3, val_loss=901.0, train_loss_epoch=795.0]\n",
      "Epoch 8:  75%|███████▍  | 50/67 [00:14<00:04,  3.49it/s, loss=853, v_num=23, train_loss_step=3.2e+3, val_loss=901.0, train_loss_epoch=795.0]\n",
      "Epoch 8:  76%|███████▌  | 51/67 [00:14<00:04,  3.46it/s, loss=853, v_num=23, train_loss_step=3.2e+3, val_loss=901.0, train_loss_epoch=795.0]\n",
      "Epoch 8:  78%|███████▊  | 52/67 [00:15<00:04,  3.33it/s, loss=853, v_num=23, train_loss_step=3.2e+3, val_loss=901.0, train_loss_epoch=795.0]\n",
      "Epoch 8:  79%|███████▉  | 53/67 [00:15<00:04,  3.33it/s, loss=853, v_num=23, train_loss_step=3.2e+3, val_loss=901.0, train_loss_epoch=795.0]\n",
      "Epoch 8:  81%|████████  | 54/67 [00:16<00:03,  3.34it/s, loss=853, v_num=23, train_loss_step=3.2e+3, val_loss=901.0, train_loss_epoch=795.0]\n",
      "Epoch 8:  82%|████████▏ | 55/67 [00:16<00:03,  3.30it/s, loss=853, v_num=23, train_loss_step=3.2e+3, val_loss=901.0, train_loss_epoch=795.0]\n",
      "Epoch 8:  84%|████████▎ | 56/67 [00:16<00:03,  3.30it/s, loss=853, v_num=23, train_loss_step=3.2e+3, val_loss=901.0, train_loss_epoch=795.0]\n",
      "Epoch 8:  85%|████████▌ | 57/67 [00:17<00:03,  3.31it/s, loss=853, v_num=23, train_loss_step=3.2e+3, val_loss=901.0, train_loss_epoch=795.0]\n",
      "Epoch 8:  87%|████████▋ | 58/67 [00:17<00:02,  3.27it/s, loss=853, v_num=23, train_loss_step=3.2e+3, val_loss=901.0, train_loss_epoch=795.0]\n",
      "Epoch 8:  88%|████████▊ | 59/67 [00:18<00:02,  3.27it/s, loss=853, v_num=23, train_loss_step=3.2e+3, val_loss=901.0, train_loss_epoch=795.0]\n",
      "Epoch 8:  90%|████████▉ | 60/67 [00:18<00:02,  3.28it/s, loss=853, v_num=23, train_loss_step=3.2e+3, val_loss=901.0, train_loss_epoch=795.0]\n",
      "Epoch 8:  91%|█████████ | 61/67 [00:18<00:01,  3.23it/s, loss=853, v_num=23, train_loss_step=3.2e+3, val_loss=901.0, train_loss_epoch=795.0]\n",
      "Epoch 8:  93%|█████████▎| 62/67 [00:19<00:01,  3.23it/s, loss=853, v_num=23, train_loss_step=3.2e+3, val_loss=901.0, train_loss_epoch=795.0]\n",
      "Epoch 8:  94%|█████████▍| 63/67 [00:19<00:01,  3.23it/s, loss=853, v_num=23, train_loss_step=3.2e+3, val_loss=901.0, train_loss_epoch=795.0]\n",
      "Epoch 8:  96%|█████████▌| 64/67 [00:19<00:00,  3.23it/s, loss=853, v_num=23, train_loss_step=3.2e+3, val_loss=901.0, train_loss_epoch=795.0]\n",
      "Epoch 8:  97%|█████████▋| 65/67 [00:20<00:00,  3.23it/s, loss=853, v_num=23, train_loss_step=3.2e+3, val_loss=901.0, train_loss_epoch=795.0]\n",
      "Epoch 8:  99%|█████████▊| 66/67 [00:20<00:00,  3.24it/s, loss=853, v_num=23, train_loss_step=3.2e+3, val_loss=901.0, train_loss_epoch=795.0]\n",
      "Epoch 8: 100%|██████████| 67/67 [00:21<00:00,  3.17it/s, loss=853, v_num=23, train_loss_step=3.2e+3, val_loss=901.0, train_loss_epoch=795.0]\n",
      "Epoch 9:  45%|████▍     | 30/67 [00:05<00:07,  5.07it/s, loss=1.07e+03, v_num=23, train_loss_step=392.0, val_loss=901.0, train_loss_epoch=904.0]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9:  46%|████▋     | 31/67 [00:07<00:09,  3.91it/s, loss=1.07e+03, v_num=23, train_loss_step=392.0, val_loss=901.0, train_loss_epoch=904.0]\n",
      "Epoch 9:  48%|████▊     | 32/67 [00:08<00:08,  3.89it/s, loss=1.07e+03, v_num=23, train_loss_step=392.0, val_loss=901.0, train_loss_epoch=904.0]\n",
      "Epoch 9:  49%|████▉     | 33/67 [00:08<00:08,  3.88it/s, loss=1.07e+03, v_num=23, train_loss_step=392.0, val_loss=901.0, train_loss_epoch=904.0]\n",
      "Epoch 9:  51%|█████     | 34/67 [00:08<00:08,  3.86it/s, loss=1.07e+03, v_num=23, train_loss_step=392.0, val_loss=901.0, train_loss_epoch=904.0]\n",
      "Epoch 9:  52%|█████▏    | 35/67 [00:09<00:08,  3.84it/s, loss=1.07e+03, v_num=23, train_loss_step=392.0, val_loss=901.0, train_loss_epoch=904.0]\n",
      "Epoch 9:  54%|█████▎    | 36/67 [00:09<00:08,  3.83it/s, loss=1.07e+03, v_num=23, train_loss_step=392.0, val_loss=901.0, train_loss_epoch=904.0]\n",
      "Epoch 9:  55%|█████▌    | 37/67 [00:09<00:07,  3.77it/s, loss=1.07e+03, v_num=23, train_loss_step=392.0, val_loss=901.0, train_loss_epoch=904.0]\n",
      "Epoch 9:  57%|█████▋    | 38/67 [00:10<00:07,  3.75it/s, loss=1.07e+03, v_num=23, train_loss_step=392.0, val_loss=901.0, train_loss_epoch=904.0]\n",
      "Epoch 9:  58%|█████▊    | 39/67 [00:10<00:07,  3.75it/s, loss=1.07e+03, v_num=23, train_loss_step=392.0, val_loss=901.0, train_loss_epoch=904.0]\n",
      "Epoch 9:  60%|█████▉    | 40/67 [00:10<00:07,  3.67it/s, loss=1.07e+03, v_num=23, train_loss_step=392.0, val_loss=901.0, train_loss_epoch=904.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 9:  61%|██████    | 41/67 [00:11<00:07,  3.62it/s, loss=1.07e+03, v_num=23, train_loss_step=392.0, val_loss=901.0, train_loss_epoch=904.0]\n",
      "Epoch 9:  63%|██████▎   | 42/67 [00:11<00:06,  3.62it/s, loss=1.07e+03, v_num=23, train_loss_step=392.0, val_loss=901.0, train_loss_epoch=904.0]\n",
      "Epoch 9:  64%|██████▍   | 43/67 [00:12<00:06,  3.43it/s, loss=1.07e+03, v_num=23, train_loss_step=392.0, val_loss=901.0, train_loss_epoch=904.0]\n",
      "Epoch 9:  66%|██████▌   | 44/67 [00:12<00:06,  3.42it/s, loss=1.07e+03, v_num=23, train_loss_step=392.0, val_loss=901.0, train_loss_epoch=904.0]\n",
      "Epoch 9:  67%|██████▋   | 45/67 [00:13<00:06,  3.43it/s, loss=1.07e+03, v_num=23, train_loss_step=392.0, val_loss=901.0, train_loss_epoch=904.0]\n",
      "Epoch 9:  69%|██████▊   | 46/67 [00:13<00:06,  3.37it/s, loss=1.07e+03, v_num=23, train_loss_step=392.0, val_loss=901.0, train_loss_epoch=904.0]\n",
      "Epoch 9:  70%|███████   | 47/67 [00:13<00:05,  3.37it/s, loss=1.07e+03, v_num=23, train_loss_step=392.0, val_loss=901.0, train_loss_epoch=904.0]\n",
      "Epoch 9:  72%|███████▏  | 48/67 [00:14<00:05,  3.37it/s, loss=1.07e+03, v_num=23, train_loss_step=392.0, val_loss=901.0, train_loss_epoch=904.0]\n",
      "Epoch 9:  73%|███████▎  | 49/67 [00:14<00:05,  3.31it/s, loss=1.07e+03, v_num=23, train_loss_step=392.0, val_loss=901.0, train_loss_epoch=904.0]\n",
      "Epoch 9:  75%|███████▍  | 50/67 [00:15<00:05,  3.31it/s, loss=1.07e+03, v_num=23, train_loss_step=392.0, val_loss=901.0, train_loss_epoch=904.0]\n",
      "Epoch 9:  76%|███████▌  | 51/67 [00:15<00:04,  3.29it/s, loss=1.07e+03, v_num=23, train_loss_step=392.0, val_loss=901.0, train_loss_epoch=904.0]\n",
      "Epoch 9:  78%|███████▊  | 52/67 [00:15<00:04,  3.26it/s, loss=1.07e+03, v_num=23, train_loss_step=392.0, val_loss=901.0, train_loss_epoch=904.0]\n",
      "Epoch 9:  79%|███████▉  | 53/67 [00:16<00:04,  3.27it/s, loss=1.07e+03, v_num=23, train_loss_step=392.0, val_loss=901.0, train_loss_epoch=904.0]\n",
      "Epoch 9:  81%|████████  | 54/67 [00:16<00:03,  3.27it/s, loss=1.07e+03, v_num=23, train_loss_step=392.0, val_loss=901.0, train_loss_epoch=904.0]\n",
      "Epoch 9:  82%|████████▏ | 55/67 [00:17<00:03,  3.24it/s, loss=1.07e+03, v_num=23, train_loss_step=392.0, val_loss=901.0, train_loss_epoch=904.0]\n",
      "Epoch 9:  84%|████████▎ | 56/67 [00:17<00:03,  3.24it/s, loss=1.07e+03, v_num=23, train_loss_step=392.0, val_loss=901.0, train_loss_epoch=904.0]\n",
      "Epoch 9:  85%|████████▌ | 57/67 [00:17<00:03,  3.25it/s, loss=1.07e+03, v_num=23, train_loss_step=392.0, val_loss=901.0, train_loss_epoch=904.0]\n",
      "Epoch 9:  87%|████████▋ | 58/67 [00:18<00:02,  3.20it/s, loss=1.07e+03, v_num=23, train_loss_step=392.0, val_loss=901.0, train_loss_epoch=904.0]\n",
      "Epoch 9:  88%|████████▊ | 59/67 [00:18<00:02,  3.21it/s, loss=1.07e+03, v_num=23, train_loss_step=392.0, val_loss=901.0, train_loss_epoch=904.0]\n",
      "Epoch 9:  90%|████████▉ | 60/67 [00:18<00:02,  3.21it/s, loss=1.07e+03, v_num=23, train_loss_step=392.0, val_loss=901.0, train_loss_epoch=904.0]\n",
      "Epoch 9:  91%|█████████ | 61/67 [00:19<00:01,  3.16it/s, loss=1.07e+03, v_num=23, train_loss_step=392.0, val_loss=901.0, train_loss_epoch=904.0]\n",
      "Epoch 9:  93%|█████████▎| 62/67 [00:19<00:01,  3.17it/s, loss=1.07e+03, v_num=23, train_loss_step=392.0, val_loss=901.0, train_loss_epoch=904.0]\n",
      "Epoch 9:  94%|█████████▍| 63/67 [00:19<00:01,  3.18it/s, loss=1.07e+03, v_num=23, train_loss_step=392.0, val_loss=901.0, train_loss_epoch=904.0]\n",
      "Epoch 9:  96%|█████████▌| 64/67 [00:20<00:00,  3.16it/s, loss=1.07e+03, v_num=23, train_loss_step=392.0, val_loss=901.0, train_loss_epoch=904.0]\n",
      "Epoch 9:  97%|█████████▋| 65/67 [00:20<00:00,  3.17it/s, loss=1.07e+03, v_num=23, train_loss_step=392.0, val_loss=901.0, train_loss_epoch=904.0]\n",
      "Epoch 9:  99%|█████████▊| 66/67 [00:20<00:00,  3.17it/s, loss=1.07e+03, v_num=23, train_loss_step=392.0, val_loss=901.0, train_loss_epoch=904.0]\n",
      "Epoch 9: 100%|██████████| 67/67 [00:21<00:00,  3.11it/s, loss=1.07e+03, v_num=23, train_loss_step=392.0, val_loss=901.0, train_loss_epoch=904.0]\n",
      "Epoch 10:  45%|████▍     | 30/67 [00:05<00:07,  5.06it/s, loss=703, v_num=23, train_loss_step=844.0, val_loss=901.0, train_loss_epoch=1.11e+3]      \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10:  46%|████▋     | 31/67 [00:07<00:09,  3.89it/s, loss=703, v_num=23, train_loss_step=844.0, val_loss=901.0, train_loss_epoch=1.11e+3]\n",
      "Epoch 10:  48%|████▊     | 32/67 [00:08<00:09,  3.88it/s, loss=703, v_num=23, train_loss_step=844.0, val_loss=901.0, train_loss_epoch=1.11e+3]\n",
      "Epoch 10:  49%|████▉     | 33/67 [00:08<00:08,  3.87it/s, loss=703, v_num=23, train_loss_step=844.0, val_loss=901.0, train_loss_epoch=1.11e+3]\n",
      "Epoch 10:  51%|█████     | 34/67 [00:09<00:09,  3.62it/s, loss=703, v_num=23, train_loss_step=844.0, val_loss=901.0, train_loss_epoch=1.11e+3]\n",
      "Epoch 10:  52%|█████▏    | 35/67 [00:09<00:08,  3.62it/s, loss=703, v_num=23, train_loss_step=844.0, val_loss=901.0, train_loss_epoch=1.11e+3]\n",
      "Epoch 10:  54%|█████▎    | 36/67 [00:09<00:08,  3.62it/s, loss=703, v_num=23, train_loss_step=844.0, val_loss=901.0, train_loss_epoch=1.11e+3]\n",
      "Epoch 10:  55%|█████▌    | 37/67 [00:10<00:08,  3.54it/s, loss=703, v_num=23, train_loss_step=844.0, val_loss=901.0, train_loss_epoch=1.11e+3]\n",
      "Epoch 10:  57%|█████▋    | 38/67 [00:10<00:08,  3.54it/s, loss=703, v_num=23, train_loss_step=844.0, val_loss=901.0, train_loss_epoch=1.11e+3]\n",
      "Epoch 10:  58%|█████▊    | 39/67 [00:11<00:07,  3.54it/s, loss=703, v_num=23, train_loss_step=844.0, val_loss=901.0, train_loss_epoch=1.11e+3]\n",
      "Epoch 10:  60%|█████▉    | 40/67 [00:11<00:07,  3.47it/s, loss=703, v_num=23, train_loss_step=844.0, val_loss=901.0, train_loss_epoch=1.11e+3]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 10:  61%|██████    | 41/67 [00:11<00:07,  3.43it/s, loss=703, v_num=23, train_loss_step=844.0, val_loss=901.0, train_loss_epoch=1.11e+3]\n",
      "Epoch 10:  63%|██████▎   | 42/67 [00:12<00:07,  3.44it/s, loss=703, v_num=23, train_loss_step=844.0, val_loss=901.0, train_loss_epoch=1.11e+3]\n",
      "Epoch 10:  64%|██████▍   | 43/67 [00:12<00:07,  3.41it/s, loss=703, v_num=23, train_loss_step=844.0, val_loss=901.0, train_loss_epoch=1.11e+3]\n",
      "Epoch 10:  66%|██████▌   | 44/67 [00:12<00:06,  3.41it/s, loss=703, v_num=23, train_loss_step=844.0, val_loss=901.0, train_loss_epoch=1.11e+3]\n",
      "Epoch 10:  67%|██████▋   | 45/67 [00:13<00:06,  3.42it/s, loss=703, v_num=23, train_loss_step=844.0, val_loss=901.0, train_loss_epoch=1.11e+3]\n",
      "Epoch 10:  69%|██████▊   | 46/67 [00:13<00:06,  3.36it/s, loss=703, v_num=23, train_loss_step=844.0, val_loss=901.0, train_loss_epoch=1.11e+3]\n",
      "Epoch 10:  70%|███████   | 47/67 [00:13<00:05,  3.37it/s, loss=703, v_num=23, train_loss_step=844.0, val_loss=901.0, train_loss_epoch=1.11e+3]\n",
      "Epoch 10:  72%|███████▏  | 48/67 [00:14<00:05,  3.37it/s, loss=703, v_num=23, train_loss_step=844.0, val_loss=901.0, train_loss_epoch=1.11e+3]\n",
      "Epoch 10:  73%|███████▎  | 49/67 [00:14<00:05,  3.32it/s, loss=703, v_num=23, train_loss_step=844.0, val_loss=901.0, train_loss_epoch=1.11e+3]\n",
      "Epoch 10:  75%|███████▍  | 50/67 [00:15<00:05,  3.32it/s, loss=703, v_num=23, train_loss_step=844.0, val_loss=901.0, train_loss_epoch=1.11e+3]\n",
      "Epoch 10:  76%|███████▌  | 51/67 [00:15<00:04,  3.30it/s, loss=703, v_num=23, train_loss_step=844.0, val_loss=901.0, train_loss_epoch=1.11e+3]\n",
      "Epoch 10:  78%|███████▊  | 52/67 [00:15<00:04,  3.28it/s, loss=703, v_num=23, train_loss_step=844.0, val_loss=901.0, train_loss_epoch=1.11e+3]\n",
      "Epoch 10:  79%|███████▉  | 53/67 [00:16<00:04,  3.29it/s, loss=703, v_num=23, train_loss_step=844.0, val_loss=901.0, train_loss_epoch=1.11e+3]\n",
      "Epoch 10:  81%|████████  | 54/67 [00:16<00:03,  3.29it/s, loss=703, v_num=23, train_loss_step=844.0, val_loss=901.0, train_loss_epoch=1.11e+3]\n",
      "Epoch 10:  82%|████████▏ | 55/67 [00:16<00:03,  3.25it/s, loss=703, v_num=23, train_loss_step=844.0, val_loss=901.0, train_loss_epoch=1.11e+3]\n",
      "Epoch 10:  84%|████████▎ | 56/67 [00:17<00:03,  3.26it/s, loss=703, v_num=23, train_loss_step=844.0, val_loss=901.0, train_loss_epoch=1.11e+3]\n",
      "Epoch 10:  85%|████████▌ | 57/67 [00:17<00:03,  3.26it/s, loss=703, v_num=23, train_loss_step=844.0, val_loss=901.0, train_loss_epoch=1.11e+3]\n",
      "Epoch 10:  87%|████████▋ | 58/67 [00:17<00:02,  3.23it/s, loss=703, v_num=23, train_loss_step=844.0, val_loss=901.0, train_loss_epoch=1.11e+3]\n",
      "Epoch 10:  88%|████████▊ | 59/67 [00:18<00:02,  3.23it/s, loss=703, v_num=23, train_loss_step=844.0, val_loss=901.0, train_loss_epoch=1.11e+3]\n",
      "Epoch 10:  90%|████████▉ | 60/67 [00:18<00:02,  3.24it/s, loss=703, v_num=23, train_loss_step=844.0, val_loss=901.0, train_loss_epoch=1.11e+3]\n",
      "Epoch 10:  91%|█████████ | 61/67 [00:19<00:01,  3.18it/s, loss=703, v_num=23, train_loss_step=844.0, val_loss=901.0, train_loss_epoch=1.11e+3]\n",
      "Epoch 10:  93%|█████████▎| 62/67 [00:19<00:01,  3.19it/s, loss=703, v_num=23, train_loss_step=844.0, val_loss=901.0, train_loss_epoch=1.11e+3]\n",
      "Epoch 10:  94%|█████████▍| 63/67 [00:19<00:01,  3.19it/s, loss=703, v_num=23, train_loss_step=844.0, val_loss=901.0, train_loss_epoch=1.11e+3]\n",
      "Epoch 10:  96%|█████████▌| 64/67 [00:20<00:00,  3.18it/s, loss=703, v_num=23, train_loss_step=844.0, val_loss=901.0, train_loss_epoch=1.11e+3]\n",
      "Epoch 10:  97%|█████████▋| 65/67 [00:20<00:00,  3.19it/s, loss=703, v_num=23, train_loss_step=844.0, val_loss=901.0, train_loss_epoch=1.11e+3]\n",
      "Epoch 10:  99%|█████████▊| 66/67 [00:20<00:00,  3.19it/s, loss=703, v_num=23, train_loss_step=844.0, val_loss=901.0, train_loss_epoch=1.11e+3]\n",
      "Epoch 10: 100%|██████████| 67/67 [00:21<00:00,  3.05it/s, loss=703, v_num=23, train_loss_step=844.0, val_loss=901.0, train_loss_epoch=1.11e+3]\n",
      "Epoch 11:  45%|████▍     | 30/67 [00:05<00:07,  5.03it/s, loss=628, v_num=23, train_loss_step=789.0, val_loss=901.0, train_loss_epoch=676.0]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 11:  46%|████▋     | 31/67 [00:07<00:09,  3.90it/s, loss=628, v_num=23, train_loss_step=789.0, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 11:  48%|████▊     | 32/67 [00:08<00:09,  3.88it/s, loss=628, v_num=23, train_loss_step=789.0, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 11:  49%|████▉     | 33/67 [00:08<00:08,  3.86it/s, loss=628, v_num=23, train_loss_step=789.0, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 11:  51%|█████     | 34/67 [00:08<00:08,  3.85it/s, loss=628, v_num=23, train_loss_step=789.0, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 11:  52%|█████▏    | 35/67 [00:09<00:08,  3.83it/s, loss=628, v_num=23, train_loss_step=789.0, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 11:  54%|█████▎    | 36/67 [00:09<00:08,  3.82it/s, loss=628, v_num=23, train_loss_step=789.0, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 11:  55%|█████▌    | 37/67 [00:09<00:07,  3.75it/s, loss=628, v_num=23, train_loss_step=789.0, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 11:  57%|█████▋    | 38/67 [00:10<00:07,  3.74it/s, loss=628, v_num=23, train_loss_step=789.0, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 11:  58%|█████▊    | 39/67 [00:10<00:07,  3.73it/s, loss=628, v_num=23, train_loss_step=789.0, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 11:  60%|█████▉    | 40/67 [00:10<00:07,  3.65it/s, loss=628, v_num=23, train_loss_step=789.0, val_loss=901.0, train_loss_epoch=676.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 11:  61%|██████    | 41/67 [00:11<00:07,  3.61it/s, loss=628, v_num=23, train_loss_step=789.0, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 11:  63%|██████▎   | 42/67 [00:11<00:06,  3.61it/s, loss=628, v_num=23, train_loss_step=789.0, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 11:  64%|██████▍   | 43/67 [00:12<00:06,  3.58it/s, loss=628, v_num=23, train_loss_step=789.0, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 11:  66%|██████▌   | 44/67 [00:12<00:06,  3.58it/s, loss=628, v_num=23, train_loss_step=789.0, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 11:  67%|██████▋   | 45/67 [00:12<00:06,  3.58it/s, loss=628, v_num=23, train_loss_step=789.0, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 11:  69%|██████▊   | 46/67 [00:13<00:05,  3.52it/s, loss=628, v_num=23, train_loss_step=789.0, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 11:  70%|███████   | 47/67 [00:13<00:05,  3.51it/s, loss=628, v_num=23, train_loss_step=789.0, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 11:  72%|███████▏  | 48/67 [00:13<00:05,  3.51it/s, loss=628, v_num=23, train_loss_step=789.0, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 11:  73%|███████▎  | 49/67 [00:14<00:05,  3.45it/s, loss=628, v_num=23, train_loss_step=789.0, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 11:  75%|███████▍  | 50/67 [00:14<00:04,  3.45it/s, loss=628, v_num=23, train_loss_step=789.0, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 11:  76%|███████▌  | 51/67 [00:14<00:04,  3.41it/s, loss=628, v_num=23, train_loss_step=789.0, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 11:  78%|███████▊  | 52/67 [00:15<00:04,  3.40it/s, loss=628, v_num=23, train_loss_step=789.0, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 11:  79%|███████▉  | 53/67 [00:15<00:04,  3.40it/s, loss=628, v_num=23, train_loss_step=789.0, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 11:  81%|████████  | 54/67 [00:15<00:03,  3.40it/s, loss=628, v_num=23, train_loss_step=789.0, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 11:  82%|████████▏ | 55/67 [00:16<00:03,  3.36it/s, loss=628, v_num=23, train_loss_step=789.0, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 11:  84%|████████▎ | 56/67 [00:16<00:03,  3.37it/s, loss=628, v_num=23, train_loss_step=789.0, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 11:  85%|████████▌ | 57/67 [00:16<00:02,  3.37it/s, loss=628, v_num=23, train_loss_step=789.0, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 11:  87%|████████▋ | 58/67 [00:17<00:02,  3.33it/s, loss=628, v_num=23, train_loss_step=789.0, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 11:  88%|████████▊ | 59/67 [00:17<00:02,  3.33it/s, loss=628, v_num=23, train_loss_step=789.0, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 11:  90%|████████▉ | 60/67 [00:17<00:02,  3.33it/s, loss=628, v_num=23, train_loss_step=789.0, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 11:  91%|█████████ | 61/67 [00:18<00:01,  3.28it/s, loss=628, v_num=23, train_loss_step=789.0, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 11:  93%|█████████▎| 62/67 [00:18<00:01,  3.28it/s, loss=628, v_num=23, train_loss_step=789.0, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 11:  94%|█████████▍| 63/67 [00:19<00:01,  3.28it/s, loss=628, v_num=23, train_loss_step=789.0, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 11:  96%|█████████▌| 64/67 [00:20<00:00,  3.17it/s, loss=628, v_num=23, train_loss_step=789.0, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 11:  97%|█████████▋| 65/67 [00:20<00:00,  3.17it/s, loss=628, v_num=23, train_loss_step=789.0, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 11:  99%|█████████▊| 66/67 [00:20<00:00,  3.18it/s, loss=628, v_num=23, train_loss_step=789.0, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 11: 100%|██████████| 67/67 [00:21<00:00,  3.12it/s, loss=628, v_num=23, train_loss_step=789.0, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 12:  45%|████▍     | 30/67 [00:06<00:07,  4.93it/s, loss=787, v_num=23, train_loss_step=1.59e+3, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 12:  46%|████▋     | 31/67 [00:08<00:09,  3.82it/s, loss=787, v_num=23, train_loss_step=1.59e+3, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 12:  48%|████▊     | 32/67 [00:08<00:09,  3.80it/s, loss=787, v_num=23, train_loss_step=1.59e+3, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 12:  49%|████▉     | 33/67 [00:08<00:08,  3.79it/s, loss=787, v_num=23, train_loss_step=1.59e+3, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 12:  51%|█████     | 34/67 [00:08<00:08,  3.78it/s, loss=787, v_num=23, train_loss_step=1.59e+3, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 12:  52%|█████▏    | 35/67 [00:09<00:08,  3.77it/s, loss=787, v_num=23, train_loss_step=1.59e+3, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 12:  54%|█████▎    | 36/67 [00:09<00:08,  3.77it/s, loss=787, v_num=23, train_loss_step=1.59e+3, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 12:  55%|█████▌    | 37/67 [00:09<00:08,  3.71it/s, loss=787, v_num=23, train_loss_step=1.59e+3, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 12:  57%|█████▋    | 38/67 [00:10<00:07,  3.69it/s, loss=787, v_num=23, train_loss_step=1.59e+3, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 12:  58%|█████▊    | 39/67 [00:10<00:07,  3.69it/s, loss=787, v_num=23, train_loss_step=1.59e+3, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 12:  60%|█████▉    | 40/67 [00:11<00:07,  3.61it/s, loss=787, v_num=23, train_loss_step=1.59e+3, val_loss=901.0, train_loss_epoch=676.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 12:  61%|██████    | 41/67 [00:11<00:07,  3.56it/s, loss=787, v_num=23, train_loss_step=1.59e+3, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 12:  63%|██████▎   | 42/67 [00:11<00:07,  3.56it/s, loss=787, v_num=23, train_loss_step=1.59e+3, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 12:  64%|██████▍   | 43/67 [00:12<00:06,  3.54it/s, loss=787, v_num=23, train_loss_step=1.59e+3, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 12:  66%|██████▌   | 44/67 [00:12<00:06,  3.53it/s, loss=787, v_num=23, train_loss_step=1.59e+3, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 12:  67%|██████▋   | 45/67 [00:12<00:06,  3.53it/s, loss=787, v_num=23, train_loss_step=1.59e+3, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 12:  69%|██████▊   | 46/67 [00:13<00:06,  3.47it/s, loss=787, v_num=23, train_loss_step=1.59e+3, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 12:  70%|███████   | 47/67 [00:13<00:05,  3.47it/s, loss=787, v_num=23, train_loss_step=1.59e+3, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 12:  72%|███████▏  | 48/67 [00:13<00:05,  3.47it/s, loss=787, v_num=23, train_loss_step=1.59e+3, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 12:  73%|███████▎  | 49/67 [00:14<00:05,  3.42it/s, loss=787, v_num=23, train_loss_step=1.59e+3, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 12:  75%|███████▍  | 50/67 [00:14<00:04,  3.42it/s, loss=787, v_num=23, train_loss_step=1.59e+3, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 12:  76%|███████▌  | 51/67 [00:15<00:04,  3.39it/s, loss=787, v_num=23, train_loss_step=1.59e+3, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 12:  78%|███████▊  | 52/67 [00:15<00:04,  3.37it/s, loss=787, v_num=23, train_loss_step=1.59e+3, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 12:  79%|███████▉  | 53/67 [00:15<00:04,  3.37it/s, loss=787, v_num=23, train_loss_step=1.59e+3, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 12:  81%|████████  | 54/67 [00:16<00:03,  3.37it/s, loss=787, v_num=23, train_loss_step=1.59e+3, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 12:  82%|████████▏ | 55/67 [00:17<00:03,  3.21it/s, loss=787, v_num=23, train_loss_step=1.59e+3, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 12:  84%|████████▎ | 56/67 [00:17<00:03,  3.21it/s, loss=787, v_num=23, train_loss_step=1.59e+3, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 12:  85%|████████▌ | 57/67 [00:17<00:03,  3.21it/s, loss=787, v_num=23, train_loss_step=1.59e+3, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 12:  87%|████████▋ | 58/67 [00:18<00:02,  3.18it/s, loss=787, v_num=23, train_loss_step=1.59e+3, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 12:  88%|████████▊ | 59/67 [00:18<00:02,  3.19it/s, loss=787, v_num=23, train_loss_step=1.59e+3, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 12:  90%|████████▉ | 60/67 [00:18<00:02,  3.19it/s, loss=787, v_num=23, train_loss_step=1.59e+3, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 12:  91%|█████████ | 61/67 [00:19<00:01,  3.14it/s, loss=787, v_num=23, train_loss_step=1.59e+3, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 12:  93%|█████████▎| 62/67 [00:19<00:01,  3.15it/s, loss=787, v_num=23, train_loss_step=1.59e+3, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 12:  94%|█████████▍| 63/67 [00:19<00:01,  3.15it/s, loss=787, v_num=23, train_loss_step=1.59e+3, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 12:  96%|█████████▌| 64/67 [00:20<00:00,  3.14it/s, loss=787, v_num=23, train_loss_step=1.59e+3, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 12:  97%|█████████▋| 65/67 [00:20<00:00,  3.15it/s, loss=787, v_num=23, train_loss_step=1.59e+3, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 12:  99%|█████████▊| 66/67 [00:20<00:00,  3.15it/s, loss=787, v_num=23, train_loss_step=1.59e+3, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 12: 100%|██████████| 67/67 [00:21<00:00,  3.08it/s, loss=787, v_num=23, train_loss_step=1.59e+3, val_loss=901.0, train_loss_epoch=676.0]\n",
      "Epoch 13:  45%|████▍     | 30/67 [00:06<00:07,  4.98it/s, loss=693, v_num=23, train_loss_step=752.0, val_loss=901.0, train_loss_epoch=727.0]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 13:  46%|████▋     | 31/67 [00:07<00:09,  3.88it/s, loss=693, v_num=23, train_loss_step=752.0, val_loss=901.0, train_loss_epoch=727.0]\n",
      "Epoch 13:  48%|████▊     | 32/67 [00:08<00:09,  3.85it/s, loss=693, v_num=23, train_loss_step=752.0, val_loss=901.0, train_loss_epoch=727.0]\n",
      "Epoch 13:  49%|████▉     | 33/67 [00:08<00:08,  3.83it/s, loss=693, v_num=23, train_loss_step=752.0, val_loss=901.0, train_loss_epoch=727.0]\n",
      "Epoch 13:  51%|█████     | 34/67 [00:08<00:08,  3.82it/s, loss=693, v_num=23, train_loss_step=752.0, val_loss=901.0, train_loss_epoch=727.0]\n",
      "Epoch 13:  52%|█████▏    | 35/67 [00:09<00:08,  3.80it/s, loss=693, v_num=23, train_loss_step=752.0, val_loss=901.0, train_loss_epoch=727.0]\n",
      "Epoch 13:  54%|█████▎    | 36/67 [00:09<00:08,  3.79it/s, loss=693, v_num=23, train_loss_step=752.0, val_loss=901.0, train_loss_epoch=727.0]\n",
      "Epoch 13:  55%|█████▌    | 37/67 [00:09<00:08,  3.73it/s, loss=693, v_num=23, train_loss_step=752.0, val_loss=901.0, train_loss_epoch=727.0]\n",
      "Epoch 13:  57%|█████▋    | 38/67 [00:10<00:07,  3.72it/s, loss=693, v_num=23, train_loss_step=752.0, val_loss=901.0, train_loss_epoch=727.0]\n",
      "Epoch 13:  58%|█████▊    | 39/67 [00:10<00:07,  3.72it/s, loss=693, v_num=23, train_loss_step=752.0, val_loss=901.0, train_loss_epoch=727.0]\n",
      "Epoch 13:  60%|█████▉    | 40/67 [00:11<00:07,  3.62it/s, loss=693, v_num=23, train_loss_step=752.0, val_loss=901.0, train_loss_epoch=727.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 13:  61%|██████    | 41/67 [00:11<00:07,  3.57it/s, loss=693, v_num=23, train_loss_step=752.0, val_loss=901.0, train_loss_epoch=727.0]\n",
      "Epoch 13:  63%|██████▎   | 42/67 [00:11<00:07,  3.57it/s, loss=693, v_num=23, train_loss_step=752.0, val_loss=901.0, train_loss_epoch=727.0]\n",
      "Epoch 13:  64%|██████▍   | 43/67 [00:12<00:06,  3.53it/s, loss=693, v_num=23, train_loss_step=752.0, val_loss=901.0, train_loss_epoch=727.0]\n",
      "Epoch 13:  66%|██████▌   | 44/67 [00:12<00:06,  3.53it/s, loss=693, v_num=23, train_loss_step=752.0, val_loss=901.0, train_loss_epoch=727.0]\n",
      "Epoch 13:  67%|██████▋   | 45/67 [00:12<00:06,  3.53it/s, loss=693, v_num=23, train_loss_step=752.0, val_loss=901.0, train_loss_epoch=727.0]\n",
      "Epoch 13:  69%|██████▊   | 46/67 [00:13<00:06,  3.47it/s, loss=693, v_num=23, train_loss_step=752.0, val_loss=901.0, train_loss_epoch=727.0]\n",
      "Epoch 13:  70%|███████   | 47/67 [00:14<00:06,  3.31it/s, loss=693, v_num=23, train_loss_step=752.0, val_loss=901.0, train_loss_epoch=727.0]\n",
      "Epoch 13:  72%|███████▏  | 48/67 [00:14<00:05,  3.31it/s, loss=693, v_num=23, train_loss_step=752.0, val_loss=901.0, train_loss_epoch=727.0]\n",
      "Epoch 13:  73%|███████▎  | 49/67 [00:15<00:05,  3.24it/s, loss=693, v_num=23, train_loss_step=752.0, val_loss=901.0, train_loss_epoch=727.0]\n",
      "Epoch 13:  75%|███████▍  | 50/67 [00:15<00:05,  3.25it/s, loss=693, v_num=23, train_loss_step=752.0, val_loss=901.0, train_loss_epoch=727.0]\n",
      "Epoch 13:  76%|███████▌  | 51/67 [00:15<00:04,  3.23it/s, loss=693, v_num=23, train_loss_step=752.0, val_loss=901.0, train_loss_epoch=727.0]\n",
      "Epoch 13:  78%|███████▊  | 52/67 [00:16<00:04,  3.20it/s, loss=693, v_num=23, train_loss_step=752.0, val_loss=901.0, train_loss_epoch=727.0]\n",
      "Epoch 13:  79%|███████▉  | 53/67 [00:16<00:04,  3.21it/s, loss=693, v_num=23, train_loss_step=752.0, val_loss=901.0, train_loss_epoch=727.0]\n",
      "Epoch 13:  81%|████████  | 54/67 [00:16<00:04,  3.21it/s, loss=693, v_num=23, train_loss_step=752.0, val_loss=901.0, train_loss_epoch=727.0]\n",
      "Epoch 13:  82%|████████▏ | 55/67 [00:17<00:03,  3.18it/s, loss=693, v_num=23, train_loss_step=752.0, val_loss=901.0, train_loss_epoch=727.0]\n",
      "Epoch 13:  84%|████████▎ | 56/67 [00:17<00:03,  3.19it/s, loss=693, v_num=23, train_loss_step=752.0, val_loss=901.0, train_loss_epoch=727.0]\n",
      "Epoch 13:  85%|████████▌ | 57/67 [00:17<00:03,  3.19it/s, loss=693, v_num=23, train_loss_step=752.0, val_loss=901.0, train_loss_epoch=727.0]\n",
      "Epoch 13:  87%|████████▋ | 58/67 [00:18<00:02,  3.16it/s, loss=693, v_num=23, train_loss_step=752.0, val_loss=901.0, train_loss_epoch=727.0]\n",
      "Epoch 13:  88%|████████▊ | 59/67 [00:18<00:02,  3.16it/s, loss=693, v_num=23, train_loss_step=752.0, val_loss=901.0, train_loss_epoch=727.0]\n",
      "Epoch 13:  90%|████████▉ | 60/67 [00:18<00:02,  3.17it/s, loss=693, v_num=23, train_loss_step=752.0, val_loss=901.0, train_loss_epoch=727.0]\n",
      "Epoch 13:  91%|█████████ | 61/67 [00:19<00:01,  3.12it/s, loss=693, v_num=23, train_loss_step=752.0, val_loss=901.0, train_loss_epoch=727.0]\n",
      "Epoch 13:  93%|█████████▎| 62/67 [00:19<00:01,  3.12it/s, loss=693, v_num=23, train_loss_step=752.0, val_loss=901.0, train_loss_epoch=727.0]\n",
      "Epoch 13:  94%|█████████▍| 63/67 [00:20<00:01,  3.13it/s, loss=693, v_num=23, train_loss_step=752.0, val_loss=901.0, train_loss_epoch=727.0]\n",
      "Epoch 13:  96%|█████████▌| 64/67 [00:20<00:00,  3.12it/s, loss=693, v_num=23, train_loss_step=752.0, val_loss=901.0, train_loss_epoch=727.0]\n",
      "Epoch 13:  97%|█████████▋| 65/67 [00:20<00:00,  3.12it/s, loss=693, v_num=23, train_loss_step=752.0, val_loss=901.0, train_loss_epoch=727.0]\n",
      "Epoch 13:  99%|█████████▊| 66/67 [00:21<00:00,  3.13it/s, loss=693, v_num=23, train_loss_step=752.0, val_loss=901.0, train_loss_epoch=727.0]\n",
      "Epoch 13: 100%|██████████| 67/67 [00:21<00:00,  3.06it/s, loss=693, v_num=23, train_loss_step=752.0, val_loss=900.0, train_loss_epoch=727.0]\n",
      "Epoch 14:  45%|████▍     | 30/67 [00:05<00:07,  5.00it/s, loss=876, v_num=23, train_loss_step=2.42e+3, val_loss=900.0, train_loss_epoch=685.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 14:  46%|████▋     | 31/67 [00:08<00:09,  3.86it/s, loss=876, v_num=23, train_loss_step=2.42e+3, val_loss=900.0, train_loss_epoch=685.0]\n",
      "Epoch 14:  48%|████▊     | 32/67 [00:08<00:09,  3.84it/s, loss=876, v_num=23, train_loss_step=2.42e+3, val_loss=900.0, train_loss_epoch=685.0]\n",
      "Epoch 14:  49%|████▉     | 33/67 [00:08<00:08,  3.82it/s, loss=876, v_num=23, train_loss_step=2.42e+3, val_loss=900.0, train_loss_epoch=685.0]\n",
      "Epoch 14:  51%|█████     | 34/67 [00:08<00:08,  3.81it/s, loss=876, v_num=23, train_loss_step=2.42e+3, val_loss=900.0, train_loss_epoch=685.0]\n",
      "Epoch 14:  52%|█████▏    | 35/67 [00:09<00:08,  3.80it/s, loss=876, v_num=23, train_loss_step=2.42e+3, val_loss=900.0, train_loss_epoch=685.0]\n",
      "Epoch 14:  54%|█████▎    | 36/67 [00:09<00:08,  3.78it/s, loss=876, v_num=23, train_loss_step=2.42e+3, val_loss=900.0, train_loss_epoch=685.0]\n",
      "Epoch 14:  55%|█████▌    | 37/67 [00:09<00:08,  3.73it/s, loss=876, v_num=23, train_loss_step=2.42e+3, val_loss=900.0, train_loss_epoch=685.0]\n",
      "Epoch 14:  57%|█████▋    | 38/67 [00:10<00:07,  3.72it/s, loss=876, v_num=23, train_loss_step=2.42e+3, val_loss=900.0, train_loss_epoch=685.0]\n",
      "Epoch 14:  58%|█████▊    | 39/67 [00:10<00:07,  3.71it/s, loss=876, v_num=23, train_loss_step=2.42e+3, val_loss=900.0, train_loss_epoch=685.0]\n",
      "Epoch 14:  60%|█████▉    | 40/67 [00:11<00:07,  3.41it/s, loss=876, v_num=23, train_loss_step=2.42e+3, val_loss=900.0, train_loss_epoch=685.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 14:  61%|██████    | 41/67 [00:12<00:07,  3.37it/s, loss=876, v_num=23, train_loss_step=2.42e+3, val_loss=900.0, train_loss_epoch=685.0]\n",
      "Epoch 14:  63%|██████▎   | 42/67 [00:12<00:07,  3.37it/s, loss=876, v_num=23, train_loss_step=2.42e+3, val_loss=900.0, train_loss_epoch=685.0]\n",
      "Epoch 14:  64%|██████▍   | 43/67 [00:12<00:07,  3.35it/s, loss=876, v_num=23, train_loss_step=2.42e+3, val_loss=900.0, train_loss_epoch=685.0]\n",
      "Epoch 14:  66%|██████▌   | 44/67 [00:13<00:06,  3.35it/s, loss=876, v_num=23, train_loss_step=2.42e+3, val_loss=900.0, train_loss_epoch=685.0]\n",
      "Epoch 14:  67%|██████▋   | 45/67 [00:13<00:06,  3.36it/s, loss=876, v_num=23, train_loss_step=2.42e+3, val_loss=900.0, train_loss_epoch=685.0]\n",
      "Epoch 14:  69%|██████▊   | 46/67 [00:13<00:06,  3.31it/s, loss=876, v_num=23, train_loss_step=2.42e+3, val_loss=900.0, train_loss_epoch=685.0]\n",
      "Epoch 14:  70%|███████   | 47/67 [00:14<00:06,  3.32it/s, loss=876, v_num=23, train_loss_step=2.42e+3, val_loss=900.0, train_loss_epoch=685.0]\n",
      "Epoch 14:  72%|███████▏  | 48/67 [00:14<00:05,  3.32it/s, loss=876, v_num=23, train_loss_step=2.42e+3, val_loss=900.0, train_loss_epoch=685.0]\n",
      "Epoch 14:  73%|███████▎  | 49/67 [00:14<00:05,  3.27it/s, loss=876, v_num=23, train_loss_step=2.42e+3, val_loss=900.0, train_loss_epoch=685.0]\n",
      "Epoch 14:  75%|███████▍  | 50/67 [00:15<00:05,  3.28it/s, loss=876, v_num=23, train_loss_step=2.42e+3, val_loss=900.0, train_loss_epoch=685.0]\n",
      "Epoch 14:  76%|███████▌  | 51/67 [00:15<00:04,  3.25it/s, loss=876, v_num=23, train_loss_step=2.42e+3, val_loss=900.0, train_loss_epoch=685.0]\n",
      "Epoch 14:  78%|███████▊  | 52/67 [00:16<00:04,  3.24it/s, loss=876, v_num=23, train_loss_step=2.42e+3, val_loss=900.0, train_loss_epoch=685.0]\n",
      "Epoch 14:  79%|███████▉  | 53/67 [00:16<00:04,  3.25it/s, loss=876, v_num=23, train_loss_step=2.42e+3, val_loss=900.0, train_loss_epoch=685.0]\n",
      "Epoch 14:  81%|████████  | 54/67 [00:16<00:04,  3.25it/s, loss=876, v_num=23, train_loss_step=2.42e+3, val_loss=900.0, train_loss_epoch=685.0]\n",
      "Epoch 14:  82%|████████▏ | 55/67 [00:17<00:03,  3.21it/s, loss=876, v_num=23, train_loss_step=2.42e+3, val_loss=900.0, train_loss_epoch=685.0]\n",
      "Epoch 14:  84%|████████▎ | 56/67 [00:17<00:03,  3.22it/s, loss=876, v_num=23, train_loss_step=2.42e+3, val_loss=900.0, train_loss_epoch=685.0]\n",
      "Epoch 14:  85%|████████▌ | 57/67 [00:17<00:03,  3.22it/s, loss=876, v_num=23, train_loss_step=2.42e+3, val_loss=900.0, train_loss_epoch=685.0]\n",
      "Epoch 14:  87%|████████▋ | 58/67 [00:18<00:02,  3.19it/s, loss=876, v_num=23, train_loss_step=2.42e+3, val_loss=900.0, train_loss_epoch=685.0]\n",
      "Epoch 14:  88%|████████▊ | 59/67 [00:18<00:02,  3.20it/s, loss=876, v_num=23, train_loss_step=2.42e+3, val_loss=900.0, train_loss_epoch=685.0]\n",
      "Epoch 14:  90%|████████▉ | 60/67 [00:18<00:02,  3.20it/s, loss=876, v_num=23, train_loss_step=2.42e+3, val_loss=900.0, train_loss_epoch=685.0]\n",
      "Epoch 14:  91%|█████████ | 61/67 [00:19<00:01,  3.15it/s, loss=876, v_num=23, train_loss_step=2.42e+3, val_loss=900.0, train_loss_epoch=685.0]\n",
      "Epoch 14:  93%|█████████▎| 62/67 [00:19<00:01,  3.16it/s, loss=876, v_num=23, train_loss_step=2.42e+3, val_loss=900.0, train_loss_epoch=685.0]\n",
      "Epoch 14:  94%|█████████▍| 63/67 [00:19<00:01,  3.16it/s, loss=876, v_num=23, train_loss_step=2.42e+3, val_loss=900.0, train_loss_epoch=685.0]\n",
      "Epoch 14:  96%|█████████▌| 64/67 [00:20<00:00,  3.14it/s, loss=876, v_num=23, train_loss_step=2.42e+3, val_loss=900.0, train_loss_epoch=685.0]\n",
      "Epoch 14:  97%|█████████▋| 65/67 [00:20<00:00,  3.15it/s, loss=876, v_num=23, train_loss_step=2.42e+3, val_loss=900.0, train_loss_epoch=685.0]\n",
      "Epoch 14:  99%|█████████▊| 66/67 [00:20<00:00,  3.15it/s, loss=876, v_num=23, train_loss_step=2.42e+3, val_loss=900.0, train_loss_epoch=685.0]\n",
      "Epoch 14: 100%|██████████| 67/67 [00:21<00:00,  3.09it/s, loss=876, v_num=23, train_loss_step=2.42e+3, val_loss=900.0, train_loss_epoch=685.0]\n",
      "Epoch 15:  45%|████▍     | 30/67 [00:06<00:07,  4.77it/s, loss=693, v_num=23, train_loss_step=382.0, val_loss=900.0, train_loss_epoch=865.0]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 15:  46%|████▋     | 31/67 [00:09<00:10,  3.44it/s, loss=693, v_num=23, train_loss_step=382.0, val_loss=900.0, train_loss_epoch=865.0]\n",
      "Epoch 15:  48%|████▊     | 32/67 [00:09<00:10,  3.44it/s, loss=693, v_num=23, train_loss_step=382.0, val_loss=900.0, train_loss_epoch=865.0]\n",
      "Epoch 15:  49%|████▉     | 33/67 [00:09<00:09,  3.43it/s, loss=693, v_num=23, train_loss_step=382.0, val_loss=900.0, train_loss_epoch=865.0]\n",
      "Epoch 15:  51%|█████     | 34/67 [00:09<00:09,  3.41it/s, loss=693, v_num=23, train_loss_step=382.0, val_loss=900.0, train_loss_epoch=865.0]\n",
      "Epoch 15:  52%|█████▏    | 35/67 [00:10<00:09,  3.42it/s, loss=693, v_num=23, train_loss_step=382.0, val_loss=900.0, train_loss_epoch=865.0]\n",
      "Epoch 15:  54%|█████▎    | 36/67 [00:10<00:09,  3.41it/s, loss=693, v_num=23, train_loss_step=382.0, val_loss=900.0, train_loss_epoch=865.0]\n",
      "Epoch 15:  55%|█████▌    | 37/67 [00:11<00:08,  3.34it/s, loss=693, v_num=23, train_loss_step=382.0, val_loss=900.0, train_loss_epoch=865.0]\n",
      "Epoch 15:  57%|█████▋    | 38/67 [00:11<00:08,  3.35it/s, loss=693, v_num=23, train_loss_step=382.0, val_loss=900.0, train_loss_epoch=865.0]\n",
      "Epoch 15:  58%|█████▊    | 39/67 [00:11<00:08,  3.35it/s, loss=693, v_num=23, train_loss_step=382.0, val_loss=900.0, train_loss_epoch=865.0]\n",
      "Epoch 15:  60%|█████▉    | 40/67 [00:12<00:08,  3.30it/s, loss=693, v_num=23, train_loss_step=382.0, val_loss=900.0, train_loss_epoch=865.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 15:  61%|██████    | 41/67 [00:13<00:08,  3.09it/s, loss=693, v_num=23, train_loss_step=382.0, val_loss=900.0, train_loss_epoch=865.0]\n",
      "Epoch 15:  63%|██████▎   | 42/67 [00:13<00:08,  3.10it/s, loss=693, v_num=23, train_loss_step=382.0, val_loss=900.0, train_loss_epoch=865.0]\n",
      "Epoch 15:  64%|██████▍   | 43/67 [00:13<00:07,  3.10it/s, loss=693, v_num=23, train_loss_step=382.0, val_loss=900.0, train_loss_epoch=865.0]\n",
      "Epoch 15:  66%|██████▌   | 44/67 [00:14<00:07,  3.11it/s, loss=693, v_num=23, train_loss_step=382.0, val_loss=900.0, train_loss_epoch=865.0]\n",
      "Epoch 15:  67%|██████▋   | 45/67 [00:14<00:07,  3.12it/s, loss=693, v_num=23, train_loss_step=382.0, val_loss=900.0, train_loss_epoch=865.0]\n",
      "Epoch 15:  69%|██████▊   | 46/67 [00:14<00:06,  3.12it/s, loss=693, v_num=23, train_loss_step=382.0, val_loss=900.0, train_loss_epoch=865.0]\n",
      "Epoch 15:  70%|███████   | 47/67 [00:15<00:06,  3.13it/s, loss=693, v_num=23, train_loss_step=382.0, val_loss=900.0, train_loss_epoch=865.0]\n",
      "Epoch 15:  72%|███████▏  | 48/67 [00:15<00:06,  3.14it/s, loss=693, v_num=23, train_loss_step=382.0, val_loss=900.0, train_loss_epoch=865.0]\n",
      "Epoch 15:  73%|███████▎  | 49/67 [00:15<00:05,  3.14it/s, loss=693, v_num=23, train_loss_step=382.0, val_loss=900.0, train_loss_epoch=865.0]\n",
      "Epoch 15:  75%|███████▍  | 50/67 [00:15<00:05,  3.15it/s, loss=693, v_num=23, train_loss_step=382.0, val_loss=900.0, train_loss_epoch=865.0]\n",
      "Epoch 15:  76%|███████▌  | 51/67 [00:16<00:05,  3.12it/s, loss=693, v_num=23, train_loss_step=382.0, val_loss=900.0, train_loss_epoch=865.0]\n",
      "Epoch 15:  78%|███████▊  | 52/67 [00:16<00:04,  3.12it/s, loss=693, v_num=23, train_loss_step=382.0, val_loss=900.0, train_loss_epoch=865.0]\n",
      "Epoch 15:  79%|███████▉  | 53/67 [00:16<00:04,  3.13it/s, loss=693, v_num=23, train_loss_step=382.0, val_loss=900.0, train_loss_epoch=865.0]\n",
      "Epoch 15:  81%|████████  | 54/67 [00:17<00:04,  3.13it/s, loss=693, v_num=23, train_loss_step=382.0, val_loss=900.0, train_loss_epoch=865.0]\n",
      "Epoch 15:  82%|████████▏ | 55/67 [00:17<00:03,  3.10it/s, loss=693, v_num=23, train_loss_step=382.0, val_loss=900.0, train_loss_epoch=865.0]\n",
      "Epoch 15:  84%|████████▎ | 56/67 [00:18<00:03,  3.11it/s, loss=693, v_num=23, train_loss_step=382.0, val_loss=900.0, train_loss_epoch=865.0]\n",
      "Epoch 15:  85%|████████▌ | 57/67 [00:18<00:03,  3.11it/s, loss=693, v_num=23, train_loss_step=382.0, val_loss=900.0, train_loss_epoch=865.0]\n",
      "Epoch 15:  87%|████████▋ | 58/67 [00:18<00:02,  3.08it/s, loss=693, v_num=23, train_loss_step=382.0, val_loss=900.0, train_loss_epoch=865.0]\n",
      "Epoch 15:  88%|████████▊ | 59/67 [00:19<00:02,  3.09it/s, loss=693, v_num=23, train_loss_step=382.0, val_loss=900.0, train_loss_epoch=865.0]\n",
      "Epoch 15:  90%|████████▉ | 60/67 [00:19<00:02,  3.10it/s, loss=693, v_num=23, train_loss_step=382.0, val_loss=900.0, train_loss_epoch=865.0]\n",
      "Epoch 15:  91%|█████████ | 61/67 [00:20<00:01,  3.04it/s, loss=693, v_num=23, train_loss_step=382.0, val_loss=900.0, train_loss_epoch=865.0]\n",
      "Epoch 15:  93%|█████████▎| 62/67 [00:20<00:01,  3.05it/s, loss=693, v_num=23, train_loss_step=382.0, val_loss=900.0, train_loss_epoch=865.0]\n",
      "Epoch 15:  94%|█████████▍| 63/67 [00:20<00:01,  3.06it/s, loss=693, v_num=23, train_loss_step=382.0, val_loss=900.0, train_loss_epoch=865.0]\n",
      "Epoch 15:  96%|█████████▌| 64/67 [00:20<00:00,  3.05it/s, loss=693, v_num=23, train_loss_step=382.0, val_loss=900.0, train_loss_epoch=865.0]\n",
      "Epoch 15:  97%|█████████▋| 65/67 [00:21<00:00,  3.06it/s, loss=693, v_num=23, train_loss_step=382.0, val_loss=900.0, train_loss_epoch=865.0]\n",
      "Epoch 15:  99%|█████████▊| 66/67 [00:21<00:00,  3.07it/s, loss=693, v_num=23, train_loss_step=382.0, val_loss=900.0, train_loss_epoch=865.0]\n",
      "Epoch 15: 100%|██████████| 67/67 [00:22<00:00,  3.01it/s, loss=693, v_num=23, train_loss_step=382.0, val_loss=900.0, train_loss_epoch=865.0]\n",
      "Epoch 16:  45%|████▍     | 30/67 [00:05<00:07,  5.13it/s, loss=947, v_num=23, train_loss_step=2.22e+3, val_loss=900.0, train_loss_epoch=646.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 16:  46%|████▋     | 31/67 [00:07<00:09,  3.94it/s, loss=947, v_num=23, train_loss_step=2.22e+3, val_loss=900.0, train_loss_epoch=646.0]\n",
      "Epoch 16:  48%|████▊     | 32/67 [00:08<00:08,  3.93it/s, loss=947, v_num=23, train_loss_step=2.22e+3, val_loss=900.0, train_loss_epoch=646.0]\n",
      "Epoch 16:  49%|████▉     | 33/67 [00:08<00:08,  3.91it/s, loss=947, v_num=23, train_loss_step=2.22e+3, val_loss=900.0, train_loss_epoch=646.0]\n",
      "Epoch 16:  51%|█████     | 34/67 [00:08<00:08,  3.90it/s, loss=947, v_num=23, train_loss_step=2.22e+3, val_loss=900.0, train_loss_epoch=646.0]\n",
      "Epoch 16:  52%|█████▏    | 35/67 [00:09<00:08,  3.88it/s, loss=947, v_num=23, train_loss_step=2.22e+3, val_loss=900.0, train_loss_epoch=646.0]\n",
      "Epoch 16:  54%|█████▎    | 36/67 [00:09<00:08,  3.87it/s, loss=947, v_num=23, train_loss_step=2.22e+3, val_loss=900.0, train_loss_epoch=646.0]\n",
      "Epoch 16:  55%|█████▌    | 37/67 [00:09<00:07,  3.78it/s, loss=947, v_num=23, train_loss_step=2.22e+3, val_loss=900.0, train_loss_epoch=646.0]\n",
      "Epoch 16:  57%|█████▋    | 38/67 [00:10<00:07,  3.77it/s, loss=947, v_num=23, train_loss_step=2.22e+3, val_loss=900.0, train_loss_epoch=646.0]\n",
      "Epoch 16:  58%|█████▊    | 39/67 [00:10<00:07,  3.76it/s, loss=947, v_num=23, train_loss_step=2.22e+3, val_loss=900.0, train_loss_epoch=646.0]\n",
      "Epoch 16:  60%|█████▉    | 40/67 [00:10<00:07,  3.69it/s, loss=947, v_num=23, train_loss_step=2.22e+3, val_loss=900.0, train_loss_epoch=646.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 16:  61%|██████    | 41/67 [00:11<00:07,  3.63it/s, loss=947, v_num=23, train_loss_step=2.22e+3, val_loss=900.0, train_loss_epoch=646.0]\n",
      "Epoch 16:  63%|██████▎   | 42/67 [00:11<00:06,  3.63it/s, loss=947, v_num=23, train_loss_step=2.22e+3, val_loss=900.0, train_loss_epoch=646.0]\n",
      "Epoch 16:  64%|██████▍   | 43/67 [00:11<00:06,  3.61it/s, loss=947, v_num=23, train_loss_step=2.22e+3, val_loss=900.0, train_loss_epoch=646.0]\n",
      "Epoch 16:  66%|██████▌   | 44/67 [00:12<00:06,  3.59it/s, loss=947, v_num=23, train_loss_step=2.22e+3, val_loss=900.0, train_loss_epoch=646.0]\n",
      "Epoch 16:  67%|██████▋   | 45/67 [00:12<00:06,  3.59it/s, loss=947, v_num=23, train_loss_step=2.22e+3, val_loss=900.0, train_loss_epoch=646.0]\n",
      "Epoch 16:  69%|██████▊   | 46/67 [00:13<00:05,  3.54it/s, loss=947, v_num=23, train_loss_step=2.22e+3, val_loss=900.0, train_loss_epoch=646.0]\n",
      "Epoch 16:  70%|███████   | 47/67 [00:13<00:05,  3.53it/s, loss=947, v_num=23, train_loss_step=2.22e+3, val_loss=900.0, train_loss_epoch=646.0]\n",
      "Epoch 16:  72%|███████▏  | 48/67 [00:13<00:05,  3.53it/s, loss=947, v_num=23, train_loss_step=2.22e+3, val_loss=900.0, train_loss_epoch=646.0]\n",
      "Epoch 16:  73%|███████▎  | 49/67 [00:14<00:05,  3.48it/s, loss=947, v_num=23, train_loss_step=2.22e+3, val_loss=900.0, train_loss_epoch=646.0]\n",
      "Epoch 16:  75%|███████▍  | 50/67 [00:14<00:04,  3.48it/s, loss=947, v_num=23, train_loss_step=2.22e+3, val_loss=900.0, train_loss_epoch=646.0]\n",
      "Epoch 16:  76%|███████▌  | 51/67 [00:14<00:04,  3.45it/s, loss=947, v_num=23, train_loss_step=2.22e+3, val_loss=900.0, train_loss_epoch=646.0]\n",
      "Epoch 16:  78%|███████▊  | 52/67 [00:15<00:04,  3.44it/s, loss=947, v_num=23, train_loss_step=2.22e+3, val_loss=900.0, train_loss_epoch=646.0]\n",
      "Epoch 16:  79%|███████▉  | 53/67 [00:15<00:04,  3.43it/s, loss=947, v_num=23, train_loss_step=2.22e+3, val_loss=900.0, train_loss_epoch=646.0]\n",
      "Epoch 16:  81%|████████  | 54/67 [00:15<00:03,  3.43it/s, loss=947, v_num=23, train_loss_step=2.22e+3, val_loss=900.0, train_loss_epoch=646.0]\n",
      "Epoch 16:  82%|████████▏ | 55/67 [00:16<00:03,  3.39it/s, loss=947, v_num=23, train_loss_step=2.22e+3, val_loss=900.0, train_loss_epoch=646.0]\n",
      "Epoch 16:  84%|████████▎ | 56/67 [00:16<00:03,  3.39it/s, loss=947, v_num=23, train_loss_step=2.22e+3, val_loss=900.0, train_loss_epoch=646.0]\n",
      "Epoch 16:  85%|████████▌ | 57/67 [00:16<00:02,  3.40it/s, loss=947, v_num=23, train_loss_step=2.22e+3, val_loss=900.0, train_loss_epoch=646.0]\n",
      "Epoch 16:  87%|████████▋ | 58/67 [00:17<00:02,  3.35it/s, loss=947, v_num=23, train_loss_step=2.22e+3, val_loss=900.0, train_loss_epoch=646.0]\n",
      "Epoch 16:  88%|████████▊ | 59/67 [00:17<00:02,  3.35it/s, loss=947, v_num=23, train_loss_step=2.22e+3, val_loss=900.0, train_loss_epoch=646.0]\n",
      "Epoch 16:  90%|████████▉ | 60/67 [00:17<00:02,  3.35it/s, loss=947, v_num=23, train_loss_step=2.22e+3, val_loss=900.0, train_loss_epoch=646.0]\n",
      "Epoch 16:  91%|█████████ | 61/67 [00:18<00:01,  3.30it/s, loss=947, v_num=23, train_loss_step=2.22e+3, val_loss=900.0, train_loss_epoch=646.0]\n",
      "Epoch 16:  93%|█████████▎| 62/67 [00:18<00:01,  3.30it/s, loss=947, v_num=23, train_loss_step=2.22e+3, val_loss=900.0, train_loss_epoch=646.0]\n",
      "Epoch 16:  94%|█████████▍| 63/67 [00:19<00:01,  3.30it/s, loss=947, v_num=23, train_loss_step=2.22e+3, val_loss=900.0, train_loss_epoch=646.0]\n",
      "Epoch 16:  96%|█████████▌| 64/67 [00:19<00:00,  3.29it/s, loss=947, v_num=23, train_loss_step=2.22e+3, val_loss=900.0, train_loss_epoch=646.0]\n",
      "Epoch 16:  97%|█████████▋| 65/67 [00:19<00:00,  3.30it/s, loss=947, v_num=23, train_loss_step=2.22e+3, val_loss=900.0, train_loss_epoch=646.0]\n",
      "Epoch 16:  99%|█████████▊| 66/67 [00:20<00:00,  3.30it/s, loss=947, v_num=23, train_loss_step=2.22e+3, val_loss=900.0, train_loss_epoch=646.0]\n",
      "Epoch 16: 100%|██████████| 67/67 [00:20<00:00,  3.23it/s, loss=947, v_num=23, train_loss_step=2.22e+3, val_loss=900.0, train_loss_epoch=646.0]\n",
      "Epoch 17:  45%|████▍     | 30/67 [00:06<00:07,  4.95it/s, loss=823, v_num=23, train_loss_step=135.0, val_loss=900.0, train_loss_epoch=867.0]       \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 17:  46%|████▋     | 31/67 [00:08<00:09,  3.86it/s, loss=823, v_num=23, train_loss_step=135.0, val_loss=900.0, train_loss_epoch=867.0]\n",
      "Epoch 17:  48%|████▊     | 32/67 [00:08<00:09,  3.83it/s, loss=823, v_num=23, train_loss_step=135.0, val_loss=900.0, train_loss_epoch=867.0]\n",
      "Epoch 17:  49%|████▉     | 33/67 [00:08<00:08,  3.82it/s, loss=823, v_num=23, train_loss_step=135.0, val_loss=900.0, train_loss_epoch=867.0]\n",
      "Epoch 17:  51%|█████     | 34/67 [00:08<00:08,  3.81it/s, loss=823, v_num=23, train_loss_step=135.0, val_loss=900.0, train_loss_epoch=867.0]\n",
      "Epoch 17:  52%|█████▏    | 35/67 [00:09<00:08,  3.79it/s, loss=823, v_num=23, train_loss_step=135.0, val_loss=900.0, train_loss_epoch=867.0]\n",
      "Epoch 17:  54%|█████▎    | 36/67 [00:09<00:08,  3.78it/s, loss=823, v_num=23, train_loss_step=135.0, val_loss=900.0, train_loss_epoch=867.0]\n",
      "Epoch 17:  55%|█████▌    | 37/67 [00:09<00:08,  3.71it/s, loss=823, v_num=23, train_loss_step=135.0, val_loss=900.0, train_loss_epoch=867.0]\n",
      "Epoch 17:  57%|█████▋    | 38/67 [00:10<00:07,  3.70it/s, loss=823, v_num=23, train_loss_step=135.0, val_loss=900.0, train_loss_epoch=867.0]\n",
      "Epoch 17:  58%|█████▊    | 39/67 [00:10<00:07,  3.70it/s, loss=823, v_num=23, train_loss_step=135.0, val_loss=900.0, train_loss_epoch=867.0]\n",
      "Epoch 17:  60%|█████▉    | 40/67 [00:11<00:07,  3.63it/s, loss=823, v_num=23, train_loss_step=135.0, val_loss=900.0, train_loss_epoch=867.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 17:  61%|██████    | 41/67 [00:11<00:07,  3.58it/s, loss=823, v_num=23, train_loss_step=135.0, val_loss=900.0, train_loss_epoch=867.0]\n",
      "Epoch 17:  63%|██████▎   | 42/67 [00:11<00:06,  3.58it/s, loss=823, v_num=23, train_loss_step=135.0, val_loss=900.0, train_loss_epoch=867.0]\n",
      "Epoch 17:  64%|██████▍   | 43/67 [00:12<00:06,  3.56it/s, loss=823, v_num=23, train_loss_step=135.0, val_loss=900.0, train_loss_epoch=867.0]\n",
      "Epoch 17:  66%|██████▌   | 44/67 [00:12<00:06,  3.55it/s, loss=823, v_num=23, train_loss_step=135.0, val_loss=900.0, train_loss_epoch=867.0]\n",
      "Epoch 17:  67%|██████▋   | 45/67 [00:12<00:06,  3.56it/s, loss=823, v_num=23, train_loss_step=135.0, val_loss=900.0, train_loss_epoch=867.0]\n",
      "Epoch 17:  69%|██████▊   | 46/67 [00:13<00:05,  3.50it/s, loss=823, v_num=23, train_loss_step=135.0, val_loss=900.0, train_loss_epoch=867.0]\n",
      "Epoch 17:  70%|███████   | 47/67 [00:13<00:05,  3.49it/s, loss=823, v_num=23, train_loss_step=135.0, val_loss=900.0, train_loss_epoch=867.0]\n",
      "Epoch 17:  72%|███████▏  | 48/67 [00:13<00:05,  3.49it/s, loss=823, v_num=23, train_loss_step=135.0, val_loss=900.0, train_loss_epoch=867.0]\n",
      "Epoch 17:  73%|███████▎  | 49/67 [00:14<00:05,  3.45it/s, loss=823, v_num=23, train_loss_step=135.0, val_loss=900.0, train_loss_epoch=867.0]\n",
      "Epoch 17:  75%|███████▍  | 50/67 [00:14<00:04,  3.45it/s, loss=823, v_num=23, train_loss_step=135.0, val_loss=900.0, train_loss_epoch=867.0]\n",
      "Epoch 17:  76%|███████▌  | 51/67 [00:14<00:04,  3.42it/s, loss=823, v_num=23, train_loss_step=135.0, val_loss=900.0, train_loss_epoch=867.0]\n",
      "Epoch 17:  78%|███████▊  | 52/67 [00:15<00:04,  3.40it/s, loss=823, v_num=23, train_loss_step=135.0, val_loss=900.0, train_loss_epoch=867.0]\n",
      "Epoch 17:  79%|███████▉  | 53/67 [00:15<00:04,  3.39it/s, loss=823, v_num=23, train_loss_step=135.0, val_loss=900.0, train_loss_epoch=867.0]\n",
      "Epoch 17:  81%|████████  | 54/67 [00:15<00:03,  3.40it/s, loss=823, v_num=23, train_loss_step=135.0, val_loss=900.0, train_loss_epoch=867.0]\n",
      "Epoch 17:  82%|████████▏ | 55/67 [00:16<00:03,  3.36it/s, loss=823, v_num=23, train_loss_step=135.0, val_loss=900.0, train_loss_epoch=867.0]\n",
      "Epoch 17:  84%|████████▎ | 56/67 [00:16<00:03,  3.36it/s, loss=823, v_num=23, train_loss_step=135.0, val_loss=900.0, train_loss_epoch=867.0]\n",
      "Epoch 17:  85%|████████▌ | 57/67 [00:16<00:02,  3.36it/s, loss=823, v_num=23, train_loss_step=135.0, val_loss=900.0, train_loss_epoch=867.0]\n",
      "Epoch 17:  87%|████████▋ | 58/67 [00:17<00:02,  3.31it/s, loss=823, v_num=23, train_loss_step=135.0, val_loss=900.0, train_loss_epoch=867.0]\n",
      "Epoch 17:  88%|████████▊ | 59/67 [00:17<00:02,  3.31it/s, loss=823, v_num=23, train_loss_step=135.0, val_loss=900.0, train_loss_epoch=867.0]\n",
      "Epoch 17:  90%|████████▉ | 60/67 [00:18<00:02,  3.32it/s, loss=823, v_num=23, train_loss_step=135.0, val_loss=900.0, train_loss_epoch=867.0]\n",
      "Epoch 17:  91%|█████████ | 61/67 [00:19<00:01,  3.13it/s, loss=823, v_num=23, train_loss_step=135.0, val_loss=900.0, train_loss_epoch=867.0]\n",
      "Epoch 17:  93%|█████████▎| 62/67 [00:19<00:01,  3.13it/s, loss=823, v_num=23, train_loss_step=135.0, val_loss=900.0, train_loss_epoch=867.0]\n",
      "Epoch 17:  94%|█████████▍| 63/67 [00:20<00:01,  3.14it/s, loss=823, v_num=23, train_loss_step=135.0, val_loss=900.0, train_loss_epoch=867.0]\n",
      "Epoch 17:  96%|█████████▌| 64/67 [00:20<00:00,  3.13it/s, loss=823, v_num=23, train_loss_step=135.0, val_loss=900.0, train_loss_epoch=867.0]\n",
      "Epoch 17:  97%|█████████▋| 65/67 [00:20<00:00,  3.14it/s, loss=823, v_num=23, train_loss_step=135.0, val_loss=900.0, train_loss_epoch=867.0]\n",
      "Epoch 17:  99%|█████████▊| 66/67 [00:20<00:00,  3.14it/s, loss=823, v_num=23, train_loss_step=135.0, val_loss=900.0, train_loss_epoch=867.0]\n",
      "Epoch 17: 100%|██████████| 67/67 [00:21<00:00,  3.08it/s, loss=823, v_num=23, train_loss_step=135.0, val_loss=900.0, train_loss_epoch=867.0]\n",
      "Epoch 18:  45%|████▍     | 30/67 [00:06<00:07,  4.99it/s, loss=585, v_num=23, train_loss_step=187.0, val_loss=900.0, train_loss_epoch=875.0]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 18:  46%|████▋     | 31/67 [00:07<00:09,  3.88it/s, loss=585, v_num=23, train_loss_step=187.0, val_loss=900.0, train_loss_epoch=875.0]\n",
      "Epoch 18:  48%|████▊     | 32/67 [00:08<00:09,  3.86it/s, loss=585, v_num=23, train_loss_step=187.0, val_loss=900.0, train_loss_epoch=875.0]\n",
      "Epoch 18:  49%|████▉     | 33/67 [00:08<00:08,  3.84it/s, loss=585, v_num=23, train_loss_step=187.0, val_loss=900.0, train_loss_epoch=875.0]\n",
      "Epoch 18:  51%|█████     | 34/67 [00:08<00:08,  3.79it/s, loss=585, v_num=23, train_loss_step=187.0, val_loss=900.0, train_loss_epoch=875.0]\n",
      "Epoch 18:  52%|█████▏    | 35/67 [00:09<00:08,  3.78it/s, loss=585, v_num=23, train_loss_step=187.0, val_loss=900.0, train_loss_epoch=875.0]\n",
      "Epoch 18:  54%|█████▎    | 36/67 [00:09<00:08,  3.78it/s, loss=585, v_num=23, train_loss_step=187.0, val_loss=900.0, train_loss_epoch=875.0]\n",
      "Epoch 18:  55%|█████▌    | 37/67 [00:10<00:08,  3.68it/s, loss=585, v_num=23, train_loss_step=187.0, val_loss=900.0, train_loss_epoch=875.0]\n",
      "Epoch 18:  57%|█████▋    | 38/67 [00:10<00:07,  3.68it/s, loss=585, v_num=23, train_loss_step=187.0, val_loss=900.0, train_loss_epoch=875.0]\n",
      "Epoch 18:  58%|█████▊    | 39/67 [00:10<00:07,  3.67it/s, loss=585, v_num=23, train_loss_step=187.0, val_loss=900.0, train_loss_epoch=875.0]\n",
      "Epoch 18:  60%|█████▉    | 40/67 [00:11<00:07,  3.59it/s, loss=585, v_num=23, train_loss_step=187.0, val_loss=900.0, train_loss_epoch=875.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 18:  61%|██████    | 41/67 [00:11<00:07,  3.55it/s, loss=585, v_num=23, train_loss_step=187.0, val_loss=900.0, train_loss_epoch=875.0]\n",
      "Epoch 18:  63%|██████▎   | 42/67 [00:11<00:07,  3.54it/s, loss=585, v_num=23, train_loss_step=187.0, val_loss=900.0, train_loss_epoch=875.0]\n",
      "Epoch 18:  64%|██████▍   | 43/67 [00:12<00:06,  3.53it/s, loss=585, v_num=23, train_loss_step=187.0, val_loss=900.0, train_loss_epoch=875.0]\n",
      "Epoch 18:  66%|██████▌   | 44/67 [00:12<00:06,  3.53it/s, loss=585, v_num=23, train_loss_step=187.0, val_loss=900.0, train_loss_epoch=875.0]\n",
      "Epoch 18:  67%|██████▋   | 45/67 [00:12<00:06,  3.52it/s, loss=585, v_num=23, train_loss_step=187.0, val_loss=900.0, train_loss_epoch=875.0]\n",
      "Epoch 18:  69%|██████▊   | 46/67 [00:13<00:06,  3.48it/s, loss=585, v_num=23, train_loss_step=187.0, val_loss=900.0, train_loss_epoch=875.0]\n",
      "Epoch 18:  70%|███████   | 47/67 [00:13<00:05,  3.48it/s, loss=585, v_num=23, train_loss_step=187.0, val_loss=900.0, train_loss_epoch=875.0]\n",
      "Epoch 18:  72%|███████▏  | 48/67 [00:13<00:05,  3.48it/s, loss=585, v_num=23, train_loss_step=187.0, val_loss=900.0, train_loss_epoch=875.0]\n",
      "Epoch 18:  73%|███████▎  | 49/67 [00:14<00:05,  3.42it/s, loss=585, v_num=23, train_loss_step=187.0, val_loss=900.0, train_loss_epoch=875.0]\n",
      "Epoch 18:  75%|███████▍  | 50/67 [00:14<00:04,  3.42it/s, loss=585, v_num=23, train_loss_step=187.0, val_loss=900.0, train_loss_epoch=875.0]\n",
      "Epoch 18:  76%|███████▌  | 51/67 [00:15<00:04,  3.39it/s, loss=585, v_num=23, train_loss_step=187.0, val_loss=900.0, train_loss_epoch=875.0]\n",
      "Epoch 18:  78%|███████▊  | 52/67 [00:16<00:04,  3.21it/s, loss=585, v_num=23, train_loss_step=187.0, val_loss=900.0, train_loss_epoch=875.0]\n",
      "Epoch 18:  79%|███████▉  | 53/67 [00:16<00:04,  3.21it/s, loss=585, v_num=23, train_loss_step=187.0, val_loss=900.0, train_loss_epoch=875.0]\n",
      "Epoch 18:  81%|████████  | 54/67 [00:16<00:04,  3.21it/s, loss=585, v_num=23, train_loss_step=187.0, val_loss=900.0, train_loss_epoch=875.0]\n",
      "Epoch 18:  82%|████████▏ | 55/67 [00:17<00:03,  3.17it/s, loss=585, v_num=23, train_loss_step=187.0, val_loss=900.0, train_loss_epoch=875.0]\n",
      "Epoch 18:  84%|████████▎ | 56/67 [00:17<00:03,  3.18it/s, loss=585, v_num=23, train_loss_step=187.0, val_loss=900.0, train_loss_epoch=875.0]\n",
      "Epoch 18:  85%|████████▌ | 57/67 [00:17<00:03,  3.18it/s, loss=585, v_num=23, train_loss_step=187.0, val_loss=900.0, train_loss_epoch=875.0]\n",
      "Epoch 18:  87%|████████▋ | 58/67 [00:18<00:02,  3.15it/s, loss=585, v_num=23, train_loss_step=187.0, val_loss=900.0, train_loss_epoch=875.0]\n",
      "Epoch 18:  88%|████████▊ | 59/67 [00:18<00:02,  3.16it/s, loss=585, v_num=23, train_loss_step=187.0, val_loss=900.0, train_loss_epoch=875.0]\n",
      "Epoch 18:  90%|████████▉ | 60/67 [00:18<00:02,  3.16it/s, loss=585, v_num=23, train_loss_step=187.0, val_loss=900.0, train_loss_epoch=875.0]\n",
      "Epoch 18:  91%|█████████ | 61/67 [00:19<00:01,  3.11it/s, loss=585, v_num=23, train_loss_step=187.0, val_loss=900.0, train_loss_epoch=875.0]\n",
      "Epoch 18:  93%|█████████▎| 62/67 [00:19<00:01,  3.12it/s, loss=585, v_num=23, train_loss_step=187.0, val_loss=900.0, train_loss_epoch=875.0]\n",
      "Epoch 18:  94%|█████████▍| 63/67 [00:20<00:01,  3.12it/s, loss=585, v_num=23, train_loss_step=187.0, val_loss=900.0, train_loss_epoch=875.0]\n",
      "Epoch 18:  96%|█████████▌| 64/67 [00:20<00:00,  3.11it/s, loss=585, v_num=23, train_loss_step=187.0, val_loss=900.0, train_loss_epoch=875.0]\n",
      "Epoch 18:  97%|█████████▋| 65/67 [00:20<00:00,  3.12it/s, loss=585, v_num=23, train_loss_step=187.0, val_loss=900.0, train_loss_epoch=875.0]\n",
      "Epoch 18:  99%|█████████▊| 66/67 [00:21<00:00,  3.12it/s, loss=585, v_num=23, train_loss_step=187.0, val_loss=900.0, train_loss_epoch=875.0]\n",
      "Epoch 18: 100%|██████████| 67/67 [00:21<00:00,  3.07it/s, loss=585, v_num=23, train_loss_step=187.0, val_loss=900.0, train_loss_epoch=875.0]\n",
      "Epoch 19:  45%|████▍     | 30/67 [00:05<00:07,  5.08it/s, loss=927, v_num=23, train_loss_step=840.0, val_loss=900.0, train_loss_epoch=599.0]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 19:  46%|████▋     | 31/67 [00:07<00:09,  3.95it/s, loss=927, v_num=23, train_loss_step=840.0, val_loss=900.0, train_loss_epoch=599.0]\n",
      "Epoch 19:  48%|████▊     | 32/67 [00:08<00:08,  3.91it/s, loss=927, v_num=23, train_loss_step=840.0, val_loss=900.0, train_loss_epoch=599.0]\n",
      "Epoch 19:  49%|████▉     | 33/67 [00:08<00:08,  3.90it/s, loss=927, v_num=23, train_loss_step=840.0, val_loss=900.0, train_loss_epoch=599.0]\n",
      "Epoch 19:  51%|█████     | 34/67 [00:08<00:08,  3.89it/s, loss=927, v_num=23, train_loss_step=840.0, val_loss=900.0, train_loss_epoch=599.0]\n",
      "Epoch 19:  52%|█████▏    | 35/67 [00:09<00:08,  3.87it/s, loss=927, v_num=23, train_loss_step=840.0, val_loss=900.0, train_loss_epoch=599.0]\n",
      "Epoch 19:  54%|█████▎    | 36/67 [00:09<00:08,  3.86it/s, loss=927, v_num=23, train_loss_step=840.0, val_loss=900.0, train_loss_epoch=599.0]\n",
      "Epoch 19:  55%|█████▌    | 37/67 [00:09<00:07,  3.77it/s, loss=927, v_num=23, train_loss_step=840.0, val_loss=900.0, train_loss_epoch=599.0]\n",
      "Epoch 19:  57%|█████▋    | 38/67 [00:10<00:07,  3.75it/s, loss=927, v_num=23, train_loss_step=840.0, val_loss=900.0, train_loss_epoch=599.0]\n",
      "Epoch 19:  58%|█████▊    | 39/67 [00:10<00:07,  3.74it/s, loss=927, v_num=23, train_loss_step=840.0, val_loss=900.0, train_loss_epoch=599.0]\n",
      "Epoch 19:  60%|█████▉    | 40/67 [00:10<00:07,  3.66it/s, loss=927, v_num=23, train_loss_step=840.0, val_loss=900.0, train_loss_epoch=599.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 19:  61%|██████    | 41/67 [00:11<00:07,  3.61it/s, loss=927, v_num=23, train_loss_step=840.0, val_loss=900.0, train_loss_epoch=599.0]\n",
      "Epoch 19:  63%|██████▎   | 42/67 [00:11<00:06,  3.61it/s, loss=927, v_num=23, train_loss_step=840.0, val_loss=900.0, train_loss_epoch=599.0]\n",
      "Epoch 19:  64%|██████▍   | 43/67 [00:12<00:07,  3.33it/s, loss=927, v_num=23, train_loss_step=840.0, val_loss=900.0, train_loss_epoch=599.0]\n",
      "Epoch 19:  66%|██████▌   | 44/67 [00:13<00:06,  3.34it/s, loss=927, v_num=23, train_loss_step=840.0, val_loss=900.0, train_loss_epoch=599.0]\n",
      "Epoch 19:  67%|██████▋   | 45/67 [00:13<00:06,  3.34it/s, loss=927, v_num=23, train_loss_step=840.0, val_loss=900.0, train_loss_epoch=599.0]\n",
      "Epoch 19:  69%|██████▊   | 46/67 [00:14<00:06,  3.27it/s, loss=927, v_num=23, train_loss_step=840.0, val_loss=900.0, train_loss_epoch=599.0]\n",
      "Epoch 19:  70%|███████   | 47/67 [00:14<00:06,  3.28it/s, loss=927, v_num=23, train_loss_step=840.0, val_loss=900.0, train_loss_epoch=599.0]\n",
      "Epoch 19:  72%|███████▏  | 48/67 [00:14<00:05,  3.29it/s, loss=927, v_num=23, train_loss_step=840.0, val_loss=900.0, train_loss_epoch=599.0]\n",
      "Epoch 19:  73%|███████▎  | 49/67 [00:15<00:05,  3.23it/s, loss=927, v_num=23, train_loss_step=840.0, val_loss=900.0, train_loss_epoch=599.0]\n",
      "Epoch 19:  75%|███████▍  | 50/67 [00:15<00:05,  3.24it/s, loss=927, v_num=23, train_loss_step=840.0, val_loss=900.0, train_loss_epoch=599.0]\n",
      "Epoch 19:  76%|███████▌  | 51/67 [00:15<00:04,  3.22it/s, loss=927, v_num=23, train_loss_step=840.0, val_loss=900.0, train_loss_epoch=599.0]\n",
      "Epoch 19:  78%|███████▊  | 52/67 [00:16<00:04,  3.20it/s, loss=927, v_num=23, train_loss_step=840.0, val_loss=900.0, train_loss_epoch=599.0]\n",
      "Epoch 19:  79%|███████▉  | 53/67 [00:16<00:04,  3.21it/s, loss=927, v_num=23, train_loss_step=840.0, val_loss=900.0, train_loss_epoch=599.0]\n",
      "Epoch 19:  81%|████████  | 54/67 [00:16<00:04,  3.21it/s, loss=927, v_num=23, train_loss_step=840.0, val_loss=900.0, train_loss_epoch=599.0]\n",
      "Epoch 19:  82%|████████▏ | 55/67 [00:17<00:03,  3.17it/s, loss=927, v_num=23, train_loss_step=840.0, val_loss=900.0, train_loss_epoch=599.0]\n",
      "Epoch 19:  84%|████████▎ | 56/67 [00:17<00:03,  3.18it/s, loss=927, v_num=23, train_loss_step=840.0, val_loss=900.0, train_loss_epoch=599.0]\n",
      "Epoch 19:  85%|████████▌ | 57/67 [00:17<00:03,  3.18it/s, loss=927, v_num=23, train_loss_step=840.0, val_loss=900.0, train_loss_epoch=599.0]\n",
      "Epoch 19:  87%|████████▋ | 58/67 [00:18<00:02,  3.15it/s, loss=927, v_num=23, train_loss_step=840.0, val_loss=900.0, train_loss_epoch=599.0]\n",
      "Epoch 19:  88%|████████▊ | 59/67 [00:18<00:02,  3.15it/s, loss=927, v_num=23, train_loss_step=840.0, val_loss=900.0, train_loss_epoch=599.0]\n",
      "Epoch 19:  90%|████████▉ | 60/67 [00:19<00:02,  3.16it/s, loss=927, v_num=23, train_loss_step=840.0, val_loss=900.0, train_loss_epoch=599.0]\n",
      "Epoch 19:  91%|█████████ | 61/67 [00:19<00:01,  3.11it/s, loss=927, v_num=23, train_loss_step=840.0, val_loss=900.0, train_loss_epoch=599.0]\n",
      "Epoch 19:  93%|█████████▎| 62/67 [00:19<00:01,  3.11it/s, loss=927, v_num=23, train_loss_step=840.0, val_loss=900.0, train_loss_epoch=599.0]\n",
      "Epoch 19:  94%|█████████▍| 63/67 [00:20<00:01,  3.12it/s, loss=927, v_num=23, train_loss_step=840.0, val_loss=900.0, train_loss_epoch=599.0]\n",
      "Epoch 19:  96%|█████████▌| 64/67 [00:20<00:00,  3.10it/s, loss=927, v_num=23, train_loss_step=840.0, val_loss=900.0, train_loss_epoch=599.0]\n",
      "Epoch 19:  97%|█████████▋| 65/67 [00:20<00:00,  3.11it/s, loss=927, v_num=23, train_loss_step=840.0, val_loss=900.0, train_loss_epoch=599.0]\n",
      "Epoch 19:  99%|█████████▊| 66/67 [00:21<00:00,  3.11it/s, loss=927, v_num=23, train_loss_step=840.0, val_loss=900.0, train_loss_epoch=599.0]\n",
      "Epoch 19: 100%|██████████| 67/67 [00:21<00:00,  3.05it/s, loss=927, v_num=23, train_loss_step=840.0, val_loss=900.0, train_loss_epoch=599.0]\n",
      "Epoch 20:  45%|████▍     | 30/67 [00:06<00:07,  4.95it/s, loss=873, v_num=23, train_loss_step=365.0, val_loss=900.0, train_loss_epoch=833.0]       \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 20:  46%|████▋     | 31/67 [00:08<00:09,  3.82it/s, loss=873, v_num=23, train_loss_step=365.0, val_loss=900.0, train_loss_epoch=833.0]\n",
      "Epoch 20:  48%|████▊     | 32/67 [00:08<00:09,  3.80it/s, loss=873, v_num=23, train_loss_step=365.0, val_loss=900.0, train_loss_epoch=833.0]\n",
      "Epoch 20:  49%|████▉     | 33/67 [00:08<00:08,  3.79it/s, loss=873, v_num=23, train_loss_step=365.0, val_loss=900.0, train_loss_epoch=833.0]\n",
      "Epoch 20:  51%|█████     | 34/67 [00:09<00:09,  3.47it/s, loss=873, v_num=23, train_loss_step=365.0, val_loss=900.0, train_loss_epoch=833.0]\n",
      "Epoch 20:  52%|█████▏    | 35/67 [00:10<00:09,  3.46it/s, loss=873, v_num=23, train_loss_step=365.0, val_loss=900.0, train_loss_epoch=833.0]\n",
      "Epoch 20:  54%|█████▎    | 36/67 [00:10<00:08,  3.46it/s, loss=873, v_num=23, train_loss_step=365.0, val_loss=900.0, train_loss_epoch=833.0]\n",
      "Epoch 20:  55%|█████▌    | 37/67 [00:10<00:08,  3.38it/s, loss=873, v_num=23, train_loss_step=365.0, val_loss=900.0, train_loss_epoch=833.0]\n",
      "Epoch 20:  57%|█████▋    | 38/67 [00:11<00:08,  3.38it/s, loss=873, v_num=23, train_loss_step=365.0, val_loss=900.0, train_loss_epoch=833.0]\n",
      "Epoch 20:  58%|█████▊    | 39/67 [00:11<00:08,  3.39it/s, loss=873, v_num=23, train_loss_step=365.0, val_loss=900.0, train_loss_epoch=833.0]\n",
      "Epoch 20:  60%|█████▉    | 40/67 [00:12<00:08,  3.33it/s, loss=873, v_num=23, train_loss_step=365.0, val_loss=900.0, train_loss_epoch=833.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 20:  61%|██████    | 41/67 [00:12<00:07,  3.30it/s, loss=873, v_num=23, train_loss_step=365.0, val_loss=900.0, train_loss_epoch=833.0]\n",
      "Epoch 20:  63%|██████▎   | 42/67 [00:12<00:07,  3.31it/s, loss=873, v_num=23, train_loss_step=365.0, val_loss=900.0, train_loss_epoch=833.0]\n",
      "Epoch 20:  64%|██████▍   | 43/67 [00:13<00:07,  3.28it/s, loss=873, v_num=23, train_loss_step=365.0, val_loss=900.0, train_loss_epoch=833.0]\n",
      "Epoch 20:  66%|██████▌   | 44/67 [00:13<00:06,  3.29it/s, loss=873, v_num=23, train_loss_step=365.0, val_loss=900.0, train_loss_epoch=833.0]\n",
      "Epoch 20:  67%|██████▋   | 45/67 [00:13<00:06,  3.29it/s, loss=873, v_num=23, train_loss_step=365.0, val_loss=900.0, train_loss_epoch=833.0]\n",
      "Epoch 20:  69%|██████▊   | 46/67 [00:14<00:06,  3.25it/s, loss=873, v_num=23, train_loss_step=365.0, val_loss=900.0, train_loss_epoch=833.0]\n",
      "Epoch 20:  70%|███████   | 47/67 [00:14<00:06,  3.25it/s, loss=873, v_num=23, train_loss_step=365.0, val_loss=900.0, train_loss_epoch=833.0]\n",
      "Epoch 20:  72%|███████▏  | 48/67 [00:14<00:05,  3.26it/s, loss=873, v_num=23, train_loss_step=365.0, val_loss=900.0, train_loss_epoch=833.0]\n",
      "Epoch 20:  73%|███████▎  | 49/67 [00:15<00:05,  3.22it/s, loss=873, v_num=23, train_loss_step=365.0, val_loss=900.0, train_loss_epoch=833.0]\n",
      "Epoch 20:  75%|███████▍  | 50/67 [00:15<00:05,  3.23it/s, loss=873, v_num=23, train_loss_step=365.0, val_loss=900.0, train_loss_epoch=833.0]\n",
      "Epoch 20:  76%|███████▌  | 51/67 [00:15<00:04,  3.20it/s, loss=873, v_num=23, train_loss_step=365.0, val_loss=900.0, train_loss_epoch=833.0]\n",
      "Epoch 20:  78%|███████▊  | 52/67 [00:16<00:04,  3.19it/s, loss=873, v_num=23, train_loss_step=365.0, val_loss=900.0, train_loss_epoch=833.0]\n",
      "Epoch 20:  79%|███████▉  | 53/67 [00:16<00:04,  3.20it/s, loss=873, v_num=23, train_loss_step=365.0, val_loss=900.0, train_loss_epoch=833.0]\n",
      "Epoch 20:  81%|████████  | 54/67 [00:16<00:04,  3.20it/s, loss=873, v_num=23, train_loss_step=365.0, val_loss=900.0, train_loss_epoch=833.0]\n",
      "Epoch 20:  82%|████████▏ | 55/67 [00:17<00:03,  3.17it/s, loss=873, v_num=23, train_loss_step=365.0, val_loss=900.0, train_loss_epoch=833.0]\n",
      "Epoch 20:  84%|████████▎ | 56/67 [00:17<00:03,  3.17it/s, loss=873, v_num=23, train_loss_step=365.0, val_loss=900.0, train_loss_epoch=833.0]\n",
      "Epoch 20:  85%|████████▌ | 57/67 [00:17<00:03,  3.18it/s, loss=873, v_num=23, train_loss_step=365.0, val_loss=900.0, train_loss_epoch=833.0]\n",
      "Epoch 20:  87%|████████▋ | 58/67 [00:18<00:02,  3.14it/s, loss=873, v_num=23, train_loss_step=365.0, val_loss=900.0, train_loss_epoch=833.0]\n",
      "Epoch 20:  88%|████████▊ | 59/67 [00:18<00:02,  3.15it/s, loss=873, v_num=23, train_loss_step=365.0, val_loss=900.0, train_loss_epoch=833.0]\n",
      "Epoch 20:  90%|████████▉ | 60/67 [00:19<00:02,  3.15it/s, loss=873, v_num=23, train_loss_step=365.0, val_loss=900.0, train_loss_epoch=833.0]\n",
      "Epoch 20:  91%|█████████ | 61/67 [00:19<00:01,  3.10it/s, loss=873, v_num=23, train_loss_step=365.0, val_loss=900.0, train_loss_epoch=833.0]\n",
      "Epoch 20:  93%|█████████▎| 62/67 [00:19<00:01,  3.11it/s, loss=873, v_num=23, train_loss_step=365.0, val_loss=900.0, train_loss_epoch=833.0]\n",
      "Epoch 20:  94%|█████████▍| 63/67 [00:20<00:01,  3.11it/s, loss=873, v_num=23, train_loss_step=365.0, val_loss=900.0, train_loss_epoch=833.0]\n",
      "Epoch 20:  96%|█████████▌| 64/67 [00:20<00:00,  3.11it/s, loss=873, v_num=23, train_loss_step=365.0, val_loss=900.0, train_loss_epoch=833.0]\n",
      "Epoch 20:  97%|█████████▋| 65/67 [00:20<00:00,  3.11it/s, loss=873, v_num=23, train_loss_step=365.0, val_loss=900.0, train_loss_epoch=833.0]\n",
      "Epoch 20:  99%|█████████▊| 66/67 [00:21<00:00,  3.12it/s, loss=873, v_num=23, train_loss_step=365.0, val_loss=900.0, train_loss_epoch=833.0]\n",
      "Epoch 20: 100%|██████████| 67/67 [00:22<00:00,  2.94it/s, loss=873, v_num=23, train_loss_step=365.0, val_loss=900.0, train_loss_epoch=833.0]\n",
      "Epoch 21:  45%|████▍     | 30/67 [00:05<00:07,  5.06it/s, loss=715, v_num=23, train_loss_step=616.0, val_loss=900.0, train_loss_epoch=902.0]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 21:  46%|████▋     | 31/67 [00:07<00:09,  3.90it/s, loss=715, v_num=23, train_loss_step=616.0, val_loss=900.0, train_loss_epoch=902.0]\n",
      "Epoch 21:  48%|████▊     | 32/67 [00:08<00:09,  3.88it/s, loss=715, v_num=23, train_loss_step=616.0, val_loss=900.0, train_loss_epoch=902.0]\n",
      "Epoch 21:  49%|████▉     | 33/67 [00:08<00:08,  3.86it/s, loss=715, v_num=23, train_loss_step=616.0, val_loss=900.0, train_loss_epoch=902.0]\n",
      "Epoch 21:  51%|█████     | 34/67 [00:08<00:08,  3.85it/s, loss=715, v_num=23, train_loss_step=616.0, val_loss=900.0, train_loss_epoch=902.0]\n",
      "Epoch 21:  52%|█████▏    | 35/67 [00:09<00:08,  3.83it/s, loss=715, v_num=23, train_loss_step=616.0, val_loss=900.0, train_loss_epoch=902.0]\n",
      "Epoch 21:  54%|█████▎    | 36/67 [00:09<00:08,  3.83it/s, loss=715, v_num=23, train_loss_step=616.0, val_loss=900.0, train_loss_epoch=902.0]\n",
      "Epoch 21:  55%|█████▌    | 37/67 [00:09<00:07,  3.77it/s, loss=715, v_num=23, train_loss_step=616.0, val_loss=900.0, train_loss_epoch=902.0]\n",
      "Epoch 21:  57%|█████▋    | 38/67 [00:10<00:07,  3.76it/s, loss=715, v_num=23, train_loss_step=616.0, val_loss=900.0, train_loss_epoch=902.0]\n",
      "Epoch 21:  58%|█████▊    | 39/67 [00:10<00:07,  3.75it/s, loss=715, v_num=23, train_loss_step=616.0, val_loss=900.0, train_loss_epoch=902.0]\n",
      "Epoch 21:  60%|█████▉    | 40/67 [00:10<00:07,  3.67it/s, loss=715, v_num=23, train_loss_step=616.0, val_loss=900.0, train_loss_epoch=902.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 21:  61%|██████    | 41/67 [00:11<00:07,  3.62it/s, loss=715, v_num=23, train_loss_step=616.0, val_loss=900.0, train_loss_epoch=902.0]\n",
      "Epoch 21:  63%|██████▎   | 42/67 [00:11<00:06,  3.62it/s, loss=715, v_num=23, train_loss_step=616.0, val_loss=900.0, train_loss_epoch=902.0]\n",
      "Epoch 21:  64%|██████▍   | 43/67 [00:11<00:06,  3.59it/s, loss=715, v_num=23, train_loss_step=616.0, val_loss=900.0, train_loss_epoch=902.0]\n",
      "Epoch 21:  66%|██████▌   | 44/67 [00:12<00:06,  3.58it/s, loss=715, v_num=23, train_loss_step=616.0, val_loss=900.0, train_loss_epoch=902.0]\n",
      "Epoch 21:  67%|██████▋   | 45/67 [00:12<00:06,  3.58it/s, loss=715, v_num=23, train_loss_step=616.0, val_loss=900.0, train_loss_epoch=902.0]\n",
      "Epoch 21:  69%|██████▊   | 46/67 [00:13<00:05,  3.53it/s, loss=715, v_num=23, train_loss_step=616.0, val_loss=900.0, train_loss_epoch=902.0]\n",
      "Epoch 21:  70%|███████   | 47/67 [00:13<00:05,  3.53it/s, loss=715, v_num=23, train_loss_step=616.0, val_loss=900.0, train_loss_epoch=902.0]\n",
      "Epoch 21:  72%|███████▏  | 48/67 [00:13<00:05,  3.53it/s, loss=715, v_num=23, train_loss_step=616.0, val_loss=900.0, train_loss_epoch=902.0]\n",
      "Epoch 21:  73%|███████▎  | 49/67 [00:14<00:05,  3.47it/s, loss=715, v_num=23, train_loss_step=616.0, val_loss=900.0, train_loss_epoch=902.0]\n",
      "Epoch 21:  75%|███████▍  | 50/67 [00:14<00:04,  3.47it/s, loss=715, v_num=23, train_loss_step=616.0, val_loss=900.0, train_loss_epoch=902.0]\n",
      "Epoch 21:  76%|███████▌  | 51/67 [00:14<00:04,  3.43it/s, loss=715, v_num=23, train_loss_step=616.0, val_loss=900.0, train_loss_epoch=902.0]\n",
      "Epoch 21:  78%|███████▊  | 52/67 [00:15<00:04,  3.43it/s, loss=715, v_num=23, train_loss_step=616.0, val_loss=900.0, train_loss_epoch=902.0]\n",
      "Epoch 21:  79%|███████▉  | 53/67 [00:15<00:04,  3.43it/s, loss=715, v_num=23, train_loss_step=616.0, val_loss=900.0, train_loss_epoch=902.0]\n",
      "Epoch 21:  81%|████████  | 54/67 [00:15<00:03,  3.43it/s, loss=715, v_num=23, train_loss_step=616.0, val_loss=900.0, train_loss_epoch=902.0]\n",
      "Epoch 21:  82%|████████▏ | 55/67 [00:16<00:03,  3.40it/s, loss=715, v_num=23, train_loss_step=616.0, val_loss=900.0, train_loss_epoch=902.0]\n",
      "Epoch 21:  84%|████████▎ | 56/67 [00:16<00:03,  3.39it/s, loss=715, v_num=23, train_loss_step=616.0, val_loss=900.0, train_loss_epoch=902.0]\n",
      "Epoch 21:  85%|████████▌ | 57/67 [00:16<00:02,  3.40it/s, loss=715, v_num=23, train_loss_step=616.0, val_loss=900.0, train_loss_epoch=902.0]\n",
      "Epoch 21:  87%|████████▋ | 58/67 [00:17<00:02,  3.36it/s, loss=715, v_num=23, train_loss_step=616.0, val_loss=900.0, train_loss_epoch=902.0]\n",
      "Epoch 21:  88%|████████▊ | 59/67 [00:17<00:02,  3.36it/s, loss=715, v_num=23, train_loss_step=616.0, val_loss=900.0, train_loss_epoch=902.0]\n",
      "Epoch 21:  90%|████████▉ | 60/67 [00:17<00:02,  3.36it/s, loss=715, v_num=23, train_loss_step=616.0, val_loss=900.0, train_loss_epoch=902.0]\n",
      "Epoch 21:  91%|█████████ | 61/67 [00:18<00:01,  3.30it/s, loss=715, v_num=23, train_loss_step=616.0, val_loss=900.0, train_loss_epoch=902.0]\n",
      "Epoch 21:  93%|█████████▎| 62/67 [00:18<00:01,  3.30it/s, loss=715, v_num=23, train_loss_step=616.0, val_loss=900.0, train_loss_epoch=902.0]\n",
      "Epoch 21:  94%|█████████▍| 63/67 [00:19<00:01,  3.31it/s, loss=715, v_num=23, train_loss_step=616.0, val_loss=900.0, train_loss_epoch=902.0]\n",
      "Epoch 21:  96%|█████████▌| 64/67 [00:19<00:00,  3.30it/s, loss=715, v_num=23, train_loss_step=616.0, val_loss=900.0, train_loss_epoch=902.0]\n",
      "Epoch 21:  97%|█████████▋| 65/67 [00:19<00:00,  3.30it/s, loss=715, v_num=23, train_loss_step=616.0, val_loss=900.0, train_loss_epoch=902.0]\n",
      "Epoch 21:  99%|█████████▊| 66/67 [00:19<00:00,  3.30it/s, loss=715, v_num=23, train_loss_step=616.0, val_loss=900.0, train_loss_epoch=902.0]\n",
      "Epoch 21: 100%|██████████| 67/67 [00:20<00:00,  3.23it/s, loss=715, v_num=23, train_loss_step=616.0, val_loss=900.0, train_loss_epoch=902.0]\n",
      "Epoch 22:  45%|████▍     | 30/67 [00:05<00:07,  5.06it/s, loss=873, v_num=23, train_loss_step=591.0, val_loss=900.0, train_loss_epoch=782.0]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 22:  46%|████▋     | 31/67 [00:07<00:09,  3.89it/s, loss=873, v_num=23, train_loss_step=591.0, val_loss=900.0, train_loss_epoch=782.0]\n",
      "Epoch 22:  48%|████▊     | 32/67 [00:08<00:09,  3.87it/s, loss=873, v_num=23, train_loss_step=591.0, val_loss=900.0, train_loss_epoch=782.0]\n",
      "Epoch 22:  49%|████▉     | 33/67 [00:08<00:08,  3.85it/s, loss=873, v_num=23, train_loss_step=591.0, val_loss=900.0, train_loss_epoch=782.0]\n",
      "Epoch 22:  51%|█████     | 34/67 [00:08<00:08,  3.84it/s, loss=873, v_num=23, train_loss_step=591.0, val_loss=900.0, train_loss_epoch=782.0]\n",
      "Epoch 22:  52%|█████▏    | 35/67 [00:09<00:08,  3.82it/s, loss=873, v_num=23, train_loss_step=591.0, val_loss=900.0, train_loss_epoch=782.0]\n",
      "Epoch 22:  54%|█████▎    | 36/67 [00:09<00:08,  3.80it/s, loss=873, v_num=23, train_loss_step=591.0, val_loss=900.0, train_loss_epoch=782.0]\n",
      "Epoch 22:  55%|█████▌    | 37/67 [00:09<00:07,  3.75it/s, loss=873, v_num=23, train_loss_step=591.0, val_loss=900.0, train_loss_epoch=782.0]\n",
      "Epoch 22:  57%|█████▋    | 38/67 [00:10<00:07,  3.75it/s, loss=873, v_num=23, train_loss_step=591.0, val_loss=900.0, train_loss_epoch=782.0]\n",
      "Epoch 22:  58%|█████▊    | 39/67 [00:10<00:07,  3.74it/s, loss=873, v_num=23, train_loss_step=591.0, val_loss=900.0, train_loss_epoch=782.0]\n",
      "Epoch 22:  60%|█████▉    | 40/67 [00:10<00:07,  3.66it/s, loss=873, v_num=23, train_loss_step=591.0, val_loss=900.0, train_loss_epoch=782.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 22:  61%|██████    | 41/67 [00:11<00:07,  3.62it/s, loss=873, v_num=23, train_loss_step=591.0, val_loss=900.0, train_loss_epoch=782.0]\n",
      "Epoch 22:  63%|██████▎   | 42/67 [00:11<00:06,  3.61it/s, loss=873, v_num=23, train_loss_step=591.0, val_loss=900.0, train_loss_epoch=782.0]\n",
      "Epoch 22:  64%|██████▍   | 43/67 [00:11<00:06,  3.59it/s, loss=873, v_num=23, train_loss_step=591.0, val_loss=900.0, train_loss_epoch=782.0]\n",
      "Epoch 22:  66%|██████▌   | 44/67 [00:12<00:06,  3.58it/s, loss=873, v_num=23, train_loss_step=591.0, val_loss=900.0, train_loss_epoch=782.0]\n",
      "Epoch 22:  67%|██████▋   | 45/67 [00:12<00:06,  3.58it/s, loss=873, v_num=23, train_loss_step=591.0, val_loss=900.0, train_loss_epoch=782.0]\n",
      "Epoch 22:  69%|██████▊   | 46/67 [00:13<00:05,  3.53it/s, loss=873, v_num=23, train_loss_step=591.0, val_loss=900.0, train_loss_epoch=782.0]\n",
      "Epoch 22:  70%|███████   | 47/67 [00:13<00:05,  3.53it/s, loss=873, v_num=23, train_loss_step=591.0, val_loss=900.0, train_loss_epoch=782.0]\n",
      "Epoch 22:  72%|███████▏  | 48/67 [00:13<00:05,  3.53it/s, loss=873, v_num=23, train_loss_step=591.0, val_loss=900.0, train_loss_epoch=782.0]\n",
      "Epoch 22:  73%|███████▎  | 49/67 [00:14<00:05,  3.46it/s, loss=873, v_num=23, train_loss_step=591.0, val_loss=900.0, train_loss_epoch=782.0]\n",
      "Epoch 22:  75%|███████▍  | 50/67 [00:14<00:04,  3.46it/s, loss=873, v_num=23, train_loss_step=591.0, val_loss=900.0, train_loss_epoch=782.0]\n",
      "Epoch 22:  76%|███████▌  | 51/67 [00:14<00:04,  3.42it/s, loss=873, v_num=23, train_loss_step=591.0, val_loss=900.0, train_loss_epoch=782.0]\n",
      "Epoch 22:  78%|███████▊  | 52/67 [00:15<00:04,  3.39it/s, loss=873, v_num=23, train_loss_step=591.0, val_loss=900.0, train_loss_epoch=782.0]\n",
      "Epoch 22:  79%|███████▉  | 53/67 [00:15<00:04,  3.39it/s, loss=873, v_num=23, train_loss_step=591.0, val_loss=900.0, train_loss_epoch=782.0]\n",
      "Epoch 22:  81%|████████  | 54/67 [00:15<00:03,  3.39it/s, loss=873, v_num=23, train_loss_step=591.0, val_loss=900.0, train_loss_epoch=782.0]\n",
      "Epoch 22:  82%|████████▏ | 55/67 [00:16<00:03,  3.36it/s, loss=873, v_num=23, train_loss_step=591.0, val_loss=900.0, train_loss_epoch=782.0]\n",
      "Epoch 22:  84%|████████▎ | 56/67 [00:16<00:03,  3.36it/s, loss=873, v_num=23, train_loss_step=591.0, val_loss=900.0, train_loss_epoch=782.0]\n",
      "Epoch 22:  85%|████████▌ | 57/67 [00:16<00:02,  3.36it/s, loss=873, v_num=23, train_loss_step=591.0, val_loss=900.0, train_loss_epoch=782.0]\n",
      "Epoch 22:  87%|████████▋ | 58/67 [00:17<00:02,  3.33it/s, loss=873, v_num=23, train_loss_step=591.0, val_loss=900.0, train_loss_epoch=782.0]\n",
      "Epoch 22:  88%|████████▊ | 59/67 [00:17<00:02,  3.33it/s, loss=873, v_num=23, train_loss_step=591.0, val_loss=900.0, train_loss_epoch=782.0]\n",
      "Epoch 22:  90%|████████▉ | 60/67 [00:17<00:02,  3.33it/s, loss=873, v_num=23, train_loss_step=591.0, val_loss=900.0, train_loss_epoch=782.0]\n",
      "Epoch 22:  91%|█████████ | 61/67 [00:18<00:01,  3.28it/s, loss=873, v_num=23, train_loss_step=591.0, val_loss=900.0, train_loss_epoch=782.0]\n",
      "Epoch 22:  93%|█████████▎| 62/67 [00:18<00:01,  3.28it/s, loss=873, v_num=23, train_loss_step=591.0, val_loss=900.0, train_loss_epoch=782.0]\n",
      "Epoch 22:  94%|█████████▍| 63/67 [00:19<00:01,  3.28it/s, loss=873, v_num=23, train_loss_step=591.0, val_loss=900.0, train_loss_epoch=782.0]\n",
      "Epoch 22:  96%|█████████▌| 64/67 [00:19<00:00,  3.28it/s, loss=873, v_num=23, train_loss_step=591.0, val_loss=900.0, train_loss_epoch=782.0]\n",
      "Epoch 22:  97%|█████████▋| 65/67 [00:19<00:00,  3.28it/s, loss=873, v_num=23, train_loss_step=591.0, val_loss=900.0, train_loss_epoch=782.0]\n",
      "Epoch 22:  99%|█████████▊| 66/67 [00:20<00:00,  3.28it/s, loss=873, v_num=23, train_loss_step=591.0, val_loss=900.0, train_loss_epoch=782.0]\n",
      "Epoch 22: 100%|██████████| 67/67 [00:20<00:00,  3.22it/s, loss=873, v_num=23, train_loss_step=591.0, val_loss=900.0, train_loss_epoch=782.0]\n",
      "Epoch 23:  45%|████▍     | 30/67 [00:05<00:07,  5.01it/s, loss=586, v_num=23, train_loss_step=484.0, val_loss=900.0, train_loss_epoch=910.0]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 23:  46%|████▋     | 31/67 [00:08<00:09,  3.86it/s, loss=586, v_num=23, train_loss_step=484.0, val_loss=900.0, train_loss_epoch=910.0]\n",
      "Epoch 23:  48%|████▊     | 32/67 [00:08<00:09,  3.84it/s, loss=586, v_num=23, train_loss_step=484.0, val_loss=900.0, train_loss_epoch=910.0]\n",
      "Epoch 23:  49%|████▉     | 33/67 [00:08<00:08,  3.83it/s, loss=586, v_num=23, train_loss_step=484.0, val_loss=900.0, train_loss_epoch=910.0]\n",
      "Epoch 23:  51%|█████     | 34/67 [00:08<00:08,  3.82it/s, loss=586, v_num=23, train_loss_step=484.0, val_loss=900.0, train_loss_epoch=910.0]\n",
      "Epoch 23:  52%|█████▏    | 35/67 [00:09<00:08,  3.80it/s, loss=586, v_num=23, train_loss_step=484.0, val_loss=900.0, train_loss_epoch=910.0]\n",
      "Epoch 23:  54%|█████▎    | 36/67 [00:09<00:08,  3.79it/s, loss=586, v_num=23, train_loss_step=484.0, val_loss=900.0, train_loss_epoch=910.0]\n",
      "Epoch 23:  55%|█████▌    | 37/67 [00:09<00:08,  3.72it/s, loss=586, v_num=23, train_loss_step=484.0, val_loss=900.0, train_loss_epoch=910.0]\n",
      "Epoch 23:  57%|█████▋    | 38/67 [00:10<00:07,  3.70it/s, loss=586, v_num=23, train_loss_step=484.0, val_loss=900.0, train_loss_epoch=910.0]\n",
      "Epoch 23:  58%|█████▊    | 39/67 [00:10<00:07,  3.70it/s, loss=586, v_num=23, train_loss_step=484.0, val_loss=900.0, train_loss_epoch=910.0]\n",
      "Epoch 23:  60%|█████▉    | 40/67 [00:11<00:07,  3.63it/s, loss=586, v_num=23, train_loss_step=484.0, val_loss=900.0, train_loss_epoch=910.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 23:  61%|██████    | 41/67 [00:11<00:07,  3.58it/s, loss=586, v_num=23, train_loss_step=484.0, val_loss=900.0, train_loss_epoch=910.0]\n",
      "Epoch 23:  63%|██████▎   | 42/67 [00:11<00:06,  3.58it/s, loss=586, v_num=23, train_loss_step=484.0, val_loss=900.0, train_loss_epoch=910.0]\n",
      "Epoch 23:  64%|██████▍   | 43/67 [00:12<00:06,  3.56it/s, loss=586, v_num=23, train_loss_step=484.0, val_loss=900.0, train_loss_epoch=910.0]\n",
      "Epoch 23:  66%|██████▌   | 44/67 [00:12<00:06,  3.55it/s, loss=586, v_num=23, train_loss_step=484.0, val_loss=900.0, train_loss_epoch=910.0]\n",
      "Epoch 23:  67%|██████▋   | 45/67 [00:12<00:06,  3.55it/s, loss=586, v_num=23, train_loss_step=484.0, val_loss=900.0, train_loss_epoch=910.0]\n",
      "Epoch 23:  69%|██████▊   | 46/67 [00:13<00:06,  3.50it/s, loss=586, v_num=23, train_loss_step=484.0, val_loss=900.0, train_loss_epoch=910.0]\n",
      "Epoch 23:  70%|███████   | 47/67 [00:13<00:05,  3.49it/s, loss=586, v_num=23, train_loss_step=484.0, val_loss=900.0, train_loss_epoch=910.0]\n",
      "Epoch 23:  72%|███████▏  | 48/67 [00:13<00:05,  3.49it/s, loss=586, v_num=23, train_loss_step=484.0, val_loss=900.0, train_loss_epoch=910.0]\n",
      "Epoch 23:  73%|███████▎  | 49/67 [00:14<00:05,  3.43it/s, loss=586, v_num=23, train_loss_step=484.0, val_loss=900.0, train_loss_epoch=910.0]\n",
      "Epoch 23:  75%|███████▍  | 50/67 [00:14<00:04,  3.43it/s, loss=586, v_num=23, train_loss_step=484.0, val_loss=900.0, train_loss_epoch=910.0]\n",
      "Epoch 23:  76%|███████▌  | 51/67 [00:14<00:04,  3.40it/s, loss=586, v_num=23, train_loss_step=484.0, val_loss=900.0, train_loss_epoch=910.0]\n",
      "Epoch 23:  78%|███████▊  | 52/67 [00:15<00:04,  3.39it/s, loss=586, v_num=23, train_loss_step=484.0, val_loss=900.0, train_loss_epoch=910.0]\n",
      "Epoch 23:  79%|███████▉  | 53/67 [00:15<00:04,  3.39it/s, loss=586, v_num=23, train_loss_step=484.0, val_loss=900.0, train_loss_epoch=910.0]\n",
      "Epoch 23:  81%|████████  | 54/67 [00:15<00:03,  3.39it/s, loss=586, v_num=23, train_loss_step=484.0, val_loss=900.0, train_loss_epoch=910.0]\n",
      "Epoch 23:  82%|████████▏ | 55/67 [00:16<00:03,  3.35it/s, loss=586, v_num=23, train_loss_step=484.0, val_loss=900.0, train_loss_epoch=910.0]\n",
      "Epoch 23:  84%|████████▎ | 56/67 [00:16<00:03,  3.35it/s, loss=586, v_num=23, train_loss_step=484.0, val_loss=900.0, train_loss_epoch=910.0]\n",
      "Epoch 23:  85%|████████▌ | 57/67 [00:16<00:02,  3.35it/s, loss=586, v_num=23, train_loss_step=484.0, val_loss=900.0, train_loss_epoch=910.0]\n",
      "Epoch 23:  87%|████████▋ | 58/67 [00:17<00:02,  3.32it/s, loss=586, v_num=23, train_loss_step=484.0, val_loss=900.0, train_loss_epoch=910.0]\n",
      "Epoch 23:  88%|████████▊ | 59/67 [00:17<00:02,  3.32it/s, loss=586, v_num=23, train_loss_step=484.0, val_loss=900.0, train_loss_epoch=910.0]\n",
      "Epoch 23:  90%|████████▉ | 60/67 [00:18<00:02,  3.32it/s, loss=586, v_num=23, train_loss_step=484.0, val_loss=900.0, train_loss_epoch=910.0]\n",
      "Epoch 23:  91%|█████████ | 61/67 [00:18<00:01,  3.26it/s, loss=586, v_num=23, train_loss_step=484.0, val_loss=900.0, train_loss_epoch=910.0]\n",
      "Epoch 23:  93%|█████████▎| 62/67 [00:18<00:01,  3.27it/s, loss=586, v_num=23, train_loss_step=484.0, val_loss=900.0, train_loss_epoch=910.0]\n",
      "Epoch 23:  94%|█████████▍| 63/67 [00:19<00:01,  3.27it/s, loss=586, v_num=23, train_loss_step=484.0, val_loss=900.0, train_loss_epoch=910.0]\n",
      "Epoch 23:  96%|█████████▌| 64/67 [00:19<00:00,  3.26it/s, loss=586, v_num=23, train_loss_step=484.0, val_loss=900.0, train_loss_epoch=910.0]\n",
      "Epoch 23:  97%|█████████▋| 65/67 [00:19<00:00,  3.26it/s, loss=586, v_num=23, train_loss_step=484.0, val_loss=900.0, train_loss_epoch=910.0]\n",
      "Epoch 23:  99%|█████████▊| 66/67 [00:20<00:00,  3.26it/s, loss=586, v_num=23, train_loss_step=484.0, val_loss=900.0, train_loss_epoch=910.0]\n",
      "Epoch 23: 100%|██████████| 67/67 [00:20<00:00,  3.20it/s, loss=586, v_num=23, train_loss_step=484.0, val_loss=900.0, train_loss_epoch=910.0]\n",
      "Epoch 24:  45%|████▍     | 30/67 [00:06<00:07,  4.95it/s, loss=663, v_num=23, train_loss_step=454.0, val_loss=900.0, train_loss_epoch=580.0]     \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 24:  46%|████▋     | 31/67 [00:08<00:09,  3.85it/s, loss=663, v_num=23, train_loss_step=454.0, val_loss=900.0, train_loss_epoch=580.0]\n",
      "Epoch 24:  48%|████▊     | 32/67 [00:08<00:09,  3.83it/s, loss=663, v_num=23, train_loss_step=454.0, val_loss=900.0, train_loss_epoch=580.0]\n",
      "Epoch 24:  49%|████▉     | 33/67 [00:08<00:08,  3.82it/s, loss=663, v_num=23, train_loss_step=454.0, val_loss=900.0, train_loss_epoch=580.0]\n",
      "Epoch 24:  51%|█████     | 34/67 [00:08<00:08,  3.80it/s, loss=663, v_num=23, train_loss_step=454.0, val_loss=900.0, train_loss_epoch=580.0]\n",
      "Epoch 24:  52%|█████▏    | 35/67 [00:09<00:08,  3.78it/s, loss=663, v_num=23, train_loss_step=454.0, val_loss=900.0, train_loss_epoch=580.0]\n",
      "Epoch 24:  54%|█████▎    | 36/67 [00:09<00:08,  3.78it/s, loss=663, v_num=23, train_loss_step=454.0, val_loss=900.0, train_loss_epoch=580.0]\n",
      "Epoch 24:  55%|█████▌    | 37/67 [00:09<00:08,  3.71it/s, loss=663, v_num=23, train_loss_step=454.0, val_loss=900.0, train_loss_epoch=580.0]\n",
      "Epoch 24:  57%|█████▋    | 38/67 [00:10<00:07,  3.70it/s, loss=663, v_num=23, train_loss_step=454.0, val_loss=900.0, train_loss_epoch=580.0]\n",
      "Epoch 24:  58%|█████▊    | 39/67 [00:10<00:07,  3.70it/s, loss=663, v_num=23, train_loss_step=454.0, val_loss=900.0, train_loss_epoch=580.0]\n",
      "Epoch 24:  60%|█████▉    | 40/67 [00:11<00:07,  3.63it/s, loss=663, v_num=23, train_loss_step=454.0, val_loss=900.0, train_loss_epoch=580.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 24:  61%|██████    | 41/67 [00:11<00:07,  3.58it/s, loss=663, v_num=23, train_loss_step=454.0, val_loss=900.0, train_loss_epoch=580.0]\n",
      "Epoch 24:  63%|██████▎   | 42/67 [00:11<00:06,  3.58it/s, loss=663, v_num=23, train_loss_step=454.0, val_loss=900.0, train_loss_epoch=580.0]\n",
      "Epoch 24:  64%|██████▍   | 43/67 [00:12<00:06,  3.55it/s, loss=663, v_num=23, train_loss_step=454.0, val_loss=900.0, train_loss_epoch=580.0]\n",
      "Epoch 24:  66%|██████▌   | 44/67 [00:12<00:06,  3.55it/s, loss=663, v_num=23, train_loss_step=454.0, val_loss=900.0, train_loss_epoch=580.0]\n",
      "Epoch 24:  67%|██████▋   | 45/67 [00:12<00:06,  3.54it/s, loss=663, v_num=23, train_loss_step=454.0, val_loss=900.0, train_loss_epoch=580.0]\n",
      "Epoch 24:  69%|██████▊   | 46/67 [00:13<00:06,  3.47it/s, loss=663, v_num=23, train_loss_step=454.0, val_loss=900.0, train_loss_epoch=580.0]\n",
      "Epoch 24:  70%|███████   | 47/67 [00:13<00:05,  3.47it/s, loss=663, v_num=23, train_loss_step=454.0, val_loss=900.0, train_loss_epoch=580.0]\n",
      "Epoch 24:  72%|███████▏  | 48/67 [00:13<00:05,  3.47it/s, loss=663, v_num=23, train_loss_step=454.0, val_loss=900.0, train_loss_epoch=580.0]\n",
      "Epoch 24:  73%|███████▎  | 49/67 [00:14<00:05,  3.42it/s, loss=663, v_num=23, train_loss_step=454.0, val_loss=900.0, train_loss_epoch=580.0]\n",
      "Epoch 24:  75%|███████▍  | 50/67 [00:14<00:04,  3.42it/s, loss=663, v_num=23, train_loss_step=454.0, val_loss=900.0, train_loss_epoch=580.0]\n",
      "Epoch 24:  76%|███████▌  | 51/67 [00:15<00:04,  3.39it/s, loss=663, v_num=23, train_loss_step=454.0, val_loss=900.0, train_loss_epoch=580.0]\n",
      "Epoch 24:  78%|███████▊  | 52/67 [00:15<00:04,  3.37it/s, loss=663, v_num=23, train_loss_step=454.0, val_loss=900.0, train_loss_epoch=580.0]\n",
      "Epoch 24:  79%|███████▉  | 53/67 [00:15<00:04,  3.37it/s, loss=663, v_num=23, train_loss_step=454.0, val_loss=900.0, train_loss_epoch=580.0]\n",
      "Epoch 24:  81%|████████  | 54/67 [00:15<00:03,  3.38it/s, loss=663, v_num=23, train_loss_step=454.0, val_loss=900.0, train_loss_epoch=580.0]\n",
      "Epoch 24:  82%|████████▏ | 55/67 [00:16<00:03,  3.34it/s, loss=663, v_num=23, train_loss_step=454.0, val_loss=900.0, train_loss_epoch=580.0]\n",
      "Epoch 24:  84%|████████▎ | 56/67 [00:16<00:03,  3.34it/s, loss=663, v_num=23, train_loss_step=454.0, val_loss=900.0, train_loss_epoch=580.0]\n",
      "Epoch 24:  85%|████████▌ | 57/67 [00:17<00:02,  3.34it/s, loss=663, v_num=23, train_loss_step=454.0, val_loss=900.0, train_loss_epoch=580.0]\n",
      "Epoch 24:  87%|████████▋ | 58/67 [00:17<00:02,  3.31it/s, loss=663, v_num=23, train_loss_step=454.0, val_loss=900.0, train_loss_epoch=580.0]\n",
      "Epoch 24:  88%|████████▊ | 59/67 [00:17<00:02,  3.31it/s, loss=663, v_num=23, train_loss_step=454.0, val_loss=900.0, train_loss_epoch=580.0]\n",
      "Epoch 24:  90%|████████▉ | 60/67 [00:18<00:02,  3.31it/s, loss=663, v_num=23, train_loss_step=454.0, val_loss=900.0, train_loss_epoch=580.0]\n",
      "Epoch 24:  91%|█████████ | 61/67 [00:19<00:01,  3.10it/s, loss=663, v_num=23, train_loss_step=454.0, val_loss=900.0, train_loss_epoch=580.0]\n",
      "Epoch 24:  93%|█████████▎| 62/67 [00:19<00:01,  3.10it/s, loss=663, v_num=23, train_loss_step=454.0, val_loss=900.0, train_loss_epoch=580.0]\n",
      "Epoch 24:  94%|█████████▍| 63/67 [00:20<00:01,  3.10it/s, loss=663, v_num=23, train_loss_step=454.0, val_loss=900.0, train_loss_epoch=580.0]\n",
      "Epoch 24:  96%|█████████▌| 64/67 [00:20<00:00,  3.10it/s, loss=663, v_num=23, train_loss_step=454.0, val_loss=900.0, train_loss_epoch=580.0]\n",
      "Epoch 24:  97%|█████████▋| 65/67 [00:20<00:00,  3.11it/s, loss=663, v_num=23, train_loss_step=454.0, val_loss=900.0, train_loss_epoch=580.0]\n",
      "Epoch 24:  99%|█████████▊| 66/67 [00:21<00:00,  3.11it/s, loss=663, v_num=23, train_loss_step=454.0, val_loss=900.0, train_loss_epoch=580.0]\n",
      "Epoch 24: 100%|██████████| 67/67 [00:21<00:00,  3.06it/s, loss=663, v_num=23, train_loss_step=454.0, val_loss=900.0, train_loss_epoch=580.0]\n",
      "Epoch 25:  45%|████▍     | 30/67 [00:05<00:07,  5.06it/s, loss=827, v_num=23, train_loss_step=339.0, val_loss=900.0, train_loss_epoch=885.0]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 25:  46%|████▋     | 31/67 [00:07<00:09,  3.90it/s, loss=827, v_num=23, train_loss_step=339.0, val_loss=900.0, train_loss_epoch=885.0]\n",
      "Epoch 25:  48%|████▊     | 32/67 [00:08<00:09,  3.87it/s, loss=827, v_num=23, train_loss_step=339.0, val_loss=900.0, train_loss_epoch=885.0]\n",
      "Epoch 25:  49%|████▉     | 33/67 [00:08<00:08,  3.86it/s, loss=827, v_num=23, train_loss_step=339.0, val_loss=900.0, train_loss_epoch=885.0]\n",
      "Epoch 25:  51%|█████     | 34/67 [00:08<00:08,  3.83it/s, loss=827, v_num=23, train_loss_step=339.0, val_loss=900.0, train_loss_epoch=885.0]\n",
      "Epoch 25:  52%|█████▏    | 35/67 [00:09<00:08,  3.82it/s, loss=827, v_num=23, train_loss_step=339.0, val_loss=900.0, train_loss_epoch=885.0]\n",
      "Epoch 25:  54%|█████▎    | 36/67 [00:09<00:08,  3.82it/s, loss=827, v_num=23, train_loss_step=339.0, val_loss=900.0, train_loss_epoch=885.0]\n",
      "Epoch 25:  55%|█████▌    | 37/67 [00:09<00:08,  3.74it/s, loss=827, v_num=23, train_loss_step=339.0, val_loss=900.0, train_loss_epoch=885.0]\n",
      "Epoch 25:  57%|█████▋    | 38/67 [00:10<00:07,  3.73it/s, loss=827, v_num=23, train_loss_step=339.0, val_loss=900.0, train_loss_epoch=885.0]\n",
      "Epoch 25:  58%|█████▊    | 39/67 [00:10<00:07,  3.73it/s, loss=827, v_num=23, train_loss_step=339.0, val_loss=900.0, train_loss_epoch=885.0]\n",
      "Epoch 25:  60%|█████▉    | 40/67 [00:11<00:07,  3.64it/s, loss=827, v_num=23, train_loss_step=339.0, val_loss=900.0, train_loss_epoch=885.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 25:  61%|██████    | 41/67 [00:11<00:07,  3.59it/s, loss=827, v_num=23, train_loss_step=339.0, val_loss=900.0, train_loss_epoch=885.0]\n",
      "Epoch 25:  63%|██████▎   | 42/67 [00:11<00:06,  3.59it/s, loss=827, v_num=23, train_loss_step=339.0, val_loss=900.0, train_loss_epoch=885.0]\n",
      "Epoch 25:  64%|██████▍   | 43/67 [00:12<00:06,  3.56it/s, loss=827, v_num=23, train_loss_step=339.0, val_loss=900.0, train_loss_epoch=885.0]\n",
      "Epoch 25:  66%|██████▌   | 44/67 [00:12<00:06,  3.56it/s, loss=827, v_num=23, train_loss_step=339.0, val_loss=900.0, train_loss_epoch=885.0]\n",
      "Epoch 25:  67%|██████▋   | 45/67 [00:12<00:06,  3.56it/s, loss=827, v_num=23, train_loss_step=339.0, val_loss=900.0, train_loss_epoch=885.0]\n",
      "Epoch 25:  69%|██████▊   | 46/67 [00:13<00:05,  3.50it/s, loss=827, v_num=23, train_loss_step=339.0, val_loss=900.0, train_loss_epoch=885.0]\n",
      "Epoch 25:  70%|███████   | 47/67 [00:13<00:05,  3.51it/s, loss=827, v_num=23, train_loss_step=339.0, val_loss=900.0, train_loss_epoch=885.0]\n",
      "Epoch 25:  72%|███████▏  | 48/67 [00:13<00:05,  3.51it/s, loss=827, v_num=23, train_loss_step=339.0, val_loss=900.0, train_loss_epoch=885.0]\n",
      "Epoch 25:  73%|███████▎  | 49/67 [00:14<00:05,  3.46it/s, loss=827, v_num=23, train_loss_step=339.0, val_loss=900.0, train_loss_epoch=885.0]\n",
      "Epoch 25:  75%|███████▍  | 50/67 [00:14<00:04,  3.46it/s, loss=827, v_num=23, train_loss_step=339.0, val_loss=900.0, train_loss_epoch=885.0]\n",
      "Epoch 25:  76%|███████▌  | 51/67 [00:14<00:04,  3.43it/s, loss=827, v_num=23, train_loss_step=339.0, val_loss=900.0, train_loss_epoch=885.0]\n",
      "Epoch 25:  78%|███████▊  | 52/67 [00:16<00:04,  3.20it/s, loss=827, v_num=23, train_loss_step=339.0, val_loss=900.0, train_loss_epoch=885.0]\n",
      "Epoch 25:  79%|███████▉  | 53/67 [00:16<00:04,  3.21it/s, loss=827, v_num=23, train_loss_step=339.0, val_loss=900.0, train_loss_epoch=885.0]\n",
      "Epoch 25:  81%|████████  | 54/67 [00:16<00:04,  3.21it/s, loss=827, v_num=23, train_loss_step=339.0, val_loss=900.0, train_loss_epoch=885.0]\n",
      "Epoch 25:  82%|████████▏ | 55/67 [00:17<00:03,  3.15it/s, loss=827, v_num=23, train_loss_step=339.0, val_loss=900.0, train_loss_epoch=885.0]\n",
      "Epoch 25:  84%|████████▎ | 56/67 [00:17<00:03,  3.15it/s, loss=827, v_num=23, train_loss_step=339.0, val_loss=900.0, train_loss_epoch=885.0]\n",
      "Epoch 25:  85%|████████▌ | 57/67 [00:18<00:03,  3.16it/s, loss=827, v_num=23, train_loss_step=339.0, val_loss=900.0, train_loss_epoch=885.0]\n",
      "Epoch 25:  87%|████████▋ | 58/67 [00:18<00:02,  3.13it/s, loss=827, v_num=23, train_loss_step=339.0, val_loss=900.0, train_loss_epoch=885.0]\n",
      "Epoch 25:  88%|████████▊ | 59/67 [00:18<00:02,  3.14it/s, loss=827, v_num=23, train_loss_step=339.0, val_loss=900.0, train_loss_epoch=885.0]\n",
      "Epoch 25:  90%|████████▉ | 60/67 [00:19<00:02,  3.14it/s, loss=827, v_num=23, train_loss_step=339.0, val_loss=900.0, train_loss_epoch=885.0]\n",
      "Epoch 25:  91%|█████████ | 61/67 [00:19<00:01,  3.09it/s, loss=827, v_num=23, train_loss_step=339.0, val_loss=900.0, train_loss_epoch=885.0]\n",
      "Epoch 25:  93%|█████████▎| 62/67 [00:20<00:01,  3.10it/s, loss=827, v_num=23, train_loss_step=339.0, val_loss=900.0, train_loss_epoch=885.0]\n",
      "Epoch 25:  94%|█████████▍| 63/67 [00:20<00:01,  3.10it/s, loss=827, v_num=23, train_loss_step=339.0, val_loss=900.0, train_loss_epoch=885.0]\n",
      "Epoch 25:  96%|█████████▌| 64/67 [00:20<00:00,  3.09it/s, loss=827, v_num=23, train_loss_step=339.0, val_loss=900.0, train_loss_epoch=885.0]\n",
      "Epoch 25:  97%|█████████▋| 65/67 [00:20<00:00,  3.10it/s, loss=827, v_num=23, train_loss_step=339.0, val_loss=900.0, train_loss_epoch=885.0]\n",
      "Epoch 25:  99%|█████████▊| 66/67 [00:21<00:00,  3.11it/s, loss=827, v_num=23, train_loss_step=339.0, val_loss=900.0, train_loss_epoch=885.0]\n",
      "Epoch 25: 100%|██████████| 67/67 [00:22<00:00,  3.04it/s, loss=827, v_num=23, train_loss_step=339.0, val_loss=900.0, train_loss_epoch=885.0]\n",
      "Epoch 26:  45%|████▍     | 30/67 [00:05<00:07,  5.08it/s, loss=806, v_num=23, train_loss_step=690.0, val_loss=900.0, train_loss_epoch=744.0]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 26:  46%|████▋     | 31/67 [00:07<00:09,  3.92it/s, loss=806, v_num=23, train_loss_step=690.0, val_loss=900.0, train_loss_epoch=744.0]\n",
      "Epoch 26:  48%|████▊     | 32/67 [00:08<00:08,  3.90it/s, loss=806, v_num=23, train_loss_step=690.0, val_loss=900.0, train_loss_epoch=744.0]\n",
      "Epoch 26:  49%|████▉     | 33/67 [00:08<00:08,  3.88it/s, loss=806, v_num=23, train_loss_step=690.0, val_loss=900.0, train_loss_epoch=744.0]\n",
      "Epoch 26:  51%|█████     | 34/67 [00:08<00:08,  3.86it/s, loss=806, v_num=23, train_loss_step=690.0, val_loss=900.0, train_loss_epoch=744.0]\n",
      "Epoch 26:  52%|█████▏    | 35/67 [00:09<00:08,  3.84it/s, loss=806, v_num=23, train_loss_step=690.0, val_loss=900.0, train_loss_epoch=744.0]\n",
      "Epoch 26:  54%|█████▎    | 36/67 [00:09<00:08,  3.83it/s, loss=806, v_num=23, train_loss_step=690.0, val_loss=900.0, train_loss_epoch=744.0]\n",
      "Epoch 26:  55%|█████▌    | 37/67 [00:09<00:08,  3.74it/s, loss=806, v_num=23, train_loss_step=690.0, val_loss=900.0, train_loss_epoch=744.0]\n",
      "Epoch 26:  57%|█████▋    | 38/67 [00:10<00:07,  3.74it/s, loss=806, v_num=23, train_loss_step=690.0, val_loss=900.0, train_loss_epoch=744.0]\n",
      "Epoch 26:  58%|█████▊    | 39/67 [00:10<00:07,  3.73it/s, loss=806, v_num=23, train_loss_step=690.0, val_loss=900.0, train_loss_epoch=744.0]\n",
      "Epoch 26:  60%|█████▉    | 40/67 [00:10<00:07,  3.65it/s, loss=806, v_num=23, train_loss_step=690.0, val_loss=900.0, train_loss_epoch=744.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 26:  61%|██████    | 41/67 [00:11<00:07,  3.60it/s, loss=806, v_num=23, train_loss_step=690.0, val_loss=900.0, train_loss_epoch=744.0]\n",
      "Epoch 26:  63%|██████▎   | 42/67 [00:11<00:06,  3.60it/s, loss=806, v_num=23, train_loss_step=690.0, val_loss=900.0, train_loss_epoch=744.0]\n",
      "Epoch 26:  64%|██████▍   | 43/67 [00:13<00:07,  3.28it/s, loss=806, v_num=23, train_loss_step=690.0, val_loss=900.0, train_loss_epoch=744.0]\n",
      "Epoch 26:  66%|██████▌   | 44/67 [00:13<00:07,  3.28it/s, loss=806, v_num=23, train_loss_step=690.0, val_loss=900.0, train_loss_epoch=744.0]\n",
      "Epoch 26:  67%|██████▋   | 45/67 [00:13<00:06,  3.29it/s, loss=806, v_num=23, train_loss_step=690.0, val_loss=900.0, train_loss_epoch=744.0]\n",
      "Epoch 26:  69%|██████▊   | 46/67 [00:14<00:06,  3.23it/s, loss=806, v_num=23, train_loss_step=690.0, val_loss=900.0, train_loss_epoch=744.0]\n",
      "Epoch 26:  70%|███████   | 47/67 [00:14<00:06,  3.24it/s, loss=806, v_num=23, train_loss_step=690.0, val_loss=900.0, train_loss_epoch=744.0]\n",
      "Epoch 26:  72%|███████▏  | 48/67 [00:14<00:05,  3.25it/s, loss=806, v_num=23, train_loss_step=690.0, val_loss=900.0, train_loss_epoch=744.0]\n",
      "Epoch 26:  73%|███████▎  | 49/67 [00:15<00:05,  3.20it/s, loss=806, v_num=23, train_loss_step=690.0, val_loss=900.0, train_loss_epoch=744.0]\n",
      "Epoch 26:  75%|███████▍  | 50/67 [00:15<00:05,  3.21it/s, loss=806, v_num=23, train_loss_step=690.0, val_loss=900.0, train_loss_epoch=744.0]\n",
      "Epoch 26:  76%|███████▌  | 51/67 [00:16<00:05,  3.19it/s, loss=806, v_num=23, train_loss_step=690.0, val_loss=900.0, train_loss_epoch=744.0]\n",
      "Epoch 26:  78%|███████▊  | 52/67 [00:16<00:04,  3.17it/s, loss=806, v_num=23, train_loss_step=690.0, val_loss=900.0, train_loss_epoch=744.0]\n",
      "Epoch 26:  79%|███████▉  | 53/67 [00:16<00:04,  3.17it/s, loss=806, v_num=23, train_loss_step=690.0, val_loss=900.0, train_loss_epoch=744.0]\n",
      "Epoch 26:  81%|████████  | 54/67 [00:16<00:04,  3.18it/s, loss=806, v_num=23, train_loss_step=690.0, val_loss=900.0, train_loss_epoch=744.0]\n",
      "Epoch 26:  82%|████████▏ | 55/67 [00:17<00:03,  3.14it/s, loss=806, v_num=23, train_loss_step=690.0, val_loss=900.0, train_loss_epoch=744.0]\n",
      "Epoch 26:  84%|████████▎ | 56/67 [00:17<00:03,  3.14it/s, loss=806, v_num=23, train_loss_step=690.0, val_loss=900.0, train_loss_epoch=744.0]\n",
      "Epoch 26:  85%|████████▌ | 57/67 [00:18<00:03,  3.15it/s, loss=806, v_num=23, train_loss_step=690.0, val_loss=900.0, train_loss_epoch=744.0]\n",
      "Epoch 26:  87%|████████▋ | 58/67 [00:18<00:02,  3.12it/s, loss=806, v_num=23, train_loss_step=690.0, val_loss=900.0, train_loss_epoch=744.0]\n",
      "Epoch 26:  88%|████████▊ | 59/67 [00:18<00:02,  3.12it/s, loss=806, v_num=23, train_loss_step=690.0, val_loss=900.0, train_loss_epoch=744.0]\n",
      "Epoch 26:  90%|████████▉ | 60/67 [00:19<00:02,  3.13it/s, loss=806, v_num=23, train_loss_step=690.0, val_loss=900.0, train_loss_epoch=744.0]\n",
      "Epoch 26:  91%|█████████ | 61/67 [00:19<00:01,  3.08it/s, loss=806, v_num=23, train_loss_step=690.0, val_loss=900.0, train_loss_epoch=744.0]\n",
      "Epoch 26:  93%|█████████▎| 62/67 [00:20<00:01,  3.09it/s, loss=806, v_num=23, train_loss_step=690.0, val_loss=900.0, train_loss_epoch=744.0]\n",
      "Epoch 26:  94%|█████████▍| 63/67 [00:20<00:01,  3.09it/s, loss=806, v_num=23, train_loss_step=690.0, val_loss=900.0, train_loss_epoch=744.0]\n",
      "Epoch 26:  96%|█████████▌| 64/67 [00:20<00:00,  3.08it/s, loss=806, v_num=23, train_loss_step=690.0, val_loss=900.0, train_loss_epoch=744.0]\n",
      "Epoch 26:  97%|█████████▋| 65/67 [00:21<00:00,  3.09it/s, loss=806, v_num=23, train_loss_step=690.0, val_loss=900.0, train_loss_epoch=744.0]\n",
      "Epoch 26:  99%|█████████▊| 66/67 [00:21<00:00,  3.10it/s, loss=806, v_num=23, train_loss_step=690.0, val_loss=900.0, train_loss_epoch=744.0]\n",
      "Epoch 26: 100%|██████████| 67/67 [00:22<00:00,  3.04it/s, loss=806, v_num=23, train_loss_step=690.0, val_loss=900.0, train_loss_epoch=744.0]\n",
      "Epoch 27:  45%|████▍     | 30/67 [00:06<00:07,  4.98it/s, loss=873, v_num=23, train_loss_step=244.0, val_loss=900.0, train_loss_epoch=753.0]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 27:  46%|████▋     | 31/67 [00:08<00:09,  3.84it/s, loss=873, v_num=23, train_loss_step=244.0, val_loss=900.0, train_loss_epoch=753.0]\n",
      "Epoch 27:  48%|████▊     | 32/67 [00:08<00:09,  3.82it/s, loss=873, v_num=23, train_loss_step=244.0, val_loss=900.0, train_loss_epoch=753.0]\n",
      "Epoch 27:  49%|████▉     | 33/67 [00:08<00:08,  3.81it/s, loss=873, v_num=23, train_loss_step=244.0, val_loss=900.0, train_loss_epoch=753.0]\n",
      "Epoch 27:  51%|█████     | 34/67 [00:10<00:09,  3.39it/s, loss=873, v_num=23, train_loss_step=244.0, val_loss=900.0, train_loss_epoch=753.0]\n",
      "Epoch 27:  52%|█████▏    | 35/67 [00:10<00:09,  3.39it/s, loss=873, v_num=23, train_loss_step=244.0, val_loss=900.0, train_loss_epoch=753.0]\n",
      "Epoch 27:  54%|█████▎    | 36/67 [00:10<00:09,  3.40it/s, loss=873, v_num=23, train_loss_step=244.0, val_loss=900.0, train_loss_epoch=753.0]\n",
      "Epoch 27:  55%|█████▌    | 37/67 [00:11<00:08,  3.34it/s, loss=873, v_num=23, train_loss_step=244.0, val_loss=900.0, train_loss_epoch=753.0]\n",
      "Epoch 27:  57%|█████▋    | 38/67 [00:11<00:08,  3.34it/s, loss=873, v_num=23, train_loss_step=244.0, val_loss=900.0, train_loss_epoch=753.0]\n",
      "Epoch 27:  58%|█████▊    | 39/67 [00:11<00:08,  3.34it/s, loss=873, v_num=23, train_loss_step=244.0, val_loss=900.0, train_loss_epoch=753.0]\n",
      "Epoch 27:  60%|█████▉    | 40/67 [00:12<00:08,  3.29it/s, loss=873, v_num=23, train_loss_step=244.0, val_loss=900.0, train_loss_epoch=753.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 27:  61%|██████    | 41/67 [00:12<00:07,  3.26it/s, loss=873, v_num=23, train_loss_step=244.0, val_loss=900.0, train_loss_epoch=753.0]\n",
      "Epoch 27:  63%|██████▎   | 42/67 [00:12<00:07,  3.26it/s, loss=873, v_num=23, train_loss_step=244.0, val_loss=900.0, train_loss_epoch=753.0]\n",
      "Epoch 27:  64%|██████▍   | 43/67 [00:13<00:07,  3.25it/s, loss=873, v_num=23, train_loss_step=244.0, val_loss=900.0, train_loss_epoch=753.0]\n",
      "Epoch 27:  66%|██████▌   | 44/67 [00:13<00:07,  3.25it/s, loss=873, v_num=23, train_loss_step=244.0, val_loss=900.0, train_loss_epoch=753.0]\n",
      "Epoch 27:  67%|██████▋   | 45/67 [00:13<00:06,  3.26it/s, loss=873, v_num=23, train_loss_step=244.0, val_loss=900.0, train_loss_epoch=753.0]\n",
      "Epoch 27:  69%|██████▊   | 46/67 [00:14<00:06,  3.20it/s, loss=873, v_num=23, train_loss_step=244.0, val_loss=900.0, train_loss_epoch=753.0]\n",
      "Epoch 27:  70%|███████   | 47/67 [00:14<00:06,  3.21it/s, loss=873, v_num=23, train_loss_step=244.0, val_loss=900.0, train_loss_epoch=753.0]\n",
      "Epoch 27:  72%|███████▏  | 48/67 [00:14<00:05,  3.22it/s, loss=873, v_num=23, train_loss_step=244.0, val_loss=900.0, train_loss_epoch=753.0]\n",
      "Epoch 27:  73%|███████▎  | 49/67 [00:15<00:05,  3.17it/s, loss=873, v_num=23, train_loss_step=244.0, val_loss=900.0, train_loss_epoch=753.0]\n",
      "Epoch 27:  75%|███████▍  | 50/67 [00:15<00:05,  3.18it/s, loss=873, v_num=23, train_loss_step=244.0, val_loss=900.0, train_loss_epoch=753.0]\n",
      "Epoch 27:  76%|███████▌  | 51/67 [00:16<00:05,  3.16it/s, loss=873, v_num=23, train_loss_step=244.0, val_loss=900.0, train_loss_epoch=753.0]\n",
      "Epoch 27:  78%|███████▊  | 52/67 [00:16<00:04,  3.13it/s, loss=873, v_num=23, train_loss_step=244.0, val_loss=900.0, train_loss_epoch=753.0]\n",
      "Epoch 27:  79%|███████▉  | 53/67 [00:16<00:04,  3.14it/s, loss=873, v_num=23, train_loss_step=244.0, val_loss=900.0, train_loss_epoch=753.0]\n",
      "Epoch 27:  81%|████████  | 54/67 [00:17<00:04,  3.15it/s, loss=873, v_num=23, train_loss_step=244.0, val_loss=900.0, train_loss_epoch=753.0]\n",
      "Epoch 27:  82%|████████▏ | 55/67 [00:17<00:03,  3.11it/s, loss=873, v_num=23, train_loss_step=244.0, val_loss=900.0, train_loss_epoch=753.0]\n",
      "Epoch 27:  84%|████████▎ | 56/67 [00:17<00:03,  3.12it/s, loss=873, v_num=23, train_loss_step=244.0, val_loss=900.0, train_loss_epoch=753.0]\n",
      "Epoch 27:  85%|████████▌ | 57/67 [00:18<00:03,  3.13it/s, loss=873, v_num=23, train_loss_step=244.0, val_loss=900.0, train_loss_epoch=753.0]\n",
      "Epoch 27:  87%|████████▋ | 58/67 [00:18<00:02,  3.10it/s, loss=873, v_num=23, train_loss_step=244.0, val_loss=900.0, train_loss_epoch=753.0]\n",
      "Epoch 27:  88%|████████▊ | 59/67 [00:19<00:02,  3.10it/s, loss=873, v_num=23, train_loss_step=244.0, val_loss=900.0, train_loss_epoch=753.0]\n",
      "Epoch 27:  90%|████████▉ | 60/67 [00:19<00:02,  3.11it/s, loss=873, v_num=23, train_loss_step=244.0, val_loss=900.0, train_loss_epoch=753.0]\n",
      "Epoch 27:  91%|█████████ | 61/67 [00:19<00:01,  3.06it/s, loss=873, v_num=23, train_loss_step=244.0, val_loss=900.0, train_loss_epoch=753.0]\n",
      "Epoch 27:  93%|█████████▎| 62/67 [00:20<00:01,  3.07it/s, loss=873, v_num=23, train_loss_step=244.0, val_loss=900.0, train_loss_epoch=753.0]\n",
      "Epoch 27:  94%|█████████▍| 63/67 [00:20<00:01,  3.08it/s, loss=873, v_num=23, train_loss_step=244.0, val_loss=900.0, train_loss_epoch=753.0]\n",
      "Epoch 27:  96%|█████████▌| 64/67 [00:20<00:00,  3.07it/s, loss=873, v_num=23, train_loss_step=244.0, val_loss=900.0, train_loss_epoch=753.0]\n",
      "Epoch 27:  97%|█████████▋| 65/67 [00:21<00:00,  3.07it/s, loss=873, v_num=23, train_loss_step=244.0, val_loss=900.0, train_loss_epoch=753.0]\n",
      "Epoch 27:  99%|█████████▊| 66/67 [00:21<00:00,  3.08it/s, loss=873, v_num=23, train_loss_step=244.0, val_loss=900.0, train_loss_epoch=753.0]\n",
      "Epoch 27: 100%|██████████| 67/67 [00:23<00:00,  2.90it/s, loss=873, v_num=23, train_loss_step=244.0, val_loss=900.0, train_loss_epoch=753.0]\n",
      "Epoch 28:  45%|████▍     | 30/67 [00:05<00:07,  5.02it/s, loss=640, v_num=23, train_loss_step=611.0, val_loss=900.0, train_loss_epoch=868.0]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 28:  46%|████▋     | 31/67 [00:07<00:09,  3.90it/s, loss=640, v_num=23, train_loss_step=611.0, val_loss=900.0, train_loss_epoch=868.0]\n",
      "Epoch 28:  48%|████▊     | 32/67 [00:08<00:09,  3.89it/s, loss=640, v_num=23, train_loss_step=611.0, val_loss=900.0, train_loss_epoch=868.0]\n",
      "Epoch 28:  49%|████▉     | 33/67 [00:08<00:08,  3.87it/s, loss=640, v_num=23, train_loss_step=611.0, val_loss=900.0, train_loss_epoch=868.0]\n",
      "Epoch 28:  51%|█████     | 34/67 [00:08<00:08,  3.85it/s, loss=640, v_num=23, train_loss_step=611.0, val_loss=900.0, train_loss_epoch=868.0]\n",
      "Epoch 28:  52%|█████▏    | 35/67 [00:09<00:08,  3.84it/s, loss=640, v_num=23, train_loss_step=611.0, val_loss=900.0, train_loss_epoch=868.0]\n",
      "Epoch 28:  54%|█████▎    | 36/67 [00:09<00:08,  3.83it/s, loss=640, v_num=23, train_loss_step=611.0, val_loss=900.0, train_loss_epoch=868.0]\n",
      "Epoch 28:  55%|█████▌    | 37/67 [00:09<00:07,  3.76it/s, loss=640, v_num=23, train_loss_step=611.0, val_loss=900.0, train_loss_epoch=868.0]\n",
      "Epoch 28:  57%|█████▋    | 38/67 [00:10<00:07,  3.75it/s, loss=640, v_num=23, train_loss_step=611.0, val_loss=900.0, train_loss_epoch=868.0]\n",
      "Epoch 28:  58%|█████▊    | 39/67 [00:10<00:07,  3.74it/s, loss=640, v_num=23, train_loss_step=611.0, val_loss=900.0, train_loss_epoch=868.0]\n",
      "Epoch 28:  60%|█████▉    | 40/67 [00:10<00:07,  3.67it/s, loss=640, v_num=23, train_loss_step=611.0, val_loss=900.0, train_loss_epoch=868.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 28:  61%|██████    | 41/67 [00:11<00:07,  3.61it/s, loss=640, v_num=23, train_loss_step=611.0, val_loss=900.0, train_loss_epoch=868.0]\n",
      "Epoch 28:  63%|██████▎   | 42/67 [00:11<00:06,  3.61it/s, loss=640, v_num=23, train_loss_step=611.0, val_loss=900.0, train_loss_epoch=868.0]\n",
      "Epoch 28:  64%|██████▍   | 43/67 [00:11<00:06,  3.59it/s, loss=640, v_num=23, train_loss_step=611.0, val_loss=900.0, train_loss_epoch=868.0]\n",
      "Epoch 28:  66%|██████▌   | 44/67 [00:12<00:06,  3.58it/s, loss=640, v_num=23, train_loss_step=611.0, val_loss=900.0, train_loss_epoch=868.0]\n",
      "Epoch 28:  67%|██████▋   | 45/67 [00:12<00:06,  3.58it/s, loss=640, v_num=23, train_loss_step=611.0, val_loss=900.0, train_loss_epoch=868.0]\n",
      "Epoch 28:  69%|██████▊   | 46/67 [00:13<00:05,  3.53it/s, loss=640, v_num=23, train_loss_step=611.0, val_loss=900.0, train_loss_epoch=868.0]\n",
      "Epoch 28:  70%|███████   | 47/67 [00:13<00:05,  3.52it/s, loss=640, v_num=23, train_loss_step=611.0, val_loss=900.0, train_loss_epoch=868.0]\n",
      "Epoch 28:  72%|███████▏  | 48/67 [00:13<00:05,  3.52it/s, loss=640, v_num=23, train_loss_step=611.0, val_loss=900.0, train_loss_epoch=868.0]\n",
      "Epoch 28:  73%|███████▎  | 49/67 [00:14<00:05,  3.47it/s, loss=640, v_num=23, train_loss_step=611.0, val_loss=900.0, train_loss_epoch=868.0]\n",
      "Epoch 28:  75%|███████▍  | 50/67 [00:14<00:04,  3.47it/s, loss=640, v_num=23, train_loss_step=611.0, val_loss=900.0, train_loss_epoch=868.0]\n",
      "Epoch 28:  76%|███████▌  | 51/67 [00:14<00:04,  3.43it/s, loss=640, v_num=23, train_loss_step=611.0, val_loss=900.0, train_loss_epoch=868.0]\n",
      "Epoch 28:  78%|███████▊  | 52/67 [00:15<00:04,  3.43it/s, loss=640, v_num=23, train_loss_step=611.0, val_loss=900.0, train_loss_epoch=868.0]\n",
      "Epoch 28:  79%|███████▉  | 53/67 [00:15<00:04,  3.43it/s, loss=640, v_num=23, train_loss_step=611.0, val_loss=900.0, train_loss_epoch=868.0]\n",
      "Epoch 28:  81%|████████  | 54/67 [00:15<00:03,  3.42it/s, loss=640, v_num=23, train_loss_step=611.0, val_loss=900.0, train_loss_epoch=868.0]\n",
      "Epoch 28:  82%|████████▏ | 55/67 [00:16<00:03,  3.39it/s, loss=640, v_num=23, train_loss_step=611.0, val_loss=900.0, train_loss_epoch=868.0]\n",
      "Epoch 28:  84%|████████▎ | 56/67 [00:16<00:03,  3.39it/s, loss=640, v_num=23, train_loss_step=611.0, val_loss=900.0, train_loss_epoch=868.0]\n",
      "Epoch 28:  85%|████████▌ | 57/67 [00:16<00:02,  3.39it/s, loss=640, v_num=23, train_loss_step=611.0, val_loss=900.0, train_loss_epoch=868.0]\n",
      "Epoch 28:  87%|████████▋ | 58/67 [00:17<00:02,  3.35it/s, loss=640, v_num=23, train_loss_step=611.0, val_loss=900.0, train_loss_epoch=868.0]\n",
      "Epoch 28:  88%|████████▊ | 59/67 [00:17<00:02,  3.35it/s, loss=640, v_num=23, train_loss_step=611.0, val_loss=900.0, train_loss_epoch=868.0]\n",
      "Epoch 28:  90%|████████▉ | 60/67 [00:17<00:02,  3.35it/s, loss=640, v_num=23, train_loss_step=611.0, val_loss=900.0, train_loss_epoch=868.0]\n",
      "Epoch 28:  91%|█████████ | 61/67 [00:18<00:01,  3.29it/s, loss=640, v_num=23, train_loss_step=611.0, val_loss=900.0, train_loss_epoch=868.0]\n",
      "Epoch 28:  93%|█████████▎| 62/67 [00:18<00:01,  3.29it/s, loss=640, v_num=23, train_loss_step=611.0, val_loss=900.0, train_loss_epoch=868.0]\n",
      "Epoch 28:  94%|█████████▍| 63/67 [00:19<00:01,  3.29it/s, loss=640, v_num=23, train_loss_step=611.0, val_loss=900.0, train_loss_epoch=868.0]\n",
      "Epoch 28:  96%|█████████▌| 64/67 [00:19<00:00,  3.29it/s, loss=640, v_num=23, train_loss_step=611.0, val_loss=900.0, train_loss_epoch=868.0]\n",
      "Epoch 28:  97%|█████████▋| 65/67 [00:19<00:00,  3.29it/s, loss=640, v_num=23, train_loss_step=611.0, val_loss=900.0, train_loss_epoch=868.0]\n",
      "Epoch 28:  99%|█████████▊| 66/67 [00:20<00:00,  3.29it/s, loss=640, v_num=23, train_loss_step=611.0, val_loss=900.0, train_loss_epoch=868.0]\n",
      "Epoch 28: 100%|██████████| 67/67 [00:20<00:00,  3.22it/s, loss=640, v_num=23, train_loss_step=611.0, val_loss=900.0, train_loss_epoch=868.0]\n",
      "Epoch 29:  45%|████▍     | 30/67 [00:06<00:07,  4.75it/s, loss=981, v_num=23, train_loss_step=4.03e+3, val_loss=900.0, train_loss_epoch=664.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 29:  46%|████▋     | 31/67 [00:08<00:09,  3.69it/s, loss=981, v_num=23, train_loss_step=4.03e+3, val_loss=900.0, train_loss_epoch=664.0]\n",
      "Epoch 29:  48%|████▊     | 32/67 [00:08<00:09,  3.68it/s, loss=981, v_num=23, train_loss_step=4.03e+3, val_loss=900.0, train_loss_epoch=664.0]\n",
      "Epoch 29:  49%|████▉     | 33/67 [00:08<00:09,  3.68it/s, loss=981, v_num=23, train_loss_step=4.03e+3, val_loss=900.0, train_loss_epoch=664.0]\n",
      "Epoch 29:  51%|█████     | 34/67 [00:09<00:09,  3.66it/s, loss=981, v_num=23, train_loss_step=4.03e+3, val_loss=900.0, train_loss_epoch=664.0]\n",
      "Epoch 29:  52%|█████▏    | 35/67 [00:09<00:08,  3.66it/s, loss=981, v_num=23, train_loss_step=4.03e+3, val_loss=900.0, train_loss_epoch=664.0]\n",
      "Epoch 29:  54%|█████▎    | 36/67 [00:09<00:08,  3.65it/s, loss=981, v_num=23, train_loss_step=4.03e+3, val_loss=900.0, train_loss_epoch=664.0]\n",
      "Epoch 29:  55%|█████▌    | 37/67 [00:10<00:08,  3.60it/s, loss=981, v_num=23, train_loss_step=4.03e+3, val_loss=900.0, train_loss_epoch=664.0]\n",
      "Epoch 29:  57%|█████▋    | 38/67 [00:10<00:08,  3.59it/s, loss=981, v_num=23, train_loss_step=4.03e+3, val_loss=900.0, train_loss_epoch=664.0]\n",
      "Epoch 29:  58%|█████▊    | 39/67 [00:10<00:07,  3.58it/s, loss=981, v_num=23, train_loss_step=4.03e+3, val_loss=900.0, train_loss_epoch=664.0]\n",
      "Epoch 29:  60%|█████▉    | 40/67 [00:11<00:07,  3.52it/s, loss=981, v_num=23, train_loss_step=4.03e+3, val_loss=900.0, train_loss_epoch=664.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 29:  61%|██████    | 41/67 [00:11<00:07,  3.48it/s, loss=981, v_num=23, train_loss_step=4.03e+3, val_loss=900.0, train_loss_epoch=664.0]\n",
      "Epoch 29:  63%|██████▎   | 42/67 [00:12<00:07,  3.48it/s, loss=981, v_num=23, train_loss_step=4.03e+3, val_loss=900.0, train_loss_epoch=664.0]\n",
      "Epoch 29:  64%|██████▍   | 43/67 [00:12<00:06,  3.45it/s, loss=981, v_num=23, train_loss_step=4.03e+3, val_loss=900.0, train_loss_epoch=664.0]\n",
      "Epoch 29:  66%|██████▌   | 44/67 [00:12<00:06,  3.46it/s, loss=981, v_num=23, train_loss_step=4.03e+3, val_loss=900.0, train_loss_epoch=664.0]\n",
      "Epoch 29:  67%|██████▋   | 45/67 [00:13<00:06,  3.45it/s, loss=981, v_num=23, train_loss_step=4.03e+3, val_loss=900.0, train_loss_epoch=664.0]\n",
      "Epoch 29:  69%|██████▊   | 46/67 [00:13<00:06,  3.40it/s, loss=981, v_num=23, train_loss_step=4.03e+3, val_loss=900.0, train_loss_epoch=664.0]\n",
      "Epoch 29:  70%|███████   | 47/67 [00:13<00:05,  3.40it/s, loss=981, v_num=23, train_loss_step=4.03e+3, val_loss=900.0, train_loss_epoch=664.0]\n",
      "Epoch 29:  72%|███████▏  | 48/67 [00:14<00:05,  3.40it/s, loss=981, v_num=23, train_loss_step=4.03e+3, val_loss=900.0, train_loss_epoch=664.0]\n",
      "Epoch 29:  73%|███████▎  | 49/67 [00:14<00:05,  3.36it/s, loss=981, v_num=23, train_loss_step=4.03e+3, val_loss=900.0, train_loss_epoch=664.0]\n",
      "Epoch 29:  75%|███████▍  | 50/67 [00:14<00:05,  3.36it/s, loss=981, v_num=23, train_loss_step=4.03e+3, val_loss=900.0, train_loss_epoch=664.0]\n",
      "Epoch 29:  76%|███████▌  | 51/67 [00:15<00:04,  3.33it/s, loss=981, v_num=23, train_loss_step=4.03e+3, val_loss=900.0, train_loss_epoch=664.0]\n",
      "Epoch 29:  78%|███████▊  | 52/67 [00:15<00:04,  3.32it/s, loss=981, v_num=23, train_loss_step=4.03e+3, val_loss=900.0, train_loss_epoch=664.0]\n",
      "Epoch 29:  79%|███████▉  | 53/67 [00:15<00:04,  3.32it/s, loss=981, v_num=23, train_loss_step=4.03e+3, val_loss=900.0, train_loss_epoch=664.0]\n",
      "Epoch 29:  81%|████████  | 54/67 [00:16<00:03,  3.32it/s, loss=981, v_num=23, train_loss_step=4.03e+3, val_loss=900.0, train_loss_epoch=664.0]\n",
      "Epoch 29:  82%|████████▏ | 55/67 [00:16<00:03,  3.28it/s, loss=981, v_num=23, train_loss_step=4.03e+3, val_loss=900.0, train_loss_epoch=664.0]\n",
      "Epoch 29:  84%|████████▎ | 56/67 [00:17<00:03,  3.29it/s, loss=981, v_num=23, train_loss_step=4.03e+3, val_loss=900.0, train_loss_epoch=664.0]\n",
      "Epoch 29:  85%|████████▌ | 57/67 [00:17<00:03,  3.29it/s, loss=981, v_num=23, train_loss_step=4.03e+3, val_loss=900.0, train_loss_epoch=664.0]\n",
      "Epoch 29:  87%|████████▋ | 58/67 [00:17<00:02,  3.25it/s, loss=981, v_num=23, train_loss_step=4.03e+3, val_loss=900.0, train_loss_epoch=664.0]\n",
      "Epoch 29:  88%|████████▊ | 59/67 [00:18<00:02,  3.26it/s, loss=981, v_num=23, train_loss_step=4.03e+3, val_loss=900.0, train_loss_epoch=664.0]\n",
      "Epoch 29:  90%|████████▉ | 60/67 [00:18<00:02,  3.26it/s, loss=981, v_num=23, train_loss_step=4.03e+3, val_loss=900.0, train_loss_epoch=664.0]\n",
      "Epoch 29:  91%|█████████ | 61/67 [00:19<00:01,  3.20it/s, loss=981, v_num=23, train_loss_step=4.03e+3, val_loss=900.0, train_loss_epoch=664.0]\n",
      "Epoch 29:  93%|█████████▎| 62/67 [00:19<00:01,  3.21it/s, loss=981, v_num=23, train_loss_step=4.03e+3, val_loss=900.0, train_loss_epoch=664.0]\n",
      "Epoch 29:  94%|█████████▍| 63/67 [00:19<00:01,  3.21it/s, loss=981, v_num=23, train_loss_step=4.03e+3, val_loss=900.0, train_loss_epoch=664.0]\n",
      "Epoch 29:  96%|█████████▌| 64/67 [00:19<00:00,  3.20it/s, loss=981, v_num=23, train_loss_step=4.03e+3, val_loss=900.0, train_loss_epoch=664.0]\n",
      "Epoch 29:  97%|█████████▋| 65/67 [00:20<00:00,  3.21it/s, loss=981, v_num=23, train_loss_step=4.03e+3, val_loss=900.0, train_loss_epoch=664.0]\n",
      "Epoch 29:  99%|█████████▊| 66/67 [00:20<00:00,  3.21it/s, loss=981, v_num=23, train_loss_step=4.03e+3, val_loss=900.0, train_loss_epoch=664.0]\n",
      "Epoch 29: 100%|██████████| 67/67 [00:21<00:00,  3.14it/s, loss=981, v_num=23, train_loss_step=4.03e+3, val_loss=900.0, train_loss_epoch=664.0]\n",
      "Epoch 30:  45%|████▍     | 30/67 [00:06<00:07,  4.98it/s, loss=693, v_num=23, train_loss_step=636.0, val_loss=900.0, train_loss_epoch=804.0]      \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 30:  46%|████▋     | 31/67 [00:08<00:09,  3.87it/s, loss=693, v_num=23, train_loss_step=636.0, val_loss=900.0, train_loss_epoch=804.0]\n",
      "Epoch 30:  48%|████▊     | 32/67 [00:08<00:09,  3.85it/s, loss=693, v_num=23, train_loss_step=636.0, val_loss=900.0, train_loss_epoch=804.0]\n",
      "Epoch 30:  49%|████▉     | 33/67 [00:08<00:08,  3.83it/s, loss=693, v_num=23, train_loss_step=636.0, val_loss=900.0, train_loss_epoch=804.0]\n",
      "Epoch 30:  51%|█████     | 34/67 [00:08<00:08,  3.82it/s, loss=693, v_num=23, train_loss_step=636.0, val_loss=900.0, train_loss_epoch=804.0]\n",
      "Epoch 30:  52%|█████▏    | 35/67 [00:09<00:08,  3.79it/s, loss=693, v_num=23, train_loss_step=636.0, val_loss=900.0, train_loss_epoch=804.0]\n",
      "Epoch 30:  54%|█████▎    | 36/67 [00:09<00:08,  3.79it/s, loss=693, v_num=23, train_loss_step=636.0, val_loss=900.0, train_loss_epoch=804.0]\n",
      "Epoch 30:  55%|█████▌    | 37/67 [00:09<00:08,  3.72it/s, loss=693, v_num=23, train_loss_step=636.0, val_loss=900.0, train_loss_epoch=804.0]\n",
      "Epoch 30:  57%|█████▋    | 38/67 [00:10<00:07,  3.71it/s, loss=693, v_num=23, train_loss_step=636.0, val_loss=900.0, train_loss_epoch=804.0]\n",
      "Epoch 30:  58%|█████▊    | 39/67 [00:10<00:07,  3.70it/s, loss=693, v_num=23, train_loss_step=636.0, val_loss=900.0, train_loss_epoch=804.0]\n",
      "Epoch 30:  60%|█████▉    | 40/67 [00:10<00:07,  3.64it/s, loss=693, v_num=23, train_loss_step=636.0, val_loss=900.0, train_loss_epoch=804.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 30:  61%|██████    | 41/67 [00:11<00:07,  3.59it/s, loss=693, v_num=23, train_loss_step=636.0, val_loss=900.0, train_loss_epoch=804.0]\n",
      "Epoch 30:  63%|██████▎   | 42/67 [00:11<00:06,  3.59it/s, loss=693, v_num=23, train_loss_step=636.0, val_loss=900.0, train_loss_epoch=804.0]\n",
      "Epoch 30:  64%|██████▍   | 43/67 [00:12<00:06,  3.54it/s, loss=693, v_num=23, train_loss_step=636.0, val_loss=900.0, train_loss_epoch=804.0]\n",
      "Epoch 30:  66%|██████▌   | 44/67 [00:12<00:06,  3.54it/s, loss=693, v_num=23, train_loss_step=636.0, val_loss=900.0, train_loss_epoch=804.0]\n",
      "Epoch 30:  67%|██████▋   | 45/67 [00:12<00:06,  3.54it/s, loss=693, v_num=23, train_loss_step=636.0, val_loss=900.0, train_loss_epoch=804.0]\n",
      "Epoch 30:  69%|██████▊   | 46/67 [00:13<00:06,  3.48it/s, loss=693, v_num=23, train_loss_step=636.0, val_loss=900.0, train_loss_epoch=804.0]\n",
      "Epoch 30:  70%|███████   | 47/67 [00:13<00:05,  3.48it/s, loss=693, v_num=23, train_loss_step=636.0, val_loss=900.0, train_loss_epoch=804.0]\n",
      "Epoch 30:  72%|███████▏  | 48/67 [00:13<00:05,  3.48it/s, loss=693, v_num=23, train_loss_step=636.0, val_loss=900.0, train_loss_epoch=804.0]\n",
      "Epoch 30:  73%|███████▎  | 49/67 [00:14<00:05,  3.43it/s, loss=693, v_num=23, train_loss_step=636.0, val_loss=900.0, train_loss_epoch=804.0]\n",
      "Epoch 30:  75%|███████▍  | 50/67 [00:14<00:04,  3.43it/s, loss=693, v_num=23, train_loss_step=636.0, val_loss=900.0, train_loss_epoch=804.0]\n",
      "Epoch 30:  76%|███████▌  | 51/67 [00:15<00:04,  3.40it/s, loss=693, v_num=23, train_loss_step=636.0, val_loss=900.0, train_loss_epoch=804.0]\n",
      "Epoch 30:  78%|███████▊  | 52/67 [00:15<00:04,  3.38it/s, loss=693, v_num=23, train_loss_step=636.0, val_loss=900.0, train_loss_epoch=804.0]\n",
      "Epoch 30:  79%|███████▉  | 53/67 [00:15<00:04,  3.39it/s, loss=693, v_num=23, train_loss_step=636.0, val_loss=900.0, train_loss_epoch=804.0]\n",
      "Epoch 30:  81%|████████  | 54/67 [00:15<00:03,  3.39it/s, loss=693, v_num=23, train_loss_step=636.0, val_loss=900.0, train_loss_epoch=804.0]\n",
      "Epoch 30:  82%|████████▏ | 55/67 [00:16<00:03,  3.35it/s, loss=693, v_num=23, train_loss_step=636.0, val_loss=900.0, train_loss_epoch=804.0]\n",
      "Epoch 30:  84%|████████▎ | 56/67 [00:16<00:03,  3.35it/s, loss=693, v_num=23, train_loss_step=636.0, val_loss=900.0, train_loss_epoch=804.0]\n",
      "Epoch 30:  85%|████████▌ | 57/67 [00:17<00:02,  3.35it/s, loss=693, v_num=23, train_loss_step=636.0, val_loss=900.0, train_loss_epoch=804.0]\n",
      "Epoch 30:  87%|████████▋ | 58/67 [00:17<00:02,  3.31it/s, loss=693, v_num=23, train_loss_step=636.0, val_loss=900.0, train_loss_epoch=804.0]\n",
      "Epoch 30:  88%|████████▊ | 59/67 [00:17<00:02,  3.32it/s, loss=693, v_num=23, train_loss_step=636.0, val_loss=900.0, train_loss_epoch=804.0]\n",
      "Epoch 30:  90%|████████▉ | 60/67 [00:18<00:02,  3.32it/s, loss=693, v_num=23, train_loss_step=636.0, val_loss=900.0, train_loss_epoch=804.0]\n",
      "Epoch 30:  91%|█████████ | 61/67 [00:18<00:01,  3.26it/s, loss=693, v_num=23, train_loss_step=636.0, val_loss=900.0, train_loss_epoch=804.0]\n",
      "Epoch 30:  93%|█████████▎| 62/67 [00:19<00:01,  3.26it/s, loss=693, v_num=23, train_loss_step=636.0, val_loss=900.0, train_loss_epoch=804.0]\n",
      "Epoch 30:  94%|█████████▍| 63/67 [00:19<00:01,  3.26it/s, loss=693, v_num=23, train_loss_step=636.0, val_loss=900.0, train_loss_epoch=804.0]\n",
      "Epoch 30:  96%|█████████▌| 64/67 [00:19<00:00,  3.25it/s, loss=693, v_num=23, train_loss_step=636.0, val_loss=900.0, train_loss_epoch=804.0]\n",
      "Epoch 30:  97%|█████████▋| 65/67 [00:19<00:00,  3.26it/s, loss=693, v_num=23, train_loss_step=636.0, val_loss=900.0, train_loss_epoch=804.0]\n",
      "Epoch 30:  99%|█████████▊| 66/67 [00:20<00:00,  3.26it/s, loss=693, v_num=23, train_loss_step=636.0, val_loss=900.0, train_loss_epoch=804.0]\n",
      "Epoch 30: 100%|██████████| 67/67 [00:20<00:00,  3.19it/s, loss=693, v_num=23, train_loss_step=636.0, val_loss=900.0, train_loss_epoch=804.0]\n",
      "Epoch 31:  45%|████▍     | 30/67 [00:06<00:07,  4.93it/s, loss=746, v_num=23, train_loss_step=1.97e+3, val_loss=900.0, train_loss_epoch=690.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 31:  46%|████▋     | 31/67 [00:08<00:09,  3.83it/s, loss=746, v_num=23, train_loss_step=1.97e+3, val_loss=900.0, train_loss_epoch=690.0]\n",
      "Epoch 31:  48%|████▊     | 32/67 [00:08<00:09,  3.81it/s, loss=746, v_num=23, train_loss_step=1.97e+3, val_loss=900.0, train_loss_epoch=690.0]\n",
      "Epoch 31:  49%|████▉     | 33/67 [00:08<00:08,  3.80it/s, loss=746, v_num=23, train_loss_step=1.97e+3, val_loss=900.0, train_loss_epoch=690.0]\n",
      "Epoch 31:  51%|█████     | 34/67 [00:08<00:08,  3.79it/s, loss=746, v_num=23, train_loss_step=1.97e+3, val_loss=900.0, train_loss_epoch=690.0]\n",
      "Epoch 31:  52%|█████▏    | 35/67 [00:09<00:08,  3.77it/s, loss=746, v_num=23, train_loss_step=1.97e+3, val_loss=900.0, train_loss_epoch=690.0]\n",
      "Epoch 31:  54%|█████▎    | 36/67 [00:09<00:08,  3.76it/s, loss=746, v_num=23, train_loss_step=1.97e+3, val_loss=900.0, train_loss_epoch=690.0]\n",
      "Epoch 31:  55%|█████▌    | 37/67 [00:09<00:08,  3.70it/s, loss=746, v_num=23, train_loss_step=1.97e+3, val_loss=900.0, train_loss_epoch=690.0]\n",
      "Epoch 31:  57%|█████▋    | 38/67 [00:10<00:07,  3.69it/s, loss=746, v_num=23, train_loss_step=1.97e+3, val_loss=900.0, train_loss_epoch=690.0]\n",
      "Epoch 31:  58%|█████▊    | 39/67 [00:10<00:07,  3.68it/s, loss=746, v_num=23, train_loss_step=1.97e+3, val_loss=900.0, train_loss_epoch=690.0]\n",
      "Epoch 31:  60%|█████▉    | 40/67 [00:11<00:07,  3.61it/s, loss=746, v_num=23, train_loss_step=1.97e+3, val_loss=900.0, train_loss_epoch=690.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 31:  61%|██████    | 41/67 [00:11<00:07,  3.56it/s, loss=746, v_num=23, train_loss_step=1.97e+3, val_loss=900.0, train_loss_epoch=690.0]\n",
      "Epoch 31:  63%|██████▎   | 42/67 [00:11<00:07,  3.56it/s, loss=746, v_num=23, train_loss_step=1.97e+3, val_loss=900.0, train_loss_epoch=690.0]\n",
      "Epoch 31:  64%|██████▍   | 43/67 [00:12<00:06,  3.53it/s, loss=746, v_num=23, train_loss_step=1.97e+3, val_loss=900.0, train_loss_epoch=690.0]\n",
      "Epoch 31:  66%|██████▌   | 44/67 [00:12<00:06,  3.53it/s, loss=746, v_num=23, train_loss_step=1.97e+3, val_loss=900.0, train_loss_epoch=690.0]\n",
      "Epoch 31:  67%|██████▋   | 45/67 [00:12<00:06,  3.53it/s, loss=746, v_num=23, train_loss_step=1.97e+3, val_loss=900.0, train_loss_epoch=690.0]\n",
      "Epoch 31:  69%|██████▊   | 46/67 [00:13<00:06,  3.47it/s, loss=746, v_num=23, train_loss_step=1.97e+3, val_loss=900.0, train_loss_epoch=690.0]\n",
      "Epoch 31:  70%|███████   | 47/67 [00:13<00:05,  3.47it/s, loss=746, v_num=23, train_loss_step=1.97e+3, val_loss=900.0, train_loss_epoch=690.0]\n",
      "Epoch 31:  72%|███████▏  | 48/67 [00:13<00:05,  3.47it/s, loss=746, v_num=23, train_loss_step=1.97e+3, val_loss=900.0, train_loss_epoch=690.0]\n",
      "Epoch 31:  73%|███████▎  | 49/67 [00:14<00:05,  3.42it/s, loss=746, v_num=23, train_loss_step=1.97e+3, val_loss=900.0, train_loss_epoch=690.0]\n",
      "Epoch 31:  75%|███████▍  | 50/67 [00:14<00:04,  3.42it/s, loss=746, v_num=23, train_loss_step=1.97e+3, val_loss=900.0, train_loss_epoch=690.0]\n",
      "Epoch 31:  76%|███████▌  | 51/67 [00:15<00:04,  3.39it/s, loss=746, v_num=23, train_loss_step=1.97e+3, val_loss=900.0, train_loss_epoch=690.0]\n",
      "Epoch 31:  78%|███████▊  | 52/67 [00:15<00:04,  3.38it/s, loss=746, v_num=23, train_loss_step=1.97e+3, val_loss=900.0, train_loss_epoch=690.0]\n",
      "Epoch 31:  79%|███████▉  | 53/67 [00:15<00:04,  3.38it/s, loss=746, v_num=23, train_loss_step=1.97e+3, val_loss=900.0, train_loss_epoch=690.0]\n",
      "Epoch 31:  81%|████████  | 54/67 [00:15<00:03,  3.38it/s, loss=746, v_num=23, train_loss_step=1.97e+3, val_loss=900.0, train_loss_epoch=690.0]\n",
      "Epoch 31:  82%|████████▏ | 55/67 [00:16<00:03,  3.34it/s, loss=746, v_num=23, train_loss_step=1.97e+3, val_loss=900.0, train_loss_epoch=690.0]\n",
      "Epoch 31:  84%|████████▎ | 56/67 [00:16<00:03,  3.35it/s, loss=746, v_num=23, train_loss_step=1.97e+3, val_loss=900.0, train_loss_epoch=690.0]\n",
      "Epoch 31:  85%|████████▌ | 57/67 [00:17<00:02,  3.35it/s, loss=746, v_num=23, train_loss_step=1.97e+3, val_loss=900.0, train_loss_epoch=690.0]\n",
      "Epoch 31:  87%|████████▋ | 58/67 [00:17<00:02,  3.31it/s, loss=746, v_num=23, train_loss_step=1.97e+3, val_loss=900.0, train_loss_epoch=690.0]\n",
      "Epoch 31:  88%|████████▊ | 59/67 [00:17<00:02,  3.31it/s, loss=746, v_num=23, train_loss_step=1.97e+3, val_loss=900.0, train_loss_epoch=690.0]\n",
      "Epoch 31:  90%|████████▉ | 60/67 [00:18<00:02,  3.31it/s, loss=746, v_num=23, train_loss_step=1.97e+3, val_loss=900.0, train_loss_epoch=690.0]\n",
      "Epoch 31:  91%|█████████ | 61/67 [00:18<00:01,  3.25it/s, loss=746, v_num=23, train_loss_step=1.97e+3, val_loss=900.0, train_loss_epoch=690.0]\n",
      "Epoch 31:  93%|█████████▎| 62/67 [00:19<00:01,  3.26it/s, loss=746, v_num=23, train_loss_step=1.97e+3, val_loss=900.0, train_loss_epoch=690.0]\n",
      "Epoch 31:  94%|█████████▍| 63/67 [00:19<00:01,  3.26it/s, loss=746, v_num=23, train_loss_step=1.97e+3, val_loss=900.0, train_loss_epoch=690.0]\n",
      "Epoch 31:  96%|█████████▌| 64/67 [00:19<00:00,  3.24it/s, loss=746, v_num=23, train_loss_step=1.97e+3, val_loss=900.0, train_loss_epoch=690.0]\n",
      "Epoch 31:  97%|█████████▋| 65/67 [00:20<00:00,  3.24it/s, loss=746, v_num=23, train_loss_step=1.97e+3, val_loss=900.0, train_loss_epoch=690.0]\n",
      "Epoch 31:  99%|█████████▊| 66/67 [00:20<00:00,  3.25it/s, loss=746, v_num=23, train_loss_step=1.97e+3, val_loss=900.0, train_loss_epoch=690.0]\n",
      "Epoch 31: 100%|██████████| 67/67 [00:21<00:00,  3.17it/s, loss=746, v_num=23, train_loss_step=1.97e+3, val_loss=900.0, train_loss_epoch=690.0]\n",
      "Epoch 32:  45%|████▍     | 30/67 [00:06<00:07,  4.97it/s, loss=857, v_num=23, train_loss_step=641.0, val_loss=900.0, train_loss_epoch=695.0]       \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 32:  46%|████▋     | 31/67 [00:08<00:09,  3.85it/s, loss=857, v_num=23, train_loss_step=641.0, val_loss=900.0, train_loss_epoch=695.0]\n",
      "Epoch 32:  48%|████▊     | 32/67 [00:08<00:09,  3.82it/s, loss=857, v_num=23, train_loss_step=641.0, val_loss=900.0, train_loss_epoch=695.0]\n",
      "Epoch 32:  49%|████▉     | 33/67 [00:08<00:08,  3.81it/s, loss=857, v_num=23, train_loss_step=641.0, val_loss=900.0, train_loss_epoch=695.0]\n",
      "Epoch 32:  51%|█████     | 34/67 [00:08<00:08,  3.80it/s, loss=857, v_num=23, train_loss_step=641.0, val_loss=900.0, train_loss_epoch=695.0]\n",
      "Epoch 32:  52%|█████▏    | 35/67 [00:09<00:08,  3.78it/s, loss=857, v_num=23, train_loss_step=641.0, val_loss=900.0, train_loss_epoch=695.0]\n",
      "Epoch 32:  54%|█████▎    | 36/67 [00:09<00:08,  3.78it/s, loss=857, v_num=23, train_loss_step=641.0, val_loss=900.0, train_loss_epoch=695.0]\n",
      "Epoch 32:  55%|█████▌    | 37/67 [00:10<00:08,  3.69it/s, loss=857, v_num=23, train_loss_step=641.0, val_loss=900.0, train_loss_epoch=695.0]\n",
      "Epoch 32:  57%|█████▋    | 38/67 [00:10<00:07,  3.68it/s, loss=857, v_num=23, train_loss_step=641.0, val_loss=900.0, train_loss_epoch=695.0]\n",
      "Epoch 32:  58%|█████▊    | 39/67 [00:10<00:07,  3.67it/s, loss=857, v_num=23, train_loss_step=641.0, val_loss=900.0, train_loss_epoch=695.0]\n",
      "Epoch 32:  60%|█████▉    | 40/67 [00:11<00:07,  3.60it/s, loss=857, v_num=23, train_loss_step=641.0, val_loss=900.0, train_loss_epoch=695.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 32:  61%|██████    | 41/67 [00:11<00:07,  3.56it/s, loss=857, v_num=23, train_loss_step=641.0, val_loss=900.0, train_loss_epoch=695.0]\n",
      "Epoch 32:  63%|██████▎   | 42/67 [00:11<00:07,  3.55it/s, loss=857, v_num=23, train_loss_step=641.0, val_loss=900.0, train_loss_epoch=695.0]\n",
      "Epoch 32:  64%|██████▍   | 43/67 [00:12<00:06,  3.51it/s, loss=857, v_num=23, train_loss_step=641.0, val_loss=900.0, train_loss_epoch=695.0]\n",
      "Epoch 32:  66%|██████▌   | 44/67 [00:12<00:06,  3.51it/s, loss=857, v_num=23, train_loss_step=641.0, val_loss=900.0, train_loss_epoch=695.0]\n",
      "Epoch 32:  67%|██████▋   | 45/67 [00:12<00:06,  3.51it/s, loss=857, v_num=23, train_loss_step=641.0, val_loss=900.0, train_loss_epoch=695.0]\n",
      "Epoch 32:  69%|██████▊   | 46/67 [00:13<00:06,  3.45it/s, loss=857, v_num=23, train_loss_step=641.0, val_loss=900.0, train_loss_epoch=695.0]\n",
      "Epoch 32:  70%|███████   | 47/67 [00:13<00:05,  3.45it/s, loss=857, v_num=23, train_loss_step=641.0, val_loss=900.0, train_loss_epoch=695.0]\n",
      "Epoch 32:  72%|███████▏  | 48/67 [00:13<00:05,  3.45it/s, loss=857, v_num=23, train_loss_step=641.0, val_loss=900.0, train_loss_epoch=695.0]\n",
      "Epoch 32:  73%|███████▎  | 49/67 [00:14<00:05,  3.40it/s, loss=857, v_num=23, train_loss_step=641.0, val_loss=900.0, train_loss_epoch=695.0]\n",
      "Epoch 32:  75%|███████▍  | 50/67 [00:14<00:04,  3.40it/s, loss=857, v_num=23, train_loss_step=641.0, val_loss=900.0, train_loss_epoch=695.0]\n",
      "Epoch 32:  76%|███████▌  | 51/67 [00:15<00:04,  3.37it/s, loss=857, v_num=23, train_loss_step=641.0, val_loss=900.0, train_loss_epoch=695.0]\n",
      "Epoch 32:  78%|███████▊  | 52/67 [00:15<00:04,  3.35it/s, loss=857, v_num=23, train_loss_step=641.0, val_loss=900.0, train_loss_epoch=695.0]\n",
      "Epoch 32:  79%|███████▉  | 53/67 [00:15<00:04,  3.36it/s, loss=857, v_num=23, train_loss_step=641.0, val_loss=900.0, train_loss_epoch=695.0]\n",
      "Epoch 32:  81%|████████  | 54/67 [00:16<00:03,  3.36it/s, loss=857, v_num=23, train_loss_step=641.0, val_loss=900.0, train_loss_epoch=695.0]\n",
      "Epoch 32:  82%|████████▏ | 55/67 [00:16<00:03,  3.32it/s, loss=857, v_num=23, train_loss_step=641.0, val_loss=900.0, train_loss_epoch=695.0]\n",
      "Epoch 32:  84%|████████▎ | 56/67 [00:16<00:03,  3.33it/s, loss=857, v_num=23, train_loss_step=641.0, val_loss=900.0, train_loss_epoch=695.0]\n",
      "Epoch 32:  85%|████████▌ | 57/67 [00:17<00:03,  3.33it/s, loss=857, v_num=23, train_loss_step=641.0, val_loss=900.0, train_loss_epoch=695.0]\n",
      "Epoch 32:  87%|████████▋ | 58/67 [00:17<00:02,  3.29it/s, loss=857, v_num=23, train_loss_step=641.0, val_loss=900.0, train_loss_epoch=695.0]\n",
      "Epoch 32:  88%|████████▊ | 59/67 [00:17<00:02,  3.29it/s, loss=857, v_num=23, train_loss_step=641.0, val_loss=900.0, train_loss_epoch=695.0]\n",
      "Epoch 32:  90%|████████▉ | 60/67 [00:18<00:02,  3.30it/s, loss=857, v_num=23, train_loss_step=641.0, val_loss=900.0, train_loss_epoch=695.0]\n",
      "Epoch 32:  91%|█████████ | 61/67 [00:18<00:01,  3.24it/s, loss=857, v_num=23, train_loss_step=641.0, val_loss=900.0, train_loss_epoch=695.0]\n",
      "Epoch 32:  93%|█████████▎| 62/67 [00:19<00:01,  3.25it/s, loss=857, v_num=23, train_loss_step=641.0, val_loss=900.0, train_loss_epoch=695.0]\n",
      "Epoch 32:  94%|█████████▍| 63/67 [00:19<00:01,  3.25it/s, loss=857, v_num=23, train_loss_step=641.0, val_loss=900.0, train_loss_epoch=695.0]\n",
      "Epoch 32:  96%|█████████▌| 64/67 [00:21<00:00,  3.05it/s, loss=857, v_num=23, train_loss_step=641.0, val_loss=900.0, train_loss_epoch=695.0]\n",
      "Epoch 32:  97%|█████████▋| 65/67 [00:21<00:00,  3.05it/s, loss=857, v_num=23, train_loss_step=641.0, val_loss=900.0, train_loss_epoch=695.0]\n",
      "Epoch 32:  99%|█████████▊| 66/67 [00:21<00:00,  3.06it/s, loss=857, v_num=23, train_loss_step=641.0, val_loss=900.0, train_loss_epoch=695.0]\n",
      "Epoch 32: 100%|██████████| 67/67 [00:22<00:00,  2.99it/s, loss=857, v_num=23, train_loss_step=641.0, val_loss=900.0, train_loss_epoch=695.0]\n",
      "Epoch 33:  45%|████▍     | 30/67 [00:06<00:07,  4.93it/s, loss=668, v_num=23, train_loss_step=1.04e+3, val_loss=900.0, train_loss_epoch=894.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 33:  46%|████▋     | 31/67 [00:08<00:09,  3.79it/s, loss=668, v_num=23, train_loss_step=1.04e+3, val_loss=900.0, train_loss_epoch=894.0]\n",
      "Epoch 33:  48%|████▊     | 32/67 [00:08<00:09,  3.78it/s, loss=668, v_num=23, train_loss_step=1.04e+3, val_loss=900.0, train_loss_epoch=894.0]\n",
      "Epoch 33:  49%|████▉     | 33/67 [00:08<00:09,  3.77it/s, loss=668, v_num=23, train_loss_step=1.04e+3, val_loss=900.0, train_loss_epoch=894.0]\n",
      "Epoch 33:  51%|█████     | 34/67 [00:09<00:08,  3.76it/s, loss=668, v_num=23, train_loss_step=1.04e+3, val_loss=900.0, train_loss_epoch=894.0]\n",
      "Epoch 33:  52%|█████▏    | 35/67 [00:09<00:08,  3.74it/s, loss=668, v_num=23, train_loss_step=1.04e+3, val_loss=900.0, train_loss_epoch=894.0]\n",
      "Epoch 33:  54%|█████▎    | 36/67 [00:09<00:08,  3.74it/s, loss=668, v_num=23, train_loss_step=1.04e+3, val_loss=900.0, train_loss_epoch=894.0]\n",
      "Epoch 33:  55%|█████▌    | 37/67 [00:10<00:08,  3.68it/s, loss=668, v_num=23, train_loss_step=1.04e+3, val_loss=900.0, train_loss_epoch=894.0]\n",
      "Epoch 33:  57%|█████▋    | 38/67 [00:10<00:07,  3.68it/s, loss=668, v_num=23, train_loss_step=1.04e+3, val_loss=900.0, train_loss_epoch=894.0]\n",
      "Epoch 33:  58%|█████▊    | 39/67 [00:10<00:07,  3.67it/s, loss=668, v_num=23, train_loss_step=1.04e+3, val_loss=900.0, train_loss_epoch=894.0]\n",
      "Epoch 33:  60%|█████▉    | 40/67 [00:11<00:07,  3.61it/s, loss=668, v_num=23, train_loss_step=1.04e+3, val_loss=900.0, train_loss_epoch=894.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 33:  61%|██████    | 41/67 [00:11<00:07,  3.56it/s, loss=668, v_num=23, train_loss_step=1.04e+3, val_loss=900.0, train_loss_epoch=894.0]\n",
      "Epoch 33:  63%|██████▎   | 42/67 [00:11<00:07,  3.56it/s, loss=668, v_num=23, train_loss_step=1.04e+3, val_loss=900.0, train_loss_epoch=894.0]\n",
      "Epoch 33:  64%|██████▍   | 43/67 [00:12<00:06,  3.53it/s, loss=668, v_num=23, train_loss_step=1.04e+3, val_loss=900.0, train_loss_epoch=894.0]\n",
      "Epoch 33:  66%|██████▌   | 44/67 [00:12<00:06,  3.53it/s, loss=668, v_num=23, train_loss_step=1.04e+3, val_loss=900.0, train_loss_epoch=894.0]\n",
      "Epoch 33:  67%|██████▋   | 45/67 [00:12<00:06,  3.54it/s, loss=668, v_num=23, train_loss_step=1.04e+3, val_loss=900.0, train_loss_epoch=894.0]\n",
      "Epoch 33:  69%|██████▊   | 46/67 [00:13<00:06,  3.47it/s, loss=668, v_num=23, train_loss_step=1.04e+3, val_loss=900.0, train_loss_epoch=894.0]\n",
      "Epoch 33:  70%|███████   | 47/67 [00:13<00:05,  3.47it/s, loss=668, v_num=23, train_loss_step=1.04e+3, val_loss=900.0, train_loss_epoch=894.0]\n",
      "Epoch 33:  72%|███████▏  | 48/67 [00:13<00:05,  3.47it/s, loss=668, v_num=23, train_loss_step=1.04e+3, val_loss=900.0, train_loss_epoch=894.0]\n",
      "Epoch 33:  73%|███████▎  | 49/67 [00:14<00:05,  3.42it/s, loss=668, v_num=23, train_loss_step=1.04e+3, val_loss=900.0, train_loss_epoch=894.0]\n",
      "Epoch 33:  75%|███████▍  | 50/67 [00:14<00:04,  3.42it/s, loss=668, v_num=23, train_loss_step=1.04e+3, val_loss=900.0, train_loss_epoch=894.0]\n",
      "Epoch 33:  76%|███████▌  | 51/67 [00:15<00:04,  3.39it/s, loss=668, v_num=23, train_loss_step=1.04e+3, val_loss=900.0, train_loss_epoch=894.0]\n",
      "Epoch 33:  78%|███████▊  | 52/67 [00:15<00:04,  3.37it/s, loss=668, v_num=23, train_loss_step=1.04e+3, val_loss=900.0, train_loss_epoch=894.0]\n",
      "Epoch 33:  79%|███████▉  | 53/67 [00:15<00:04,  3.37it/s, loss=668, v_num=23, train_loss_step=1.04e+3, val_loss=900.0, train_loss_epoch=894.0]\n",
      "Epoch 33:  81%|████████  | 54/67 [00:15<00:03,  3.38it/s, loss=668, v_num=23, train_loss_step=1.04e+3, val_loss=900.0, train_loss_epoch=894.0]\n",
      "Epoch 33:  82%|████████▏ | 55/67 [00:17<00:03,  3.09it/s, loss=668, v_num=23, train_loss_step=1.04e+3, val_loss=900.0, train_loss_epoch=894.0]\n",
      "Epoch 33:  84%|████████▎ | 56/67 [00:18<00:03,  3.10it/s, loss=668, v_num=23, train_loss_step=1.04e+3, val_loss=900.0, train_loss_epoch=894.0]\n",
      "Epoch 33:  85%|████████▌ | 57/67 [00:18<00:03,  3.11it/s, loss=668, v_num=23, train_loss_step=1.04e+3, val_loss=900.0, train_loss_epoch=894.0]\n",
      "Epoch 33:  87%|████████▋ | 58/67 [00:18<00:02,  3.07it/s, loss=668, v_num=23, train_loss_step=1.04e+3, val_loss=900.0, train_loss_epoch=894.0]\n",
      "Epoch 33:  88%|████████▊ | 59/67 [00:19<00:02,  3.08it/s, loss=668, v_num=23, train_loss_step=1.04e+3, val_loss=900.0, train_loss_epoch=894.0]\n",
      "Epoch 33:  90%|████████▉ | 60/67 [00:19<00:02,  3.09it/s, loss=668, v_num=23, train_loss_step=1.04e+3, val_loss=900.0, train_loss_epoch=894.0]\n",
      "Epoch 33:  91%|█████████ | 61/67 [00:20<00:01,  3.04it/s, loss=668, v_num=23, train_loss_step=1.04e+3, val_loss=900.0, train_loss_epoch=894.0]\n",
      "Epoch 33:  93%|█████████▎| 62/67 [00:20<00:01,  3.05it/s, loss=668, v_num=23, train_loss_step=1.04e+3, val_loss=900.0, train_loss_epoch=894.0]\n",
      "Epoch 33:  94%|█████████▍| 63/67 [00:20<00:01,  3.06it/s, loss=668, v_num=23, train_loss_step=1.04e+3, val_loss=900.0, train_loss_epoch=894.0]\n",
      "Epoch 33:  96%|█████████▌| 64/67 [00:20<00:00,  3.05it/s, loss=668, v_num=23, train_loss_step=1.04e+3, val_loss=900.0, train_loss_epoch=894.0]\n",
      "Epoch 33:  97%|█████████▋| 65/67 [00:21<00:00,  3.06it/s, loss=668, v_num=23, train_loss_step=1.04e+3, val_loss=900.0, train_loss_epoch=894.0]\n",
      "Epoch 33:  99%|█████████▊| 66/67 [00:21<00:00,  3.07it/s, loss=668, v_num=23, train_loss_step=1.04e+3, val_loss=900.0, train_loss_epoch=894.0]\n",
      "Epoch 33: 100%|██████████| 67/67 [00:22<00:00,  3.01it/s, loss=668, v_num=23, train_loss_step=1.04e+3, val_loss=900.0, train_loss_epoch=894.0]\n",
      "Epoch 34:  45%|████▍     | 30/67 [00:06<00:07,  4.91it/s, loss=857, v_num=23, train_loss_step=1.81e+3, val_loss=900.0, train_loss_epoch=656.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 34:  46%|████▋     | 31/67 [00:08<00:09,  3.80it/s, loss=857, v_num=23, train_loss_step=1.81e+3, val_loss=900.0, train_loss_epoch=656.0]\n",
      "Epoch 34:  48%|████▊     | 32/67 [00:08<00:09,  3.78it/s, loss=857, v_num=23, train_loss_step=1.81e+3, val_loss=900.0, train_loss_epoch=656.0]\n",
      "Epoch 34:  49%|████▉     | 33/67 [00:08<00:09,  3.77it/s, loss=857, v_num=23, train_loss_step=1.81e+3, val_loss=900.0, train_loss_epoch=656.0]\n",
      "Epoch 34:  51%|█████     | 34/67 [00:09<00:08,  3.76it/s, loss=857, v_num=23, train_loss_step=1.81e+3, val_loss=900.0, train_loss_epoch=656.0]\n",
      "Epoch 34:  52%|█████▏    | 35/67 [00:09<00:08,  3.74it/s, loss=857, v_num=23, train_loss_step=1.81e+3, val_loss=900.0, train_loss_epoch=656.0]\n",
      "Epoch 34:  54%|█████▎    | 36/67 [00:09<00:08,  3.73it/s, loss=857, v_num=23, train_loss_step=1.81e+3, val_loss=900.0, train_loss_epoch=656.0]\n",
      "Epoch 34:  55%|█████▌    | 37/67 [00:10<00:08,  3.67it/s, loss=857, v_num=23, train_loss_step=1.81e+3, val_loss=900.0, train_loss_epoch=656.0]\n",
      "Epoch 34:  57%|█████▋    | 38/67 [00:10<00:07,  3.67it/s, loss=857, v_num=23, train_loss_step=1.81e+3, val_loss=900.0, train_loss_epoch=656.0]\n",
      "Epoch 34:  58%|█████▊    | 39/67 [00:10<00:07,  3.66it/s, loss=857, v_num=23, train_loss_step=1.81e+3, val_loss=900.0, train_loss_epoch=656.0]\n",
      "Epoch 34:  60%|█████▉    | 40/67 [00:11<00:07,  3.59it/s, loss=857, v_num=23, train_loss_step=1.81e+3, val_loss=900.0, train_loss_epoch=656.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 34:  61%|██████    | 41/67 [00:11<00:07,  3.55it/s, loss=857, v_num=23, train_loss_step=1.81e+3, val_loss=900.0, train_loss_epoch=656.0]\n",
      "Epoch 34:  63%|██████▎   | 42/67 [00:11<00:07,  3.54it/s, loss=857, v_num=23, train_loss_step=1.81e+3, val_loss=900.0, train_loss_epoch=656.0]\n",
      "Epoch 34:  64%|██████▍   | 43/67 [00:12<00:06,  3.51it/s, loss=857, v_num=23, train_loss_step=1.81e+3, val_loss=900.0, train_loss_epoch=656.0]\n",
      "Epoch 34:  66%|██████▌   | 44/67 [00:12<00:06,  3.51it/s, loss=857, v_num=23, train_loss_step=1.81e+3, val_loss=900.0, train_loss_epoch=656.0]\n",
      "Epoch 34:  67%|██████▋   | 45/67 [00:12<00:06,  3.51it/s, loss=857, v_num=23, train_loss_step=1.81e+3, val_loss=900.0, train_loss_epoch=656.0]\n",
      "Epoch 34:  69%|██████▊   | 46/67 [00:14<00:06,  3.15it/s, loss=857, v_num=23, train_loss_step=1.81e+3, val_loss=900.0, train_loss_epoch=656.0]\n",
      "Epoch 34:  70%|███████   | 47/67 [00:14<00:06,  3.15it/s, loss=857, v_num=23, train_loss_step=1.81e+3, val_loss=900.0, train_loss_epoch=656.0]\n",
      "Epoch 34:  72%|███████▏  | 48/67 [00:15<00:06,  3.16it/s, loss=857, v_num=23, train_loss_step=1.81e+3, val_loss=900.0, train_loss_epoch=656.0]\n",
      "Epoch 34:  73%|███████▎  | 49/67 [00:15<00:05,  3.12it/s, loss=857, v_num=23, train_loss_step=1.81e+3, val_loss=900.0, train_loss_epoch=656.0]\n",
      "Epoch 34:  75%|███████▍  | 50/67 [00:15<00:05,  3.13it/s, loss=857, v_num=23, train_loss_step=1.81e+3, val_loss=900.0, train_loss_epoch=656.0]\n",
      "Epoch 34:  76%|███████▌  | 51/67 [00:16<00:05,  3.10it/s, loss=857, v_num=23, train_loss_step=1.81e+3, val_loss=900.0, train_loss_epoch=656.0]\n",
      "Epoch 34:  78%|███████▊  | 52/67 [00:16<00:04,  3.10it/s, loss=857, v_num=23, train_loss_step=1.81e+3, val_loss=900.0, train_loss_epoch=656.0]\n",
      "Epoch 34:  79%|███████▉  | 53/67 [00:17<00:04,  3.11it/s, loss=857, v_num=23, train_loss_step=1.81e+3, val_loss=900.0, train_loss_epoch=656.0]\n",
      "Epoch 34:  81%|████████  | 54/67 [00:17<00:04,  3.11it/s, loss=857, v_num=23, train_loss_step=1.81e+3, val_loss=900.0, train_loss_epoch=656.0]\n",
      "Epoch 34:  82%|████████▏ | 55/67 [00:17<00:03,  3.08it/s, loss=857, v_num=23, train_loss_step=1.81e+3, val_loss=900.0, train_loss_epoch=656.0]\n",
      "Epoch 34:  84%|████████▎ | 56/67 [00:18<00:03,  3.08it/s, loss=857, v_num=23, train_loss_step=1.81e+3, val_loss=900.0, train_loss_epoch=656.0]\n",
      "Epoch 34:  85%|████████▌ | 57/67 [00:18<00:03,  3.09it/s, loss=857, v_num=23, train_loss_step=1.81e+3, val_loss=900.0, train_loss_epoch=656.0]\n",
      "Epoch 34:  87%|████████▋ | 58/67 [00:19<00:02,  3.05it/s, loss=857, v_num=23, train_loss_step=1.81e+3, val_loss=900.0, train_loss_epoch=656.0]\n",
      "Epoch 34:  88%|████████▊ | 59/67 [00:19<00:02,  3.05it/s, loss=857, v_num=23, train_loss_step=1.81e+3, val_loss=900.0, train_loss_epoch=656.0]\n",
      "Epoch 34:  90%|████████▉ | 60/67 [00:19<00:02,  3.06it/s, loss=857, v_num=23, train_loss_step=1.81e+3, val_loss=900.0, train_loss_epoch=656.0]\n",
      "Epoch 34:  91%|█████████ | 61/67 [00:20<00:01,  3.01it/s, loss=857, v_num=23, train_loss_step=1.81e+3, val_loss=900.0, train_loss_epoch=656.0]\n",
      "Epoch 34:  93%|█████████▎| 62/67 [00:20<00:01,  3.01it/s, loss=857, v_num=23, train_loss_step=1.81e+3, val_loss=900.0, train_loss_epoch=656.0]\n",
      "Epoch 34:  94%|█████████▍| 63/67 [00:20<00:01,  3.02it/s, loss=857, v_num=23, train_loss_step=1.81e+3, val_loss=900.0, train_loss_epoch=656.0]\n",
      "Epoch 34:  96%|█████████▌| 64/67 [00:21<00:00,  3.01it/s, loss=857, v_num=23, train_loss_step=1.81e+3, val_loss=900.0, train_loss_epoch=656.0]\n",
      "Epoch 34:  97%|█████████▋| 65/67 [00:21<00:00,  3.02it/s, loss=857, v_num=23, train_loss_step=1.81e+3, val_loss=900.0, train_loss_epoch=656.0]\n",
      "Epoch 34:  99%|█████████▊| 66/67 [00:21<00:00,  3.03it/s, loss=857, v_num=23, train_loss_step=1.81e+3, val_loss=900.0, train_loss_epoch=656.0]\n",
      "Epoch 34: 100%|██████████| 67/67 [00:22<00:00,  2.97it/s, loss=857, v_num=23, train_loss_step=1.81e+3, val_loss=899.0, train_loss_epoch=656.0]\n",
      "Epoch 35:  45%|████▍     | 30/67 [00:05<00:07,  5.06it/s, loss=629, v_num=23, train_loss_step=407.0, val_loss=899.0, train_loss_epoch=819.0]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 35:  46%|████▋     | 31/67 [00:07<00:09,  3.91it/s, loss=629, v_num=23, train_loss_step=407.0, val_loss=899.0, train_loss_epoch=819.0]\n",
      "Epoch 35:  48%|████▊     | 32/67 [00:08<00:09,  3.88it/s, loss=629, v_num=23, train_loss_step=407.0, val_loss=899.0, train_loss_epoch=819.0]\n",
      "Epoch 35:  49%|████▉     | 33/67 [00:08<00:08,  3.86it/s, loss=629, v_num=23, train_loss_step=407.0, val_loss=899.0, train_loss_epoch=819.0]\n",
      "Epoch 35:  51%|█████     | 34/67 [00:08<00:08,  3.85it/s, loss=629, v_num=23, train_loss_step=407.0, val_loss=899.0, train_loss_epoch=819.0]\n",
      "Epoch 35:  52%|█████▏    | 35/67 [00:09<00:08,  3.83it/s, loss=629, v_num=23, train_loss_step=407.0, val_loss=899.0, train_loss_epoch=819.0]\n",
      "Epoch 35:  54%|█████▎    | 36/67 [00:09<00:08,  3.82it/s, loss=629, v_num=23, train_loss_step=407.0, val_loss=899.0, train_loss_epoch=819.0]\n",
      "Epoch 35:  55%|█████▌    | 37/67 [00:11<00:09,  3.33it/s, loss=629, v_num=23, train_loss_step=407.0, val_loss=899.0, train_loss_epoch=819.0]\n",
      "Epoch 35:  57%|█████▋    | 38/67 [00:11<00:08,  3.33it/s, loss=629, v_num=23, train_loss_step=407.0, val_loss=899.0, train_loss_epoch=819.0]\n",
      "Epoch 35:  58%|█████▊    | 39/67 [00:11<00:08,  3.33it/s, loss=629, v_num=23, train_loss_step=407.0, val_loss=899.0, train_loss_epoch=819.0]\n",
      "Epoch 35:  60%|█████▉    | 40/67 [00:12<00:08,  3.28it/s, loss=629, v_num=23, train_loss_step=407.0, val_loss=899.0, train_loss_epoch=819.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 35:  61%|██████    | 41/67 [00:12<00:08,  3.25it/s, loss=629, v_num=23, train_loss_step=407.0, val_loss=899.0, train_loss_epoch=819.0]\n",
      "Epoch 35:  63%|██████▎   | 42/67 [00:12<00:07,  3.25it/s, loss=629, v_num=23, train_loss_step=407.0, val_loss=899.0, train_loss_epoch=819.0]\n",
      "Epoch 35:  64%|██████▍   | 43/67 [00:13<00:07,  3.24it/s, loss=629, v_num=23, train_loss_step=407.0, val_loss=899.0, train_loss_epoch=819.0]\n",
      "Epoch 35:  66%|██████▌   | 44/67 [00:13<00:07,  3.24it/s, loss=629, v_num=23, train_loss_step=407.0, val_loss=899.0, train_loss_epoch=819.0]\n",
      "Epoch 35:  67%|██████▋   | 45/67 [00:13<00:06,  3.24it/s, loss=629, v_num=23, train_loss_step=407.0, val_loss=899.0, train_loss_epoch=819.0]\n",
      "Epoch 35:  69%|██████▊   | 46/67 [00:14<00:06,  3.21it/s, loss=629, v_num=23, train_loss_step=407.0, val_loss=899.0, train_loss_epoch=819.0]\n",
      "Epoch 35:  70%|███████   | 47/67 [00:14<00:06,  3.21it/s, loss=629, v_num=23, train_loss_step=407.0, val_loss=899.0, train_loss_epoch=819.0]\n",
      "Epoch 35:  72%|███████▏  | 48/67 [00:14<00:05,  3.22it/s, loss=629, v_num=23, train_loss_step=407.0, val_loss=899.0, train_loss_epoch=819.0]\n",
      "Epoch 35:  73%|███████▎  | 49/67 [00:15<00:05,  3.18it/s, loss=629, v_num=23, train_loss_step=407.0, val_loss=899.0, train_loss_epoch=819.0]\n",
      "Epoch 35:  75%|███████▍  | 50/67 [00:15<00:05,  3.18it/s, loss=629, v_num=23, train_loss_step=407.0, val_loss=899.0, train_loss_epoch=819.0]\n",
      "Epoch 35:  76%|███████▌  | 51/67 [00:16<00:05,  3.16it/s, loss=629, v_num=23, train_loss_step=407.0, val_loss=899.0, train_loss_epoch=819.0]\n",
      "Epoch 35:  78%|███████▊  | 52/67 [00:16<00:04,  3.15it/s, loss=629, v_num=23, train_loss_step=407.0, val_loss=899.0, train_loss_epoch=819.0]\n",
      "Epoch 35:  79%|███████▉  | 53/67 [00:16<00:04,  3.15it/s, loss=629, v_num=23, train_loss_step=407.0, val_loss=899.0, train_loss_epoch=819.0]\n",
      "Epoch 35:  81%|████████  | 54/67 [00:17<00:04,  3.16it/s, loss=629, v_num=23, train_loss_step=407.0, val_loss=899.0, train_loss_epoch=819.0]\n",
      "Epoch 35:  82%|████████▏ | 55/67 [00:17<00:03,  3.13it/s, loss=629, v_num=23, train_loss_step=407.0, val_loss=899.0, train_loss_epoch=819.0]\n",
      "Epoch 35:  84%|████████▎ | 56/67 [00:17<00:03,  3.13it/s, loss=629, v_num=23, train_loss_step=407.0, val_loss=899.0, train_loss_epoch=819.0]\n",
      "Epoch 35:  85%|████████▌ | 57/67 [00:18<00:03,  3.14it/s, loss=629, v_num=23, train_loss_step=407.0, val_loss=899.0, train_loss_epoch=819.0]\n",
      "Epoch 35:  87%|████████▋ | 58/67 [00:18<00:02,  3.11it/s, loss=629, v_num=23, train_loss_step=407.0, val_loss=899.0, train_loss_epoch=819.0]\n",
      "Epoch 35:  88%|████████▊ | 59/67 [00:18<00:02,  3.11it/s, loss=629, v_num=23, train_loss_step=407.0, val_loss=899.0, train_loss_epoch=819.0]\n",
      "Epoch 35:  90%|████████▉ | 60/67 [00:19<00:02,  3.12it/s, loss=629, v_num=23, train_loss_step=407.0, val_loss=899.0, train_loss_epoch=819.0]\n",
      "Epoch 35:  91%|█████████ | 61/67 [00:19<00:01,  3.07it/s, loss=629, v_num=23, train_loss_step=407.0, val_loss=899.0, train_loss_epoch=819.0]\n",
      "Epoch 35:  93%|█████████▎| 62/67 [00:20<00:01,  3.08it/s, loss=629, v_num=23, train_loss_step=407.0, val_loss=899.0, train_loss_epoch=819.0]\n",
      "Epoch 35:  94%|█████████▍| 63/67 [00:20<00:01,  3.08it/s, loss=629, v_num=23, train_loss_step=407.0, val_loss=899.0, train_loss_epoch=819.0]\n",
      "Epoch 35:  96%|█████████▌| 64/67 [00:20<00:00,  3.08it/s, loss=629, v_num=23, train_loss_step=407.0, val_loss=899.0, train_loss_epoch=819.0]\n",
      "Epoch 35:  97%|█████████▋| 65/67 [00:21<00:00,  3.09it/s, loss=629, v_num=23, train_loss_step=407.0, val_loss=899.0, train_loss_epoch=819.0]\n",
      "Epoch 35:  99%|█████████▊| 66/67 [00:21<00:00,  3.09it/s, loss=629, v_num=23, train_loss_step=407.0, val_loss=899.0, train_loss_epoch=819.0]\n",
      "Epoch 35: 100%|██████████| 67/67 [00:23<00:00,  2.87it/s, loss=629, v_num=23, train_loss_step=407.0, val_loss=900.0, train_loss_epoch=819.0]\n",
      "Epoch 36:  45%|████▍     | 30/67 [00:05<00:07,  5.11it/s, loss=664, v_num=23, train_loss_step=1.69e+3, val_loss=900.0, train_loss_epoch=604.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 36:  46%|████▋     | 31/67 [00:07<00:09,  3.90it/s, loss=664, v_num=23, train_loss_step=1.69e+3, val_loss=900.0, train_loss_epoch=604.0]\n",
      "Epoch 36:  48%|████▊     | 32/67 [00:08<00:09,  3.88it/s, loss=664, v_num=23, train_loss_step=1.69e+3, val_loss=900.0, train_loss_epoch=604.0]\n",
      "Epoch 36:  49%|████▉     | 33/67 [00:08<00:08,  3.86it/s, loss=664, v_num=23, train_loss_step=1.69e+3, val_loss=900.0, train_loss_epoch=604.0]\n",
      "Epoch 36:  51%|█████     | 34/67 [00:08<00:08,  3.85it/s, loss=664, v_num=23, train_loss_step=1.69e+3, val_loss=900.0, train_loss_epoch=604.0]\n",
      "Epoch 36:  52%|█████▏    | 35/67 [00:09<00:08,  3.84it/s, loss=664, v_num=23, train_loss_step=1.69e+3, val_loss=900.0, train_loss_epoch=604.0]\n",
      "Epoch 36:  54%|█████▎    | 36/67 [00:09<00:08,  3.82it/s, loss=664, v_num=23, train_loss_step=1.69e+3, val_loss=900.0, train_loss_epoch=604.0]\n",
      "Epoch 36:  55%|█████▌    | 37/67 [00:09<00:08,  3.74it/s, loss=664, v_num=23, train_loss_step=1.69e+3, val_loss=900.0, train_loss_epoch=604.0]\n",
      "Epoch 36:  57%|█████▋    | 38/67 [00:10<00:07,  3.73it/s, loss=664, v_num=23, train_loss_step=1.69e+3, val_loss=900.0, train_loss_epoch=604.0]\n",
      "Epoch 36:  58%|█████▊    | 39/67 [00:10<00:07,  3.72it/s, loss=664, v_num=23, train_loss_step=1.69e+3, val_loss=900.0, train_loss_epoch=604.0]\n",
      "Epoch 36:  60%|█████▉    | 40/67 [00:10<00:07,  3.65it/s, loss=664, v_num=23, train_loss_step=1.69e+3, val_loss=900.0, train_loss_epoch=604.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 36:  61%|██████    | 41/67 [00:11<00:07,  3.60it/s, loss=664, v_num=23, train_loss_step=1.69e+3, val_loss=900.0, train_loss_epoch=604.0]\n",
      "Epoch 36:  63%|██████▎   | 42/67 [00:11<00:06,  3.60it/s, loss=664, v_num=23, train_loss_step=1.69e+3, val_loss=900.0, train_loss_epoch=604.0]\n",
      "Epoch 36:  64%|██████▍   | 43/67 [00:12<00:06,  3.57it/s, loss=664, v_num=23, train_loss_step=1.69e+3, val_loss=900.0, train_loss_epoch=604.0]\n",
      "Epoch 36:  66%|██████▌   | 44/67 [00:12<00:06,  3.56it/s, loss=664, v_num=23, train_loss_step=1.69e+3, val_loss=900.0, train_loss_epoch=604.0]\n",
      "Epoch 36:  67%|██████▋   | 45/67 [00:12<00:06,  3.56it/s, loss=664, v_num=23, train_loss_step=1.69e+3, val_loss=900.0, train_loss_epoch=604.0]\n",
      "Epoch 36:  69%|██████▊   | 46/67 [00:13<00:05,  3.51it/s, loss=664, v_num=23, train_loss_step=1.69e+3, val_loss=900.0, train_loss_epoch=604.0]\n",
      "Epoch 36:  70%|███████   | 47/67 [00:13<00:05,  3.51it/s, loss=664, v_num=23, train_loss_step=1.69e+3, val_loss=900.0, train_loss_epoch=604.0]\n",
      "Epoch 36:  72%|███████▏  | 48/67 [00:13<00:05,  3.50it/s, loss=664, v_num=23, train_loss_step=1.69e+3, val_loss=900.0, train_loss_epoch=604.0]\n",
      "Epoch 36:  73%|███████▎  | 49/67 [00:14<00:05,  3.46it/s, loss=664, v_num=23, train_loss_step=1.69e+3, val_loss=900.0, train_loss_epoch=604.0]\n",
      "Epoch 36:  75%|███████▍  | 50/67 [00:14<00:04,  3.46it/s, loss=664, v_num=23, train_loss_step=1.69e+3, val_loss=900.0, train_loss_epoch=604.0]\n",
      "Epoch 36:  76%|███████▌  | 51/67 [00:14<00:04,  3.43it/s, loss=664, v_num=23, train_loss_step=1.69e+3, val_loss=900.0, train_loss_epoch=604.0]\n",
      "Epoch 36:  78%|███████▊  | 52/67 [00:15<00:04,  3.41it/s, loss=664, v_num=23, train_loss_step=1.69e+3, val_loss=900.0, train_loss_epoch=604.0]\n",
      "Epoch 36:  79%|███████▉  | 53/67 [00:15<00:04,  3.41it/s, loss=664, v_num=23, train_loss_step=1.69e+3, val_loss=900.0, train_loss_epoch=604.0]\n",
      "Epoch 36:  81%|████████  | 54/67 [00:15<00:03,  3.41it/s, loss=664, v_num=23, train_loss_step=1.69e+3, val_loss=900.0, train_loss_epoch=604.0]\n",
      "Epoch 36:  82%|████████▏ | 55/67 [00:16<00:03,  3.37it/s, loss=664, v_num=23, train_loss_step=1.69e+3, val_loss=900.0, train_loss_epoch=604.0]\n",
      "Epoch 36:  84%|████████▎ | 56/67 [00:16<00:03,  3.37it/s, loss=664, v_num=23, train_loss_step=1.69e+3, val_loss=900.0, train_loss_epoch=604.0]\n",
      "Epoch 36:  85%|████████▌ | 57/67 [00:16<00:02,  3.38it/s, loss=664, v_num=23, train_loss_step=1.69e+3, val_loss=900.0, train_loss_epoch=604.0]\n",
      "Epoch 36:  87%|████████▋ | 58/67 [00:17<00:02,  3.34it/s, loss=664, v_num=23, train_loss_step=1.69e+3, val_loss=900.0, train_loss_epoch=604.0]\n",
      "Epoch 36:  88%|████████▊ | 59/67 [00:17<00:02,  3.34it/s, loss=664, v_num=23, train_loss_step=1.69e+3, val_loss=900.0, train_loss_epoch=604.0]\n",
      "Epoch 36:  90%|████████▉ | 60/67 [00:17<00:02,  3.34it/s, loss=664, v_num=23, train_loss_step=1.69e+3, val_loss=900.0, train_loss_epoch=604.0]\n",
      "Epoch 36:  91%|█████████ | 61/67 [00:18<00:01,  3.28it/s, loss=664, v_num=23, train_loss_step=1.69e+3, val_loss=900.0, train_loss_epoch=604.0]\n",
      "Epoch 36:  93%|█████████▎| 62/67 [00:18<00:01,  3.28it/s, loss=664, v_num=23, train_loss_step=1.69e+3, val_loss=900.0, train_loss_epoch=604.0]\n",
      "Epoch 36:  94%|█████████▍| 63/67 [00:19<00:01,  3.28it/s, loss=664, v_num=23, train_loss_step=1.69e+3, val_loss=900.0, train_loss_epoch=604.0]\n",
      "Epoch 36:  96%|█████████▌| 64/67 [00:19<00:00,  3.28it/s, loss=664, v_num=23, train_loss_step=1.69e+3, val_loss=900.0, train_loss_epoch=604.0]\n",
      "Epoch 36:  97%|█████████▋| 65/67 [00:19<00:00,  3.28it/s, loss=664, v_num=23, train_loss_step=1.69e+3, val_loss=900.0, train_loss_epoch=604.0]\n",
      "Epoch 36:  99%|█████████▊| 66/67 [00:20<00:00,  3.28it/s, loss=664, v_num=23, train_loss_step=1.69e+3, val_loss=900.0, train_loss_epoch=604.0]\n",
      "Epoch 36: 100%|██████████| 67/67 [00:20<00:00,  3.22it/s, loss=664, v_num=23, train_loss_step=1.69e+3, val_loss=899.0, train_loss_epoch=604.0]\n",
      "Epoch 37:  45%|████▍     | 30/67 [00:05<00:07,  5.01it/s, loss=660, v_num=23, train_loss_step=1.87e+3, val_loss=899.0, train_loss_epoch=652.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 37:  46%|████▋     | 31/67 [00:08<00:09,  3.83it/s, loss=660, v_num=23, train_loss_step=1.87e+3, val_loss=899.0, train_loss_epoch=652.0]\n",
      "Epoch 37:  48%|████▊     | 32/67 [00:08<00:09,  3.82it/s, loss=660, v_num=23, train_loss_step=1.87e+3, val_loss=899.0, train_loss_epoch=652.0]\n",
      "Epoch 37:  49%|████▉     | 33/67 [00:08<00:08,  3.81it/s, loss=660, v_num=23, train_loss_step=1.87e+3, val_loss=899.0, train_loss_epoch=652.0]\n",
      "Epoch 37:  51%|█████     | 34/67 [00:08<00:08,  3.79it/s, loss=660, v_num=23, train_loss_step=1.87e+3, val_loss=899.0, train_loss_epoch=652.0]\n",
      "Epoch 37:  52%|█████▏    | 35/67 [00:09<00:08,  3.78it/s, loss=660, v_num=23, train_loss_step=1.87e+3, val_loss=899.0, train_loss_epoch=652.0]\n",
      "Epoch 37:  54%|█████▎    | 36/67 [00:09<00:08,  3.76it/s, loss=660, v_num=23, train_loss_step=1.87e+3, val_loss=899.0, train_loss_epoch=652.0]\n",
      "Epoch 37:  55%|█████▌    | 37/67 [00:09<00:08,  3.70it/s, loss=660, v_num=23, train_loss_step=1.87e+3, val_loss=899.0, train_loss_epoch=652.0]\n",
      "Epoch 37:  57%|█████▋    | 38/67 [00:10<00:07,  3.69it/s, loss=660, v_num=23, train_loss_step=1.87e+3, val_loss=899.0, train_loss_epoch=652.0]\n",
      "Epoch 37:  58%|█████▊    | 39/67 [00:10<00:07,  3.69it/s, loss=660, v_num=23, train_loss_step=1.87e+3, val_loss=899.0, train_loss_epoch=652.0]\n",
      "Epoch 37:  60%|█████▉    | 40/67 [00:11<00:07,  3.62it/s, loss=660, v_num=23, train_loss_step=1.87e+3, val_loss=899.0, train_loss_epoch=652.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 37:  61%|██████    | 41/67 [00:11<00:07,  3.57it/s, loss=660, v_num=23, train_loss_step=1.87e+3, val_loss=899.0, train_loss_epoch=652.0]\n",
      "Epoch 37:  63%|██████▎   | 42/67 [00:11<00:07,  3.57it/s, loss=660, v_num=23, train_loss_step=1.87e+3, val_loss=899.0, train_loss_epoch=652.0]\n",
      "Epoch 37:  64%|██████▍   | 43/67 [00:12<00:06,  3.54it/s, loss=660, v_num=23, train_loss_step=1.87e+3, val_loss=899.0, train_loss_epoch=652.0]\n",
      "Epoch 37:  66%|██████▌   | 44/67 [00:12<00:06,  3.54it/s, loss=660, v_num=23, train_loss_step=1.87e+3, val_loss=899.0, train_loss_epoch=652.0]\n",
      "Epoch 37:  67%|██████▋   | 45/67 [00:12<00:06,  3.53it/s, loss=660, v_num=23, train_loss_step=1.87e+3, val_loss=899.0, train_loss_epoch=652.0]\n",
      "Epoch 37:  69%|██████▊   | 46/67 [00:13<00:06,  3.48it/s, loss=660, v_num=23, train_loss_step=1.87e+3, val_loss=899.0, train_loss_epoch=652.0]\n",
      "Epoch 37:  70%|███████   | 47/67 [00:13<00:05,  3.48it/s, loss=660, v_num=23, train_loss_step=1.87e+3, val_loss=899.0, train_loss_epoch=652.0]\n",
      "Epoch 37:  72%|███████▏  | 48/67 [00:13<00:05,  3.48it/s, loss=660, v_num=23, train_loss_step=1.87e+3, val_loss=899.0, train_loss_epoch=652.0]\n",
      "Epoch 37:  73%|███████▎  | 49/67 [00:14<00:05,  3.43it/s, loss=660, v_num=23, train_loss_step=1.87e+3, val_loss=899.0, train_loss_epoch=652.0]\n",
      "Epoch 37:  75%|███████▍  | 50/67 [00:14<00:04,  3.43it/s, loss=660, v_num=23, train_loss_step=1.87e+3, val_loss=899.0, train_loss_epoch=652.0]\n",
      "Epoch 37:  76%|███████▌  | 51/67 [00:14<00:04,  3.40it/s, loss=660, v_num=23, train_loss_step=1.87e+3, val_loss=899.0, train_loss_epoch=652.0]\n",
      "Epoch 37:  78%|███████▊  | 52/67 [00:15<00:04,  3.39it/s, loss=660, v_num=23, train_loss_step=1.87e+3, val_loss=899.0, train_loss_epoch=652.0]\n",
      "Epoch 37:  79%|███████▉  | 53/67 [00:15<00:04,  3.39it/s, loss=660, v_num=23, train_loss_step=1.87e+3, val_loss=899.0, train_loss_epoch=652.0]\n",
      "Epoch 37:  81%|████████  | 54/67 [00:15<00:03,  3.39it/s, loss=660, v_num=23, train_loss_step=1.87e+3, val_loss=899.0, train_loss_epoch=652.0]\n",
      "Epoch 37:  82%|████████▏ | 55/67 [00:16<00:03,  3.35it/s, loss=660, v_num=23, train_loss_step=1.87e+3, val_loss=899.0, train_loss_epoch=652.0]\n",
      "Epoch 37:  84%|████████▎ | 56/67 [00:16<00:03,  3.35it/s, loss=660, v_num=23, train_loss_step=1.87e+3, val_loss=899.0, train_loss_epoch=652.0]\n",
      "Epoch 37:  85%|████████▌ | 57/67 [00:17<00:02,  3.35it/s, loss=660, v_num=23, train_loss_step=1.87e+3, val_loss=899.0, train_loss_epoch=652.0]\n",
      "Epoch 37:  87%|████████▋ | 58/67 [00:17<00:02,  3.31it/s, loss=660, v_num=23, train_loss_step=1.87e+3, val_loss=899.0, train_loss_epoch=652.0]\n",
      "Epoch 37:  88%|████████▊ | 59/67 [00:17<00:02,  3.31it/s, loss=660, v_num=23, train_loss_step=1.87e+3, val_loss=899.0, train_loss_epoch=652.0]\n",
      "Epoch 37:  90%|████████▉ | 60/67 [00:18<00:02,  3.31it/s, loss=660, v_num=23, train_loss_step=1.87e+3, val_loss=899.0, train_loss_epoch=652.0]\n",
      "Epoch 37:  91%|█████████ | 61/67 [00:18<00:01,  3.24it/s, loss=660, v_num=23, train_loss_step=1.87e+3, val_loss=899.0, train_loss_epoch=652.0]\n",
      "Epoch 37:  93%|█████████▎| 62/67 [00:19<00:01,  3.25it/s, loss=660, v_num=23, train_loss_step=1.87e+3, val_loss=899.0, train_loss_epoch=652.0]\n",
      "Epoch 37:  94%|█████████▍| 63/67 [00:19<00:01,  3.25it/s, loss=660, v_num=23, train_loss_step=1.87e+3, val_loss=899.0, train_loss_epoch=652.0]\n",
      "Epoch 37:  96%|█████████▌| 64/67 [00:19<00:00,  3.24it/s, loss=660, v_num=23, train_loss_step=1.87e+3, val_loss=899.0, train_loss_epoch=652.0]\n",
      "Epoch 37:  97%|█████████▋| 65/67 [00:20<00:00,  3.25it/s, loss=660, v_num=23, train_loss_step=1.87e+3, val_loss=899.0, train_loss_epoch=652.0]\n",
      "Epoch 37:  99%|█████████▊| 66/67 [00:20<00:00,  3.25it/s, loss=660, v_num=23, train_loss_step=1.87e+3, val_loss=899.0, train_loss_epoch=652.0]\n",
      "Epoch 37: 100%|██████████| 67/67 [00:21<00:00,  3.18it/s, loss=660, v_num=23, train_loss_step=1.87e+3, val_loss=899.0, train_loss_epoch=652.0]\n",
      "Epoch 38:  45%|████▍     | 30/67 [00:05<00:07,  5.05it/s, loss=725, v_num=23, train_loss_step=638.0, val_loss=899.0, train_loss_epoch=680.0]       \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 38:  46%|████▋     | 31/67 [00:07<00:09,  3.92it/s, loss=725, v_num=23, train_loss_step=638.0, val_loss=899.0, train_loss_epoch=680.0]\n",
      "Epoch 38:  48%|████▊     | 32/67 [00:08<00:09,  3.89it/s, loss=725, v_num=23, train_loss_step=638.0, val_loss=899.0, train_loss_epoch=680.0]\n",
      "Epoch 38:  49%|████▉     | 33/67 [00:08<00:08,  3.88it/s, loss=725, v_num=23, train_loss_step=638.0, val_loss=899.0, train_loss_epoch=680.0]\n",
      "Epoch 38:  51%|█████     | 34/67 [00:08<00:08,  3.86it/s, loss=725, v_num=23, train_loss_step=638.0, val_loss=899.0, train_loss_epoch=680.0]\n",
      "Epoch 38:  52%|█████▏    | 35/67 [00:09<00:08,  3.84it/s, loss=725, v_num=23, train_loss_step=638.0, val_loss=899.0, train_loss_epoch=680.0]\n",
      "Epoch 38:  54%|█████▎    | 36/67 [00:09<00:08,  3.83it/s, loss=725, v_num=23, train_loss_step=638.0, val_loss=899.0, train_loss_epoch=680.0]\n",
      "Epoch 38:  55%|█████▌    | 37/67 [00:09<00:07,  3.79it/s, loss=725, v_num=23, train_loss_step=638.0, val_loss=899.0, train_loss_epoch=680.0]\n",
      "Epoch 38:  57%|█████▋    | 38/67 [00:10<00:07,  3.77it/s, loss=725, v_num=23, train_loss_step=638.0, val_loss=899.0, train_loss_epoch=680.0]\n",
      "Epoch 38:  58%|█████▊    | 39/67 [00:10<00:07,  3.76it/s, loss=725, v_num=23, train_loss_step=638.0, val_loss=899.0, train_loss_epoch=680.0]\n",
      "Epoch 38:  60%|█████▉    | 40/67 [00:10<00:07,  3.69it/s, loss=725, v_num=23, train_loss_step=638.0, val_loss=899.0, train_loss_epoch=680.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 38:  61%|██████    | 41/67 [00:11<00:07,  3.63it/s, loss=725, v_num=23, train_loss_step=638.0, val_loss=899.0, train_loss_epoch=680.0]\n",
      "Epoch 38:  63%|██████▎   | 42/67 [00:11<00:06,  3.63it/s, loss=725, v_num=23, train_loss_step=638.0, val_loss=899.0, train_loss_epoch=680.0]\n",
      "Epoch 38:  64%|██████▍   | 43/67 [00:11<00:06,  3.61it/s, loss=725, v_num=23, train_loss_step=638.0, val_loss=899.0, train_loss_epoch=680.0]\n",
      "Epoch 38:  66%|██████▌   | 44/67 [00:12<00:06,  3.60it/s, loss=725, v_num=23, train_loss_step=638.0, val_loss=899.0, train_loss_epoch=680.0]\n",
      "Epoch 38:  67%|██████▋   | 45/67 [00:12<00:06,  3.60it/s, loss=725, v_num=23, train_loss_step=638.0, val_loss=899.0, train_loss_epoch=680.0]\n",
      "Epoch 38:  69%|██████▊   | 46/67 [00:12<00:05,  3.54it/s, loss=725, v_num=23, train_loss_step=638.0, val_loss=899.0, train_loss_epoch=680.0]\n",
      "Epoch 38:  70%|███████   | 47/67 [00:13<00:05,  3.54it/s, loss=725, v_num=23, train_loss_step=638.0, val_loss=899.0, train_loss_epoch=680.0]\n",
      "Epoch 38:  72%|███████▏  | 48/67 [00:13<00:05,  3.54it/s, loss=725, v_num=23, train_loss_step=638.0, val_loss=899.0, train_loss_epoch=680.0]\n",
      "Epoch 38:  73%|███████▎  | 49/67 [00:14<00:05,  3.48it/s, loss=725, v_num=23, train_loss_step=638.0, val_loss=899.0, train_loss_epoch=680.0]\n",
      "Epoch 38:  75%|███████▍  | 50/67 [00:14<00:04,  3.48it/s, loss=725, v_num=23, train_loss_step=638.0, val_loss=899.0, train_loss_epoch=680.0]\n",
      "Epoch 38:  76%|███████▌  | 51/67 [00:14<00:04,  3.45it/s, loss=725, v_num=23, train_loss_step=638.0, val_loss=899.0, train_loss_epoch=680.0]\n",
      "Epoch 38:  78%|███████▊  | 52/67 [00:15<00:04,  3.43it/s, loss=725, v_num=23, train_loss_step=638.0, val_loss=899.0, train_loss_epoch=680.0]\n",
      "Epoch 38:  79%|███████▉  | 53/67 [00:15<00:04,  3.43it/s, loss=725, v_num=23, train_loss_step=638.0, val_loss=899.0, train_loss_epoch=680.0]\n",
      "Epoch 38:  81%|████████  | 54/67 [00:15<00:03,  3.42it/s, loss=725, v_num=23, train_loss_step=638.0, val_loss=899.0, train_loss_epoch=680.0]\n",
      "Epoch 38:  82%|████████▏ | 55/67 [00:16<00:03,  3.36it/s, loss=725, v_num=23, train_loss_step=638.0, val_loss=899.0, train_loss_epoch=680.0]\n",
      "Epoch 38:  84%|████████▎ | 56/67 [00:16<00:03,  3.37it/s, loss=725, v_num=23, train_loss_step=638.0, val_loss=899.0, train_loss_epoch=680.0]\n",
      "Epoch 38:  85%|████████▌ | 57/67 [00:16<00:02,  3.37it/s, loss=725, v_num=23, train_loss_step=638.0, val_loss=899.0, train_loss_epoch=680.0]\n",
      "Epoch 38:  87%|████████▋ | 58/67 [00:17<00:02,  3.32it/s, loss=725, v_num=23, train_loss_step=638.0, val_loss=899.0, train_loss_epoch=680.0]\n",
      "Epoch 38:  88%|████████▊ | 59/67 [00:17<00:02,  3.32it/s, loss=725, v_num=23, train_loss_step=638.0, val_loss=899.0, train_loss_epoch=680.0]\n",
      "Epoch 38:  90%|████████▉ | 60/67 [00:18<00:02,  3.32it/s, loss=725, v_num=23, train_loss_step=638.0, val_loss=899.0, train_loss_epoch=680.0]\n",
      "Epoch 38:  91%|█████████ | 61/67 [00:18<00:01,  3.27it/s, loss=725, v_num=23, train_loss_step=638.0, val_loss=899.0, train_loss_epoch=680.0]\n",
      "Epoch 38:  93%|█████████▎| 62/67 [00:18<00:01,  3.27it/s, loss=725, v_num=23, train_loss_step=638.0, val_loss=899.0, train_loss_epoch=680.0]\n",
      "Epoch 38:  94%|█████████▍| 63/67 [00:19<00:01,  3.27it/s, loss=725, v_num=23, train_loss_step=638.0, val_loss=899.0, train_loss_epoch=680.0]\n",
      "Epoch 38:  96%|█████████▌| 64/67 [00:19<00:00,  3.27it/s, loss=725, v_num=23, train_loss_step=638.0, val_loss=899.0, train_loss_epoch=680.0]\n",
      "Epoch 38:  97%|█████████▋| 65/67 [00:19<00:00,  3.27it/s, loss=725, v_num=23, train_loss_step=638.0, val_loss=899.0, train_loss_epoch=680.0]\n",
      "Epoch 38:  99%|█████████▊| 66/67 [00:20<00:00,  3.27it/s, loss=725, v_num=23, train_loss_step=638.0, val_loss=899.0, train_loss_epoch=680.0]\n",
      "Epoch 38: 100%|██████████| 67/67 [00:20<00:00,  3.20it/s, loss=725, v_num=23, train_loss_step=638.0, val_loss=899.0, train_loss_epoch=680.0]\n",
      "Epoch 39:  45%|████▍     | 30/67 [00:06<00:07,  4.84it/s, loss=806, v_num=23, train_loss_step=1.37e+3, val_loss=899.0, train_loss_epoch=940.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 39:  46%|████▋     | 31/67 [00:08<00:09,  3.76it/s, loss=806, v_num=23, train_loss_step=1.37e+3, val_loss=899.0, train_loss_epoch=940.0]\n",
      "Epoch 39:  48%|████▊     | 32/67 [00:08<00:09,  3.74it/s, loss=806, v_num=23, train_loss_step=1.37e+3, val_loss=899.0, train_loss_epoch=940.0]\n",
      "Epoch 39:  49%|████▉     | 33/67 [00:08<00:09,  3.73it/s, loss=806, v_num=23, train_loss_step=1.37e+3, val_loss=899.0, train_loss_epoch=940.0]\n",
      "Epoch 39:  51%|█████     | 34/67 [00:09<00:08,  3.72it/s, loss=806, v_num=23, train_loss_step=1.37e+3, val_loss=899.0, train_loss_epoch=940.0]\n",
      "Epoch 39:  52%|█████▏    | 35/67 [00:09<00:08,  3.70it/s, loss=806, v_num=23, train_loss_step=1.37e+3, val_loss=899.0, train_loss_epoch=940.0]\n",
      "Epoch 39:  54%|█████▎    | 36/67 [00:09<00:08,  3.70it/s, loss=806, v_num=23, train_loss_step=1.37e+3, val_loss=899.0, train_loss_epoch=940.0]\n",
      "Epoch 39:  55%|█████▌    | 37/67 [00:10<00:08,  3.64it/s, loss=806, v_num=23, train_loss_step=1.37e+3, val_loss=899.0, train_loss_epoch=940.0]\n",
      "Epoch 39:  57%|█████▋    | 38/67 [00:10<00:07,  3.63it/s, loss=806, v_num=23, train_loss_step=1.37e+3, val_loss=899.0, train_loss_epoch=940.0]\n",
      "Epoch 39:  58%|█████▊    | 39/67 [00:10<00:07,  3.62it/s, loss=806, v_num=23, train_loss_step=1.37e+3, val_loss=899.0, train_loss_epoch=940.0]\n",
      "Epoch 39:  60%|█████▉    | 40/67 [00:11<00:07,  3.55it/s, loss=806, v_num=23, train_loss_step=1.37e+3, val_loss=899.0, train_loss_epoch=940.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 39:  61%|██████    | 41/67 [00:11<00:07,  3.52it/s, loss=806, v_num=23, train_loss_step=1.37e+3, val_loss=899.0, train_loss_epoch=940.0]\n",
      "Epoch 39:  63%|██████▎   | 42/67 [00:11<00:07,  3.52it/s, loss=806, v_num=23, train_loss_step=1.37e+3, val_loss=899.0, train_loss_epoch=940.0]\n",
      "Epoch 39:  64%|██████▍   | 43/67 [00:12<00:06,  3.49it/s, loss=806, v_num=23, train_loss_step=1.37e+3, val_loss=899.0, train_loss_epoch=940.0]\n",
      "Epoch 39:  66%|██████▌   | 44/67 [00:12<00:06,  3.49it/s, loss=806, v_num=23, train_loss_step=1.37e+3, val_loss=899.0, train_loss_epoch=940.0]\n",
      "Epoch 39:  67%|██████▋   | 45/67 [00:12<00:06,  3.49it/s, loss=806, v_num=23, train_loss_step=1.37e+3, val_loss=899.0, train_loss_epoch=940.0]\n",
      "Epoch 39:  69%|██████▊   | 46/67 [00:13<00:06,  3.43it/s, loss=806, v_num=23, train_loss_step=1.37e+3, val_loss=899.0, train_loss_epoch=940.0]\n",
      "Epoch 39:  70%|███████   | 47/67 [00:13<00:05,  3.43it/s, loss=806, v_num=23, train_loss_step=1.37e+3, val_loss=899.0, train_loss_epoch=940.0]\n",
      "Epoch 39:  72%|███████▏  | 48/67 [00:13<00:05,  3.43it/s, loss=806, v_num=23, train_loss_step=1.37e+3, val_loss=899.0, train_loss_epoch=940.0]\n",
      "Epoch 39:  73%|███████▎  | 49/67 [00:14<00:05,  3.38it/s, loss=806, v_num=23, train_loss_step=1.37e+3, val_loss=899.0, train_loss_epoch=940.0]\n",
      "Epoch 39:  75%|███████▍  | 50/67 [00:14<00:05,  3.38it/s, loss=806, v_num=23, train_loss_step=1.37e+3, val_loss=899.0, train_loss_epoch=940.0]\n",
      "Epoch 39:  76%|███████▌  | 51/67 [00:15<00:04,  3.35it/s, loss=806, v_num=23, train_loss_step=1.37e+3, val_loss=899.0, train_loss_epoch=940.0]\n",
      "Epoch 39:  78%|███████▊  | 52/67 [00:15<00:04,  3.34it/s, loss=806, v_num=23, train_loss_step=1.37e+3, val_loss=899.0, train_loss_epoch=940.0]\n",
      "Epoch 39:  79%|███████▉  | 53/67 [00:15<00:04,  3.35it/s, loss=806, v_num=23, train_loss_step=1.37e+3, val_loss=899.0, train_loss_epoch=940.0]\n",
      "Epoch 39:  81%|████████  | 54/67 [00:16<00:03,  3.35it/s, loss=806, v_num=23, train_loss_step=1.37e+3, val_loss=899.0, train_loss_epoch=940.0]\n",
      "Epoch 39:  82%|████████▏ | 55/67 [00:16<00:03,  3.31it/s, loss=806, v_num=23, train_loss_step=1.37e+3, val_loss=899.0, train_loss_epoch=940.0]\n",
      "Epoch 39:  84%|████████▎ | 56/67 [00:16<00:03,  3.31it/s, loss=806, v_num=23, train_loss_step=1.37e+3, val_loss=899.0, train_loss_epoch=940.0]\n",
      "Epoch 39:  85%|████████▌ | 57/67 [00:17<00:03,  3.31it/s, loss=806, v_num=23, train_loss_step=1.37e+3, val_loss=899.0, train_loss_epoch=940.0]\n",
      "Epoch 39:  87%|████████▋ | 58/67 [00:17<00:02,  3.28it/s, loss=806, v_num=23, train_loss_step=1.37e+3, val_loss=899.0, train_loss_epoch=940.0]\n",
      "Epoch 39:  88%|████████▊ | 59/67 [00:17<00:02,  3.28it/s, loss=806, v_num=23, train_loss_step=1.37e+3, val_loss=899.0, train_loss_epoch=940.0]\n",
      "Epoch 39:  90%|████████▉ | 60/67 [00:18<00:02,  3.28it/s, loss=806, v_num=23, train_loss_step=1.37e+3, val_loss=899.0, train_loss_epoch=940.0]\n",
      "Epoch 39:  91%|█████████ | 61/67 [00:18<00:01,  3.23it/s, loss=806, v_num=23, train_loss_step=1.37e+3, val_loss=899.0, train_loss_epoch=940.0]\n",
      "Epoch 39:  93%|█████████▎| 62/67 [00:19<00:01,  3.23it/s, loss=806, v_num=23, train_loss_step=1.37e+3, val_loss=899.0, train_loss_epoch=940.0]\n",
      "Epoch 39:  94%|█████████▍| 63/67 [00:19<00:01,  3.23it/s, loss=806, v_num=23, train_loss_step=1.37e+3, val_loss=899.0, train_loss_epoch=940.0]\n",
      "Epoch 39:  96%|█████████▌| 64/67 [00:19<00:00,  3.23it/s, loss=806, v_num=23, train_loss_step=1.37e+3, val_loss=899.0, train_loss_epoch=940.0]\n",
      "Epoch 39:  97%|█████████▋| 65/67 [00:20<00:00,  3.24it/s, loss=806, v_num=23, train_loss_step=1.37e+3, val_loss=899.0, train_loss_epoch=940.0]\n",
      "Epoch 39:  99%|█████████▊| 66/67 [00:20<00:00,  3.24it/s, loss=806, v_num=23, train_loss_step=1.37e+3, val_loss=899.0, train_loss_epoch=940.0]\n",
      "Epoch 39: 100%|██████████| 67/67 [00:21<00:00,  3.17it/s, loss=806, v_num=23, train_loss_step=1.37e+3, val_loss=899.0, train_loss_epoch=940.0]\n",
      "Epoch 40:  45%|████▍     | 30/67 [00:06<00:07,  4.96it/s, loss=678, v_num=23, train_loss_step=814.0, val_loss=899.0, train_loss_epoch=786.0]       \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 40:  46%|████▋     | 31/67 [00:08<00:09,  3.82it/s, loss=678, v_num=23, train_loss_step=814.0, val_loss=899.0, train_loss_epoch=786.0]\n",
      "Epoch 40:  48%|████▊     | 32/67 [00:08<00:09,  3.80it/s, loss=678, v_num=23, train_loss_step=814.0, val_loss=899.0, train_loss_epoch=786.0]\n",
      "Epoch 40:  49%|████▉     | 33/67 [00:08<00:08,  3.79it/s, loss=678, v_num=23, train_loss_step=814.0, val_loss=899.0, train_loss_epoch=786.0]\n",
      "Epoch 40:  51%|█████     | 34/67 [00:08<00:08,  3.78it/s, loss=678, v_num=23, train_loss_step=814.0, val_loss=899.0, train_loss_epoch=786.0]\n",
      "Epoch 40:  52%|█████▏    | 35/67 [00:09<00:08,  3.77it/s, loss=678, v_num=23, train_loss_step=814.0, val_loss=899.0, train_loss_epoch=786.0]\n",
      "Epoch 40:  54%|█████▎    | 36/67 [00:09<00:08,  3.76it/s, loss=678, v_num=23, train_loss_step=814.0, val_loss=899.0, train_loss_epoch=786.0]\n",
      "Epoch 40:  55%|█████▌    | 37/67 [00:09<00:08,  3.71it/s, loss=678, v_num=23, train_loss_step=814.0, val_loss=899.0, train_loss_epoch=786.0]\n",
      "Epoch 40:  57%|█████▋    | 38/67 [00:10<00:07,  3.70it/s, loss=678, v_num=23, train_loss_step=814.0, val_loss=899.0, train_loss_epoch=786.0]\n",
      "Epoch 40:  58%|█████▊    | 39/67 [00:10<00:07,  3.69it/s, loss=678, v_num=23, train_loss_step=814.0, val_loss=899.0, train_loss_epoch=786.0]\n",
      "Epoch 40:  60%|█████▉    | 40/67 [00:11<00:07,  3.63it/s, loss=678, v_num=23, train_loss_step=814.0, val_loss=899.0, train_loss_epoch=786.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 40:  61%|██████    | 41/67 [00:11<00:07,  3.57it/s, loss=678, v_num=23, train_loss_step=814.0, val_loss=899.0, train_loss_epoch=786.0]\n",
      "Epoch 40:  63%|██████▎   | 42/67 [00:11<00:07,  3.57it/s, loss=678, v_num=23, train_loss_step=814.0, val_loss=899.0, train_loss_epoch=786.0]\n",
      "Epoch 40:  64%|██████▍   | 43/67 [00:12<00:06,  3.55it/s, loss=678, v_num=23, train_loss_step=814.0, val_loss=899.0, train_loss_epoch=786.0]\n",
      "Epoch 40:  66%|██████▌   | 44/67 [00:12<00:06,  3.55it/s, loss=678, v_num=23, train_loss_step=814.0, val_loss=899.0, train_loss_epoch=786.0]\n",
      "Epoch 40:  67%|██████▋   | 45/67 [00:12<00:06,  3.55it/s, loss=678, v_num=23, train_loss_step=814.0, val_loss=899.0, train_loss_epoch=786.0]\n",
      "Epoch 40:  69%|██████▊   | 46/67 [00:13<00:06,  3.49it/s, loss=678, v_num=23, train_loss_step=814.0, val_loss=899.0, train_loss_epoch=786.0]\n",
      "Epoch 40:  70%|███████   | 47/67 [00:13<00:05,  3.49it/s, loss=678, v_num=23, train_loss_step=814.0, val_loss=899.0, train_loss_epoch=786.0]\n",
      "Epoch 40:  72%|███████▏  | 48/67 [00:13<00:05,  3.49it/s, loss=678, v_num=23, train_loss_step=814.0, val_loss=899.0, train_loss_epoch=786.0]\n",
      "Epoch 40:  73%|███████▎  | 49/67 [00:14<00:05,  3.44it/s, loss=678, v_num=23, train_loss_step=814.0, val_loss=899.0, train_loss_epoch=786.0]\n",
      "Epoch 40:  75%|███████▍  | 50/67 [00:14<00:04,  3.43it/s, loss=678, v_num=23, train_loss_step=814.0, val_loss=899.0, train_loss_epoch=786.0]\n",
      "Epoch 40:  76%|███████▌  | 51/67 [00:14<00:04,  3.40it/s, loss=678, v_num=23, train_loss_step=814.0, val_loss=899.0, train_loss_epoch=786.0]\n",
      "Epoch 40:  78%|███████▊  | 52/67 [00:15<00:04,  3.40it/s, loss=678, v_num=23, train_loss_step=814.0, val_loss=899.0, train_loss_epoch=786.0]\n",
      "Epoch 40:  79%|███████▉  | 53/67 [00:15<00:04,  3.39it/s, loss=678, v_num=23, train_loss_step=814.0, val_loss=899.0, train_loss_epoch=786.0]\n",
      "Epoch 40:  81%|████████  | 54/67 [00:15<00:03,  3.39it/s, loss=678, v_num=23, train_loss_step=814.0, val_loss=899.0, train_loss_epoch=786.0]\n",
      "Epoch 40:  82%|████████▏ | 55/67 [00:16<00:03,  3.36it/s, loss=678, v_num=23, train_loss_step=814.0, val_loss=899.0, train_loss_epoch=786.0]\n",
      "Epoch 40:  84%|████████▎ | 56/67 [00:16<00:03,  3.35it/s, loss=678, v_num=23, train_loss_step=814.0, val_loss=899.0, train_loss_epoch=786.0]\n",
      "Epoch 40:  85%|████████▌ | 57/67 [00:17<00:02,  3.35it/s, loss=678, v_num=23, train_loss_step=814.0, val_loss=899.0, train_loss_epoch=786.0]\n",
      "Epoch 40:  87%|████████▋ | 58/67 [00:17<00:02,  3.33it/s, loss=678, v_num=23, train_loss_step=814.0, val_loss=899.0, train_loss_epoch=786.0]\n",
      "Epoch 40:  88%|████████▊ | 59/67 [00:17<00:02,  3.32it/s, loss=678, v_num=23, train_loss_step=814.0, val_loss=899.0, train_loss_epoch=786.0]\n",
      "Epoch 40:  90%|████████▉ | 60/67 [00:18<00:02,  3.32it/s, loss=678, v_num=23, train_loss_step=814.0, val_loss=899.0, train_loss_epoch=786.0]\n",
      "Epoch 40:  91%|█████████ | 61/67 [00:18<00:01,  3.27it/s, loss=678, v_num=23, train_loss_step=814.0, val_loss=899.0, train_loss_epoch=786.0]\n",
      "Epoch 40:  93%|█████████▎| 62/67 [00:18<00:01,  3.27it/s, loss=678, v_num=23, train_loss_step=814.0, val_loss=899.0, train_loss_epoch=786.0]\n",
      "Epoch 40:  94%|█████████▍| 63/67 [00:19<00:01,  3.27it/s, loss=678, v_num=23, train_loss_step=814.0, val_loss=899.0, train_loss_epoch=786.0]\n",
      "Epoch 40:  96%|█████████▌| 64/67 [00:19<00:00,  3.26it/s, loss=678, v_num=23, train_loss_step=814.0, val_loss=899.0, train_loss_epoch=786.0]\n",
      "Epoch 40:  97%|█████████▋| 65/67 [00:19<00:00,  3.27it/s, loss=678, v_num=23, train_loss_step=814.0, val_loss=899.0, train_loss_epoch=786.0]\n",
      "Epoch 40:  99%|█████████▊| 66/67 [00:20<00:00,  3.27it/s, loss=678, v_num=23, train_loss_step=814.0, val_loss=899.0, train_loss_epoch=786.0]\n",
      "Epoch 40: 100%|██████████| 67/67 [00:20<00:00,  3.20it/s, loss=678, v_num=23, train_loss_step=814.0, val_loss=899.0, train_loss_epoch=786.0]\n",
      "Epoch 41:  45%|████▍     | 30/67 [00:06<00:07,  4.91it/s, loss=729, v_num=23, train_loss_step=177.0, val_loss=899.0, train_loss_epoch=823.0]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 41:  46%|████▋     | 31/67 [00:08<00:09,  3.80it/s, loss=729, v_num=23, train_loss_step=177.0, val_loss=899.0, train_loss_epoch=823.0]\n",
      "Epoch 41:  48%|████▊     | 32/67 [00:08<00:09,  3.78it/s, loss=729, v_num=23, train_loss_step=177.0, val_loss=899.0, train_loss_epoch=823.0]\n",
      "Epoch 41:  49%|████▉     | 33/67 [00:08<00:09,  3.77it/s, loss=729, v_num=23, train_loss_step=177.0, val_loss=899.0, train_loss_epoch=823.0]\n",
      "Epoch 41:  51%|█████     | 34/67 [00:09<00:08,  3.75it/s, loss=729, v_num=23, train_loss_step=177.0, val_loss=899.0, train_loss_epoch=823.0]\n",
      "Epoch 41:  52%|█████▏    | 35/67 [00:09<00:08,  3.74it/s, loss=729, v_num=23, train_loss_step=177.0, val_loss=899.0, train_loss_epoch=823.0]\n",
      "Epoch 41:  54%|█████▎    | 36/67 [00:09<00:08,  3.74it/s, loss=729, v_num=23, train_loss_step=177.0, val_loss=899.0, train_loss_epoch=823.0]\n",
      "Epoch 41:  55%|█████▌    | 37/67 [00:10<00:08,  3.66it/s, loss=729, v_num=23, train_loss_step=177.0, val_loss=899.0, train_loss_epoch=823.0]\n",
      "Epoch 41:  57%|█████▋    | 38/67 [00:10<00:07,  3.66it/s, loss=729, v_num=23, train_loss_step=177.0, val_loss=899.0, train_loss_epoch=823.0]\n",
      "Epoch 41:  58%|█████▊    | 39/67 [00:10<00:07,  3.65it/s, loss=729, v_num=23, train_loss_step=177.0, val_loss=899.0, train_loss_epoch=823.0]\n",
      "Epoch 41:  60%|█████▉    | 40/67 [00:11<00:07,  3.58it/s, loss=729, v_num=23, train_loss_step=177.0, val_loss=899.0, train_loss_epoch=823.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 41:  61%|██████    | 41/67 [00:11<00:07,  3.53it/s, loss=729, v_num=23, train_loss_step=177.0, val_loss=899.0, train_loss_epoch=823.0]\n",
      "Epoch 41:  63%|██████▎   | 42/67 [00:11<00:07,  3.53it/s, loss=729, v_num=23, train_loss_step=177.0, val_loss=899.0, train_loss_epoch=823.0]\n",
      "Epoch 41:  64%|██████▍   | 43/67 [00:12<00:06,  3.50it/s, loss=729, v_num=23, train_loss_step=177.0, val_loss=899.0, train_loss_epoch=823.0]\n",
      "Epoch 41:  66%|██████▌   | 44/67 [00:12<00:06,  3.50it/s, loss=729, v_num=23, train_loss_step=177.0, val_loss=899.0, train_loss_epoch=823.0]\n",
      "Epoch 41:  67%|██████▋   | 45/67 [00:12<00:06,  3.49it/s, loss=729, v_num=23, train_loss_step=177.0, val_loss=899.0, train_loss_epoch=823.0]\n",
      "Epoch 41:  69%|██████▊   | 46/67 [00:13<00:06,  3.44it/s, loss=729, v_num=23, train_loss_step=177.0, val_loss=899.0, train_loss_epoch=823.0]\n",
      "Epoch 41:  70%|███████   | 47/67 [00:13<00:05,  3.44it/s, loss=729, v_num=23, train_loss_step=177.0, val_loss=899.0, train_loss_epoch=823.0]\n",
      "Epoch 41:  72%|███████▏  | 48/67 [00:13<00:05,  3.44it/s, loss=729, v_num=23, train_loss_step=177.0, val_loss=899.0, train_loss_epoch=823.0]\n",
      "Epoch 41:  73%|███████▎  | 49/67 [00:14<00:05,  3.39it/s, loss=729, v_num=23, train_loss_step=177.0, val_loss=899.0, train_loss_epoch=823.0]\n",
      "Epoch 41:  75%|███████▍  | 50/67 [00:14<00:05,  3.39it/s, loss=729, v_num=23, train_loss_step=177.0, val_loss=899.0, train_loss_epoch=823.0]\n",
      "Epoch 41:  76%|███████▌  | 51/67 [00:15<00:04,  3.37it/s, loss=729, v_num=23, train_loss_step=177.0, val_loss=899.0, train_loss_epoch=823.0]\n",
      "Epoch 41:  78%|███████▊  | 52/67 [00:15<00:04,  3.35it/s, loss=729, v_num=23, train_loss_step=177.0, val_loss=899.0, train_loss_epoch=823.0]\n",
      "Epoch 41:  79%|███████▉  | 53/67 [00:15<00:04,  3.35it/s, loss=729, v_num=23, train_loss_step=177.0, val_loss=899.0, train_loss_epoch=823.0]\n",
      "Epoch 41:  81%|████████  | 54/67 [00:16<00:03,  3.36it/s, loss=729, v_num=23, train_loss_step=177.0, val_loss=899.0, train_loss_epoch=823.0]\n",
      "Epoch 41:  82%|████████▏ | 55/67 [00:16<00:03,  3.32it/s, loss=729, v_num=23, train_loss_step=177.0, val_loss=899.0, train_loss_epoch=823.0]\n",
      "Epoch 41:  84%|████████▎ | 56/67 [00:16<00:03,  3.32it/s, loss=729, v_num=23, train_loss_step=177.0, val_loss=899.0, train_loss_epoch=823.0]\n",
      "Epoch 41:  85%|████████▌ | 57/67 [00:17<00:03,  3.32it/s, loss=729, v_num=23, train_loss_step=177.0, val_loss=899.0, train_loss_epoch=823.0]\n",
      "Epoch 41:  87%|████████▋ | 58/67 [00:17<00:02,  3.29it/s, loss=729, v_num=23, train_loss_step=177.0, val_loss=899.0, train_loss_epoch=823.0]\n",
      "Epoch 41:  88%|████████▊ | 59/67 [00:17<00:02,  3.29it/s, loss=729, v_num=23, train_loss_step=177.0, val_loss=899.0, train_loss_epoch=823.0]\n",
      "Epoch 41:  90%|████████▉ | 60/67 [00:18<00:02,  3.29it/s, loss=729, v_num=23, train_loss_step=177.0, val_loss=899.0, train_loss_epoch=823.0]\n",
      "Epoch 41:  91%|█████████ | 61/67 [00:18<00:01,  3.24it/s, loss=729, v_num=23, train_loss_step=177.0, val_loss=899.0, train_loss_epoch=823.0]\n",
      "Epoch 41:  93%|█████████▎| 62/67 [00:19<00:01,  3.24it/s, loss=729, v_num=23, train_loss_step=177.0, val_loss=899.0, train_loss_epoch=823.0]\n",
      "Epoch 41:  94%|█████████▍| 63/67 [00:19<00:01,  3.25it/s, loss=729, v_num=23, train_loss_step=177.0, val_loss=899.0, train_loss_epoch=823.0]\n",
      "Epoch 41:  96%|█████████▌| 64/67 [00:19<00:00,  3.24it/s, loss=729, v_num=23, train_loss_step=177.0, val_loss=899.0, train_loss_epoch=823.0]\n",
      "Epoch 41:  97%|█████████▋| 65/67 [00:20<00:00,  3.24it/s, loss=729, v_num=23, train_loss_step=177.0, val_loss=899.0, train_loss_epoch=823.0]\n",
      "Epoch 41:  99%|█████████▊| 66/67 [00:20<00:00,  3.25it/s, loss=729, v_num=23, train_loss_step=177.0, val_loss=899.0, train_loss_epoch=823.0]\n",
      "Epoch 41: 100%|██████████| 67/67 [00:21<00:00,  3.18it/s, loss=729, v_num=23, train_loss_step=177.0, val_loss=899.0, train_loss_epoch=823.0]\n",
      "Epoch 42:  45%|████▍     | 30/67 [00:06<00:07,  4.98it/s, loss=649, v_num=23, train_loss_step=255.0, val_loss=899.0, train_loss_epoch=734.0]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 42:  46%|████▋     | 31/67 [00:08<00:09,  3.86it/s, loss=649, v_num=23, train_loss_step=255.0, val_loss=899.0, train_loss_epoch=734.0]\n",
      "Epoch 42:  48%|████▊     | 32/67 [00:08<00:09,  3.84it/s, loss=649, v_num=23, train_loss_step=255.0, val_loss=899.0, train_loss_epoch=734.0]\n",
      "Epoch 42:  49%|████▉     | 33/67 [00:08<00:08,  3.82it/s, loss=649, v_num=23, train_loss_step=255.0, val_loss=899.0, train_loss_epoch=734.0]\n",
      "Epoch 42:  51%|█████     | 34/67 [00:08<00:08,  3.81it/s, loss=649, v_num=23, train_loss_step=255.0, val_loss=899.0, train_loss_epoch=734.0]\n",
      "Epoch 42:  52%|█████▏    | 35/67 [00:09<00:08,  3.80it/s, loss=649, v_num=23, train_loss_step=255.0, val_loss=899.0, train_loss_epoch=734.0]\n",
      "Epoch 42:  54%|█████▎    | 36/67 [00:09<00:08,  3.79it/s, loss=649, v_num=23, train_loss_step=255.0, val_loss=899.0, train_loss_epoch=734.0]\n",
      "Epoch 42:  55%|█████▌    | 37/67 [00:09<00:08,  3.70it/s, loss=649, v_num=23, train_loss_step=255.0, val_loss=899.0, train_loss_epoch=734.0]\n",
      "Epoch 42:  57%|█████▋    | 38/67 [00:10<00:07,  3.70it/s, loss=649, v_num=23, train_loss_step=255.0, val_loss=899.0, train_loss_epoch=734.0]\n",
      "Epoch 42:  58%|█████▊    | 39/67 [00:10<00:07,  3.69it/s, loss=649, v_num=23, train_loss_step=255.0, val_loss=899.0, train_loss_epoch=734.0]\n",
      "Epoch 42:  60%|█████▉    | 40/67 [00:11<00:07,  3.61it/s, loss=649, v_num=23, train_loss_step=255.0, val_loss=899.0, train_loss_epoch=734.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 42:  61%|██████    | 41/67 [00:11<00:07,  3.57it/s, loss=649, v_num=23, train_loss_step=255.0, val_loss=899.0, train_loss_epoch=734.0]\n",
      "Epoch 42:  63%|██████▎   | 42/67 [00:11<00:07,  3.56it/s, loss=649, v_num=23, train_loss_step=255.0, val_loss=899.0, train_loss_epoch=734.0]\n",
      "Epoch 42:  64%|██████▍   | 43/67 [00:12<00:06,  3.54it/s, loss=649, v_num=23, train_loss_step=255.0, val_loss=899.0, train_loss_epoch=734.0]\n",
      "Epoch 42:  66%|██████▌   | 44/67 [00:12<00:06,  3.54it/s, loss=649, v_num=23, train_loss_step=255.0, val_loss=899.0, train_loss_epoch=734.0]\n",
      "Epoch 42:  67%|██████▋   | 45/67 [00:12<00:06,  3.53it/s, loss=649, v_num=23, train_loss_step=255.0, val_loss=899.0, train_loss_epoch=734.0]\n",
      "Epoch 42:  69%|██████▊   | 46/67 [00:13<00:06,  3.47it/s, loss=649, v_num=23, train_loss_step=255.0, val_loss=899.0, train_loss_epoch=734.0]\n",
      "Epoch 42:  70%|███████   | 47/67 [00:13<00:05,  3.47it/s, loss=649, v_num=23, train_loss_step=255.0, val_loss=899.0, train_loss_epoch=734.0]\n",
      "Epoch 42:  72%|███████▏  | 48/67 [00:13<00:05,  3.47it/s, loss=649, v_num=23, train_loss_step=255.0, val_loss=899.0, train_loss_epoch=734.0]\n",
      "Epoch 42:  73%|███████▎  | 49/67 [00:14<00:05,  3.42it/s, loss=649, v_num=23, train_loss_step=255.0, val_loss=899.0, train_loss_epoch=734.0]\n",
      "Epoch 42:  75%|███████▍  | 50/67 [00:14<00:04,  3.42it/s, loss=649, v_num=23, train_loss_step=255.0, val_loss=899.0, train_loss_epoch=734.0]\n",
      "Epoch 42:  76%|███████▌  | 51/67 [00:15<00:04,  3.39it/s, loss=649, v_num=23, train_loss_step=255.0, val_loss=899.0, train_loss_epoch=734.0]\n",
      "Epoch 42:  78%|███████▊  | 52/67 [00:15<00:04,  3.37it/s, loss=649, v_num=23, train_loss_step=255.0, val_loss=899.0, train_loss_epoch=734.0]\n",
      "Epoch 42:  79%|███████▉  | 53/67 [00:15<00:04,  3.37it/s, loss=649, v_num=23, train_loss_step=255.0, val_loss=899.0, train_loss_epoch=734.0]\n",
      "Epoch 42:  81%|████████  | 54/67 [00:16<00:03,  3.37it/s, loss=649, v_num=23, train_loss_step=255.0, val_loss=899.0, train_loss_epoch=734.0]\n",
      "Epoch 42:  82%|████████▏ | 55/67 [00:16<00:03,  3.34it/s, loss=649, v_num=23, train_loss_step=255.0, val_loss=899.0, train_loss_epoch=734.0]\n",
      "Epoch 42:  84%|████████▎ | 56/67 [00:16<00:03,  3.34it/s, loss=649, v_num=23, train_loss_step=255.0, val_loss=899.0, train_loss_epoch=734.0]\n",
      "Epoch 42:  85%|████████▌ | 57/67 [00:17<00:02,  3.34it/s, loss=649, v_num=23, train_loss_step=255.0, val_loss=899.0, train_loss_epoch=734.0]\n",
      "Epoch 42:  87%|████████▋ | 58/67 [00:17<00:02,  3.31it/s, loss=649, v_num=23, train_loss_step=255.0, val_loss=899.0, train_loss_epoch=734.0]\n",
      "Epoch 42:  88%|████████▊ | 59/67 [00:17<00:02,  3.31it/s, loss=649, v_num=23, train_loss_step=255.0, val_loss=899.0, train_loss_epoch=734.0]\n",
      "Epoch 42:  90%|████████▉ | 60/67 [00:18<00:02,  3.31it/s, loss=649, v_num=23, train_loss_step=255.0, val_loss=899.0, train_loss_epoch=734.0]\n",
      "Epoch 42:  91%|█████████ | 61/67 [00:18<00:01,  3.26it/s, loss=649, v_num=23, train_loss_step=255.0, val_loss=899.0, train_loss_epoch=734.0]\n",
      "Epoch 42:  93%|█████████▎| 62/67 [00:19<00:01,  3.26it/s, loss=649, v_num=23, train_loss_step=255.0, val_loss=899.0, train_loss_epoch=734.0]\n",
      "Epoch 42:  94%|█████████▍| 63/67 [00:19<00:01,  3.26it/s, loss=649, v_num=23, train_loss_step=255.0, val_loss=899.0, train_loss_epoch=734.0]\n",
      "Epoch 42:  96%|█████████▌| 64/67 [00:19<00:00,  3.25it/s, loss=649, v_num=23, train_loss_step=255.0, val_loss=899.0, train_loss_epoch=734.0]\n",
      "Epoch 42:  97%|█████████▋| 65/67 [00:19<00:00,  3.26it/s, loss=649, v_num=23, train_loss_step=255.0, val_loss=899.0, train_loss_epoch=734.0]\n",
      "Epoch 42:  99%|█████████▊| 66/67 [00:20<00:00,  3.26it/s, loss=649, v_num=23, train_loss_step=255.0, val_loss=899.0, train_loss_epoch=734.0]\n",
      "Epoch 42: 100%|██████████| 67/67 [00:22<00:00,  2.99it/s, loss=649, v_num=23, train_loss_step=255.0, val_loss=899.0, train_loss_epoch=734.0]\n",
      "Epoch 43:  45%|████▍     | 30/67 [00:06<00:07,  4.84it/s, loss=686, v_num=23, train_loss_step=1.28e+3, val_loss=899.0, train_loss_epoch=673.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 43:  46%|████▋     | 31/67 [00:08<00:09,  3.75it/s, loss=686, v_num=23, train_loss_step=1.28e+3, val_loss=899.0, train_loss_epoch=673.0]\n",
      "Epoch 43:  48%|████▊     | 32/67 [00:08<00:09,  3.74it/s, loss=686, v_num=23, train_loss_step=1.28e+3, val_loss=899.0, train_loss_epoch=673.0]\n",
      "Epoch 43:  49%|████▉     | 33/67 [00:08<00:09,  3.73it/s, loss=686, v_num=23, train_loss_step=1.28e+3, val_loss=899.0, train_loss_epoch=673.0]\n",
      "Epoch 43:  51%|█████     | 34/67 [00:09<00:08,  3.72it/s, loss=686, v_num=23, train_loss_step=1.28e+3, val_loss=899.0, train_loss_epoch=673.0]\n",
      "Epoch 43:  52%|█████▏    | 35/67 [00:09<00:08,  3.70it/s, loss=686, v_num=23, train_loss_step=1.28e+3, val_loss=899.0, train_loss_epoch=673.0]\n",
      "Epoch 43:  54%|█████▎    | 36/67 [00:09<00:08,  3.70it/s, loss=686, v_num=23, train_loss_step=1.28e+3, val_loss=899.0, train_loss_epoch=673.0]\n",
      "Epoch 43:  55%|█████▌    | 37/67 [00:10<00:08,  3.64it/s, loss=686, v_num=23, train_loss_step=1.28e+3, val_loss=899.0, train_loss_epoch=673.0]\n",
      "Epoch 43:  57%|█████▋    | 38/67 [00:10<00:07,  3.63it/s, loss=686, v_num=23, train_loss_step=1.28e+3, val_loss=899.0, train_loss_epoch=673.0]\n",
      "Epoch 43:  58%|█████▊    | 39/67 [00:10<00:07,  3.63it/s, loss=686, v_num=23, train_loss_step=1.28e+3, val_loss=899.0, train_loss_epoch=673.0]\n",
      "Epoch 43:  60%|█████▉    | 40/67 [00:11<00:07,  3.56it/s, loss=686, v_num=23, train_loss_step=1.28e+3, val_loss=899.0, train_loss_epoch=673.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 43:  61%|██████    | 41/67 [00:11<00:07,  3.50it/s, loss=686, v_num=23, train_loss_step=1.28e+3, val_loss=899.0, train_loss_epoch=673.0]\n",
      "Epoch 43:  63%|██████▎   | 42/67 [00:11<00:07,  3.50it/s, loss=686, v_num=23, train_loss_step=1.28e+3, val_loss=899.0, train_loss_epoch=673.0]\n",
      "Epoch 43:  64%|██████▍   | 43/67 [00:12<00:06,  3.50it/s, loss=686, v_num=23, train_loss_step=1.28e+3, val_loss=899.0, train_loss_epoch=673.0]\n",
      "Epoch 43:  66%|██████▌   | 44/67 [00:12<00:06,  3.49it/s, loss=686, v_num=23, train_loss_step=1.28e+3, val_loss=899.0, train_loss_epoch=673.0]\n",
      "Epoch 43:  67%|██████▋   | 45/67 [00:12<00:06,  3.50it/s, loss=686, v_num=23, train_loss_step=1.28e+3, val_loss=899.0, train_loss_epoch=673.0]\n",
      "Epoch 43:  69%|██████▊   | 46/67 [00:13<00:06,  3.43it/s, loss=686, v_num=23, train_loss_step=1.28e+3, val_loss=899.0, train_loss_epoch=673.0]\n",
      "Epoch 43:  70%|███████   | 47/67 [00:13<00:05,  3.42it/s, loss=686, v_num=23, train_loss_step=1.28e+3, val_loss=899.0, train_loss_epoch=673.0]\n",
      "Epoch 43:  72%|███████▏  | 48/67 [00:14<00:05,  3.42it/s, loss=686, v_num=23, train_loss_step=1.28e+3, val_loss=899.0, train_loss_epoch=673.0]\n",
      "Epoch 43:  73%|███████▎  | 49/67 [00:14<00:05,  3.37it/s, loss=686, v_num=23, train_loss_step=1.28e+3, val_loss=899.0, train_loss_epoch=673.0]\n",
      "Epoch 43:  75%|███████▍  | 50/67 [00:14<00:05,  3.37it/s, loss=686, v_num=23, train_loss_step=1.28e+3, val_loss=899.0, train_loss_epoch=673.0]\n",
      "Epoch 43:  76%|███████▌  | 51/67 [00:15<00:04,  3.35it/s, loss=686, v_num=23, train_loss_step=1.28e+3, val_loss=899.0, train_loss_epoch=673.0]\n",
      "Epoch 43:  78%|███████▊  | 52/67 [00:15<00:04,  3.34it/s, loss=686, v_num=23, train_loss_step=1.28e+3, val_loss=899.0, train_loss_epoch=673.0]\n",
      "Epoch 43:  79%|███████▉  | 53/67 [00:15<00:04,  3.34it/s, loss=686, v_num=23, train_loss_step=1.28e+3, val_loss=899.0, train_loss_epoch=673.0]\n",
      "Epoch 43:  81%|████████  | 54/67 [00:16<00:03,  3.35it/s, loss=686, v_num=23, train_loss_step=1.28e+3, val_loss=899.0, train_loss_epoch=673.0]\n",
      "Epoch 43:  82%|████████▏ | 55/67 [00:16<00:03,  3.31it/s, loss=686, v_num=23, train_loss_step=1.28e+3, val_loss=899.0, train_loss_epoch=673.0]\n",
      "Epoch 43:  84%|████████▎ | 56/67 [00:16<00:03,  3.31it/s, loss=686, v_num=23, train_loss_step=1.28e+3, val_loss=899.0, train_loss_epoch=673.0]\n",
      "Epoch 43:  85%|████████▌ | 57/67 [00:17<00:03,  3.32it/s, loss=686, v_num=23, train_loss_step=1.28e+3, val_loss=899.0, train_loss_epoch=673.0]\n",
      "Epoch 43:  87%|████████▋ | 58/67 [00:19<00:02,  3.01it/s, loss=686, v_num=23, train_loss_step=1.28e+3, val_loss=899.0, train_loss_epoch=673.0]\n",
      "Epoch 43:  88%|████████▊ | 59/67 [00:19<00:02,  3.01it/s, loss=686, v_num=23, train_loss_step=1.28e+3, val_loss=899.0, train_loss_epoch=673.0]\n",
      "Epoch 43:  90%|████████▉ | 60/67 [00:19<00:02,  3.02it/s, loss=686, v_num=23, train_loss_step=1.28e+3, val_loss=899.0, train_loss_epoch=673.0]\n",
      "Epoch 43:  91%|█████████ | 61/67 [00:20<00:02,  2.98it/s, loss=686, v_num=23, train_loss_step=1.28e+3, val_loss=899.0, train_loss_epoch=673.0]\n",
      "Epoch 43:  93%|█████████▎| 62/67 [00:20<00:01,  2.99it/s, loss=686, v_num=23, train_loss_step=1.28e+3, val_loss=899.0, train_loss_epoch=673.0]\n",
      "Epoch 43:  94%|█████████▍| 63/67 [00:21<00:01,  2.99it/s, loss=686, v_num=23, train_loss_step=1.28e+3, val_loss=899.0, train_loss_epoch=673.0]\n",
      "Epoch 43:  96%|█████████▌| 64/67 [00:21<00:01,  2.99it/s, loss=686, v_num=23, train_loss_step=1.28e+3, val_loss=899.0, train_loss_epoch=673.0]\n",
      "Epoch 43:  97%|█████████▋| 65/67 [00:21<00:00,  3.00it/s, loss=686, v_num=23, train_loss_step=1.28e+3, val_loss=899.0, train_loss_epoch=673.0]\n",
      "Epoch 43:  99%|█████████▊| 66/67 [00:21<00:00,  3.01it/s, loss=686, v_num=23, train_loss_step=1.28e+3, val_loss=899.0, train_loss_epoch=673.0]\n",
      "Epoch 43: 100%|██████████| 67/67 [00:22<00:00,  2.95it/s, loss=686, v_num=23, train_loss_step=1.28e+3, val_loss=899.0, train_loss_epoch=673.0]\n",
      "Epoch 44:  45%|████▍     | 30/67 [00:06<00:07,  4.98it/s, loss=714, v_num=23, train_loss_step=1.14e+3, val_loss=899.0, train_loss_epoch=642.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 44:  46%|████▋     | 31/67 [00:08<00:09,  3.85it/s, loss=714, v_num=23, train_loss_step=1.14e+3, val_loss=899.0, train_loss_epoch=642.0]\n",
      "Epoch 44:  48%|████▊     | 32/67 [00:08<00:09,  3.82it/s, loss=714, v_num=23, train_loss_step=1.14e+3, val_loss=899.0, train_loss_epoch=642.0]\n",
      "Epoch 44:  49%|████▉     | 33/67 [00:08<00:08,  3.81it/s, loss=714, v_num=23, train_loss_step=1.14e+3, val_loss=899.0, train_loss_epoch=642.0]\n",
      "Epoch 44:  51%|█████     | 34/67 [00:08<00:08,  3.79it/s, loss=714, v_num=23, train_loss_step=1.14e+3, val_loss=899.0, train_loss_epoch=642.0]\n",
      "Epoch 44:  52%|█████▏    | 35/67 [00:09<00:08,  3.78it/s, loss=714, v_num=23, train_loss_step=1.14e+3, val_loss=899.0, train_loss_epoch=642.0]\n",
      "Epoch 44:  54%|█████▎    | 36/67 [00:09<00:08,  3.78it/s, loss=714, v_num=23, train_loss_step=1.14e+3, val_loss=899.0, train_loss_epoch=642.0]\n",
      "Epoch 44:  55%|█████▌    | 37/67 [00:09<00:08,  3.70it/s, loss=714, v_num=23, train_loss_step=1.14e+3, val_loss=899.0, train_loss_epoch=642.0]\n",
      "Epoch 44:  57%|█████▋    | 38/67 [00:10<00:07,  3.69it/s, loss=714, v_num=23, train_loss_step=1.14e+3, val_loss=899.0, train_loss_epoch=642.0]\n",
      "Epoch 44:  58%|█████▊    | 39/67 [00:10<00:07,  3.69it/s, loss=714, v_num=23, train_loss_step=1.14e+3, val_loss=899.0, train_loss_epoch=642.0]\n",
      "Epoch 44:  60%|█████▉    | 40/67 [00:11<00:07,  3.61it/s, loss=714, v_num=23, train_loss_step=1.14e+3, val_loss=899.0, train_loss_epoch=642.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 44:  61%|██████    | 41/67 [00:11<00:07,  3.57it/s, loss=714, v_num=23, train_loss_step=1.14e+3, val_loss=899.0, train_loss_epoch=642.0]\n",
      "Epoch 44:  63%|██████▎   | 42/67 [00:11<00:07,  3.56it/s, loss=714, v_num=23, train_loss_step=1.14e+3, val_loss=899.0, train_loss_epoch=642.0]\n",
      "Epoch 44:  64%|██████▍   | 43/67 [00:12<00:06,  3.55it/s, loss=714, v_num=23, train_loss_step=1.14e+3, val_loss=899.0, train_loss_epoch=642.0]\n",
      "Epoch 44:  66%|██████▌   | 44/67 [00:12<00:06,  3.54it/s, loss=714, v_num=23, train_loss_step=1.14e+3, val_loss=899.0, train_loss_epoch=642.0]\n",
      "Epoch 44:  67%|██████▋   | 45/67 [00:12<00:06,  3.54it/s, loss=714, v_num=23, train_loss_step=1.14e+3, val_loss=899.0, train_loss_epoch=642.0]\n",
      "Epoch 44:  69%|██████▊   | 46/67 [00:13<00:06,  3.49it/s, loss=714, v_num=23, train_loss_step=1.14e+3, val_loss=899.0, train_loss_epoch=642.0]\n",
      "Epoch 44:  70%|███████   | 47/67 [00:13<00:05,  3.48it/s, loss=714, v_num=23, train_loss_step=1.14e+3, val_loss=899.0, train_loss_epoch=642.0]\n",
      "Epoch 44:  72%|███████▏  | 48/67 [00:13<00:05,  3.48it/s, loss=714, v_num=23, train_loss_step=1.14e+3, val_loss=899.0, train_loss_epoch=642.0]\n",
      "Epoch 44:  73%|███████▎  | 49/67 [00:15<00:05,  3.10it/s, loss=714, v_num=23, train_loss_step=1.14e+3, val_loss=899.0, train_loss_epoch=642.0]\n",
      "Epoch 44:  75%|███████▍  | 50/67 [00:16<00:05,  3.10it/s, loss=714, v_num=23, train_loss_step=1.14e+3, val_loss=899.0, train_loss_epoch=642.0]\n",
      "Epoch 44:  76%|███████▌  | 51/67 [00:16<00:05,  3.09it/s, loss=714, v_num=23, train_loss_step=1.14e+3, val_loss=899.0, train_loss_epoch=642.0]\n",
      "Epoch 44:  78%|███████▊  | 52/67 [00:16<00:04,  3.08it/s, loss=714, v_num=23, train_loss_step=1.14e+3, val_loss=899.0, train_loss_epoch=642.0]\n",
      "Epoch 44:  79%|███████▉  | 53/67 [00:17<00:04,  3.08it/s, loss=714, v_num=23, train_loss_step=1.14e+3, val_loss=899.0, train_loss_epoch=642.0]\n",
      "Epoch 44:  81%|████████  | 54/67 [00:17<00:04,  3.09it/s, loss=714, v_num=23, train_loss_step=1.14e+3, val_loss=899.0, train_loss_epoch=642.0]\n",
      "Epoch 44:  82%|████████▏ | 55/67 [00:17<00:03,  3.06it/s, loss=714, v_num=23, train_loss_step=1.14e+3, val_loss=899.0, train_loss_epoch=642.0]\n",
      "Epoch 44:  84%|████████▎ | 56/67 [00:18<00:03,  3.06it/s, loss=714, v_num=23, train_loss_step=1.14e+3, val_loss=899.0, train_loss_epoch=642.0]\n",
      "Epoch 44:  85%|████████▌ | 57/67 [00:18<00:03,  3.07it/s, loss=714, v_num=23, train_loss_step=1.14e+3, val_loss=899.0, train_loss_epoch=642.0]\n",
      "Epoch 44:  87%|████████▋ | 58/67 [00:19<00:02,  3.04it/s, loss=714, v_num=23, train_loss_step=1.14e+3, val_loss=899.0, train_loss_epoch=642.0]\n",
      "Epoch 44:  88%|████████▊ | 59/67 [00:19<00:02,  3.05it/s, loss=714, v_num=23, train_loss_step=1.14e+3, val_loss=899.0, train_loss_epoch=642.0]\n",
      "Epoch 44:  90%|████████▉ | 60/67 [00:19<00:02,  3.06it/s, loss=714, v_num=23, train_loss_step=1.14e+3, val_loss=899.0, train_loss_epoch=642.0]\n",
      "Epoch 44:  91%|█████████ | 61/67 [00:20<00:01,  3.01it/s, loss=714, v_num=23, train_loss_step=1.14e+3, val_loss=899.0, train_loss_epoch=642.0]\n",
      "Epoch 44:  93%|█████████▎| 62/67 [00:20<00:01,  3.02it/s, loss=714, v_num=23, train_loss_step=1.14e+3, val_loss=899.0, train_loss_epoch=642.0]\n",
      "Epoch 44:  94%|█████████▍| 63/67 [00:20<00:01,  3.02it/s, loss=714, v_num=23, train_loss_step=1.14e+3, val_loss=899.0, train_loss_epoch=642.0]\n",
      "Epoch 44:  96%|█████████▌| 64/67 [00:21<00:00,  3.02it/s, loss=714, v_num=23, train_loss_step=1.14e+3, val_loss=899.0, train_loss_epoch=642.0]\n",
      "Epoch 44:  97%|█████████▋| 65/67 [00:21<00:00,  3.02it/s, loss=714, v_num=23, train_loss_step=1.14e+3, val_loss=899.0, train_loss_epoch=642.0]\n",
      "Epoch 44:  99%|█████████▊| 66/67 [00:21<00:00,  3.03it/s, loss=714, v_num=23, train_loss_step=1.14e+3, val_loss=899.0, train_loss_epoch=642.0]\n",
      "Epoch 44: 100%|██████████| 67/67 [00:22<00:00,  2.97it/s, loss=714, v_num=23, train_loss_step=1.14e+3, val_loss=898.0, train_loss_epoch=642.0]\n",
      "Epoch 45:  45%|████▍     | 30/67 [00:06<00:07,  5.00it/s, loss=645, v_num=23, train_loss_step=249.0, val_loss=898.0, train_loss_epoch=807.0]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 45:  46%|████▋     | 31/67 [00:08<00:09,  3.84it/s, loss=645, v_num=23, train_loss_step=249.0, val_loss=898.0, train_loss_epoch=807.0]\n",
      "Epoch 45:  48%|████▊     | 32/67 [00:08<00:09,  3.82it/s, loss=645, v_num=23, train_loss_step=249.0, val_loss=898.0, train_loss_epoch=807.0]\n",
      "Epoch 45:  49%|████▉     | 33/67 [00:08<00:08,  3.81it/s, loss=645, v_num=23, train_loss_step=249.0, val_loss=898.0, train_loss_epoch=807.0]\n",
      "Epoch 45:  51%|█████     | 34/67 [00:08<00:08,  3.80it/s, loss=645, v_num=23, train_loss_step=249.0, val_loss=898.0, train_loss_epoch=807.0]\n",
      "Epoch 45:  52%|█████▏    | 35/67 [00:09<00:08,  3.79it/s, loss=645, v_num=23, train_loss_step=249.0, val_loss=898.0, train_loss_epoch=807.0]\n",
      "Epoch 45:  54%|█████▎    | 36/67 [00:09<00:08,  3.77it/s, loss=645, v_num=23, train_loss_step=249.0, val_loss=898.0, train_loss_epoch=807.0]\n",
      "Epoch 45:  55%|█████▌    | 37/67 [00:11<00:09,  3.19it/s, loss=645, v_num=23, train_loss_step=249.0, val_loss=898.0, train_loss_epoch=807.0]\n",
      "Epoch 45:  57%|█████▋    | 38/67 [00:11<00:09,  3.20it/s, loss=645, v_num=23, train_loss_step=249.0, val_loss=898.0, train_loss_epoch=807.0]\n",
      "Epoch 45:  58%|█████▊    | 39/67 [00:12<00:08,  3.20it/s, loss=645, v_num=23, train_loss_step=249.0, val_loss=898.0, train_loss_epoch=807.0]\n",
      "Epoch 45:  60%|█████▉    | 40/67 [00:12<00:08,  3.15it/s, loss=645, v_num=23, train_loss_step=249.0, val_loss=898.0, train_loss_epoch=807.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 45:  61%|██████    | 41/67 [00:13<00:08,  3.12it/s, loss=645, v_num=23, train_loss_step=249.0, val_loss=898.0, train_loss_epoch=807.0]\n",
      "Epoch 45:  63%|██████▎   | 42/67 [00:13<00:08,  3.12it/s, loss=645, v_num=23, train_loss_step=249.0, val_loss=898.0, train_loss_epoch=807.0]\n",
      "Epoch 45:  64%|██████▍   | 43/67 [00:13<00:07,  3.11it/s, loss=645, v_num=23, train_loss_step=249.0, val_loss=898.0, train_loss_epoch=807.0]\n",
      "Epoch 45:  66%|██████▌   | 44/67 [00:14<00:07,  3.11it/s, loss=645, v_num=23, train_loss_step=249.0, val_loss=898.0, train_loss_epoch=807.0]\n",
      "Epoch 45:  67%|██████▋   | 45/67 [00:14<00:07,  3.12it/s, loss=645, v_num=23, train_loss_step=249.0, val_loss=898.0, train_loss_epoch=807.0]\n",
      "Epoch 45:  69%|██████▊   | 46/67 [00:14<00:06,  3.09it/s, loss=645, v_num=23, train_loss_step=249.0, val_loss=898.0, train_loss_epoch=807.0]\n",
      "Epoch 45:  70%|███████   | 47/67 [00:15<00:06,  3.10it/s, loss=645, v_num=23, train_loss_step=249.0, val_loss=898.0, train_loss_epoch=807.0]\n",
      "Epoch 45:  72%|███████▏  | 48/67 [00:15<00:06,  3.10it/s, loss=645, v_num=23, train_loss_step=249.0, val_loss=898.0, train_loss_epoch=807.0]\n",
      "Epoch 45:  73%|███████▎  | 49/67 [00:15<00:05,  3.07it/s, loss=645, v_num=23, train_loss_step=249.0, val_loss=898.0, train_loss_epoch=807.0]\n",
      "Epoch 45:  75%|███████▍  | 50/67 [00:16<00:05,  3.07it/s, loss=645, v_num=23, train_loss_step=249.0, val_loss=898.0, train_loss_epoch=807.0]\n",
      "Epoch 45:  76%|███████▌  | 51/67 [00:16<00:05,  3.05it/s, loss=645, v_num=23, train_loss_step=249.0, val_loss=898.0, train_loss_epoch=807.0]\n",
      "Epoch 45:  78%|███████▊  | 52/67 [00:17<00:04,  3.04it/s, loss=645, v_num=23, train_loss_step=249.0, val_loss=898.0, train_loss_epoch=807.0]\n",
      "Epoch 45:  79%|███████▉  | 53/67 [00:17<00:04,  3.05it/s, loss=645, v_num=23, train_loss_step=249.0, val_loss=898.0, train_loss_epoch=807.0]\n",
      "Epoch 45:  81%|████████  | 54/67 [00:17<00:04,  3.05it/s, loss=645, v_num=23, train_loss_step=249.0, val_loss=898.0, train_loss_epoch=807.0]\n",
      "Epoch 45:  82%|████████▏ | 55/67 [00:18<00:03,  3.03it/s, loss=645, v_num=23, train_loss_step=249.0, val_loss=898.0, train_loss_epoch=807.0]\n",
      "Epoch 45:  84%|████████▎ | 56/67 [00:18<00:03,  3.03it/s, loss=645, v_num=23, train_loss_step=249.0, val_loss=898.0, train_loss_epoch=807.0]\n",
      "Epoch 45:  85%|████████▌ | 57/67 [00:18<00:03,  3.04it/s, loss=645, v_num=23, train_loss_step=249.0, val_loss=898.0, train_loss_epoch=807.0]\n",
      "Epoch 45:  87%|████████▋ | 58/67 [00:19<00:02,  3.02it/s, loss=645, v_num=23, train_loss_step=249.0, val_loss=898.0, train_loss_epoch=807.0]\n",
      "Epoch 45:  88%|████████▊ | 59/67 [00:19<00:02,  3.02it/s, loss=645, v_num=23, train_loss_step=249.0, val_loss=898.0, train_loss_epoch=807.0]\n",
      "Epoch 45:  90%|████████▉ | 60/67 [00:19<00:02,  3.03it/s, loss=645, v_num=23, train_loss_step=249.0, val_loss=898.0, train_loss_epoch=807.0]\n",
      "Epoch 45:  91%|█████████ | 61/67 [00:20<00:02,  2.99it/s, loss=645, v_num=23, train_loss_step=249.0, val_loss=898.0, train_loss_epoch=807.0]\n",
      "Epoch 45:  93%|█████████▎| 62/67 [00:20<00:01,  2.99it/s, loss=645, v_num=23, train_loss_step=249.0, val_loss=898.0, train_loss_epoch=807.0]\n",
      "Epoch 45:  94%|█████████▍| 63/67 [00:21<00:01,  3.00it/s, loss=645, v_num=23, train_loss_step=249.0, val_loss=898.0, train_loss_epoch=807.0]\n",
      "Epoch 45:  96%|█████████▌| 64/67 [00:21<00:01,  2.99it/s, loss=645, v_num=23, train_loss_step=249.0, val_loss=898.0, train_loss_epoch=807.0]\n",
      "Epoch 45:  97%|█████████▋| 65/67 [00:21<00:00,  3.00it/s, loss=645, v_num=23, train_loss_step=249.0, val_loss=898.0, train_loss_epoch=807.0]\n",
      "Epoch 45:  99%|█████████▊| 66/67 [00:21<00:00,  3.01it/s, loss=645, v_num=23, train_loss_step=249.0, val_loss=898.0, train_loss_epoch=807.0]\n",
      "Epoch 45: 100%|██████████| 67/67 [00:22<00:00,  2.96it/s, loss=645, v_num=23, train_loss_step=249.0, val_loss=898.0, train_loss_epoch=807.0]\n",
      "Epoch 46:  45%|████▍     | 30/67 [00:07<00:08,  4.13it/s, loss=915, v_num=23, train_loss_step=654.0, val_loss=898.0, train_loss_epoch=645.0]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 46:  46%|████▋     | 31/67 [00:11<00:12,  2.81it/s, loss=915, v_num=23, train_loss_step=654.0, val_loss=898.0, train_loss_epoch=645.0]\n",
      "Epoch 46:  48%|████▊     | 32/67 [00:11<00:12,  2.83it/s, loss=915, v_num=23, train_loss_step=654.0, val_loss=898.0, train_loss_epoch=645.0]\n",
      "Epoch 46:  49%|████▉     | 33/67 [00:11<00:11,  2.85it/s, loss=915, v_num=23, train_loss_step=654.0, val_loss=898.0, train_loss_epoch=645.0]\n",
      "Epoch 46:  51%|█████     | 34/67 [00:11<00:11,  2.85it/s, loss=915, v_num=23, train_loss_step=654.0, val_loss=898.0, train_loss_epoch=645.0]\n",
      "Epoch 46:  52%|█████▏    | 35/67 [00:12<00:11,  2.87it/s, loss=915, v_num=23, train_loss_step=654.0, val_loss=898.0, train_loss_epoch=645.0]\n",
      "Epoch 46:  54%|█████▎    | 36/67 [00:12<00:10,  2.89it/s, loss=915, v_num=23, train_loss_step=654.0, val_loss=898.0, train_loss_epoch=645.0]\n",
      "Epoch 46:  55%|█████▌    | 37/67 [00:12<00:10,  2.85it/s, loss=915, v_num=23, train_loss_step=654.0, val_loss=898.0, train_loss_epoch=645.0]\n",
      "Epoch 46:  57%|█████▋    | 38/67 [00:13<00:10,  2.86it/s, loss=915, v_num=23, train_loss_step=654.0, val_loss=898.0, train_loss_epoch=645.0]\n",
      "Epoch 46:  58%|█████▊    | 39/67 [00:13<00:09,  2.88it/s, loss=915, v_num=23, train_loss_step=654.0, val_loss=898.0, train_loss_epoch=645.0]\n",
      "Epoch 46:  60%|█████▉    | 40/67 [00:14<00:09,  2.84it/s, loss=915, v_num=23, train_loss_step=654.0, val_loss=898.0, train_loss_epoch=645.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 46:  61%|██████    | 41/67 [00:15<00:10,  2.56it/s, loss=915, v_num=23, train_loss_step=654.0, val_loss=898.0, train_loss_epoch=645.0]\n",
      "Epoch 46:  63%|██████▎   | 42/67 [00:16<00:09,  2.58it/s, loss=915, v_num=23, train_loss_step=654.0, val_loss=898.0, train_loss_epoch=645.0]\n",
      "Epoch 46:  64%|██████▍   | 43/67 [00:16<00:09,  2.60it/s, loss=915, v_num=23, train_loss_step=654.0, val_loss=898.0, train_loss_epoch=645.0]\n",
      "Epoch 46:  66%|██████▌   | 44/67 [00:16<00:08,  2.61it/s, loss=915, v_num=23, train_loss_step=654.0, val_loss=898.0, train_loss_epoch=645.0]\n",
      "Epoch 46:  67%|██████▋   | 45/67 [00:17<00:08,  2.63it/s, loss=915, v_num=23, train_loss_step=654.0, val_loss=898.0, train_loss_epoch=645.0]\n",
      "Epoch 46:  69%|██████▊   | 46/67 [00:17<00:07,  2.64it/s, loss=915, v_num=23, train_loss_step=654.0, val_loss=898.0, train_loss_epoch=645.0]\n",
      "Epoch 46:  70%|███████   | 47/67 [00:17<00:07,  2.65it/s, loss=915, v_num=23, train_loss_step=654.0, val_loss=898.0, train_loss_epoch=645.0]\n",
      "Epoch 46:  72%|███████▏  | 48/67 [00:17<00:07,  2.67it/s, loss=915, v_num=23, train_loss_step=654.0, val_loss=898.0, train_loss_epoch=645.0]\n",
      "Epoch 46:  73%|███████▎  | 49/67 [00:18<00:06,  2.68it/s, loss=915, v_num=23, train_loss_step=654.0, val_loss=898.0, train_loss_epoch=645.0]\n",
      "Epoch 46:  75%|███████▍  | 50/67 [00:18<00:06,  2.69it/s, loss=915, v_num=23, train_loss_step=654.0, val_loss=898.0, train_loss_epoch=645.0]\n",
      "Epoch 46:  76%|███████▌  | 51/67 [00:19<00:05,  2.68it/s, loss=915, v_num=23, train_loss_step=654.0, val_loss=898.0, train_loss_epoch=645.0]\n",
      "Epoch 46:  78%|███████▊  | 52/67 [00:19<00:05,  2.69it/s, loss=915, v_num=23, train_loss_step=654.0, val_loss=898.0, train_loss_epoch=645.0]\n",
      "Epoch 46:  79%|███████▉  | 53/67 [00:19<00:05,  2.70it/s, loss=915, v_num=23, train_loss_step=654.0, val_loss=898.0, train_loss_epoch=645.0]\n",
      "Epoch 46:  81%|████████  | 54/67 [00:19<00:04,  2.71it/s, loss=915, v_num=23, train_loss_step=654.0, val_loss=898.0, train_loss_epoch=645.0]\n",
      "Epoch 46:  82%|████████▏ | 55/67 [00:20<00:04,  2.72it/s, loss=915, v_num=23, train_loss_step=654.0, val_loss=898.0, train_loss_epoch=645.0]\n",
      "Epoch 46:  84%|████████▎ | 56/67 [00:20<00:04,  2.73it/s, loss=915, v_num=23, train_loss_step=654.0, val_loss=898.0, train_loss_epoch=645.0]\n",
      "Epoch 46:  85%|████████▌ | 57/67 [00:20<00:03,  2.74it/s, loss=915, v_num=23, train_loss_step=654.0, val_loss=898.0, train_loss_epoch=645.0]\n",
      "Epoch 46:  87%|████████▋ | 58/67 [00:21<00:03,  2.75it/s, loss=915, v_num=23, train_loss_step=654.0, val_loss=898.0, train_loss_epoch=645.0]\n",
      "Epoch 46:  88%|████████▊ | 59/67 [00:21<00:02,  2.76it/s, loss=915, v_num=23, train_loss_step=654.0, val_loss=898.0, train_loss_epoch=645.0]\n",
      "Epoch 46:  90%|████████▉ | 60/67 [00:21<00:02,  2.77it/s, loss=915, v_num=23, train_loss_step=654.0, val_loss=898.0, train_loss_epoch=645.0]\n",
      "Epoch 46:  91%|█████████ | 61/67 [00:22<00:02,  2.76it/s, loss=915, v_num=23, train_loss_step=654.0, val_loss=898.0, train_loss_epoch=645.0]\n",
      "Epoch 46:  93%|█████████▎| 62/67 [00:22<00:01,  2.77it/s, loss=915, v_num=23, train_loss_step=654.0, val_loss=898.0, train_loss_epoch=645.0]\n",
      "Epoch 46:  94%|█████████▍| 63/67 [00:22<00:01,  2.78it/s, loss=915, v_num=23, train_loss_step=654.0, val_loss=898.0, train_loss_epoch=645.0]\n",
      "Epoch 46:  96%|█████████▌| 64/67 [00:23<00:01,  2.78it/s, loss=915, v_num=23, train_loss_step=654.0, val_loss=898.0, train_loss_epoch=645.0]\n",
      "Epoch 46:  97%|█████████▋| 65/67 [00:23<00:00,  2.78it/s, loss=915, v_num=23, train_loss_step=654.0, val_loss=898.0, train_loss_epoch=645.0]\n",
      "Epoch 46:  99%|█████████▊| 66/67 [00:23<00:00,  2.78it/s, loss=915, v_num=23, train_loss_step=654.0, val_loss=898.0, train_loss_epoch=645.0]\n",
      "Epoch 46: 100%|██████████| 67/67 [00:24<00:00,  2.74it/s, loss=915, v_num=23, train_loss_step=654.0, val_loss=898.0, train_loss_epoch=645.0]\n",
      "Epoch 47:  45%|████▍     | 30/67 [00:06<00:07,  4.93it/s, loss=827, v_num=23, train_loss_step=395.0, val_loss=898.0, train_loss_epoch=830.0]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 47:  46%|████▋     | 31/67 [00:08<00:09,  3.79it/s, loss=827, v_num=23, train_loss_step=395.0, val_loss=898.0, train_loss_epoch=830.0]\n",
      "Epoch 47:  48%|████▊     | 32/67 [00:08<00:09,  3.78it/s, loss=827, v_num=23, train_loss_step=395.0, val_loss=898.0, train_loss_epoch=830.0]\n",
      "Epoch 47:  49%|████▉     | 33/67 [00:08<00:09,  3.77it/s, loss=827, v_num=23, train_loss_step=395.0, val_loss=898.0, train_loss_epoch=830.0]\n",
      "Epoch 47:  51%|█████     | 34/67 [00:09<00:08,  3.76it/s, loss=827, v_num=23, train_loss_step=395.0, val_loss=898.0, train_loss_epoch=830.0]\n",
      "Epoch 47:  52%|█████▏    | 35/67 [00:09<00:08,  3.75it/s, loss=827, v_num=23, train_loss_step=395.0, val_loss=898.0, train_loss_epoch=830.0]\n",
      "Epoch 47:  54%|█████▎    | 36/67 [00:09<00:08,  3.73it/s, loss=827, v_num=23, train_loss_step=395.0, val_loss=898.0, train_loss_epoch=830.0]\n",
      "Epoch 47:  55%|█████▌    | 37/67 [00:10<00:08,  3.68it/s, loss=827, v_num=23, train_loss_step=395.0, val_loss=898.0, train_loss_epoch=830.0]\n",
      "Epoch 47:  57%|█████▋    | 38/67 [00:10<00:07,  3.67it/s, loss=827, v_num=23, train_loss_step=395.0, val_loss=898.0, train_loss_epoch=830.0]\n",
      "Epoch 47:  58%|█████▊    | 39/67 [00:10<00:07,  3.67it/s, loss=827, v_num=23, train_loss_step=395.0, val_loss=898.0, train_loss_epoch=830.0]\n",
      "Epoch 47:  60%|█████▉    | 40/67 [00:11<00:07,  3.59it/s, loss=827, v_num=23, train_loss_step=395.0, val_loss=898.0, train_loss_epoch=830.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 47:  61%|██████    | 41/67 [00:11<00:07,  3.55it/s, loss=827, v_num=23, train_loss_step=395.0, val_loss=898.0, train_loss_epoch=830.0]\n",
      "Epoch 47:  63%|██████▎   | 42/67 [00:11<00:07,  3.54it/s, loss=827, v_num=23, train_loss_step=395.0, val_loss=898.0, train_loss_epoch=830.0]\n",
      "Epoch 47:  64%|██████▍   | 43/67 [00:12<00:06,  3.52it/s, loss=827, v_num=23, train_loss_step=395.0, val_loss=898.0, train_loss_epoch=830.0]\n",
      "Epoch 47:  66%|██████▌   | 44/67 [00:12<00:06,  3.51it/s, loss=827, v_num=23, train_loss_step=395.0, val_loss=898.0, train_loss_epoch=830.0]\n",
      "Epoch 47:  67%|██████▋   | 45/67 [00:12<00:06,  3.51it/s, loss=827, v_num=23, train_loss_step=395.0, val_loss=898.0, train_loss_epoch=830.0]\n",
      "Epoch 47:  69%|██████▊   | 46/67 [00:13<00:06,  3.46it/s, loss=827, v_num=23, train_loss_step=395.0, val_loss=898.0, train_loss_epoch=830.0]\n",
      "Epoch 47:  70%|███████   | 47/67 [00:13<00:05,  3.46it/s, loss=827, v_num=23, train_loss_step=395.0, val_loss=898.0, train_loss_epoch=830.0]\n",
      "Epoch 47:  72%|███████▏  | 48/67 [00:13<00:05,  3.46it/s, loss=827, v_num=23, train_loss_step=395.0, val_loss=898.0, train_loss_epoch=830.0]\n",
      "Epoch 47:  73%|███████▎  | 49/67 [00:14<00:05,  3.41it/s, loss=827, v_num=23, train_loss_step=395.0, val_loss=898.0, train_loss_epoch=830.0]\n",
      "Epoch 47:  75%|███████▍  | 50/67 [00:14<00:04,  3.41it/s, loss=827, v_num=23, train_loss_step=395.0, val_loss=898.0, train_loss_epoch=830.0]\n",
      "Epoch 47:  76%|███████▌  | 51/67 [00:15<00:04,  3.38it/s, loss=827, v_num=23, train_loss_step=395.0, val_loss=898.0, train_loss_epoch=830.0]\n",
      "Epoch 47:  78%|███████▊  | 52/67 [00:15<00:04,  3.37it/s, loss=827, v_num=23, train_loss_step=395.0, val_loss=898.0, train_loss_epoch=830.0]\n",
      "Epoch 47:  79%|███████▉  | 53/67 [00:15<00:04,  3.37it/s, loss=827, v_num=23, train_loss_step=395.0, val_loss=898.0, train_loss_epoch=830.0]\n",
      "Epoch 47:  81%|████████  | 54/67 [00:16<00:03,  3.37it/s, loss=827, v_num=23, train_loss_step=395.0, val_loss=898.0, train_loss_epoch=830.0]\n",
      "Epoch 47:  82%|████████▏ | 55/67 [00:16<00:03,  3.33it/s, loss=827, v_num=23, train_loss_step=395.0, val_loss=898.0, train_loss_epoch=830.0]\n",
      "Epoch 47:  84%|████████▎ | 56/67 [00:16<00:03,  3.32it/s, loss=827, v_num=23, train_loss_step=395.0, val_loss=898.0, train_loss_epoch=830.0]\n",
      "Epoch 47:  85%|████████▌ | 57/67 [00:17<00:03,  3.32it/s, loss=827, v_num=23, train_loss_step=395.0, val_loss=898.0, train_loss_epoch=830.0]\n",
      "Epoch 47:  87%|████████▋ | 58/67 [00:17<00:02,  3.30it/s, loss=827, v_num=23, train_loss_step=395.0, val_loss=898.0, train_loss_epoch=830.0]\n",
      "Epoch 47:  88%|████████▊ | 59/67 [00:17<00:02,  3.28it/s, loss=827, v_num=23, train_loss_step=395.0, val_loss=898.0, train_loss_epoch=830.0]\n",
      "Epoch 47:  90%|████████▉ | 60/67 [00:18<00:02,  3.29it/s, loss=827, v_num=23, train_loss_step=395.0, val_loss=898.0, train_loss_epoch=830.0]\n",
      "Epoch 47:  91%|█████████ | 61/67 [00:18<00:01,  3.25it/s, loss=827, v_num=23, train_loss_step=395.0, val_loss=898.0, train_loss_epoch=830.0]\n",
      "Epoch 47:  93%|█████████▎| 62/67 [00:19<00:01,  3.24it/s, loss=827, v_num=23, train_loss_step=395.0, val_loss=898.0, train_loss_epoch=830.0]\n",
      "Epoch 47:  94%|█████████▍| 63/67 [00:19<00:01,  3.25it/s, loss=827, v_num=23, train_loss_step=395.0, val_loss=898.0, train_loss_epoch=830.0]\n",
      "Epoch 47:  96%|█████████▌| 64/67 [00:19<00:00,  3.25it/s, loss=827, v_num=23, train_loss_step=395.0, val_loss=898.0, train_loss_epoch=830.0]\n",
      "Epoch 47:  97%|█████████▋| 65/67 [00:20<00:00,  3.23it/s, loss=827, v_num=23, train_loss_step=395.0, val_loss=898.0, train_loss_epoch=830.0]\n",
      "Epoch 47:  99%|█████████▊| 66/67 [00:20<00:00,  3.23it/s, loss=827, v_num=23, train_loss_step=395.0, val_loss=898.0, train_loss_epoch=830.0]\n",
      "Epoch 47: 100%|██████████| 67/67 [00:21<00:00,  3.18it/s, loss=827, v_num=23, train_loss_step=395.0, val_loss=898.0, train_loss_epoch=830.0]\n",
      "Epoch 48:  45%|████▍     | 30/67 [00:06<00:07,  4.92it/s, loss=487, v_num=23, train_loss_step=394.0, val_loss=898.0, train_loss_epoch=777.0]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 48:  46%|████▋     | 31/67 [00:08<00:09,  3.77it/s, loss=487, v_num=23, train_loss_step=394.0, val_loss=898.0, train_loss_epoch=777.0]\n",
      "Epoch 48:  48%|████▊     | 32/67 [00:08<00:09,  3.76it/s, loss=487, v_num=23, train_loss_step=394.0, val_loss=898.0, train_loss_epoch=777.0]\n",
      "Epoch 48:  49%|████▉     | 33/67 [00:08<00:09,  3.76it/s, loss=487, v_num=23, train_loss_step=394.0, val_loss=898.0, train_loss_epoch=777.0]\n",
      "Epoch 48:  51%|█████     | 34/67 [00:09<00:08,  3.73it/s, loss=487, v_num=23, train_loss_step=394.0, val_loss=898.0, train_loss_epoch=777.0]\n",
      "Epoch 48:  52%|█████▏    | 35/67 [00:09<00:08,  3.72it/s, loss=487, v_num=23, train_loss_step=394.0, val_loss=898.0, train_loss_epoch=777.0]\n",
      "Epoch 48:  54%|█████▎    | 36/67 [00:09<00:08,  3.72it/s, loss=487, v_num=23, train_loss_step=394.0, val_loss=898.0, train_loss_epoch=777.0]\n",
      "Epoch 48:  55%|█████▌    | 37/67 [00:10<00:08,  3.64it/s, loss=487, v_num=23, train_loss_step=394.0, val_loss=898.0, train_loss_epoch=777.0]\n",
      "Epoch 48:  57%|█████▋    | 38/67 [00:10<00:07,  3.64it/s, loss=487, v_num=23, train_loss_step=394.0, val_loss=898.0, train_loss_epoch=777.0]\n",
      "Epoch 48:  58%|█████▊    | 39/67 [00:10<00:07,  3.64it/s, loss=487, v_num=23, train_loss_step=394.0, val_loss=898.0, train_loss_epoch=777.0]\n",
      "Epoch 48:  60%|█████▉    | 40/67 [00:11<00:07,  3.55it/s, loss=487, v_num=23, train_loss_step=394.0, val_loss=898.0, train_loss_epoch=777.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 48:  61%|██████    | 41/67 [00:11<00:07,  3.51it/s, loss=487, v_num=23, train_loss_step=394.0, val_loss=898.0, train_loss_epoch=777.0]\n",
      "Epoch 48:  63%|██████▎   | 42/67 [00:11<00:07,  3.52it/s, loss=487, v_num=23, train_loss_step=394.0, val_loss=898.0, train_loss_epoch=777.0]\n",
      "Epoch 48:  64%|██████▍   | 43/67 [00:12<00:06,  3.45it/s, loss=487, v_num=23, train_loss_step=394.0, val_loss=898.0, train_loss_epoch=777.0]\n",
      "Epoch 48:  66%|██████▌   | 44/67 [00:12<00:06,  3.45it/s, loss=487, v_num=23, train_loss_step=394.0, val_loss=898.0, train_loss_epoch=777.0]\n",
      "Epoch 48:  67%|██████▋   | 45/67 [00:13<00:06,  3.46it/s, loss=487, v_num=23, train_loss_step=394.0, val_loss=898.0, train_loss_epoch=777.0]\n",
      "Epoch 48:  69%|██████▊   | 46/67 [00:13<00:06,  3.37it/s, loss=487, v_num=23, train_loss_step=394.0, val_loss=898.0, train_loss_epoch=777.0]\n",
      "Epoch 48:  70%|███████   | 47/67 [00:13<00:05,  3.37it/s, loss=487, v_num=23, train_loss_step=394.0, val_loss=898.0, train_loss_epoch=777.0]\n",
      "Epoch 48:  72%|███████▏  | 48/67 [00:14<00:05,  3.38it/s, loss=487, v_num=23, train_loss_step=394.0, val_loss=898.0, train_loss_epoch=777.0]\n",
      "Epoch 48:  73%|███████▎  | 49/67 [00:14<00:05,  3.32it/s, loss=487, v_num=23, train_loss_step=394.0, val_loss=898.0, train_loss_epoch=777.0]\n",
      "Epoch 48:  75%|███████▍  | 50/67 [00:15<00:05,  3.33it/s, loss=487, v_num=23, train_loss_step=394.0, val_loss=898.0, train_loss_epoch=777.0]\n",
      "Epoch 48:  76%|███████▌  | 51/67 [00:15<00:04,  3.30it/s, loss=487, v_num=23, train_loss_step=394.0, val_loss=898.0, train_loss_epoch=777.0]\n",
      "Epoch 48:  78%|███████▊  | 52/67 [00:15<00:04,  3.29it/s, loss=487, v_num=23, train_loss_step=394.0, val_loss=898.0, train_loss_epoch=777.0]\n",
      "Epoch 48:  79%|███████▉  | 53/67 [00:16<00:04,  3.29it/s, loss=487, v_num=23, train_loss_step=394.0, val_loss=898.0, train_loss_epoch=777.0]\n",
      "Epoch 48:  81%|████████  | 54/67 [00:16<00:03,  3.30it/s, loss=487, v_num=23, train_loss_step=394.0, val_loss=898.0, train_loss_epoch=777.0]\n",
      "Epoch 48:  82%|████████▏ | 55/67 [00:16<00:03,  3.26it/s, loss=487, v_num=23, train_loss_step=394.0, val_loss=898.0, train_loss_epoch=777.0]\n",
      "Epoch 48:  84%|████████▎ | 56/67 [00:17<00:03,  3.26it/s, loss=487, v_num=23, train_loss_step=394.0, val_loss=898.0, train_loss_epoch=777.0]\n",
      "Epoch 48:  85%|████████▌ | 57/67 [00:17<00:03,  3.27it/s, loss=487, v_num=23, train_loss_step=394.0, val_loss=898.0, train_loss_epoch=777.0]\n",
      "Epoch 48:  87%|████████▋ | 58/67 [00:17<00:02,  3.23it/s, loss=487, v_num=23, train_loss_step=394.0, val_loss=898.0, train_loss_epoch=777.0]\n",
      "Epoch 48:  88%|████████▊ | 59/67 [00:18<00:02,  3.24it/s, loss=487, v_num=23, train_loss_step=394.0, val_loss=898.0, train_loss_epoch=777.0]\n",
      "Epoch 48:  90%|████████▉ | 60/67 [00:18<00:02,  3.24it/s, loss=487, v_num=23, train_loss_step=394.0, val_loss=898.0, train_loss_epoch=777.0]\n",
      "Epoch 48:  91%|█████████ | 61/67 [00:19<00:01,  3.19it/s, loss=487, v_num=23, train_loss_step=394.0, val_loss=898.0, train_loss_epoch=777.0]\n",
      "Epoch 48:  93%|█████████▎| 62/67 [00:19<00:01,  3.19it/s, loss=487, v_num=23, train_loss_step=394.0, val_loss=898.0, train_loss_epoch=777.0]\n",
      "Epoch 48:  94%|█████████▍| 63/67 [00:19<00:01,  3.20it/s, loss=487, v_num=23, train_loss_step=394.0, val_loss=898.0, train_loss_epoch=777.0]\n",
      "Epoch 48:  96%|█████████▌| 64/67 [00:20<00:00,  3.19it/s, loss=487, v_num=23, train_loss_step=394.0, val_loss=898.0, train_loss_epoch=777.0]\n",
      "Epoch 48:  97%|█████████▋| 65/67 [00:20<00:00,  3.19it/s, loss=487, v_num=23, train_loss_step=394.0, val_loss=898.0, train_loss_epoch=777.0]\n",
      "Epoch 48:  99%|█████████▊| 66/67 [00:20<00:00,  3.20it/s, loss=487, v_num=23, train_loss_step=394.0, val_loss=898.0, train_loss_epoch=777.0]\n",
      "Epoch 48: 100%|██████████| 67/67 [00:21<00:00,  3.13it/s, loss=487, v_num=23, train_loss_step=394.0, val_loss=898.0, train_loss_epoch=777.0]\n",
      "Epoch 49:  45%|████▍     | 30/67 [00:06<00:07,  4.92it/s, loss=776, v_num=23, train_loss_step=885.0, val_loss=898.0, train_loss_epoch=518.0]       \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 49:  46%|████▋     | 31/67 [00:08<00:09,  3.79it/s, loss=776, v_num=23, train_loss_step=885.0, val_loss=898.0, train_loss_epoch=518.0]\n",
      "Epoch 49:  48%|████▊     | 32/67 [00:08<00:09,  3.78it/s, loss=776, v_num=23, train_loss_step=885.0, val_loss=898.0, train_loss_epoch=518.0]\n",
      "Epoch 49:  49%|████▉     | 33/67 [00:08<00:09,  3.77it/s, loss=776, v_num=23, train_loss_step=885.0, val_loss=898.0, train_loss_epoch=518.0]\n",
      "Epoch 49:  51%|█████     | 34/67 [00:09<00:08,  3.76it/s, loss=776, v_num=23, train_loss_step=885.0, val_loss=898.0, train_loss_epoch=518.0]\n",
      "Epoch 49:  52%|█████▏    | 35/67 [00:09<00:08,  3.74it/s, loss=776, v_num=23, train_loss_step=885.0, val_loss=898.0, train_loss_epoch=518.0]\n",
      "Epoch 49:  54%|█████▎    | 36/67 [00:09<00:08,  3.73it/s, loss=776, v_num=23, train_loss_step=885.0, val_loss=898.0, train_loss_epoch=518.0]\n",
      "Epoch 49:  55%|█████▌    | 37/67 [00:10<00:08,  3.66it/s, loss=776, v_num=23, train_loss_step=885.0, val_loss=898.0, train_loss_epoch=518.0]\n",
      "Epoch 49:  57%|█████▋    | 38/67 [00:10<00:07,  3.66it/s, loss=776, v_num=23, train_loss_step=885.0, val_loss=898.0, train_loss_epoch=518.0]\n",
      "Epoch 49:  58%|█████▊    | 39/67 [00:10<00:07,  3.65it/s, loss=776, v_num=23, train_loss_step=885.0, val_loss=898.0, train_loss_epoch=518.0]\n",
      "Epoch 49:  60%|█████▉    | 40/67 [00:11<00:07,  3.58it/s, loss=776, v_num=23, train_loss_step=885.0, val_loss=898.0, train_loss_epoch=518.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 49:  61%|██████    | 41/67 [00:11<00:07,  3.54it/s, loss=776, v_num=23, train_loss_step=885.0, val_loss=898.0, train_loss_epoch=518.0]\n",
      "Epoch 49:  63%|██████▎   | 42/67 [00:11<00:07,  3.53it/s, loss=776, v_num=23, train_loss_step=885.0, val_loss=898.0, train_loss_epoch=518.0]\n",
      "Epoch 49:  64%|██████▍   | 43/67 [00:12<00:06,  3.52it/s, loss=776, v_num=23, train_loss_step=885.0, val_loss=898.0, train_loss_epoch=518.0]\n",
      "Epoch 49:  66%|██████▌   | 44/67 [00:12<00:06,  3.52it/s, loss=776, v_num=23, train_loss_step=885.0, val_loss=898.0, train_loss_epoch=518.0]\n",
      "Epoch 49:  67%|██████▋   | 45/67 [00:12<00:06,  3.52it/s, loss=776, v_num=23, train_loss_step=885.0, val_loss=898.0, train_loss_epoch=518.0]\n",
      "Epoch 49:  69%|██████▊   | 46/67 [00:13<00:06,  3.46it/s, loss=776, v_num=23, train_loss_step=885.0, val_loss=898.0, train_loss_epoch=518.0]\n",
      "Epoch 49:  70%|███████   | 47/67 [00:13<00:05,  3.47it/s, loss=776, v_num=23, train_loss_step=885.0, val_loss=898.0, train_loss_epoch=518.0]\n",
      "Epoch 49:  72%|███████▏  | 48/67 [00:13<00:05,  3.46it/s, loss=776, v_num=23, train_loss_step=885.0, val_loss=898.0, train_loss_epoch=518.0]\n",
      "Epoch 49:  73%|███████▎  | 49/67 [00:14<00:05,  3.42it/s, loss=776, v_num=23, train_loss_step=885.0, val_loss=898.0, train_loss_epoch=518.0]\n",
      "Epoch 49:  75%|███████▍  | 50/67 [00:14<00:04,  3.42it/s, loss=776, v_num=23, train_loss_step=885.0, val_loss=898.0, train_loss_epoch=518.0]\n",
      "Epoch 49:  76%|███████▌  | 51/67 [00:15<00:04,  3.39it/s, loss=776, v_num=23, train_loss_step=885.0, val_loss=898.0, train_loss_epoch=518.0]\n",
      "Epoch 49:  78%|███████▊  | 52/67 [00:15<00:04,  3.36it/s, loss=776, v_num=23, train_loss_step=885.0, val_loss=898.0, train_loss_epoch=518.0]\n",
      "Epoch 49:  79%|███████▉  | 53/67 [00:15<00:04,  3.36it/s, loss=776, v_num=23, train_loss_step=885.0, val_loss=898.0, train_loss_epoch=518.0]\n",
      "Epoch 49:  81%|████████  | 54/67 [00:16<00:03,  3.36it/s, loss=776, v_num=23, train_loss_step=885.0, val_loss=898.0, train_loss_epoch=518.0]\n",
      "Epoch 49:  82%|████████▏ | 55/67 [00:16<00:03,  3.32it/s, loss=776, v_num=23, train_loss_step=885.0, val_loss=898.0, train_loss_epoch=518.0]\n",
      "Epoch 49:  84%|████████▎ | 56/67 [00:16<00:03,  3.32it/s, loss=776, v_num=23, train_loss_step=885.0, val_loss=898.0, train_loss_epoch=518.0]\n",
      "Epoch 49:  85%|████████▌ | 57/67 [00:17<00:03,  3.32it/s, loss=776, v_num=23, train_loss_step=885.0, val_loss=898.0, train_loss_epoch=518.0]\n",
      "Epoch 49:  87%|████████▋ | 58/67 [00:17<00:02,  3.29it/s, loss=776, v_num=23, train_loss_step=885.0, val_loss=898.0, train_loss_epoch=518.0]\n",
      "Epoch 49:  88%|████████▊ | 59/67 [00:17<00:02,  3.29it/s, loss=776, v_num=23, train_loss_step=885.0, val_loss=898.0, train_loss_epoch=518.0]\n",
      "Epoch 49:  90%|████████▉ | 60/67 [00:18<00:02,  3.29it/s, loss=776, v_num=23, train_loss_step=885.0, val_loss=898.0, train_loss_epoch=518.0]\n",
      "Epoch 49:  91%|█████████ | 61/67 [00:18<00:01,  3.24it/s, loss=776, v_num=23, train_loss_step=885.0, val_loss=898.0, train_loss_epoch=518.0]\n",
      "Epoch 49:  93%|█████████▎| 62/67 [00:19<00:01,  3.24it/s, loss=776, v_num=23, train_loss_step=885.0, val_loss=898.0, train_loss_epoch=518.0]\n",
      "Epoch 49:  94%|█████████▍| 63/67 [00:19<00:01,  3.24it/s, loss=776, v_num=23, train_loss_step=885.0, val_loss=898.0, train_loss_epoch=518.0]\n",
      "Epoch 49:  96%|█████████▌| 64/67 [00:19<00:00,  3.24it/s, loss=776, v_num=23, train_loss_step=885.0, val_loss=898.0, train_loss_epoch=518.0]\n",
      "Epoch 49:  97%|█████████▋| 65/67 [00:20<00:00,  3.24it/s, loss=776, v_num=23, train_loss_step=885.0, val_loss=898.0, train_loss_epoch=518.0]\n",
      "Epoch 49:  99%|█████████▊| 66/67 [00:20<00:00,  3.25it/s, loss=776, v_num=23, train_loss_step=885.0, val_loss=898.0, train_loss_epoch=518.0]\n",
      "Epoch 49: 100%|██████████| 67/67 [00:21<00:00,  3.18it/s, loss=776, v_num=23, train_loss_step=885.0, val_loss=899.0, train_loss_epoch=518.0]\n",
      "Epoch 50:  45%|████▍     | 30/67 [00:06<00:07,  4.94it/s, loss=817, v_num=23, train_loss_step=305.0, val_loss=899.0, train_loss_epoch=876.0]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 50:  46%|████▋     | 31/67 [00:08<00:09,  3.76it/s, loss=817, v_num=23, train_loss_step=305.0, val_loss=899.0, train_loss_epoch=876.0]\n",
      "Epoch 50:  48%|████▊     | 32/67 [00:08<00:09,  3.75it/s, loss=817, v_num=23, train_loss_step=305.0, val_loss=899.0, train_loss_epoch=876.0]\n",
      "Epoch 50:  49%|████▉     | 33/67 [00:08<00:09,  3.75it/s, loss=817, v_num=23, train_loss_step=305.0, val_loss=899.0, train_loss_epoch=876.0]\n",
      "Epoch 50:  51%|█████     | 34/67 [00:09<00:08,  3.72it/s, loss=817, v_num=23, train_loss_step=305.0, val_loss=899.0, train_loss_epoch=876.0]\n",
      "Epoch 50:  52%|█████▏    | 35/67 [00:09<00:08,  3.71it/s, loss=817, v_num=23, train_loss_step=305.0, val_loss=899.0, train_loss_epoch=876.0]\n",
      "Epoch 50:  54%|█████▎    | 36/67 [00:09<00:08,  3.71it/s, loss=817, v_num=23, train_loss_step=305.0, val_loss=899.0, train_loss_epoch=876.0]\n",
      "Epoch 50:  55%|█████▌    | 37/67 [00:10<00:08,  3.65it/s, loss=817, v_num=23, train_loss_step=305.0, val_loss=899.0, train_loss_epoch=876.0]\n",
      "Epoch 50:  57%|█████▋    | 38/67 [00:10<00:07,  3.65it/s, loss=817, v_num=23, train_loss_step=305.0, val_loss=899.0, train_loss_epoch=876.0]\n",
      "Epoch 50:  58%|█████▊    | 39/67 [00:10<00:07,  3.65it/s, loss=817, v_num=23, train_loss_step=305.0, val_loss=899.0, train_loss_epoch=876.0]\n",
      "Epoch 50:  60%|█████▉    | 40/67 [00:11<00:07,  3.56it/s, loss=817, v_num=23, train_loss_step=305.0, val_loss=899.0, train_loss_epoch=876.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 50:  61%|██████    | 41/67 [00:11<00:07,  3.52it/s, loss=817, v_num=23, train_loss_step=305.0, val_loss=899.0, train_loss_epoch=876.0]\n",
      "Epoch 50:  63%|██████▎   | 42/67 [00:11<00:07,  3.52it/s, loss=817, v_num=23, train_loss_step=305.0, val_loss=899.0, train_loss_epoch=876.0]\n",
      "Epoch 50:  64%|██████▍   | 43/67 [00:12<00:06,  3.49it/s, loss=817, v_num=23, train_loss_step=305.0, val_loss=899.0, train_loss_epoch=876.0]\n",
      "Epoch 50:  66%|██████▌   | 44/67 [00:12<00:06,  3.49it/s, loss=817, v_num=23, train_loss_step=305.0, val_loss=899.0, train_loss_epoch=876.0]\n",
      "Epoch 50:  67%|██████▋   | 45/67 [00:12<00:06,  3.50it/s, loss=817, v_num=23, train_loss_step=305.0, val_loss=899.0, train_loss_epoch=876.0]\n",
      "Epoch 50:  69%|██████▊   | 46/67 [00:13<00:06,  3.44it/s, loss=817, v_num=23, train_loss_step=305.0, val_loss=899.0, train_loss_epoch=876.0]\n",
      "Epoch 50:  70%|███████   | 47/67 [00:13<00:05,  3.44it/s, loss=817, v_num=23, train_loss_step=305.0, val_loss=899.0, train_loss_epoch=876.0]\n",
      "Epoch 50:  72%|███████▏  | 48/67 [00:13<00:05,  3.45it/s, loss=817, v_num=23, train_loss_step=305.0, val_loss=899.0, train_loss_epoch=876.0]\n",
      "Epoch 50:  73%|███████▎  | 49/67 [00:14<00:05,  3.40it/s, loss=817, v_num=23, train_loss_step=305.0, val_loss=899.0, train_loss_epoch=876.0]\n",
      "Epoch 50:  75%|███████▍  | 50/67 [00:14<00:04,  3.40it/s, loss=817, v_num=23, train_loss_step=305.0, val_loss=899.0, train_loss_epoch=876.0]\n",
      "Epoch 50:  76%|███████▌  | 51/67 [00:15<00:04,  3.37it/s, loss=817, v_num=23, train_loss_step=305.0, val_loss=899.0, train_loss_epoch=876.0]\n",
      "Epoch 50:  78%|███████▊  | 52/67 [00:15<00:04,  3.36it/s, loss=817, v_num=23, train_loss_step=305.0, val_loss=899.0, train_loss_epoch=876.0]\n",
      "Epoch 50:  79%|███████▉  | 53/67 [00:15<00:04,  3.36it/s, loss=817, v_num=23, train_loss_step=305.0, val_loss=899.0, train_loss_epoch=876.0]\n",
      "Epoch 50:  81%|████████  | 54/67 [00:16<00:03,  3.37it/s, loss=817, v_num=23, train_loss_step=305.0, val_loss=899.0, train_loss_epoch=876.0]\n",
      "Epoch 50:  82%|████████▏ | 55/67 [00:16<00:03,  3.32it/s, loss=817, v_num=23, train_loss_step=305.0, val_loss=899.0, train_loss_epoch=876.0]\n",
      "Epoch 50:  84%|████████▎ | 56/67 [00:16<00:03,  3.33it/s, loss=817, v_num=23, train_loss_step=305.0, val_loss=899.0, train_loss_epoch=876.0]\n",
      "Epoch 50:  85%|████████▌ | 57/67 [00:17<00:03,  3.33it/s, loss=817, v_num=23, train_loss_step=305.0, val_loss=899.0, train_loss_epoch=876.0]\n",
      "Epoch 50:  87%|████████▋ | 58/67 [00:17<00:02,  3.29it/s, loss=817, v_num=23, train_loss_step=305.0, val_loss=899.0, train_loss_epoch=876.0]\n",
      "Epoch 50:  88%|████████▊ | 59/67 [00:17<00:02,  3.29it/s, loss=817, v_num=23, train_loss_step=305.0, val_loss=899.0, train_loss_epoch=876.0]\n",
      "Epoch 50:  90%|████████▉ | 60/67 [00:18<00:02,  3.30it/s, loss=817, v_num=23, train_loss_step=305.0, val_loss=899.0, train_loss_epoch=876.0]\n",
      "Epoch 50:  91%|█████████ | 61/67 [00:18<00:01,  3.24it/s, loss=817, v_num=23, train_loss_step=305.0, val_loss=899.0, train_loss_epoch=876.0]\n",
      "Epoch 50:  93%|█████████▎| 62/67 [00:19<00:01,  3.24it/s, loss=817, v_num=23, train_loss_step=305.0, val_loss=899.0, train_loss_epoch=876.0]\n",
      "Epoch 50:  94%|█████████▍| 63/67 [00:19<00:01,  3.25it/s, loss=817, v_num=23, train_loss_step=305.0, val_loss=899.0, train_loss_epoch=876.0]\n",
      "Epoch 50:  96%|█████████▌| 64/67 [00:19<00:00,  3.24it/s, loss=817, v_num=23, train_loss_step=305.0, val_loss=899.0, train_loss_epoch=876.0]\n",
      "Epoch 50:  97%|█████████▋| 65/67 [00:20<00:00,  3.24it/s, loss=817, v_num=23, train_loss_step=305.0, val_loss=899.0, train_loss_epoch=876.0]\n",
      "Epoch 50:  99%|█████████▊| 66/67 [00:20<00:00,  3.24it/s, loss=817, v_num=23, train_loss_step=305.0, val_loss=899.0, train_loss_epoch=876.0]\n",
      "Epoch 50: 100%|██████████| 67/67 [00:21<00:00,  3.18it/s, loss=817, v_num=23, train_loss_step=305.0, val_loss=898.0, train_loss_epoch=876.0]\n",
      "Epoch 51:  45%|████▍     | 30/67 [00:06<00:07,  4.96it/s, loss=688, v_num=23, train_loss_step=57.20, val_loss=898.0, train_loss_epoch=724.0]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 51:  46%|████▋     | 31/67 [00:08<00:09,  3.83it/s, loss=688, v_num=23, train_loss_step=57.20, val_loss=898.0, train_loss_epoch=724.0]\n",
      "Epoch 51:  48%|████▊     | 32/67 [00:08<00:09,  3.82it/s, loss=688, v_num=23, train_loss_step=57.20, val_loss=898.0, train_loss_epoch=724.0]\n",
      "Epoch 51:  49%|████▉     | 33/67 [00:08<00:08,  3.80it/s, loss=688, v_num=23, train_loss_step=57.20, val_loss=898.0, train_loss_epoch=724.0]\n",
      "Epoch 51:  51%|█████     | 34/67 [00:08<00:08,  3.79it/s, loss=688, v_num=23, train_loss_step=57.20, val_loss=898.0, train_loss_epoch=724.0]\n",
      "Epoch 51:  52%|█████▏    | 35/67 [00:09<00:08,  3.77it/s, loss=688, v_num=23, train_loss_step=57.20, val_loss=898.0, train_loss_epoch=724.0]\n",
      "Epoch 51:  54%|█████▎    | 36/67 [00:09<00:08,  3.77it/s, loss=688, v_num=23, train_loss_step=57.20, val_loss=898.0, train_loss_epoch=724.0]\n",
      "Epoch 51:  55%|█████▌    | 37/67 [00:09<00:08,  3.72it/s, loss=688, v_num=23, train_loss_step=57.20, val_loss=898.0, train_loss_epoch=724.0]\n",
      "Epoch 51:  57%|█████▋    | 38/67 [00:10<00:07,  3.71it/s, loss=688, v_num=23, train_loss_step=57.20, val_loss=898.0, train_loss_epoch=724.0]\n",
      "Epoch 51:  58%|█████▊    | 39/67 [00:10<00:07,  3.69it/s, loss=688, v_num=23, train_loss_step=57.20, val_loss=898.0, train_loss_epoch=724.0]\n",
      "Epoch 51:  60%|█████▉    | 40/67 [00:11<00:07,  3.63it/s, loss=688, v_num=23, train_loss_step=57.20, val_loss=898.0, train_loss_epoch=724.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 51:  61%|██████    | 41/67 [00:11<00:07,  3.58it/s, loss=688, v_num=23, train_loss_step=57.20, val_loss=898.0, train_loss_epoch=724.0]\n",
      "Epoch 51:  63%|██████▎   | 42/67 [00:11<00:06,  3.57it/s, loss=688, v_num=23, train_loss_step=57.20, val_loss=898.0, train_loss_epoch=724.0]\n",
      "Epoch 51:  64%|██████▍   | 43/67 [00:12<00:06,  3.56it/s, loss=688, v_num=23, train_loss_step=57.20, val_loss=898.0, train_loss_epoch=724.0]\n",
      "Epoch 51:  66%|██████▌   | 44/67 [00:12<00:06,  3.55it/s, loss=688, v_num=23, train_loss_step=57.20, val_loss=898.0, train_loss_epoch=724.0]\n",
      "Epoch 51:  67%|██████▋   | 45/67 [00:12<00:06,  3.55it/s, loss=688, v_num=23, train_loss_step=57.20, val_loss=898.0, train_loss_epoch=724.0]\n",
      "Epoch 51:  69%|██████▊   | 46/67 [00:13<00:06,  3.49it/s, loss=688, v_num=23, train_loss_step=57.20, val_loss=898.0, train_loss_epoch=724.0]\n",
      "Epoch 51:  70%|███████   | 47/67 [00:13<00:05,  3.49it/s, loss=688, v_num=23, train_loss_step=57.20, val_loss=898.0, train_loss_epoch=724.0]\n",
      "Epoch 51:  72%|███████▏  | 48/67 [00:13<00:05,  3.49it/s, loss=688, v_num=23, train_loss_step=57.20, val_loss=898.0, train_loss_epoch=724.0]\n",
      "Epoch 51:  73%|███████▎  | 49/67 [00:14<00:05,  3.43it/s, loss=688, v_num=23, train_loss_step=57.20, val_loss=898.0, train_loss_epoch=724.0]\n",
      "Epoch 51:  75%|███████▍  | 50/67 [00:14<00:04,  3.43it/s, loss=688, v_num=23, train_loss_step=57.20, val_loss=898.0, train_loss_epoch=724.0]\n",
      "Epoch 51:  76%|███████▌  | 51/67 [00:15<00:04,  3.40it/s, loss=688, v_num=23, train_loss_step=57.20, val_loss=898.0, train_loss_epoch=724.0]\n",
      "Epoch 51:  78%|███████▊  | 52/67 [00:15<00:04,  3.38it/s, loss=688, v_num=23, train_loss_step=57.20, val_loss=898.0, train_loss_epoch=724.0]\n",
      "Epoch 51:  79%|███████▉  | 53/67 [00:15<00:04,  3.38it/s, loss=688, v_num=23, train_loss_step=57.20, val_loss=898.0, train_loss_epoch=724.0]\n",
      "Epoch 51:  81%|████████  | 54/67 [00:15<00:03,  3.38it/s, loss=688, v_num=23, train_loss_step=57.20, val_loss=898.0, train_loss_epoch=724.0]\n",
      "Epoch 51:  82%|████████▏ | 55/67 [00:16<00:03,  3.34it/s, loss=688, v_num=23, train_loss_step=57.20, val_loss=898.0, train_loss_epoch=724.0]\n",
      "Epoch 51:  84%|████████▎ | 56/67 [00:16<00:03,  3.34it/s, loss=688, v_num=23, train_loss_step=57.20, val_loss=898.0, train_loss_epoch=724.0]\n",
      "Epoch 51:  85%|████████▌ | 57/67 [00:17<00:02,  3.35it/s, loss=688, v_num=23, train_loss_step=57.20, val_loss=898.0, train_loss_epoch=724.0]\n",
      "Epoch 51:  87%|████████▋ | 58/67 [00:17<00:02,  3.31it/s, loss=688, v_num=23, train_loss_step=57.20, val_loss=898.0, train_loss_epoch=724.0]\n",
      "Epoch 51:  88%|████████▊ | 59/67 [00:17<00:02,  3.31it/s, loss=688, v_num=23, train_loss_step=57.20, val_loss=898.0, train_loss_epoch=724.0]\n",
      "Epoch 51:  90%|████████▉ | 60/67 [00:18<00:02,  3.31it/s, loss=688, v_num=23, train_loss_step=57.20, val_loss=898.0, train_loss_epoch=724.0]\n",
      "Epoch 51:  91%|█████████ | 61/67 [00:18<00:01,  3.26it/s, loss=688, v_num=23, train_loss_step=57.20, val_loss=898.0, train_loss_epoch=724.0]\n",
      "Epoch 51:  93%|█████████▎| 62/67 [00:19<00:01,  3.26it/s, loss=688, v_num=23, train_loss_step=57.20, val_loss=898.0, train_loss_epoch=724.0]\n",
      "Epoch 51:  94%|█████████▍| 63/67 [00:19<00:01,  3.26it/s, loss=688, v_num=23, train_loss_step=57.20, val_loss=898.0, train_loss_epoch=724.0]\n",
      "Epoch 51:  96%|█████████▌| 64/67 [00:19<00:00,  3.25it/s, loss=688, v_num=23, train_loss_step=57.20, val_loss=898.0, train_loss_epoch=724.0]\n",
      "Epoch 51:  97%|█████████▋| 65/67 [00:19<00:00,  3.26it/s, loss=688, v_num=23, train_loss_step=57.20, val_loss=898.0, train_loss_epoch=724.0]\n",
      "Epoch 51:  99%|█████████▊| 66/67 [00:20<00:00,  3.26it/s, loss=688, v_num=23, train_loss_step=57.20, val_loss=898.0, train_loss_epoch=724.0]\n",
      "Epoch 51: 100%|██████████| 67/67 [00:20<00:00,  3.19it/s, loss=688, v_num=23, train_loss_step=57.20, val_loss=898.0, train_loss_epoch=724.0]\n",
      "Epoch 52:  45%|████▍     | 30/67 [00:06<00:07,  4.88it/s, loss=598, v_num=23, train_loss_step=460.0, val_loss=898.0, train_loss_epoch=823.0]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 52:  46%|████▋     | 31/67 [00:08<00:09,  3.75it/s, loss=598, v_num=23, train_loss_step=460.0, val_loss=898.0, train_loss_epoch=823.0]\n",
      "Epoch 52:  48%|████▊     | 32/67 [00:08<00:09,  3.73it/s, loss=598, v_num=23, train_loss_step=460.0, val_loss=898.0, train_loss_epoch=823.0]\n",
      "Epoch 52:  49%|████▉     | 33/67 [00:08<00:09,  3.72it/s, loss=598, v_num=23, train_loss_step=460.0, val_loss=898.0, train_loss_epoch=823.0]\n",
      "Epoch 52:  51%|█████     | 34/67 [00:09<00:08,  3.71it/s, loss=598, v_num=23, train_loss_step=460.0, val_loss=898.0, train_loss_epoch=823.0]\n",
      "Epoch 52:  52%|█████▏    | 35/67 [00:09<00:08,  3.69it/s, loss=598, v_num=23, train_loss_step=460.0, val_loss=898.0, train_loss_epoch=823.0]\n",
      "Epoch 52:  54%|█████▎    | 36/67 [00:09<00:08,  3.68it/s, loss=598, v_num=23, train_loss_step=460.0, val_loss=898.0, train_loss_epoch=823.0]\n",
      "Epoch 52:  55%|█████▌    | 37/67 [00:10<00:08,  3.64it/s, loss=598, v_num=23, train_loss_step=460.0, val_loss=898.0, train_loss_epoch=823.0]\n",
      "Epoch 52:  57%|█████▋    | 38/67 [00:10<00:07,  3.63it/s, loss=598, v_num=23, train_loss_step=460.0, val_loss=898.0, train_loss_epoch=823.0]\n",
      "Epoch 52:  58%|█████▊    | 39/67 [00:10<00:07,  3.63it/s, loss=598, v_num=23, train_loss_step=460.0, val_loss=898.0, train_loss_epoch=823.0]\n",
      "Epoch 52:  60%|█████▉    | 40/67 [00:11<00:07,  3.56it/s, loss=598, v_num=23, train_loss_step=460.0, val_loss=898.0, train_loss_epoch=823.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 52:  61%|██████    | 41/67 [00:11<00:07,  3.51it/s, loss=598, v_num=23, train_loss_step=460.0, val_loss=898.0, train_loss_epoch=823.0]\n",
      "Epoch 52:  63%|██████▎   | 42/67 [00:11<00:07,  3.51it/s, loss=598, v_num=23, train_loss_step=460.0, val_loss=898.0, train_loss_epoch=823.0]\n",
      "Epoch 52:  64%|██████▍   | 43/67 [00:12<00:06,  3.49it/s, loss=598, v_num=23, train_loss_step=460.0, val_loss=898.0, train_loss_epoch=823.0]\n",
      "Epoch 52:  66%|██████▌   | 44/67 [00:12<00:06,  3.49it/s, loss=598, v_num=23, train_loss_step=460.0, val_loss=898.0, train_loss_epoch=823.0]\n",
      "Epoch 52:  67%|██████▋   | 45/67 [00:12<00:06,  3.49it/s, loss=598, v_num=23, train_loss_step=460.0, val_loss=898.0, train_loss_epoch=823.0]\n",
      "Epoch 52:  69%|██████▊   | 46/67 [00:13<00:06,  3.44it/s, loss=598, v_num=23, train_loss_step=460.0, val_loss=898.0, train_loss_epoch=823.0]\n",
      "Epoch 52:  70%|███████   | 47/67 [00:13<00:05,  3.44it/s, loss=598, v_num=23, train_loss_step=460.0, val_loss=898.0, train_loss_epoch=823.0]\n",
      "Epoch 52:  72%|███████▏  | 48/67 [00:13<00:05,  3.44it/s, loss=598, v_num=23, train_loss_step=460.0, val_loss=898.0, train_loss_epoch=823.0]\n",
      "Epoch 52:  73%|███████▎  | 49/67 [00:14<00:05,  3.40it/s, loss=598, v_num=23, train_loss_step=460.0, val_loss=898.0, train_loss_epoch=823.0]\n",
      "Epoch 52:  75%|███████▍  | 50/67 [00:14<00:05,  3.39it/s, loss=598, v_num=23, train_loss_step=460.0, val_loss=898.0, train_loss_epoch=823.0]\n",
      "Epoch 52:  76%|███████▌  | 51/67 [00:15<00:04,  3.37it/s, loss=598, v_num=23, train_loss_step=460.0, val_loss=898.0, train_loss_epoch=823.0]\n",
      "Epoch 52:  78%|███████▊  | 52/67 [00:15<00:04,  3.36it/s, loss=598, v_num=23, train_loss_step=460.0, val_loss=898.0, train_loss_epoch=823.0]\n",
      "Epoch 52:  79%|███████▉  | 53/67 [00:15<00:04,  3.35it/s, loss=598, v_num=23, train_loss_step=460.0, val_loss=898.0, train_loss_epoch=823.0]\n",
      "Epoch 52:  81%|████████  | 54/67 [00:16<00:03,  3.36it/s, loss=598, v_num=23, train_loss_step=460.0, val_loss=898.0, train_loss_epoch=823.0]\n",
      "Epoch 52:  82%|████████▏ | 55/67 [00:16<00:03,  3.32it/s, loss=598, v_num=23, train_loss_step=460.0, val_loss=898.0, train_loss_epoch=823.0]\n",
      "Epoch 52:  84%|████████▎ | 56/67 [00:16<00:03,  3.32it/s, loss=598, v_num=23, train_loss_step=460.0, val_loss=898.0, train_loss_epoch=823.0]\n",
      "Epoch 52:  85%|████████▌ | 57/67 [00:17<00:03,  3.33it/s, loss=598, v_num=23, train_loss_step=460.0, val_loss=898.0, train_loss_epoch=823.0]\n",
      "Epoch 52:  87%|████████▋ | 58/67 [00:17<00:02,  3.29it/s, loss=598, v_num=23, train_loss_step=460.0, val_loss=898.0, train_loss_epoch=823.0]\n",
      "Epoch 52:  88%|████████▊ | 59/67 [00:17<00:02,  3.29it/s, loss=598, v_num=23, train_loss_step=460.0, val_loss=898.0, train_loss_epoch=823.0]\n",
      "Epoch 52:  90%|████████▉ | 60/67 [00:18<00:02,  3.30it/s, loss=598, v_num=23, train_loss_step=460.0, val_loss=898.0, train_loss_epoch=823.0]\n",
      "Epoch 52:  91%|█████████ | 61/67 [00:18<00:01,  3.24it/s, loss=598, v_num=23, train_loss_step=460.0, val_loss=898.0, train_loss_epoch=823.0]\n",
      "Epoch 52:  93%|█████████▎| 62/67 [00:19<00:01,  3.24it/s, loss=598, v_num=23, train_loss_step=460.0, val_loss=898.0, train_loss_epoch=823.0]\n",
      "Epoch 52:  94%|█████████▍| 63/67 [00:19<00:01,  3.24it/s, loss=598, v_num=23, train_loss_step=460.0, val_loss=898.0, train_loss_epoch=823.0]\n",
      "Epoch 52:  96%|█████████▌| 64/67 [00:19<00:00,  3.25it/s, loss=598, v_num=23, train_loss_step=460.0, val_loss=898.0, train_loss_epoch=823.0]\n",
      "Epoch 52:  97%|█████████▋| 65/67 [00:20<00:00,  3.25it/s, loss=598, v_num=23, train_loss_step=460.0, val_loss=898.0, train_loss_epoch=823.0]\n",
      "Epoch 52:  99%|█████████▊| 66/67 [00:20<00:00,  3.25it/s, loss=598, v_num=23, train_loss_step=460.0, val_loss=898.0, train_loss_epoch=823.0]\n",
      "Epoch 52: 100%|██████████| 67/67 [00:21<00:00,  3.19it/s, loss=598, v_num=23, train_loss_step=460.0, val_loss=898.0, train_loss_epoch=823.0]\n",
      "Epoch 53:  45%|████▍     | 30/67 [00:06<00:07,  4.84it/s, loss=734, v_num=23, train_loss_step=767.0, val_loss=898.0, train_loss_epoch=739.0]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 53:  46%|████▋     | 31/67 [00:08<00:09,  3.77it/s, loss=734, v_num=23, train_loss_step=767.0, val_loss=898.0, train_loss_epoch=739.0]\n",
      "Epoch 53:  48%|████▊     | 32/67 [00:08<00:09,  3.75it/s, loss=734, v_num=23, train_loss_step=767.0, val_loss=898.0, train_loss_epoch=739.0]\n",
      "Epoch 53:  49%|████▉     | 33/67 [00:08<00:09,  3.74it/s, loss=734, v_num=23, train_loss_step=767.0, val_loss=898.0, train_loss_epoch=739.0]\n",
      "Epoch 53:  51%|█████     | 34/67 [00:09<00:08,  3.73it/s, loss=734, v_num=23, train_loss_step=767.0, val_loss=898.0, train_loss_epoch=739.0]\n",
      "Epoch 53:  52%|█████▏    | 35/67 [00:09<00:08,  3.72it/s, loss=734, v_num=23, train_loss_step=767.0, val_loss=898.0, train_loss_epoch=739.0]\n",
      "Epoch 53:  54%|█████▎    | 36/67 [00:09<00:08,  3.71it/s, loss=734, v_num=23, train_loss_step=767.0, val_loss=898.0, train_loss_epoch=739.0]\n",
      "Epoch 53:  55%|█████▌    | 37/67 [00:10<00:08,  3.66it/s, loss=734, v_num=23, train_loss_step=767.0, val_loss=898.0, train_loss_epoch=739.0]\n",
      "Epoch 53:  57%|█████▋    | 38/67 [00:10<00:07,  3.66it/s, loss=734, v_num=23, train_loss_step=767.0, val_loss=898.0, train_loss_epoch=739.0]\n",
      "Epoch 53:  58%|█████▊    | 39/67 [00:10<00:07,  3.65it/s, loss=734, v_num=23, train_loss_step=767.0, val_loss=898.0, train_loss_epoch=739.0]\n",
      "Epoch 53:  60%|█████▉    | 40/67 [00:11<00:07,  3.58it/s, loss=734, v_num=23, train_loss_step=767.0, val_loss=898.0, train_loss_epoch=739.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 53:  61%|██████    | 41/67 [00:11<00:07,  3.54it/s, loss=734, v_num=23, train_loss_step=767.0, val_loss=898.0, train_loss_epoch=739.0]\n",
      "Epoch 53:  63%|██████▎   | 42/67 [00:11<00:07,  3.54it/s, loss=734, v_num=23, train_loss_step=767.0, val_loss=898.0, train_loss_epoch=739.0]\n",
      "Epoch 53:  64%|██████▍   | 43/67 [00:12<00:06,  3.52it/s, loss=734, v_num=23, train_loss_step=767.0, val_loss=898.0, train_loss_epoch=739.0]\n",
      "Epoch 53:  66%|██████▌   | 44/67 [00:12<00:06,  3.52it/s, loss=734, v_num=23, train_loss_step=767.0, val_loss=898.0, train_loss_epoch=739.0]\n",
      "Epoch 53:  67%|██████▋   | 45/67 [00:12<00:06,  3.51it/s, loss=734, v_num=23, train_loss_step=767.0, val_loss=898.0, train_loss_epoch=739.0]\n",
      "Epoch 53:  69%|██████▊   | 46/67 [00:13<00:06,  3.46it/s, loss=734, v_num=23, train_loss_step=767.0, val_loss=898.0, train_loss_epoch=739.0]\n",
      "Epoch 53:  70%|███████   | 47/67 [00:13<00:05,  3.46it/s, loss=734, v_num=23, train_loss_step=767.0, val_loss=898.0, train_loss_epoch=739.0]\n",
      "Epoch 53:  72%|███████▏  | 48/67 [00:13<00:05,  3.46it/s, loss=734, v_num=23, train_loss_step=767.0, val_loss=898.0, train_loss_epoch=739.0]\n",
      "Epoch 53:  73%|███████▎  | 49/67 [00:14<00:05,  3.40it/s, loss=734, v_num=23, train_loss_step=767.0, val_loss=898.0, train_loss_epoch=739.0]\n",
      "Epoch 53:  75%|███████▍  | 50/67 [00:14<00:05,  3.40it/s, loss=734, v_num=23, train_loss_step=767.0, val_loss=898.0, train_loss_epoch=739.0]\n",
      "Epoch 53:  76%|███████▌  | 51/67 [00:15<00:04,  3.37it/s, loss=734, v_num=23, train_loss_step=767.0, val_loss=898.0, train_loss_epoch=739.0]\n",
      "Epoch 53:  78%|███████▊  | 52/67 [00:15<00:04,  3.36it/s, loss=734, v_num=23, train_loss_step=767.0, val_loss=898.0, train_loss_epoch=739.0]\n",
      "Epoch 53:  79%|███████▉  | 53/67 [00:15<00:04,  3.36it/s, loss=734, v_num=23, train_loss_step=767.0, val_loss=898.0, train_loss_epoch=739.0]\n",
      "Epoch 53:  81%|████████  | 54/67 [00:16<00:03,  3.37it/s, loss=734, v_num=23, train_loss_step=767.0, val_loss=898.0, train_loss_epoch=739.0]\n",
      "Epoch 53:  82%|████████▏ | 55/67 [00:16<00:03,  3.33it/s, loss=734, v_num=23, train_loss_step=767.0, val_loss=898.0, train_loss_epoch=739.0]\n",
      "Epoch 53:  84%|████████▎ | 56/67 [00:16<00:03,  3.33it/s, loss=734, v_num=23, train_loss_step=767.0, val_loss=898.0, train_loss_epoch=739.0]\n",
      "Epoch 53:  85%|████████▌ | 57/67 [00:17<00:02,  3.33it/s, loss=734, v_num=23, train_loss_step=767.0, val_loss=898.0, train_loss_epoch=739.0]\n",
      "Epoch 53:  87%|████████▋ | 58/67 [00:17<00:02,  3.30it/s, loss=734, v_num=23, train_loss_step=767.0, val_loss=898.0, train_loss_epoch=739.0]\n",
      "Epoch 53:  88%|████████▊ | 59/67 [00:17<00:02,  3.30it/s, loss=734, v_num=23, train_loss_step=767.0, val_loss=898.0, train_loss_epoch=739.0]\n",
      "Epoch 53:  90%|████████▉ | 60/67 [00:18<00:02,  3.31it/s, loss=734, v_num=23, train_loss_step=767.0, val_loss=898.0, train_loss_epoch=739.0]\n",
      "Epoch 53:  91%|█████████ | 61/67 [00:18<00:01,  3.23it/s, loss=734, v_num=23, train_loss_step=767.0, val_loss=898.0, train_loss_epoch=739.0]\n",
      "Epoch 53:  93%|█████████▎| 62/67 [00:19<00:01,  3.23it/s, loss=734, v_num=23, train_loss_step=767.0, val_loss=898.0, train_loss_epoch=739.0]\n",
      "Epoch 53:  94%|█████████▍| 63/67 [00:19<00:01,  3.24it/s, loss=734, v_num=23, train_loss_step=767.0, val_loss=898.0, train_loss_epoch=739.0]\n",
      "Epoch 53:  96%|█████████▌| 64/67 [00:19<00:00,  3.21it/s, loss=734, v_num=23, train_loss_step=767.0, val_loss=898.0, train_loss_epoch=739.0]\n",
      "Epoch 53:  97%|█████████▋| 65/67 [00:20<00:00,  3.21it/s, loss=734, v_num=23, train_loss_step=767.0, val_loss=898.0, train_loss_epoch=739.0]\n",
      "Epoch 53:  99%|█████████▊| 66/67 [00:20<00:00,  3.22it/s, loss=734, v_num=23, train_loss_step=767.0, val_loss=898.0, train_loss_epoch=739.0]\n",
      "Epoch 53: 100%|██████████| 67/67 [00:21<00:00,  3.15it/s, loss=734, v_num=23, train_loss_step=767.0, val_loss=898.0, train_loss_epoch=739.0]\n",
      "Epoch 54:  45%|████▍     | 30/67 [00:06<00:07,  4.81it/s, loss=635, v_num=23, train_loss_step=428.0, val_loss=898.0, train_loss_epoch=711.0]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 54:  46%|████▋     | 31/67 [00:08<00:09,  3.75it/s, loss=635, v_num=23, train_loss_step=428.0, val_loss=898.0, train_loss_epoch=711.0]\n",
      "Epoch 54:  48%|████▊     | 32/67 [00:08<00:09,  3.73it/s, loss=635, v_num=23, train_loss_step=428.0, val_loss=898.0, train_loss_epoch=711.0]\n",
      "Epoch 54:  49%|████▉     | 33/67 [00:08<00:09,  3.73it/s, loss=635, v_num=23, train_loss_step=428.0, val_loss=898.0, train_loss_epoch=711.0]\n",
      "Epoch 54:  51%|█████     | 34/67 [00:09<00:08,  3.72it/s, loss=635, v_num=23, train_loss_step=428.0, val_loss=898.0, train_loss_epoch=711.0]\n",
      "Epoch 54:  52%|█████▏    | 35/67 [00:09<00:08,  3.70it/s, loss=635, v_num=23, train_loss_step=428.0, val_loss=898.0, train_loss_epoch=711.0]\n",
      "Epoch 54:  54%|█████▎    | 36/67 [00:09<00:08,  3.69it/s, loss=635, v_num=23, train_loss_step=428.0, val_loss=898.0, train_loss_epoch=711.0]\n",
      "Epoch 54:  55%|█████▌    | 37/67 [00:10<00:08,  3.65it/s, loss=635, v_num=23, train_loss_step=428.0, val_loss=898.0, train_loss_epoch=711.0]\n",
      "Epoch 54:  57%|█████▋    | 38/67 [00:10<00:07,  3.64it/s, loss=635, v_num=23, train_loss_step=428.0, val_loss=898.0, train_loss_epoch=711.0]\n",
      "Epoch 54:  58%|█████▊    | 39/67 [00:10<00:07,  3.63it/s, loss=635, v_num=23, train_loss_step=428.0, val_loss=898.0, train_loss_epoch=711.0]\n",
      "Epoch 54:  60%|█████▉    | 40/67 [00:11<00:07,  3.56it/s, loss=635, v_num=23, train_loss_step=428.0, val_loss=898.0, train_loss_epoch=711.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 54:  61%|██████    | 41/67 [00:11<00:07,  3.51it/s, loss=635, v_num=23, train_loss_step=428.0, val_loss=898.0, train_loss_epoch=711.0]\n",
      "Epoch 54:  63%|██████▎   | 42/67 [00:11<00:07,  3.52it/s, loss=635, v_num=23, train_loss_step=428.0, val_loss=898.0, train_loss_epoch=711.0]\n",
      "Epoch 54:  64%|██████▍   | 43/67 [00:12<00:06,  3.49it/s, loss=635, v_num=23, train_loss_step=428.0, val_loss=898.0, train_loss_epoch=711.0]\n",
      "Epoch 54:  66%|██████▌   | 44/67 [00:12<00:06,  3.48it/s, loss=635, v_num=23, train_loss_step=428.0, val_loss=898.0, train_loss_epoch=711.0]\n",
      "Epoch 54:  67%|██████▋   | 45/67 [00:12<00:06,  3.49it/s, loss=635, v_num=23, train_loss_step=428.0, val_loss=898.0, train_loss_epoch=711.0]\n",
      "Epoch 54:  69%|██████▊   | 46/67 [00:13<00:06,  3.42it/s, loss=635, v_num=23, train_loss_step=428.0, val_loss=898.0, train_loss_epoch=711.0]\n",
      "Epoch 54:  70%|███████   | 47/67 [00:13<00:05,  3.42it/s, loss=635, v_num=23, train_loss_step=428.0, val_loss=898.0, train_loss_epoch=711.0]\n",
      "Epoch 54:  72%|███████▏  | 48/67 [00:14<00:05,  3.43it/s, loss=635, v_num=23, train_loss_step=428.0, val_loss=898.0, train_loss_epoch=711.0]\n",
      "Epoch 54:  73%|███████▎  | 49/67 [00:14<00:05,  3.37it/s, loss=635, v_num=23, train_loss_step=428.0, val_loss=898.0, train_loss_epoch=711.0]\n",
      "Epoch 54:  75%|███████▍  | 50/67 [00:14<00:05,  3.37it/s, loss=635, v_num=23, train_loss_step=428.0, val_loss=898.0, train_loss_epoch=711.0]\n",
      "Epoch 54:  76%|███████▌  | 51/67 [00:15<00:04,  3.35it/s, loss=635, v_num=23, train_loss_step=428.0, val_loss=898.0, train_loss_epoch=711.0]\n",
      "Epoch 54:  78%|███████▊  | 52/67 [00:15<00:04,  3.33it/s, loss=635, v_num=23, train_loss_step=428.0, val_loss=898.0, train_loss_epoch=711.0]\n",
      "Epoch 54:  79%|███████▉  | 53/67 [00:15<00:04,  3.33it/s, loss=635, v_num=23, train_loss_step=428.0, val_loss=898.0, train_loss_epoch=711.0]\n",
      "Epoch 54:  81%|████████  | 54/67 [00:16<00:03,  3.34it/s, loss=635, v_num=23, train_loss_step=428.0, val_loss=898.0, train_loss_epoch=711.0]\n",
      "Epoch 54:  82%|████████▏ | 55/67 [00:16<00:03,  3.30it/s, loss=635, v_num=23, train_loss_step=428.0, val_loss=898.0, train_loss_epoch=711.0]\n",
      "Epoch 54:  84%|████████▎ | 56/67 [00:16<00:03,  3.30it/s, loss=635, v_num=23, train_loss_step=428.0, val_loss=898.0, train_loss_epoch=711.0]\n",
      "Epoch 54:  85%|████████▌ | 57/67 [00:17<00:03,  3.30it/s, loss=635, v_num=23, train_loss_step=428.0, val_loss=898.0, train_loss_epoch=711.0]\n",
      "Epoch 54:  87%|████████▋ | 58/67 [00:17<00:02,  3.27it/s, loss=635, v_num=23, train_loss_step=428.0, val_loss=898.0, train_loss_epoch=711.0]\n",
      "Epoch 54:  88%|████████▊ | 59/67 [00:18<00:02,  3.27it/s, loss=635, v_num=23, train_loss_step=428.0, val_loss=898.0, train_loss_epoch=711.0]\n",
      "Epoch 54:  90%|████████▉ | 60/67 [00:18<00:02,  3.27it/s, loss=635, v_num=23, train_loss_step=428.0, val_loss=898.0, train_loss_epoch=711.0]\n",
      "Epoch 54:  91%|█████████ | 61/67 [00:18<00:01,  3.22it/s, loss=635, v_num=23, train_loss_step=428.0, val_loss=898.0, train_loss_epoch=711.0]\n",
      "Epoch 54:  93%|█████████▎| 62/67 [00:19<00:01,  3.22it/s, loss=635, v_num=23, train_loss_step=428.0, val_loss=898.0, train_loss_epoch=711.0]\n",
      "Epoch 54:  94%|█████████▍| 63/67 [00:19<00:01,  3.23it/s, loss=635, v_num=23, train_loss_step=428.0, val_loss=898.0, train_loss_epoch=711.0]\n",
      "Epoch 54:  96%|█████████▌| 64/67 [00:19<00:00,  3.22it/s, loss=635, v_num=23, train_loss_step=428.0, val_loss=898.0, train_loss_epoch=711.0]\n",
      "Epoch 54:  97%|█████████▋| 65/67 [00:20<00:00,  3.22it/s, loss=635, v_num=23, train_loss_step=428.0, val_loss=898.0, train_loss_epoch=711.0]\n",
      "Epoch 54:  99%|█████████▊| 66/67 [00:20<00:00,  3.23it/s, loss=635, v_num=23, train_loss_step=428.0, val_loss=898.0, train_loss_epoch=711.0]\n",
      "Epoch 54: 100%|██████████| 67/67 [00:21<00:00,  3.16it/s, loss=635, v_num=23, train_loss_step=428.0, val_loss=898.0, train_loss_epoch=711.0]\n",
      "Epoch 55:  45%|████▍     | 30/67 [00:06<00:07,  4.92it/s, loss=666, v_num=23, train_loss_step=620.0, val_loss=898.0, train_loss_epoch=692.0]       \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 55:  46%|████▋     | 31/67 [00:08<00:09,  3.77it/s, loss=666, v_num=23, train_loss_step=620.0, val_loss=898.0, train_loss_epoch=692.0]\n",
      "Epoch 55:  48%|████▊     | 32/67 [00:08<00:09,  3.76it/s, loss=666, v_num=23, train_loss_step=620.0, val_loss=898.0, train_loss_epoch=692.0]\n",
      "Epoch 55:  49%|████▉     | 33/67 [00:08<00:09,  3.75it/s, loss=666, v_num=23, train_loss_step=620.0, val_loss=898.0, train_loss_epoch=692.0]\n",
      "Epoch 55:  51%|█████     | 34/67 [00:09<00:08,  3.74it/s, loss=666, v_num=23, train_loss_step=620.0, val_loss=898.0, train_loss_epoch=692.0]\n",
      "Epoch 55:  52%|█████▏    | 35/67 [00:09<00:08,  3.74it/s, loss=666, v_num=23, train_loss_step=620.0, val_loss=898.0, train_loss_epoch=692.0]\n",
      "Epoch 55:  54%|█████▎    | 36/67 [00:09<00:08,  3.73it/s, loss=666, v_num=23, train_loss_step=620.0, val_loss=898.0, train_loss_epoch=692.0]\n",
      "Epoch 55:  55%|█████▌    | 37/67 [00:10<00:08,  3.66it/s, loss=666, v_num=23, train_loss_step=620.0, val_loss=898.0, train_loss_epoch=692.0]\n",
      "Epoch 55:  57%|█████▋    | 38/67 [00:10<00:07,  3.66it/s, loss=666, v_num=23, train_loss_step=620.0, val_loss=898.0, train_loss_epoch=692.0]\n",
      "Epoch 55:  58%|█████▊    | 39/67 [00:10<00:07,  3.66it/s, loss=666, v_num=23, train_loss_step=620.0, val_loss=898.0, train_loss_epoch=692.0]\n",
      "Epoch 55:  60%|█████▉    | 40/67 [00:11<00:07,  3.57it/s, loss=666, v_num=23, train_loss_step=620.0, val_loss=898.0, train_loss_epoch=692.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 55:  61%|██████    | 41/67 [00:11<00:07,  3.53it/s, loss=666, v_num=23, train_loss_step=620.0, val_loss=898.0, train_loss_epoch=692.0]\n",
      "Epoch 55:  63%|██████▎   | 42/67 [00:11<00:07,  3.52it/s, loss=666, v_num=23, train_loss_step=620.0, val_loss=898.0, train_loss_epoch=692.0]\n",
      "Epoch 55:  64%|██████▍   | 43/67 [00:12<00:06,  3.50it/s, loss=666, v_num=23, train_loss_step=620.0, val_loss=898.0, train_loss_epoch=692.0]\n",
      "Epoch 55:  66%|██████▌   | 44/67 [00:12<00:06,  3.50it/s, loss=666, v_num=23, train_loss_step=620.0, val_loss=898.0, train_loss_epoch=692.0]\n",
      "Epoch 55:  67%|██████▋   | 45/67 [00:12<00:06,  3.50it/s, loss=666, v_num=23, train_loss_step=620.0, val_loss=898.0, train_loss_epoch=692.0]\n",
      "Epoch 55:  69%|██████▊   | 46/67 [00:13<00:06,  3.44it/s, loss=666, v_num=23, train_loss_step=620.0, val_loss=898.0, train_loss_epoch=692.0]\n",
      "Epoch 55:  70%|███████   | 47/67 [00:13<00:05,  3.44it/s, loss=666, v_num=23, train_loss_step=620.0, val_loss=898.0, train_loss_epoch=692.0]\n",
      "Epoch 55:  72%|███████▏  | 48/67 [00:13<00:05,  3.44it/s, loss=666, v_num=23, train_loss_step=620.0, val_loss=898.0, train_loss_epoch=692.0]\n",
      "Epoch 55:  73%|███████▎  | 49/67 [00:14<00:05,  3.39it/s, loss=666, v_num=23, train_loss_step=620.0, val_loss=898.0, train_loss_epoch=692.0]\n",
      "Epoch 55:  75%|███████▍  | 50/67 [00:14<00:05,  3.40it/s, loss=666, v_num=23, train_loss_step=620.0, val_loss=898.0, train_loss_epoch=692.0]\n",
      "Epoch 55:  76%|███████▌  | 51/67 [00:15<00:04,  3.36it/s, loss=666, v_num=23, train_loss_step=620.0, val_loss=898.0, train_loss_epoch=692.0]\n",
      "Epoch 55:  78%|███████▊  | 52/67 [00:15<00:04,  3.35it/s, loss=666, v_num=23, train_loss_step=620.0, val_loss=898.0, train_loss_epoch=692.0]\n",
      "Epoch 55:  79%|███████▉  | 53/67 [00:15<00:04,  3.35it/s, loss=666, v_num=23, train_loss_step=620.0, val_loss=898.0, train_loss_epoch=692.0]\n",
      "Epoch 55:  81%|████████  | 54/67 [00:16<00:03,  3.35it/s, loss=666, v_num=23, train_loss_step=620.0, val_loss=898.0, train_loss_epoch=692.0]\n",
      "Epoch 55:  82%|████████▏ | 55/67 [00:16<00:03,  3.31it/s, loss=666, v_num=23, train_loss_step=620.0, val_loss=898.0, train_loss_epoch=692.0]\n",
      "Epoch 55:  84%|████████▎ | 56/67 [00:16<00:03,  3.32it/s, loss=666, v_num=23, train_loss_step=620.0, val_loss=898.0, train_loss_epoch=692.0]\n",
      "Epoch 55:  85%|████████▌ | 57/67 [00:17<00:03,  3.32it/s, loss=666, v_num=23, train_loss_step=620.0, val_loss=898.0, train_loss_epoch=692.0]\n",
      "Epoch 55:  87%|████████▋ | 58/67 [00:17<00:02,  3.28it/s, loss=666, v_num=23, train_loss_step=620.0, val_loss=898.0, train_loss_epoch=692.0]\n",
      "Epoch 55:  88%|████████▊ | 59/67 [00:17<00:02,  3.29it/s, loss=666, v_num=23, train_loss_step=620.0, val_loss=898.0, train_loss_epoch=692.0]\n",
      "Epoch 55:  90%|████████▉ | 60/67 [00:18<00:02,  3.29it/s, loss=666, v_num=23, train_loss_step=620.0, val_loss=898.0, train_loss_epoch=692.0]\n",
      "Epoch 55:  91%|█████████ | 61/67 [00:20<00:02,  2.91it/s, loss=666, v_num=23, train_loss_step=620.0, val_loss=898.0, train_loss_epoch=692.0]\n",
      "Epoch 55:  93%|█████████▎| 62/67 [00:21<00:01,  2.92it/s, loss=666, v_num=23, train_loss_step=620.0, val_loss=898.0, train_loss_epoch=692.0]\n",
      "Epoch 55:  94%|█████████▍| 63/67 [00:21<00:01,  2.93it/s, loss=666, v_num=23, train_loss_step=620.0, val_loss=898.0, train_loss_epoch=692.0]\n",
      "Epoch 55:  96%|█████████▌| 64/67 [00:21<00:01,  2.92it/s, loss=666, v_num=23, train_loss_step=620.0, val_loss=898.0, train_loss_epoch=692.0]\n",
      "Epoch 55:  97%|█████████▋| 65/67 [00:22<00:00,  2.93it/s, loss=666, v_num=23, train_loss_step=620.0, val_loss=898.0, train_loss_epoch=692.0]\n",
      "Epoch 55:  99%|█████████▊| 66/67 [00:22<00:00,  2.94it/s, loss=666, v_num=23, train_loss_step=620.0, val_loss=898.0, train_loss_epoch=692.0]\n",
      "Epoch 55: 100%|██████████| 67/67 [00:23<00:00,  2.89it/s, loss=666, v_num=23, train_loss_step=620.0, val_loss=898.0, train_loss_epoch=692.0]\n",
      "Epoch 56:  45%|████▍     | 30/67 [00:06<00:07,  4.76it/s, loss=1.02e+03, v_num=23, train_loss_step=371.0, val_loss=898.0, train_loss_epoch=879.0]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 56:  46%|████▋     | 31/67 [00:08<00:09,  3.66it/s, loss=1.02e+03, v_num=23, train_loss_step=371.0, val_loss=898.0, train_loss_epoch=879.0]\n",
      "Epoch 56:  48%|████▊     | 32/67 [00:08<00:09,  3.66it/s, loss=1.02e+03, v_num=23, train_loss_step=371.0, val_loss=898.0, train_loss_epoch=879.0]\n",
      "Epoch 56:  49%|████▉     | 33/67 [00:09<00:09,  3.66it/s, loss=1.02e+03, v_num=23, train_loss_step=371.0, val_loss=898.0, train_loss_epoch=879.0]\n",
      "Epoch 56:  51%|█████     | 34/67 [00:09<00:09,  3.64it/s, loss=1.02e+03, v_num=23, train_loss_step=371.0, val_loss=898.0, train_loss_epoch=879.0]\n",
      "Epoch 56:  52%|█████▏    | 35/67 [00:09<00:08,  3.63it/s, loss=1.02e+03, v_num=23, train_loss_step=371.0, val_loss=898.0, train_loss_epoch=879.0]\n",
      "Epoch 56:  54%|█████▎    | 36/67 [00:09<00:08,  3.63it/s, loss=1.02e+03, v_num=23, train_loss_step=371.0, val_loss=898.0, train_loss_epoch=879.0]\n",
      "Epoch 56:  55%|█████▌    | 37/67 [00:10<00:08,  3.58it/s, loss=1.02e+03, v_num=23, train_loss_step=371.0, val_loss=898.0, train_loss_epoch=879.0]\n",
      "Epoch 56:  57%|█████▋    | 38/67 [00:10<00:08,  3.57it/s, loss=1.02e+03, v_num=23, train_loss_step=371.0, val_loss=898.0, train_loss_epoch=879.0]\n",
      "Epoch 56:  58%|█████▊    | 39/67 [00:10<00:07,  3.56it/s, loss=1.02e+03, v_num=23, train_loss_step=371.0, val_loss=898.0, train_loss_epoch=879.0]\n",
      "Epoch 56:  60%|█████▉    | 40/67 [00:11<00:07,  3.50it/s, loss=1.02e+03, v_num=23, train_loss_step=371.0, val_loss=898.0, train_loss_epoch=879.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 56:  61%|██████    | 41/67 [00:11<00:07,  3.45it/s, loss=1.02e+03, v_num=23, train_loss_step=371.0, val_loss=898.0, train_loss_epoch=879.0]\n",
      "Epoch 56:  63%|██████▎   | 42/67 [00:12<00:07,  3.45it/s, loss=1.02e+03, v_num=23, train_loss_step=371.0, val_loss=898.0, train_loss_epoch=879.0]\n",
      "Epoch 56:  64%|██████▍   | 43/67 [00:12<00:06,  3.45it/s, loss=1.02e+03, v_num=23, train_loss_step=371.0, val_loss=898.0, train_loss_epoch=879.0]\n",
      "Epoch 56:  66%|██████▌   | 44/67 [00:12<00:06,  3.45it/s, loss=1.02e+03, v_num=23, train_loss_step=371.0, val_loss=898.0, train_loss_epoch=879.0]\n",
      "Epoch 56:  67%|██████▋   | 45/67 [00:13<00:06,  3.44it/s, loss=1.02e+03, v_num=23, train_loss_step=371.0, val_loss=898.0, train_loss_epoch=879.0]\n",
      "Epoch 56:  69%|██████▊   | 46/67 [00:13<00:06,  3.39it/s, loss=1.02e+03, v_num=23, train_loss_step=371.0, val_loss=898.0, train_loss_epoch=879.0]\n",
      "Epoch 56:  70%|███████   | 47/67 [00:13<00:05,  3.39it/s, loss=1.02e+03, v_num=23, train_loss_step=371.0, val_loss=898.0, train_loss_epoch=879.0]\n",
      "Epoch 56:  72%|███████▏  | 48/67 [00:14<00:05,  3.39it/s, loss=1.02e+03, v_num=23, train_loss_step=371.0, val_loss=898.0, train_loss_epoch=879.0]\n",
      "Epoch 56:  73%|███████▎  | 49/67 [00:14<00:05,  3.35it/s, loss=1.02e+03, v_num=23, train_loss_step=371.0, val_loss=898.0, train_loss_epoch=879.0]\n",
      "Epoch 56:  75%|███████▍  | 50/67 [00:14<00:05,  3.35it/s, loss=1.02e+03, v_num=23, train_loss_step=371.0, val_loss=898.0, train_loss_epoch=879.0]\n",
      "Epoch 56:  76%|███████▌  | 51/67 [00:15<00:04,  3.32it/s, loss=1.02e+03, v_num=23, train_loss_step=371.0, val_loss=898.0, train_loss_epoch=879.0]\n",
      "Epoch 56:  78%|███████▊  | 52/67 [00:17<00:05,  2.94it/s, loss=1.02e+03, v_num=23, train_loss_step=371.0, val_loss=898.0, train_loss_epoch=879.0]\n",
      "Epoch 56:  79%|███████▉  | 53/67 [00:17<00:04,  2.95it/s, loss=1.02e+03, v_num=23, train_loss_step=371.0, val_loss=898.0, train_loss_epoch=879.0]\n",
      "Epoch 56:  81%|████████  | 54/67 [00:18<00:04,  2.96it/s, loss=1.02e+03, v_num=23, train_loss_step=371.0, val_loss=898.0, train_loss_epoch=879.0]\n",
      "Epoch 56:  82%|████████▏ | 55/67 [00:18<00:04,  2.94it/s, loss=1.02e+03, v_num=23, train_loss_step=371.0, val_loss=898.0, train_loss_epoch=879.0]\n",
      "Epoch 56:  84%|████████▎ | 56/67 [00:19<00:03,  2.95it/s, loss=1.02e+03, v_num=23, train_loss_step=371.0, val_loss=898.0, train_loss_epoch=879.0]\n",
      "Epoch 56:  85%|████████▌ | 57/67 [00:19<00:03,  2.95it/s, loss=1.02e+03, v_num=23, train_loss_step=371.0, val_loss=898.0, train_loss_epoch=879.0]\n",
      "Epoch 56:  87%|████████▋ | 58/67 [00:19<00:03,  2.93it/s, loss=1.02e+03, v_num=23, train_loss_step=371.0, val_loss=898.0, train_loss_epoch=879.0]\n",
      "Epoch 56:  88%|████████▊ | 59/67 [00:20<00:02,  2.94it/s, loss=1.02e+03, v_num=23, train_loss_step=371.0, val_loss=898.0, train_loss_epoch=879.0]\n",
      "Epoch 56:  90%|████████▉ | 60/67 [00:20<00:02,  2.95it/s, loss=1.02e+03, v_num=23, train_loss_step=371.0, val_loss=898.0, train_loss_epoch=879.0]\n",
      "Epoch 56:  91%|█████████ | 61/67 [00:20<00:02,  2.91it/s, loss=1.02e+03, v_num=23, train_loss_step=371.0, val_loss=898.0, train_loss_epoch=879.0]\n",
      "Epoch 56:  93%|█████████▎| 62/67 [00:21<00:01,  2.91it/s, loss=1.02e+03, v_num=23, train_loss_step=371.0, val_loss=898.0, train_loss_epoch=879.0]\n",
      "Epoch 56:  94%|█████████▍| 63/67 [00:21<00:01,  2.92it/s, loss=1.02e+03, v_num=23, train_loss_step=371.0, val_loss=898.0, train_loss_epoch=879.0]\n",
      "Epoch 56:  96%|█████████▌| 64/67 [00:21<00:01,  2.92it/s, loss=1.02e+03, v_num=23, train_loss_step=371.0, val_loss=898.0, train_loss_epoch=879.0]\n",
      "Epoch 56:  97%|█████████▋| 65/67 [00:22<00:00,  2.93it/s, loss=1.02e+03, v_num=23, train_loss_step=371.0, val_loss=898.0, train_loss_epoch=879.0]\n",
      "Epoch 56:  99%|█████████▊| 66/67 [00:22<00:00,  2.93it/s, loss=1.02e+03, v_num=23, train_loss_step=371.0, val_loss=898.0, train_loss_epoch=879.0]\n",
      "Epoch 56: 100%|██████████| 67/67 [00:23<00:00,  2.88it/s, loss=1.02e+03, v_num=23, train_loss_step=371.0, val_loss=898.0, train_loss_epoch=879.0]\n",
      "Epoch 57:  45%|████▍     | 30/67 [00:05<00:07,  5.04it/s, loss=593, v_num=23, train_loss_step=588.0, val_loss=898.0, train_loss_epoch=863.0]      \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 57:  46%|████▋     | 31/67 [00:08<00:09,  3.83it/s, loss=593, v_num=23, train_loss_step=588.0, val_loss=898.0, train_loss_epoch=863.0]\n",
      "Epoch 57:  48%|████▊     | 32/67 [00:08<00:09,  3.82it/s, loss=593, v_num=23, train_loss_step=588.0, val_loss=898.0, train_loss_epoch=863.0]\n",
      "Epoch 57:  49%|████▉     | 33/67 [00:08<00:08,  3.81it/s, loss=593, v_num=23, train_loss_step=588.0, val_loss=898.0, train_loss_epoch=863.0]\n",
      "Epoch 57:  51%|█████     | 34/67 [00:08<00:08,  3.79it/s, loss=593, v_num=23, train_loss_step=588.0, val_loss=898.0, train_loss_epoch=863.0]\n",
      "Epoch 57:  52%|█████▏    | 35/67 [00:09<00:08,  3.78it/s, loss=593, v_num=23, train_loss_step=588.0, val_loss=898.0, train_loss_epoch=863.0]\n",
      "Epoch 57:  54%|█████▎    | 36/67 [00:09<00:08,  3.78it/s, loss=593, v_num=23, train_loss_step=588.0, val_loss=898.0, train_loss_epoch=863.0]\n",
      "Epoch 57:  55%|█████▌    | 37/67 [00:10<00:08,  3.68it/s, loss=593, v_num=23, train_loss_step=588.0, val_loss=898.0, train_loss_epoch=863.0]\n",
      "Epoch 57:  57%|█████▋    | 38/67 [00:10<00:07,  3.68it/s, loss=593, v_num=23, train_loss_step=588.0, val_loss=898.0, train_loss_epoch=863.0]\n",
      "Epoch 57:  58%|█████▊    | 39/67 [00:10<00:07,  3.67it/s, loss=593, v_num=23, train_loss_step=588.0, val_loss=898.0, train_loss_epoch=863.0]\n",
      "Epoch 57:  60%|█████▉    | 40/67 [00:11<00:07,  3.59it/s, loss=593, v_num=23, train_loss_step=588.0, val_loss=898.0, train_loss_epoch=863.0]wtahsd!!\n",
      "wtahsd!!\n",
      "\n",
      "Epoch 57:  61%|██████    | 41/67 [00:11<00:07,  3.55it/s, loss=593, v_num=23, train_loss_step=588.0, val_loss=898.0, train_loss_epoch=863.0]\n",
      "Epoch 57:  63%|██████▎   | 42/67 [00:11<00:07,  3.55it/s, loss=593, v_num=23, train_loss_step=588.0, val_loss=898.0, train_loss_epoch=863.0]\n",
      "Epoch 57:  64%|██████▍   | 43/67 [00:14<00:07,  3.01it/s, loss=593, v_num=23, train_loss_step=588.0, val_loss=898.0, train_loss_epoch=863.0]\n",
      "Epoch 57:  66%|██████▌   | 44/67 [00:14<00:07,  3.02it/s, loss=593, v_num=23, train_loss_step=588.0, val_loss=898.0, train_loss_epoch=863.0]\n",
      "Epoch 57:  67%|██████▋   | 45/67 [00:14<00:07,  3.03it/s, loss=593, v_num=23, train_loss_step=588.0, val_loss=898.0, train_loss_epoch=863.0]\n",
      "Epoch 57:  69%|██████▊   | 46/67 [00:15<00:06,  3.00it/s, loss=593, v_num=23, train_loss_step=588.0, val_loss=898.0, train_loss_epoch=863.0]\n",
      "Epoch 57:  70%|███████   | 47/67 [00:15<00:06,  3.01it/s, loss=593, v_num=23, train_loss_step=588.0, val_loss=898.0, train_loss_epoch=863.0]\n",
      "Epoch 57:  72%|███████▏  | 48/67 [00:15<00:06,  3.02it/s, loss=593, v_num=23, train_loss_step=588.0, val_loss=898.0, train_loss_epoch=863.0]\n",
      "Epoch 57:  73%|███████▎  | 49/67 [00:16<00:06,  2.98it/s, loss=593, v_num=23, train_loss_step=588.0, val_loss=898.0, train_loss_epoch=863.0]\n",
      "Epoch 57:  75%|███████▍  | 50/67 [00:16<00:05,  2.99it/s, loss=593, v_num=23, train_loss_step=588.0, val_loss=898.0, train_loss_epoch=863.0]\n",
      "Epoch 57:  76%|███████▌  | 51/67 [00:17<00:05,  2.98it/s, loss=593, v_num=23, train_loss_step=588.0, val_loss=898.0, train_loss_epoch=863.0]\n",
      "Epoch 57:  78%|███████▊  | 52/67 [00:17<00:05,  2.97it/s, loss=593, v_num=23, train_loss_step=588.0, val_loss=898.0, train_loss_epoch=863.0]\n",
      "Epoch 57:  79%|███████▉  | 53/67 [00:17<00:04,  2.98it/s, loss=593, v_num=23, train_loss_step=588.0, val_loss=898.0, train_loss_epoch=863.0]\n",
      "Epoch 57:  81%|████████  | 54/67 [00:18<00:04,  2.98it/s, loss=593, v_num=23, train_loss_step=588.0, val_loss=898.0, train_loss_epoch=863.0]\n",
      "Epoch 57:  82%|████████▏ | 55/67 [00:18<00:04,  2.96it/s, loss=593, v_num=23, train_loss_step=588.0, val_loss=898.0, train_loss_epoch=863.0]\n",
      "Epoch 57:  84%|████████▎ | 56/67 [00:18<00:03,  2.97it/s, loss=593, v_num=23, train_loss_step=588.0, val_loss=898.0, train_loss_epoch=863.0]\n",
      "Epoch 57:  85%|████████▌ | 57/67 [00:19<00:03,  2.98it/s, loss=593, v_num=23, train_loss_step=588.0, val_loss=898.0, train_loss_epoch=863.0]\n",
      "Epoch 57:  87%|████████▋ | 58/67 [00:19<00:03,  2.94it/s, loss=593, v_num=23, train_loss_step=588.0, val_loss=898.0, train_loss_epoch=863.0]\n",
      "Epoch 57:  88%|████████▊ | 59/67 [00:20<00:02,  2.95it/s, loss=593, v_num=23, train_loss_step=588.0, val_loss=898.0, train_loss_epoch=863.0]\n",
      "Epoch 57:  90%|████████▉ | 60/67 [00:20<00:02,  2.96it/s, loss=593, v_num=23, train_loss_step=588.0, val_loss=898.0, train_loss_epoch=863.0]\n",
      "Epoch 57:  91%|█████████ | 61/67 [00:20<00:02,  2.91it/s, loss=593, v_num=23, train_loss_step=588.0, val_loss=898.0, train_loss_epoch=863.0]\n",
      "Epoch 57:  93%|█████████▎| 62/67 [00:21<00:01,  2.92it/s, loss=593, v_num=23, train_loss_step=588.0, val_loss=898.0, train_loss_epoch=863.0]\n",
      "Epoch 57:  94%|█████████▍| 63/67 [00:21<00:01,  2.93it/s, loss=593, v_num=23, train_loss_step=588.0, val_loss=898.0, train_loss_epoch=863.0]\n",
      "Epoch 57:  96%|█████████▌| 64/67 [00:21<00:01,  2.92it/s, loss=593, v_num=23, train_loss_step=588.0, val_loss=898.0, train_loss_epoch=863.0]\n",
      "Epoch 57:  97%|█████████▋| 65/67 [00:22<00:00,  2.93it/s, loss=593, v_num=23, train_loss_step=588.0, val_loss=898.0, train_loss_epoch=863.0]\n",
      "Epoch 57:  99%|█████████▊| 66/67 [00:22<00:00,  2.94it/s, loss=593, v_num=23, train_loss_step=588.0, val_loss=898.0, train_loss_epoch=863.0]\n",
      "Epoch 57: 100%|██████████| 67/67 [00:23<00:00,  2.89it/s, loss=593, v_num=23, train_loss_step=588.0, val_loss=898.0, train_loss_epoch=863.0]\n",
      "Epoch 58:  45%|████▍     | 30/67 [00:06<00:07,  4.94it/s, loss=831, v_num=23, train_loss_step=653.0, val_loss=898.0, train_loss_epoch=574.0]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "# fit network\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model according to the validation loss\n",
    "# (given that we use early stopping, this is not necessarily the last epoch)\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "print(best_model_path)\n",
    "best_model_name = best_model_path.split('/')[-1]\n",
    "print('best_model_name = ',best_model_name)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# raw predictions are a dictionary from which all kind of information including quantiles can be extracted\n",
    "raw_predictions, x = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)\n",
    "print('hola')\n",
    "\n",
    "for idx in range(5):  # plot 10 examples\n",
    "    best_tft.plot_prediction(x, raw_predictions, idx=idx, add_loss_to_title=True);\n",
    "    \n",
    "predictions, x = best_tft.predict(val_dataloader, return_x=True)\n",
    "predictions_vs_actuals = best_tft.calculate_prediction_actual_by_variable(x, predictions)\n",
    "best_tft.plot_prediction_actual_by_variable(predictions_vs_actuals);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcualte root mean squared error on validation set\n",
    "actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
    "val_predictions = best_tft.predict(val_dataloader)\n",
    "criterion = nn.MSELoss()\n",
    "rmse_val = torch.sqrt(criterion(actuals,val_predictions)).item()\n",
    "print('rmse_val = ',rmse_val)\n",
    "#rmse_val =  4.774808883666992"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in range(1):\n",
    "    print(actuals[i],val_predictions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('max_prediction_length:',max_prediction_length)\n",
    "print('max_encoder_length   :',max_encoder_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select last 30 days from data (max_encoder_length is 24)\n",
    "encoder_data = df_train[lambda x: x.date_block_num > x.date_block_num.max() - max_encoder_length]\n",
    "\n",
    "print(encoder_data['date_block_num'].min(),encoder_data['date_block_num'].max())\n",
    "#print(encoder_data['DATE'].min(),encoder_data['DATE'].max())\n",
    "encoder_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "last_data = df_train[df_train['date_block_num'].isin([idx  -  max_prediction_length for idx in df_test['date_block_num'].unique()])]\n",
    "last_data['date_block_num'] = last_data['date_block_num'] + max_prediction_length\n",
    "\n",
    "decoder_data = pd.merge(df_test[[col for col in df_test.columns if 'Demanda' not in col]], \n",
    "        last_data[['date_block_num','Z_MODELO','Z_PUNTO_VENTA','Z_GAMA',\"Demanda\"]+statistics_columns],\n",
    "        on = ['date_block_num', 'Z_MODELO','Z_PUNTO_VENTA','Z_GAMA',],\n",
    "                        how='left'\n",
    "        )\n",
    "\n",
    "\n",
    "encoder_data.replace([np.inf, -np.inf, np.nan],0,inplace=True)\n",
    "decoder_data.replace([np.inf, -np.inf, np.nan],0,inplace=True)\n",
    "\n",
    "\n",
    "# combine encoder and decoder data\n",
    "new_prediction_data = pd.concat([encoder_data, decoder_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_data[['date_block_num','Z_MODELO','Z_PUNTO_VENTA','Z_GAMA',\"Demanda\"]+statistics_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "aux = encoder_data['Demanda'].values\n",
    "\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "plt.plot(aux,'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "aux = decoder_data['Demanda'].values\n",
    "\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "plt.plot(aux,'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "new_raw_predictions, new_x = best_tft.predict(new_prediction_data, mode=\"raw\", return_x=True)\n",
    "\n",
    "for idx in range(10):  # plot 10 examples\n",
    "    best_tft.plot_prediction(new_x, new_raw_predictions, idx=idx, show_future_observed=False);\n",
    "    \n",
    "interpretation = best_tft.interpret_output(new_raw_predictions, reduction=\"sum\")\n",
    "best_tft.plot_interpretation(interpretation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_raw_predictions = best_tft.predict(new_prediction_data, mode=\"prediction\", return_x=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(new_raw_predictions.numpy()).T\n",
    "predictions['date_block_num'] = sorted(df_test['date_block_num'].unique())\n",
    "predictions = pd.melt(predictions, id_vars=['date_block_num'])\n",
    "predictions = predictions.sort_values(['date_block_num', 'variable']).reset_index(drop=True)\n",
    "df_test[['date_block_num','Z_MODELO','Z_PUNTO_VENTA','Z_GAMA']].sort_values(['date_block_num', 'Z_MODELO','Z_PUNTO_VENTA','Z_GAMA']).reset_index(drop=True)\n",
    "df_test2 = df_test.join(predictions['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "reverse_mapping_file = '../../utils/reverse_dict_mapping_list.txt'\n",
    "\n",
    "with open(reverse_mapping_file, 'rb') as f:\n",
    "    reverse_mapping = pickle.load( f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reverse_mapping#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#descriptive_columns = ['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA']\n",
    "descriptive_columns = ['Z_MARCA', 'Z_GAMA', 'Z_MODELO',\n",
    "                       'Z_DEPARTAMENTO', 'Z_PUNTO_VENTA']\n",
    "i=0\n",
    "for column in descriptive_columns:\n",
    "    if column in df_test2.columns:\n",
    "        df_test2[column] = df_test2[column].map(reverse_mapping[i])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inv_dict_dates = {v: k for k, v in dict_dates.items()}\n",
    "df_test2['Z_WEEK'] = df_test2['date_block_num'].map(inv_dict_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA','Z_WEEK']:\n",
    "    df_test2[column] = df_test2[column].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test2['ID'] = df_test2['Z_MODELO'] + '|' + df_test2['Z_PUNTO_VENTA'] + '|' + df_test2['Z_GAMA'] + '|' + df_test2['Z_WEEK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test2['Demanda'] = np.maximum(df_test2['value'],0)\n",
    "submission = df_test2[['Z_WEEK','ID','Demanda']]#.groupby('ID').sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.boxplot(['Demanda'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "aux = df_train['Demanda'].values\n",
    "\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "plt.plot(aux,'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "prediction = submission['Demanda'].values\n",
    "df_baseline = pd.read_csv('../../results/Submission_37.csv')\n",
    "real = df_baseline['Demanda']\n",
    "\n",
    "\n",
    "y_actual = df_baseline['Demanda']\n",
    "y_predicted = submission['Demanda'].values\n",
    "\n",
    "rms = mean_squared_error(y_actual, y_predicted, squared=False)\n",
    "rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = plt.figure(figsize=(30,5),facecolor='white')\n",
    "\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(prediction,'bo')\n",
    "plt.title('prediction')\n",
    "\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(real,'go')\n",
    "plt.title('real')\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(prediction,'bo',alpha=0.7,label='prediction')\n",
    "plt.plot(real,'go',alpha=0.1,label='real')\n",
    "plt.title('real')\n",
    "\n",
    "plt.suptitle(best_model_name+'_MAX_EPOCHS_'+str(MAX_EPOCHS)+'_val_'+str(np.round(rmse_val,2))+'_refencia_best_pred_rmse_'+str(np.round(rms,2))+'_PREDICTIONS')\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(\n",
    "            '../../results/models/tft/'+best_model_name+'_MAX_EPOCHS_'+str(MAX_EPOCHS)+'_val_'+str(np.round(rmse_val,2))+'_refencia_best_pred_rmse_'+str(np.round(rms,2))+'_PREDICTIONS.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux[aux>=300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_actual = df_baseline['Demanda']\n",
    "y_predicted = submission['Demanda'].values\n",
    "\n",
    "rms = mean_squared_error(y_actual, y_predicted, squared=False)\n",
    "rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rmse_val = 3\n",
    "submission['Demanda_real'] = df_baseline['Demanda']\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "tendencia_semanal = submission[['Z_WEEK','Demanda','Demanda_real']].groupby(['Z_WEEK']).sum().reset_index()\n",
    "\n",
    "graph = pd.melt(tendencia_semanal,id_vars=['Z_WEEK'],value_vars=['Demanda','Demanda_real'],)\n",
    "\n",
    "#fig = px.line(graph,x='Z_WEEK',y='value',color='variable')\n",
    "#fig.show()\n",
    "import seaborn as sns\n",
    "sns.set(style='white')\n",
    "sns_fig = sns.catplot(x=\"Z_WEEK\", y=\"value\", hue=\"variable\", kind=\"point\", data=graph, height=4.27, aspect=25.7/8.27,facecolor='w')\n",
    "plt.title(best_model_name+'_MAX_EPOCHS_'+str(MAX_EPOCHS)+'_val_'+str(np.round(rmse_val,2))+'_refencia_best_pred_rmse_'+str(np.round(rms,2))+'_DEMANDA_SEMANAL_TOTAL')\n",
    "fig = sns_fig.figure\n",
    "fig.savefig(\n",
    "            '../../results/models/tft/'+best_model_name+'_MAX_EPOCHS_'+str(MAX_EPOCHS)+'_val_'+str(np.round(rmse_val,2))+'_refencia_best_pred_rmse_'+str(np.round(rms,2))+'_DEMANDA_SEMANAL_TOTAL.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submission['Demanda'] = 0.9#submission['value']\n",
    "import numpy as np\n",
    "\n",
    "submission[['ID', 'Demanda']].to_csv('../../results/models/tft/'+best_model_name+'_MAX_EPOCHS_'+str(MAX_EPOCHS)+'_val_'+str(np.round(rmse_val,2))+'_refencia_best_pred_rmse_'+str(np.round(rms,2))+'_SUBMISSION.csv', index = False, sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "bb936826e9998cf8c0ce19ca18d08375e9e9e4488ea4df72de7138dd75138a24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
