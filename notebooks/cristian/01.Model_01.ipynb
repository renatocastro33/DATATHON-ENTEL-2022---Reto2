{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from time import sleep\n",
    "\n",
    "def get_distance_from_paydays(date):\n",
    "    end_of_month = date.daysinmonth\n",
    "    distance_to_1st = 0 if date.day >=15 else 15 - date.day\n",
    "    distance_to15th = 0 if date.day < 15 else end_of_month - date.day\n",
    "    return distance_to_1st + distance_to15th\n",
    "\n",
    "def std(x): return np.std(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../../dataset/train/train_converted.csv')\n",
    "df_test  = pd.read_csv('../../dataset/test/test_converted.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Z_MARCA</th>\n",
       "      <th>Z_GAMA</th>\n",
       "      <th>Z_MODELO</th>\n",
       "      <th>Z_DEPARTAMENTO</th>\n",
       "      <th>Z_PUNTO_VENTA</th>\n",
       "      <th>Z_WEEK</th>\n",
       "      <th>Demanda</th>\n",
       "      <th>Z_WEEK_DATE</th>\n",
       "      <th>Z_DAY</th>\n",
       "      <th>Z_MONTH</th>\n",
       "      <th>Z_YEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MAR_5</td>\n",
       "      <td>GAM_3</td>\n",
       "      <td>MOD_318</td>\n",
       "      <td>DEP_12</td>\n",
       "      <td>PVENT_26</td>\n",
       "      <td>SEMANA_01</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-05-17</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MAR_3</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>MOD_305</td>\n",
       "      <td>DEP_1</td>\n",
       "      <td>PVENT_212</td>\n",
       "      <td>SEMANA_01</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-05-17</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Z_MARCA Z_GAMA Z_MODELO Z_DEPARTAMENTO Z_PUNTO_VENTA     Z_WEEK  Demanda  \\\n",
       "0   MAR_5  GAM_3  MOD_318         DEP_12      PVENT_26  SEMANA_01        0   \n",
       "1   MAR_3  GAM_1  MOD_305          DEP_1     PVENT_212  SEMANA_01        0   \n",
       "\n",
       "  Z_WEEK_DATE  Z_DAY  Z_MONTH  Z_YEAR  \n",
       "0  2021-05-17     17        5    2021  \n",
       "1  2021-05-17     17        5    2021  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Z_MARCA</th>\n",
       "      <th>Z_GAMA</th>\n",
       "      <th>Z_MODELO</th>\n",
       "      <th>Z_DEPARTAMENTO</th>\n",
       "      <th>Z_PUNTO_VENTA</th>\n",
       "      <th>Z_WEEK</th>\n",
       "      <th>Demanda</th>\n",
       "      <th>Z_WEEK_DATE</th>\n",
       "      <th>Z_DAY</th>\n",
       "      <th>Z_MONTH</th>\n",
       "      <th>Z_YEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MAR_5</td>\n",
       "      <td>GAM_3</td>\n",
       "      <td>MOD_318</td>\n",
       "      <td>DEP_12</td>\n",
       "      <td>PVENT_26</td>\n",
       "      <td>SEMANA_51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-05-02</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MAR_3</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>MOD_305</td>\n",
       "      <td>DEP_1</td>\n",
       "      <td>PVENT_212</td>\n",
       "      <td>SEMANA_51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-05-02</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Z_MARCA Z_GAMA Z_MODELO Z_DEPARTAMENTO Z_PUNTO_VENTA     Z_WEEK  Demanda  \\\n",
       "0   MAR_5  GAM_3  MOD_318         DEP_12      PVENT_26  SEMANA_51      NaN   \n",
       "1   MAR_3  GAM_1  MOD_305          DEP_1     PVENT_212  SEMANA_51      NaN   \n",
       "\n",
       "  Z_WEEK_DATE  Z_DAY  Z_MONTH  Z_YEAR  \n",
       "0  2022-05-02      2        5    2022  \n",
       "1  2022-05-02      2        5    2022  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA','Z_WEEK','Z_WEEK_DATE','Demanda']].groupby(['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA','Z_WEEK','Z_WEEK_DATE']).sum().reset_index()\n",
    "df_test = df_test[['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA','Z_WEEK','Z_WEEK_DATE','Demanda']].groupby(['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA','Z_WEEK','Z_WEEK_DATE']).sum().reset_index()\n",
    "df_train.replace(['',np.inf, -np.inf, np.nan],0,inplace=True)\n",
    "df_test.replace(['',np.inf, -np.inf, np.nan],0,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating date_block_num ...\n",
      "(2358650, 6) (471730, 6)\n",
      "(2358650, 7) (471730, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_934/1812446820.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train.replace(['',np.inf, -np.inf, np.nan],0,inplace=True)\n",
      "/tmp/ipykernel_934/1812446820.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test.replace(['',np.inf, -np.inf, np.nan],0,inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating date_block_num completed!\n",
      "Preprocessing TRAINING DATASET ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_934/1812446820.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['Z_WEEK_DATE'] = pd.to_datetime(df_train['Z_WEEK_DATE'])\n",
      "/tmp/ipykernel_934/1812446820.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['days_from_payday'] = df_train['Z_WEEK_DATE'].apply(get_distance_from_paydays)\n",
      "i:   0%|\u001b[32m                                                  \u001b[0m| 0/7 [00:00<?, ?it/s]\u001b[0m\n",
      "i:  14%|\u001b[32m██████                                    \u001b[0m| 1/7 [00:02<00:12,  2.01s/it]\u001b[0m\u001b[A\n",
      "j:   0%|\u001b[31m                                                  \u001b[0m| 0/4 [00:01<?, ?it/s]\u001b[0m\u001b[A\n",
      "j:   0%|\u001b[31m                                                  \u001b[0m| 0/4 [00:00<?, ?it/s]\u001b[0m\u001b[A\n",
      "j:  25%|\u001b[31m██████████▌                               \u001b[0m| 1/4 [00:01<00:03,  1.03s/it]\u001b[0m\u001b[A\n",
      "j:  75%|\u001b[31m███████████████████████████████▌          \u001b[0m| 3/4 [00:01<00:00,  1.68it/s]\u001b[0m\u001b[A\n",
      "j: 5it [00:02,  1.92it/s]                                                       \u001b[A\n",
      "j: 6it [00:03,  1.61it/s]\u001b[A\n",
      "i: 100%|\u001b[32m██████████████████████████████████████████\u001b[0m| 7/7 [00:34<00:00,  4.92s/it]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing TRAINING DATASET COMPLETED!\n",
      "Preprocessing TESTING DATASET ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_934/1812446820.py:96: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['Z_WEEK_DATE'] = pd.to_datetime(df_test['Z_WEEK_DATE'])\n",
      "/tmp/ipykernel_934/1812446820.py:97: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['days_from_payday'] = df_test['Z_WEEK_DATE'].apply(get_distance_from_paydays)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing TESTING DATASET COMPLETED!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print('Creating date_block_num ...')\n",
    "N_submission = df_test.shape[0]\n",
    "N_sales      = df_train.shape[0]\n",
    "\n",
    "print(df_train.shape,df_test.shape)\n",
    "df_auxiliar = pd.concat([df_train,df_test])\n",
    "df_auxiliar.replace([np.inf, -np.inf, np.nan],0,inplace=True)\n",
    "\n",
    "\n",
    "dates = df_auxiliar['Z_WEEK'].unique()\n",
    "\n",
    "date = df_auxiliar['Z_WEEK'].min()\n",
    "maxi = df_auxiliar['Z_WEEK'].max()\n",
    "\n",
    "\n",
    "dict_dates = {}\n",
    "for idx,date in enumerate(dates):\n",
    "    dict_dates[date] =idx\n",
    "    \n",
    "    \n",
    "df_auxiliar['date_block_num'] = df_auxiliar['Z_WEEK'].replace(dict_dates)\n",
    "\n",
    "df_train, df_test = df_auxiliar[:N_sales], df_auxiliar[N_sales:]\n",
    "print(df_train.shape,df_test.shape)\n",
    "\n",
    "df_train.replace(['',np.inf, -np.inf, np.nan],0,inplace=True)\n",
    "df_test.replace(['',np.inf, -np.inf, np.nan],0,inplace=True)\n",
    "\n",
    "print('Creating date_block_num completed!')\n",
    "\n",
    "\n",
    "print('Preprocessing TRAINING DATASET ...')\n",
    "\n",
    "\n",
    "df_train['Z_WEEK_DATE'] = pd.to_datetime(df_train['Z_WEEK_DATE'])\n",
    "df_train['days_from_payday'] = df_train['Z_WEEK_DATE'].apply(get_distance_from_paydays)\n",
    "\n",
    "\n",
    "statistics_columns = [ ]\n",
    "\n",
    "bar1 = tqdm([\n",
    "    ['Z_MODELO'],\n",
    "    ['Z_PUNTO_VENTA'],\n",
    "    ['Z_GAMA'],\n",
    "    ['Z_MODELO','Z_PUNTO_VENTA'],\n",
    "    ['Z_MODELO','Z_GAMA'],\n",
    "    ['Z_PUNTO_VENTA','Z_GAMA'],\n",
    "    ['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA']], position=0, desc=\"i\",colour='green', ncols=80)\n",
    "time.sleep(1)\n",
    "\n",
    "bar2 = tqdm(['mean','std','max','min'], position=1, desc=\"j\", colour='red', ncols=80, leave=False)\n",
    "time.sleep(1)\n",
    "\n",
    "unique_columns = [ ]\n",
    "        \n",
    "for column_names in bar1:\n",
    "    bar1.update()\n",
    "    bar2.refresh()  #force print final state\n",
    "    bar2.reset()  #reuse bar\n",
    "    time.sleep(0.1)\n",
    "    for statistic in bar2:\n",
    "        \n",
    "        new_column_name = statistic+'_sales_by_'+'_'.join(column_names)\n",
    "        #df_train[new_column_name] = df_train.groupby([\"Z_WEEK_DATE\"]+column_names, observed=True).Demanda.transform(statistic)\n",
    "        if statistic == 'mean':\n",
    "            df_agg = df_train.groupby([\"Z_WEEK\"]+column_names, observed=True).Demanda.mean()\n",
    "        if statistic == 'std':\n",
    "            df_agg = df_train.groupby([\"Z_WEEK\"]+column_names, observed=True).Demanda.std(ddof=0)\n",
    "        if statistic == 'max':\n",
    "            df_agg = df_train.groupby([\"Z_WEEK\"]+column_names, observed=True).Demanda.max()\n",
    "        if statistic == 'min':\n",
    "            df_agg = df_train.groupby([\"Z_WEEK\"]+column_names, observed=True).Demanda.min()\n",
    "        if df_agg.shape[0] >= df_train.shape[0]*0.7:\n",
    "            unique_columns.append([[\"Z_WEEK\"]+column_names,new_column_name])\n",
    "            continue\n",
    "        \n",
    "        df_agg = df_agg.reset_index()\n",
    "        df_agg.columns = df_agg.columns.str.replace('Demanda', new_column_name)\n",
    "        \n",
    "        df_train = df_train.merge(df_agg,on=[\"Z_WEEK\"]+column_names,how='left')\n",
    "        statistics_columns.append(new_column_name)\n",
    "        bar2.update()\n",
    "        time.sleep(0.05)\n",
    "    \n",
    "df_train['dayofweek'] = df_train['Z_WEEK_DATE'].dt.dayofweek.astype('str').astype('category')\n",
    "df_train['month'] = df_train['Z_WEEK_DATE'].dt.month.astype('str').astype('category')\n",
    "df_train['dayofyear'] = df_train['Z_WEEK_DATE'].dt.dayofyear.astype('str').astype('category')\n",
    "\n",
    "df_train.drop(columns=['Z_WEEK_DATE'],inplace=True)\n",
    "df_train.drop(columns=['Z_WEEK'],inplace=True)\n",
    "\n",
    "print('Preprocessing TRAINING DATASET COMPLETED!')\n",
    "print('Preprocessing TESTING DATASET ...')\n",
    "\n",
    "\n",
    "df_test['Z_WEEK_DATE'] = pd.to_datetime(df_test['Z_WEEK_DATE'])\n",
    "df_test['days_from_payday'] = df_test['Z_WEEK_DATE'].apply(get_distance_from_paydays)\n",
    "\n",
    "inv_dict_dates = {v: k for k, v in dict_dates.items()}\n",
    "#df_test['Z_WEEK'] = df_test['date_block_num'].map(inv_dict_dates)\n",
    "df_test = df_test[['date_block_num','Z_MODELO','Z_PUNTO_VENTA','Z_GAMA',\"Demanda\",\"Z_WEEK_DATE\"]]\n",
    "\n",
    "df_test['dayofweek'] = df_test['Z_WEEK_DATE'].dt.dayofweek.astype('str').astype('category')\n",
    "df_test['month'] = df_test['Z_WEEK_DATE'].dt.month.astype('str').astype('category')\n",
    "df_test['dayofyear'] = df_test['Z_WEEK_DATE'].dt.dayofyear.astype('str').astype('category')\n",
    "\n",
    "\n",
    "\n",
    "df_test['days_from_payday'] = df_test['Z_WEEK_DATE'].apply(get_distance_from_paydays)\n",
    "df_test.drop(columns=['Z_WEEK_DATE'],inplace=True)\n",
    "\n",
    "print('Preprocessing TESTING DATASET COMPLETED!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date_block_num  Z_MODELO  Z_GAMA\n",
       "0               MOD_1     GAM_1     0.000000\n",
       "                MOD_10    GAM_1     0.000000\n",
       "                MOD_100   GAM_1     0.000000\n",
       "                MOD_101   GAM_2     0.000000\n",
       "                MOD_102   GAM_2     0.926386\n",
       "                                      ...   \n",
       "49              MOD_95    GAM_1     0.574438\n",
       "                MOD_96    GAM_1     0.936901\n",
       "                MOD_97    GAM_5     0.000000\n",
       "                MOD_98    GAM_1     0.000000\n",
       "                MOD_99    GAM_3     8.812424\n",
       "Name: Demanda, Length: 15900, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.groupby([\"date_block_num\"]+['Z_MODELO','Z_GAMA'], observed=True).Demanda.std(ddof=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Z_WEEK', 'Z_MODELO', 'Z_PUNTO_VENTA'],\n",
       "  'mean_sales_by_Z_MODELO_Z_PUNTO_VENTA'],\n",
       " [['Z_WEEK', 'Z_MODELO', 'Z_PUNTO_VENTA'],\n",
       "  'std_sales_by_Z_MODELO_Z_PUNTO_VENTA'],\n",
       " [['Z_WEEK', 'Z_MODELO', 'Z_PUNTO_VENTA'],\n",
       "  'max_sales_by_Z_MODELO_Z_PUNTO_VENTA'],\n",
       " [['Z_WEEK', 'Z_MODELO', 'Z_PUNTO_VENTA'],\n",
       "  'min_sales_by_Z_MODELO_Z_PUNTO_VENTA'],\n",
       " [['Z_WEEK', 'Z_MODELO', 'Z_PUNTO_VENTA', 'Z_GAMA'],\n",
       "  'mean_sales_by_Z_MODELO_Z_PUNTO_VENTA_Z_GAMA'],\n",
       " [['Z_WEEK', 'Z_MODELO', 'Z_PUNTO_VENTA', 'Z_GAMA'],\n",
       "  'std_sales_by_Z_MODELO_Z_PUNTO_VENTA_Z_GAMA'],\n",
       " [['Z_WEEK', 'Z_MODELO', 'Z_PUNTO_VENTA', 'Z_GAMA'],\n",
       "  'max_sales_by_Z_MODELO_Z_PUNTO_VENTA_Z_GAMA'],\n",
       " [['Z_WEEK', 'Z_MODELO', 'Z_PUNTO_VENTA', 'Z_GAMA'],\n",
       "  'min_sales_by_Z_MODELO_Z_PUNTO_VENTA_Z_GAMA']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_934/2159503274.py:1: FutureWarning: null_counts is deprecated. Use show_counts instead\n",
      "  df_train.info(verbose=True, null_counts=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2358650 entries, 0 to 2358649\n",
      "Data columns (total 29 columns):\n",
      " #   Column                              Non-Null Count    Dtype   \n",
      "---  ------                              --------------    -----   \n",
      " 0   Z_MODELO                            2358650 non-null  object  \n",
      " 1   Z_PUNTO_VENTA                       2358650 non-null  object  \n",
      " 2   Z_GAMA                              2358650 non-null  object  \n",
      " 3   Demanda                             2358650 non-null  float64 \n",
      " 4   date_block_num                      2358650 non-null  int64   \n",
      " 5   days_from_payday                    2358650 non-null  int64   \n",
      " 6   mean_sales_by_Z_MODELO              2358650 non-null  float64 \n",
      " 7   std_sales_by_Z_MODELO               2358650 non-null  float64 \n",
      " 8   max_sales_by_Z_MODELO               2358650 non-null  float64 \n",
      " 9   min_sales_by_Z_MODELO               2358650 non-null  float64 \n",
      " 10  mean_sales_by_Z_PUNTO_VENTA         2358650 non-null  float64 \n",
      " 11  std_sales_by_Z_PUNTO_VENTA          2358650 non-null  float64 \n",
      " 12  max_sales_by_Z_PUNTO_VENTA          2358650 non-null  float64 \n",
      " 13  min_sales_by_Z_PUNTO_VENTA          2358650 non-null  float64 \n",
      " 14  mean_sales_by_Z_GAMA                2358650 non-null  float64 \n",
      " 15  std_sales_by_Z_GAMA                 2358650 non-null  float64 \n",
      " 16  max_sales_by_Z_GAMA                 2358650 non-null  float64 \n",
      " 17  min_sales_by_Z_GAMA                 2358650 non-null  float64 \n",
      " 18  mean_sales_by_Z_MODELO_Z_GAMA       2358650 non-null  float64 \n",
      " 19  std_sales_by_Z_MODELO_Z_GAMA        2358650 non-null  float64 \n",
      " 20  max_sales_by_Z_MODELO_Z_GAMA        2358650 non-null  float64 \n",
      " 21  min_sales_by_Z_MODELO_Z_GAMA        2358650 non-null  float64 \n",
      " 22  mean_sales_by_Z_PUNTO_VENTA_Z_GAMA  2358650 non-null  float64 \n",
      " 23  std_sales_by_Z_PUNTO_VENTA_Z_GAMA   2358650 non-null  float64 \n",
      " 24  max_sales_by_Z_PUNTO_VENTA_Z_GAMA   2358650 non-null  float64 \n",
      " 25  min_sales_by_Z_PUNTO_VENTA_Z_GAMA   2358650 non-null  float64 \n",
      " 26  dayofweek                           2358650 non-null  category\n",
      " 27  month                               2358650 non-null  category\n",
      " 28  dayofyear                           2358650 non-null  category\n",
      "dtypes: category(3), float64(21), int64(2), object(3)\n",
      "memory usage: 492.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info(verbose=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2358650, 29)\n",
      "['Z_MODELO', 'Z_PUNTO_VENTA', 'Z_GAMA', 'Demanda', 'date_block_num', 'days_from_payday', 'mean_sales_by_Z_MODELO', 'std_sales_by_Z_MODELO', 'max_sales_by_Z_MODELO', 'min_sales_by_Z_MODELO', 'mean_sales_by_Z_PUNTO_VENTA', 'std_sales_by_Z_PUNTO_VENTA', 'max_sales_by_Z_PUNTO_VENTA', 'min_sales_by_Z_PUNTO_VENTA', 'mean_sales_by_Z_GAMA', 'std_sales_by_Z_GAMA', 'max_sales_by_Z_GAMA', 'min_sales_by_Z_GAMA', 'mean_sales_by_Z_MODELO_Z_GAMA', 'std_sales_by_Z_MODELO_Z_GAMA', 'max_sales_by_Z_MODELO_Z_GAMA', 'min_sales_by_Z_MODELO_Z_GAMA', 'mean_sales_by_Z_PUNTO_VENTA_Z_GAMA', 'std_sales_by_Z_PUNTO_VENTA_Z_GAMA', 'max_sales_by_Z_PUNTO_VENTA_Z_GAMA', 'min_sales_by_Z_PUNTO_VENTA_Z_GAMA', 'dayofweek', 'month', 'dayofyear']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Z_MODELO</th>\n",
       "      <th>Z_PUNTO_VENTA</th>\n",
       "      <th>Z_GAMA</th>\n",
       "      <th>Demanda</th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>days_from_payday</th>\n",
       "      <th>mean_sales_by_Z_MODELO</th>\n",
       "      <th>std_sales_by_Z_MODELO</th>\n",
       "      <th>max_sales_by_Z_MODELO</th>\n",
       "      <th>min_sales_by_Z_MODELO</th>\n",
       "      <th>...</th>\n",
       "      <th>std_sales_by_Z_MODELO_Z_GAMA</th>\n",
       "      <th>max_sales_by_Z_MODELO_Z_GAMA</th>\n",
       "      <th>min_sales_by_Z_MODELO_Z_GAMA</th>\n",
       "      <th>mean_sales_by_Z_PUNTO_VENTA_Z_GAMA</th>\n",
       "      <th>std_sales_by_Z_PUNTO_VENTA_Z_GAMA</th>\n",
       "      <th>max_sales_by_Z_PUNTO_VENTA_Z_GAMA</th>\n",
       "      <th>min_sales_by_Z_PUNTO_VENTA_Z_GAMA</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>month</th>\n",
       "      <th>dayofyear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.358696</td>\n",
       "      <td>76.613312</td>\n",
       "      <td>502.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.217391</td>\n",
       "      <td>96.465113</td>\n",
       "      <td>708.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Z_MODELO Z_PUNTO_VENTA Z_GAMA  Demanda  date_block_num  days_from_payday  \\\n",
       "0    MOD_1       PVENT_1  GAM_1      0.0               0                14   \n",
       "1    MOD_1       PVENT_1  GAM_1      0.0               1                 7   \n",
       "\n",
       "   mean_sales_by_Z_MODELO  std_sales_by_Z_MODELO  max_sales_by_Z_MODELO  \\\n",
       "0                     0.0                    0.0                    0.0   \n",
       "1                     0.0                    0.0                    0.0   \n",
       "\n",
       "   min_sales_by_Z_MODELO  ...  std_sales_by_Z_MODELO_Z_GAMA  \\\n",
       "0                    0.0  ...                           0.0   \n",
       "1                    0.0  ...                           0.0   \n",
       "\n",
       "   max_sales_by_Z_MODELO_Z_GAMA  min_sales_by_Z_MODELO_Z_GAMA  \\\n",
       "0                           0.0                           0.0   \n",
       "1                           0.0                           0.0   \n",
       "\n",
       "   mean_sales_by_Z_PUNTO_VENTA_Z_GAMA  std_sales_by_Z_PUNTO_VENTA_Z_GAMA  \\\n",
       "0                           21.358696                          76.613312   \n",
       "1                           26.217391                          96.465113   \n",
       "\n",
       "   max_sales_by_Z_PUNTO_VENTA_Z_GAMA  min_sales_by_Z_PUNTO_VENTA_Z_GAMA  \\\n",
       "0                              502.0                                0.0   \n",
       "1                              708.0                                0.0   \n",
       "\n",
       "   dayofweek  month  dayofyear  \n",
       "0          0      5        137  \n",
       "1          0      5        144  \n",
       "\n",
       "[2 rows x 29 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "print(list(df_train.columns))\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2358650, 29)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics_columns :  20\n",
      "(471730, 9)\n",
      "['date_block_num', 'Z_MODELO', 'Z_PUNTO_VENTA', 'Z_GAMA', 'Demanda', 'dayofweek', 'month', 'dayofyear', 'days_from_payday']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>Z_MODELO</th>\n",
       "      <th>Z_PUNTO_VENTA</th>\n",
       "      <th>Z_GAMA</th>\n",
       "      <th>Demanda</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>month</th>\n",
       "      <th>dayofyear</th>\n",
       "      <th>days_from_payday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>122</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51</td>\n",
       "      <td>MOD_1</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>129</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   date_block_num Z_MODELO Z_PUNTO_VENTA Z_GAMA  Demanda dayofweek month  \\\n",
       "0              50    MOD_1       PVENT_1  GAM_1      0.0         0     5   \n",
       "1              51    MOD_1       PVENT_1  GAM_1      0.0         0     5   \n",
       "\n",
       "  dayofyear  days_from_payday  \n",
       "0       122                13  \n",
       "1       129                 6  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('statistics_columns : ',len(statistics_columns))\n",
    "print(df_test.shape)\n",
    "print(list(df_test.columns))\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Z_MODELO</th>\n",
       "      <th>Z_PUNTO_VENTA</th>\n",
       "      <th>Z_GAMA</th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>Demanda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24050</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24100</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_10</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24150</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_100</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24200</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_101</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24250</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_102</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46650</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_95</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46700</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_96</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46750</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_97</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46800</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_98</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46850</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_99</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>457 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Z_MODELO Z_PUNTO_VENTA Z_GAMA  date_block_num  Demanda\n",
       "24050   MOD_10       PVENT_1  GAM_1               0      0.0\n",
       "24100   MOD_10      PVENT_10  GAM_1               0      0.0\n",
       "24150   MOD_10     PVENT_100  GAM_1               0      0.0\n",
       "24200   MOD_10     PVENT_101  GAM_1               0      0.0\n",
       "24250   MOD_10     PVENT_102  GAM_1               0      0.0\n",
       "...        ...           ...    ...             ...      ...\n",
       "46650   MOD_10      PVENT_95  GAM_1               0      0.0\n",
       "46700   MOD_10      PVENT_96  GAM_1               0      0.0\n",
       "46750   MOD_10      PVENT_97  GAM_1               0      0.0\n",
       "46800   MOD_10      PVENT_98  GAM_1               0      0.0\n",
       "46850   MOD_10      PVENT_99  GAM_1               0      0.0\n",
       "\n",
       "[457 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA','date_block_num','Demanda']][(df_train['Z_MODELO']=='MOD_10')&(df_train['date_block_num']==0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MOD_1      24050\n",
       "MOD_3      23950\n",
       "MOD_2      23950\n",
       "MOD_5      23500\n",
       "MOD_4      23500\n",
       "           ...  \n",
       "MOD_294       50\n",
       "MOD_295       50\n",
       "MOD_296       50\n",
       "MOD_297       50\n",
       "MOD_287       50\n",
       "Name: Z_MODELO, Length: 318, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"Z_MODELO\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GAM_1    1039250\n",
       "GAM_2     612450\n",
       "GAM_3     278800\n",
       "GAM_4     222650\n",
       "GAM_5     185950\n",
       "GAM_6      19500\n",
       "GAM_7         50\n",
       "Name: Z_GAMA, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"Z_GAMA\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z_MODELO =  MOD_1  # GAMAS = 1\n",
      "Z_MODELO =  MOD_3  # GAMAS = 1\n",
      "Z_MODELO =  MOD_2  # GAMAS = 1\n",
      "Z_MODELO =  MOD_5  # GAMAS = 1\n",
      "Z_MODELO =  MOD_4  # GAMAS = 1\n",
      "Z_MODELO =  MOD_7  # GAMAS = 1\n",
      "Z_MODELO =  MOD_6  # GAMAS = 1\n",
      "Z_MODELO =  MOD_8  # GAMAS = 1\n",
      "Z_MODELO =  MOD_9  # GAMAS = 1\n",
      "Z_MODELO =  MOD_10  # GAMAS = 1\n",
      "Z_MODELO =  MOD_102  # GAMAS = 1\n"
     ]
    }
   ],
   "source": [
    "for z_modelo in list(df_train[\"Z_MODELO\"].value_counts().index.values)[:10]+['MOD_102']:\n",
    "    aux = df_train[(df_train['Z_MODELO']==z_modelo)]['Z_GAMA'].value_counts()\n",
    "    print('Z_MODELO = ',z_modelo,' # GAMAS =',aux.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z_MODELO =  MOD_1  # Z_PUNTO_VENTA = 481\n",
      "Z_MODELO =  MOD_3  # Z_PUNTO_VENTA = 479\n",
      "Z_MODELO =  MOD_2  # Z_PUNTO_VENTA = 479\n",
      "Z_MODELO =  MOD_5  # Z_PUNTO_VENTA = 470\n",
      "Z_MODELO =  MOD_4  # Z_PUNTO_VENTA = 470\n",
      "Z_MODELO =  MOD_7  # Z_PUNTO_VENTA = 463\n",
      "Z_MODELO =  MOD_6  # Z_PUNTO_VENTA = 463\n",
      "Z_MODELO =  MOD_8  # Z_PUNTO_VENTA = 462\n",
      "Z_MODELO =  MOD_9  # Z_PUNTO_VENTA = 462\n",
      "Z_MODELO =  MOD_10  # Z_PUNTO_VENTA = 457\n",
      "Z_MODELO =  MOD_102  # Z_PUNTO_VENTA = 226\n"
     ]
    }
   ],
   "source": [
    "for z_modelo in list(df_train[\"Z_MODELO\"].value_counts().index.values)[:10]+['MOD_102']:\n",
    "    aux = df_train[(df_train['Z_MODELO']==z_modelo)]['Z_PUNTO_VENTA'].value_counts()\n",
    "    print('Z_MODELO = ',z_modelo,' # Z_PUNTO_VENTA =',aux.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GAM_1    22850\n",
       "Name: Z_GAMA, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA','date_block_num','Demanda']][(df_train['Z_MODELO']=='MOD_10')]['Z_GAMA'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MOD_1GAM_1      24050\n",
       "MOD_3GAM_4      23950\n",
       "MOD_2GAM_1      23950\n",
       "MOD_5GAM_1      23500\n",
       "MOD_4GAM_1      23500\n",
       "                ...  \n",
       "MOD_294GAM_2       50\n",
       "MOD_295GAM_3       50\n",
       "MOD_296GAM_4       50\n",
       "MOD_297GAM_3       50\n",
       "MOD_287GAM_1       50\n",
       "Length: 318, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux = df_train[\"Z_MODELO\"].astype(str) + df_train[\"Z_GAMA\"]\n",
    "aux.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Z_MODELO</th>\n",
       "      <th>Z_PUNTO_VENTA</th>\n",
       "      <th>Z_GAMA</th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>Demanda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24050</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24051</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24052</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24053</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24054</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24055</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24056</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24057</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24058</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24059</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24060</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24061</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>11</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24062</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>12</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24063</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>13</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24064</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>14</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24065</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>15</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24066</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>16</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24067</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>17</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24068</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>18</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24069</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>19</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24070</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>20</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24071</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>21</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24072</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>22</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24073</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>23</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24074</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>24</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24075</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>25</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24076</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>26</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24077</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>27</td>\n",
       "      <td>110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24078</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>28</td>\n",
       "      <td>134.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24079</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>29</td>\n",
       "      <td>225.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24080</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>30</td>\n",
       "      <td>271.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24081</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>31</td>\n",
       "      <td>225.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24082</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>32</td>\n",
       "      <td>173.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24083</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>33</td>\n",
       "      <td>214.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24084</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>34</td>\n",
       "      <td>233.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24085</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>35</td>\n",
       "      <td>186.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24086</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>36</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24087</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>37</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24088</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>38</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24089</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>39</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24090</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>40</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24091</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>41</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24092</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>42</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24093</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>43</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24094</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>44</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24095</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>45</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24096</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>46</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24097</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>47</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24098</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>48</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24099</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>49</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Z_MODELO Z_PUNTO_VENTA Z_GAMA  date_block_num  Demanda\n",
       "24050   MOD_10       PVENT_1  GAM_1               0      0.0\n",
       "24051   MOD_10       PVENT_1  GAM_1               1      0.0\n",
       "24052   MOD_10       PVENT_1  GAM_1               2      0.0\n",
       "24053   MOD_10       PVENT_1  GAM_1               3      0.0\n",
       "24054   MOD_10       PVENT_1  GAM_1               4      0.0\n",
       "24055   MOD_10       PVENT_1  GAM_1               5      0.0\n",
       "24056   MOD_10       PVENT_1  GAM_1               6      0.0\n",
       "24057   MOD_10       PVENT_1  GAM_1               7      0.0\n",
       "24058   MOD_10       PVENT_1  GAM_1               8      0.0\n",
       "24059   MOD_10       PVENT_1  GAM_1               9      0.0\n",
       "24060   MOD_10       PVENT_1  GAM_1              10      0.0\n",
       "24061   MOD_10       PVENT_1  GAM_1              11     23.0\n",
       "24062   MOD_10       PVENT_1  GAM_1              12     29.0\n",
       "24063   MOD_10       PVENT_1  GAM_1              13      4.0\n",
       "24064   MOD_10       PVENT_1  GAM_1              14      6.0\n",
       "24065   MOD_10       PVENT_1  GAM_1              15      3.0\n",
       "24066   MOD_10       PVENT_1  GAM_1              16      8.0\n",
       "24067   MOD_10       PVENT_1  GAM_1              17      4.0\n",
       "24068   MOD_10       PVENT_1  GAM_1              18      2.0\n",
       "24069   MOD_10       PVENT_1  GAM_1              19      2.0\n",
       "24070   MOD_10       PVENT_1  GAM_1              20      5.0\n",
       "24071   MOD_10       PVENT_1  GAM_1              21      9.0\n",
       "24072   MOD_10       PVENT_1  GAM_1              22     23.0\n",
       "24073   MOD_10       PVENT_1  GAM_1              23     79.0\n",
       "24074   MOD_10       PVENT_1  GAM_1              24     72.0\n",
       "24075   MOD_10       PVENT_1  GAM_1              25     89.0\n",
       "24076   MOD_10       PVENT_1  GAM_1              26     95.0\n",
       "24077   MOD_10       PVENT_1  GAM_1              27    110.0\n",
       "24078   MOD_10       PVENT_1  GAM_1              28    134.0\n",
       "24079   MOD_10       PVENT_1  GAM_1              29    225.0\n",
       "24080   MOD_10       PVENT_1  GAM_1              30    271.0\n",
       "24081   MOD_10       PVENT_1  GAM_1              31    225.0\n",
       "24082   MOD_10       PVENT_1  GAM_1              32    173.0\n",
       "24083   MOD_10       PVENT_1  GAM_1              33    214.0\n",
       "24084   MOD_10       PVENT_1  GAM_1              34    233.0\n",
       "24085   MOD_10       PVENT_1  GAM_1              35    186.0\n",
       "24086   MOD_10       PVENT_1  GAM_1              36     77.0\n",
       "24087   MOD_10       PVENT_1  GAM_1              37     36.0\n",
       "24088   MOD_10       PVENT_1  GAM_1              38     17.0\n",
       "24089   MOD_10       PVENT_1  GAM_1              39     18.0\n",
       "24090   MOD_10       PVENT_1  GAM_1              40     15.0\n",
       "24091   MOD_10       PVENT_1  GAM_1              41     13.0\n",
       "24092   MOD_10       PVENT_1  GAM_1              42     21.0\n",
       "24093   MOD_10       PVENT_1  GAM_1              43     24.0\n",
       "24094   MOD_10       PVENT_1  GAM_1              44     24.0\n",
       "24095   MOD_10       PVENT_1  GAM_1              45     32.0\n",
       "24096   MOD_10       PVENT_1  GAM_1              46     35.0\n",
       "24097   MOD_10       PVENT_1  GAM_1              47      3.0\n",
       "24098   MOD_10       PVENT_1  GAM_1              48      5.0\n",
       "24099   MOD_10       PVENT_1  GAM_1              49     10.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA','date_block_num','Demanda']][(df_train['Z_MODELO']=='MOD_10')&(df_train['Z_PUNTO_VENTA']=='PVENT_1')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Z_MODELO</th>\n",
       "      <th>Z_PUNTO_VENTA</th>\n",
       "      <th>Z_GAMA</th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>Demanda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24060</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_1</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Z_MODELO Z_PUNTO_VENTA Z_GAMA  date_block_num  Demanda\n",
       "24060   MOD_10       PVENT_1  GAM_1              10      0.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA','date_block_num','Demanda']][(df_train['Z_MODELO']=='MOD_10')&(df_train['Z_PUNTO_VENTA']=='PVENT_1')&(df_train['date_block_num']==10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Z_MODELO</th>\n",
       "      <th>Z_PUNTO_VENTA</th>\n",
       "      <th>Z_GAMA</th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>Demanda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29600</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29601</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29602</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29603</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29604</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29605</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29606</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29607</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29608</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29609</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29610</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>10</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29611</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>11</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29612</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>12</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29613</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>13</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29614</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>14</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29615</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>15</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29616</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>16</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29617</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>17</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29618</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>18</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29619</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>19</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29620</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>20</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29621</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>21</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29622</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>22</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29623</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>23</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29624</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>24</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29625</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>25</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29626</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>26</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29627</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>27</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29628</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>28</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29629</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>29</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29630</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>30</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29631</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>31</td>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29632</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>32</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29633</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>33</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29634</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>34</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29635</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>35</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29636</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>36</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29637</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>37</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29638</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>38</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29639</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>39</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29640</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>40</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29641</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>41</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29642</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>42</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29643</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>43</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29644</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>44</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29645</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>45</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29646</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>46</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29647</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>47</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29648</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>48</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29649</th>\n",
       "      <td>MOD_10</td>\n",
       "      <td>PVENT_2</td>\n",
       "      <td>GAM_1</td>\n",
       "      <td>49</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Z_MODELO Z_PUNTO_VENTA Z_GAMA  date_block_num  Demanda\n",
       "29600   MOD_10       PVENT_2  GAM_1               0      0.0\n",
       "29601   MOD_10       PVENT_2  GAM_1               1      0.0\n",
       "29602   MOD_10       PVENT_2  GAM_1               2      0.0\n",
       "29603   MOD_10       PVENT_2  GAM_1               3      0.0\n",
       "29604   MOD_10       PVENT_2  GAM_1               4      0.0\n",
       "29605   MOD_10       PVENT_2  GAM_1               5      0.0\n",
       "29606   MOD_10       PVENT_2  GAM_1               6      0.0\n",
       "29607   MOD_10       PVENT_2  GAM_1               7      0.0\n",
       "29608   MOD_10       PVENT_2  GAM_1               8      0.0\n",
       "29609   MOD_10       PVENT_2  GAM_1               9      0.0\n",
       "29610   MOD_10       PVENT_2  GAM_1              10      2.0\n",
       "29611   MOD_10       PVENT_2  GAM_1              11     13.0\n",
       "29612   MOD_10       PVENT_2  GAM_1              12     13.0\n",
       "29613   MOD_10       PVENT_2  GAM_1              13      7.0\n",
       "29614   MOD_10       PVENT_2  GAM_1              14      5.0\n",
       "29615   MOD_10       PVENT_2  GAM_1              15      3.0\n",
       "29616   MOD_10       PVENT_2  GAM_1              16      4.0\n",
       "29617   MOD_10       PVENT_2  GAM_1              17      2.0\n",
       "29618   MOD_10       PVENT_2  GAM_1              18      3.0\n",
       "29619   MOD_10       PVENT_2  GAM_1              19      1.0\n",
       "29620   MOD_10       PVENT_2  GAM_1              20      1.0\n",
       "29621   MOD_10       PVENT_2  GAM_1              21      4.0\n",
       "29622   MOD_10       PVENT_2  GAM_1              22      9.0\n",
       "29623   MOD_10       PVENT_2  GAM_1              23     25.0\n",
       "29624   MOD_10       PVENT_2  GAM_1              24     21.0\n",
       "29625   MOD_10       PVENT_2  GAM_1              25     27.0\n",
       "29626   MOD_10       PVENT_2  GAM_1              26     23.0\n",
       "29627   MOD_10       PVENT_2  GAM_1              27     44.0\n",
       "29628   MOD_10       PVENT_2  GAM_1              28     43.0\n",
       "29629   MOD_10       PVENT_2  GAM_1              29     57.0\n",
       "29630   MOD_10       PVENT_2  GAM_1              30     94.0\n",
       "29631   MOD_10       PVENT_2  GAM_1              31     71.0\n",
       "29632   MOD_10       PVENT_2  GAM_1              32     62.0\n",
       "29633   MOD_10       PVENT_2  GAM_1              33     88.0\n",
       "29634   MOD_10       PVENT_2  GAM_1              34    100.0\n",
       "29635   MOD_10       PVENT_2  GAM_1              35     76.0\n",
       "29636   MOD_10       PVENT_2  GAM_1              36     45.0\n",
       "29637   MOD_10       PVENT_2  GAM_1              37     17.0\n",
       "29638   MOD_10       PVENT_2  GAM_1              38     16.0\n",
       "29639   MOD_10       PVENT_2  GAM_1              39     13.0\n",
       "29640   MOD_10       PVENT_2  GAM_1              40     12.0\n",
       "29641   MOD_10       PVENT_2  GAM_1              41     19.0\n",
       "29642   MOD_10       PVENT_2  GAM_1              42      8.0\n",
       "29643   MOD_10       PVENT_2  GAM_1              43     18.0\n",
       "29644   MOD_10       PVENT_2  GAM_1              44     18.0\n",
       "29645   MOD_10       PVENT_2  GAM_1              45     24.0\n",
       "29646   MOD_10       PVENT_2  GAM_1              46     14.0\n",
       "29647   MOD_10       PVENT_2  GAM_1              47      4.0\n",
       "29648   MOD_10       PVENT_2  GAM_1              48      4.0\n",
       "29649   MOD_10       PVENT_2  GAM_1              49     11.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA','date_block_num','Demanda']][(df_train['Z_MODELO']=='MOD_10')&(df_train['Z_PUNTO_VENTA']=='PVENT_2')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(df_train['Demanda'],bins=100)\n",
    "plt.title('train demanda hist')\n",
    "plt.subplot(1,2,2)\n",
    "plt.boxplot(df_train['Demanda'],vert=False)\n",
    "plt.title('train demanda boxplot')\n",
    "plt.suptitle('TRAIN Demanda distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(np.log10(df_train['Demanda']+0.1),bins=100)\n",
    "plt.title('train demanda hist')\n",
    "plt.subplot(1,2,2)\n",
    "plt.boxplot(np.log10(df_train['Demanda']+0.1),vert=False)\n",
    "plt.title('train demanda boxplot')\n",
    "plt.suptitle('TRAIN Demanda distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has\n",
      "                not been set for this class (SMAPE). The property determines if `update` by\n",
      "                default needs access to the full metric state. If this is not the case, significant speedups can be\n",
      "                achieved and we recommend setting this to `False`.\n",
      "                We provide an checking function\n",
      "                `from torchmetrics.utilities import check_forward_full_state_property`\n",
      "                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,\n",
      "                default for now) or if `full_state_update=False` can be used safely.\n",
      "                \n",
      "  warnings.warn(*args, **kwargs)\n",
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "import pytorch_forecasting\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer,EncoderNormalizer\n",
    "\n",
    "from pytorch_forecasting.metrics import SMAPE, PoissonLoss, QuantileLoss\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "\"\"\"Point metrics for forecasting a single point per time step.\"\"\"\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from pytorch_forecasting.metrics import SMAPE, MAE,RMSE\n",
    "\n",
    "import scipy.stats\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import rnn\n",
    "\n",
    "from pytorch_forecasting.metrics import MultiHorizonMetric\n",
    "from pytorch_forecasting.utils import create_mask, unpack_sequence, unsqueeze_like\n",
    "\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "# configure network and trainer\n",
    "pl.seed_everything(42)\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_steps = df_test['date_block_num'].nunique()\n",
    "prediction_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['date_block_num'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'day_of_week', 'day', 'is_month_end', 'day_of_year',  'is_quarter_start', 'year', 'month', 'is_year_start', 'is_month_start', \n",
    "# 'I103','S103', 'C101','I100' , 'C100', 'ID', 'I102','S102',, 'S101', 'S100', 'item_id', 'date_block_num', 'I101'\n",
    "max_prediction_length = prediction_steps\n",
    "\n",
    "max_encoder_length = 40\n",
    "\n",
    "training_cutoff = df_train['date_block_num'].max() - max_prediction_length\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    df_train[lambda x: x['date_block_num'] <= training_cutoff],\n",
    "    time_idx='date_block_num',\n",
    "    target=\"Demanda\",\n",
    "    group_ids=['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA'],\n",
    "    min_encoder_length= max_encoder_length // 2,   \n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "        \n",
    "    static_categoricals=['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA'],\n",
    "    \n",
    "    time_varying_unknown_categoricals=[\n",
    "                                     \"month\", \n",
    "                                     \"dayofweek\",\n",
    "                                     \"dayofyear\"],\n",
    "    \n",
    "    time_varying_unknown_reals=[\"date_block_num\",'days_from_payday'],\n",
    "                                #[\"date_block_num\",\"Demanda\"],\n",
    "    time_varying_known_categoricals=[],  \n",
    "\n",
    "    time_varying_known_reals= statistics_columns,#'date_block_num'],\n",
    "       \n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA'], transformation=\"softplus\"\n",
    "    ),  # use softplus and normalize by group    \n",
    "    \n",
    "    categorical_encoders={\n",
    "        #\"Z_MARCA\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "                          \n",
    "                          \"Z_GAMA\":  pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "                          \"Z_MODELO\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "                          #\"Z_DEPARTAMENTO\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "                          \"Z_PUNTO_VENTA\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "                          \"dayofweek\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "                          \"month\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "                          \"dayofyear\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "                         #\"date_block_num\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "                         },\n",
    "    #,\n",
    "    #                      \"item_id\":pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "    #                     },\n",
    "    #'day_of_week', 'day', 'is_month_end', 'day_of_year',  'is_quarter_start', 'year', 'month', 'is_year_start', 'is_month_start']},\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = TimeSeriesDataSet.from_dataset(training, df_train, predict=True, stop_randomization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "val_dataloader   = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pytorch_forecasting.metrics.point.TweedieLoss"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "\n",
    "pytorch_forecasting.metrics.point.TweedieLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting.metrics import TweedieLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pytorch_forecasting.metrics.point import TweedieLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import torch\n",
    "\n",
    "# calculate baseline mean absolute error, i.e. predict next value as the last available value from the history\n",
    "actuals = torch.cat([y for x, (y, weight) in iter(val_dataloader)])\n",
    "baseline_predictions = Baseline().predict(val_dataloader)\n",
    "mase_val = (actuals - baseline_predictions).abs().mean().item()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "rmse_val = torch.sqrt(criterion(actuals,baseline_predictions))\n",
    "print('baseline - mase_val = ',mase_val)\n",
    "print('baseline - rmse_val = ',rmse_val)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "composite_metric = SMAPE() + 1e-4 * MAE()\n",
    "\n",
    "class RMSE2(MultiHorizonMetric):\n",
    "    \"\"\"\n",
    "    Root mean square error\n",
    "\n",
    "    Defined as ``(y_pred - target)**2``\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, reduction=\"sqrt-mean\", **kwargs):\n",
    "        super().__init__(reduction=reduction, **kwargs)\n",
    "\n",
    "    def loss(self, y_pred: Dict[str, torch.Tensor], target):\n",
    "        loss = torch.pow(self.to_prediction(y_pred) - target, 2)\n",
    "        return loss\n",
    "    \n",
    "class TweedieLoss(MultiHorizonMetric):\n",
    "    \"\"\"\n",
    "    Tweedie loss\n",
    "    Tweedie regression with log-link. It might be useful, e.g., for modeling total\n",
    "    loss in insurance, or for any target that might be tweedie-distributed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, reduction=\"mean\", p: float = 1.9, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            p (float, optional): tweedie variance power which is greater equal\n",
    "                1.0 and smaller 2.0. Close to ``2`` shifts to\n",
    "                Gamma distribution and close to ``1`` shifts to Poisson distribution.\n",
    "                Defaults to 1.5.\n",
    "            reduction (str, optional): How to reduce the loss. Defaults to \"mean\".\n",
    "        \"\"\"\n",
    "        super().__init__(reduction=reduction, **kwargs)\n",
    "        assert 1 <= p < 2, \"p must be in range [1, 2]\"\n",
    "        self.p = p\n",
    "\n",
    "    def to_prediction(self, out: Dict[str, torch.Tensor]):\n",
    "        rate = torch.exp(super().to_prediction(out))\n",
    "        return rate\n",
    "\n",
    "    def loss(self, y_pred, y_true):\n",
    "        y_pred = super().to_prediction(y_pred)\n",
    "        a = y_true * torch.exp(y_pred * (1 - self.p)) / (1 - self.p)\n",
    "        b = torch.exp(y_pred * (2 - self.p)) / (2 - self.p)\n",
    "        loss = -a + b\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-05 07:10:20,660]\u001b[0m A new study created in memory with name: no-name-267a6f8c-953c-4dd2-93dd-08abcaf389b9\u001b[0m\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Finding best initial lr: 100%|██████████| 100/100 [00:51<00:00,  1.95it/s]`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Finding best initial lr: 100%|██████████| 100/100 [00:51<00:00,  1.95it/s]\n",
      "Restoring states from the checkpoint path at /notebooks/entel-2022/DATATHON-ENTEL-2022---Reto2/notebooks/cristian/.lr_find_60a61b99-c204-47fe-81b3-32e02bfa8bb2.ckpt\n",
      "\u001b[32m[I 2022-08-05 07:11:23,004]\u001b[0m Using learning rate of 0.00479\u001b[0m\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "\u001b[32m[I 2022-08-05 07:35:40,771]\u001b[0m Trial 0 finished with value: 3.2069363594055176 and parameters: {'gradient_clip_val': 0.049151528852465355, 'hidden_size': 45, 'dropout': 0.1269481992555201, 'hidden_continuous_size': 10, 'attention_head_size': 3, 'learning_rate': 0.0047863009232263845}. Best is trial 0 with value: 3.2069363594055176.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gradient_clip_val': 0.049151528852465355, 'hidden_size': 45, 'dropout': 0.1269481992555201, 'hidden_continuous_size': 10, 'attention_head_size': 3, 'learning_rate': 0.0047863009232263845}\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    import pickle\n",
    "\n",
    "    from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "\n",
    "    # create study\n",
    "    study = optimize_hyperparameters(\n",
    "        train_dataloader,\n",
    "        val_dataloader,\n",
    "        model_path=\"optuna_test\",\n",
    "        n_trials=10,\n",
    "        max_epochs=20,\n",
    "        gradient_clip_val_range=(0.01, 1.0),\n",
    "        hidden_size_range=(8, 64),\n",
    "        hidden_continuous_size_range=(8, 64),\n",
    "        attention_head_size_range=(1, 4),\n",
    "        learning_rate_range=(0.001, 0.1),\n",
    "        dropout_range=(0.1, 0.3),\n",
    "        trainer_kwargs=dict(limit_train_batches=30, log_every_n_steps=15, gpus=1),\n",
    "        reduce_on_plateau_patience=4,\n",
    "        use_learning_rate_finder=True,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n",
    "        timeout=300,\n",
    "        #loss=QuantileLoss()\n",
    "        loss=RMSE()\n",
    "        #loss=TweedieLoss(p=1.1)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # show best hyperparameters\n",
    "    print(study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Early Stopping \n",
    "MIN_DELTA  = 1e-4\n",
    "PATIENCE   = 30\n",
    "\n",
    "#PL Trainer\n",
    "MAX_EPOCHS = 10000\n",
    "GPUS = 1\n",
    "\n",
    "if True:\n",
    "    GRADIENT_CLIP_VAL=study.best_trial.params['gradient_clip_val']\n",
    "    LIMIT_TRAIN_BATCHES=30\n",
    "\n",
    "    #Fusion Transformer\n",
    "    LR = study.best_trial.params['learning_rate']\n",
    "    HIDDEN_SIZE = study.best_trial.params['hidden_size']\n",
    "    DROPOUT = study.best_trial.params['dropout']\n",
    "    ATTENTION_HEAD_SIZE = study.best_trial.params['attention_head_size']\n",
    "    HIDDEN_CONTINUOUS_SIZE = study.best_trial.params['hidden_continuous_size']\n",
    "    \n",
    "    \n",
    "\n",
    "else:\n",
    "\n",
    "    #study = {'gradient_clip_val': 0.5899880996240897, 'hidden_size': 24, 'dropout': 0.18045896986283255, \n",
    "    # 'hidden_continuous_size': 11, 'attention_head_size': 2, 'learning_rate': 0.012427775478680268}\n",
    "    \n",
    "    study = {'gradient_clip_val': 0.3468483254885978, 'hidden_size': 32, 'dropout': 0.11031796015695508, \n",
    "            'hidden_continuous_size': 19, 'attention_head_size': 15, 'learning_rate': 0.04289257757804779}\n",
    "    \n",
    "    GRADIENT_CLIP_VAL=study['gradient_clip_val']\n",
    "    LIMIT_TRAIN_BATCHES=30\n",
    "\n",
    "    #Fusion Transformer\n",
    "    LR = study['learning_rate']\n",
    "    HIDDEN_SIZE = study['hidden_size']\n",
    "    DROPOUT = study['dropout']\n",
    "    ATTENTION_HEAD_SIZE = study['attention_head_size']\n",
    "    HIDDEN_CONTINUOUS_SIZE = study['hidden_continuous_size']\n",
    "    \n",
    "OUTPUT_SIZE= 1\n",
    "REDUCE_ON_PLATEAU_PATIENCE=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ATTENTION_HEAD_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_prediction_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 225.4k\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "\n",
    "# configure network and trainer\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=MIN_DELTA, patience=PATIENCE, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    gpus=GPUS,\n",
    "    #weights_summary=\"top\",\n",
    "    gradient_clip_val=GRADIENT_CLIP_VAL,\n",
    "    limit_train_batches=LIMIT_TRAIN_BATCHES,#oment in for training, running valiation every 30 batches\n",
    "    #fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    "    log_every_n_steps=10\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=LR,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    attention_head_size=ATTENTION_HEAD_SIZE,\n",
    "    dropout=DROPOUT,\n",
    "    hidden_continuous_size=HIDDEN_CONTINUOUS_SIZE,\n",
    "    output_size=OUTPUT_SIZE,# 7 quantiles by default\n",
    "    \n",
    "    #loss=QuantileLoss(),\n",
    "    loss=RMSE(),\n",
    "    #loss=TweedieLoss(p=1.1),\n",
    "    log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "    reduce_on_plateau_patience=REDUCE_ON_PLATEAU_PATIENCE,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | RMSE                            | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 35.3 K\n",
      "3  | prescalers                         | ModuleDict                      | 520   \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 5.7 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 43.8 K\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 37.4 K\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 8.4 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 8.4 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 8.4 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 8.4 K \n",
      "11 | lstm_encoder                       | LSTM                            | 16.6 K\n",
      "12 | lstm_decoder                       | LSTM                            | 16.6 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 4.1 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 90    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 10.4 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 5.5 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 4.2 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 8.4 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 4.2 K \n",
      "20 | output_layer                       | Linear                          | 46    \n",
      "----------------------------------------------------------------------------------------\n",
      "225 K     Trainable params\n",
      "0         Non-trainable params\n",
      "225 K     Total params\n",
      "0.902     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  45%|████▍     | 30/67 [00:15<00:19,  1.94it/s, loss=2.55, v_num=32, train_loss_step=1.630]\n",
      "                                       \n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  46%|████▋     | 31/67 [00:17<00:20,  1.75it/s, loss=2.55, v_num=32, train_loss_step=1.630]\n",
      "Epoch 0:  48%|████▊     | 32/67 [00:19<00:21,  1.66it/s, loss=2.55, v_num=32, train_loss_step=1.630]\n",
      "Epoch 0:  49%|████▉     | 33/67 [00:20<00:21,  1.59it/s, loss=2.55, v_num=32, train_loss_step=1.630]\n",
      "Epoch 0:  51%|█████     | 34/67 [00:22<00:21,  1.53it/s, loss=2.55, v_num=32, train_loss_step=1.630]\n",
      "Epoch 0:  52%|█████▏    | 35/67 [00:23<00:21,  1.47it/s, loss=2.55, v_num=32, train_loss_step=1.630]\n",
      "Epoch 0:  54%|█████▎    | 36/67 [00:25<00:21,  1.42it/s, loss=2.55, v_num=32, train_loss_step=1.630]\n",
      "Epoch 0:  55%|█████▌    | 37/67 [00:26<00:21,  1.37it/s, loss=2.55, v_num=32, train_loss_step=1.630]\n",
      "Epoch 0:  57%|█████▋    | 38/67 [00:28<00:21,  1.32it/s, loss=2.55, v_num=32, train_loss_step=1.630]\n",
      "Epoch 0:  58%|█████▊    | 39/67 [00:30<00:21,  1.29it/s, loss=2.55, v_num=32, train_loss_step=1.630]\n",
      "Epoch 0:  60%|█████▉    | 40/67 [00:31<00:21,  1.26it/s, loss=2.55, v_num=32, train_loss_step=1.630]\n",
      "Epoch 0:  61%|██████    | 41/67 [00:33<00:21,  1.22it/s, loss=2.55, v_num=32, train_loss_step=1.630]\n",
      "Epoch 0:  63%|██████▎   | 42/67 [00:35<00:20,  1.20it/s, loss=2.55, v_num=32, train_loss_step=1.630]\n",
      "Epoch 0:  64%|██████▍   | 43/67 [00:36<00:20,  1.18it/s, loss=2.55, v_num=32, train_loss_step=1.630]\n",
      "Epoch 0:  66%|██████▌   | 44/67 [00:38<00:19,  1.16it/s, loss=2.55, v_num=32, train_loss_step=1.630]\n",
      "Epoch 0:  67%|██████▋   | 45/67 [00:39<00:19,  1.13it/s, loss=2.55, v_num=32, train_loss_step=1.630]\n",
      "Epoch 0:  69%|██████▊   | 46/67 [00:41<00:18,  1.11it/s, loss=2.55, v_num=32, train_loss_step=1.630]\n",
      "Epoch 0:  70%|███████   | 47/67 [00:42<00:18,  1.10it/s, loss=2.55, v_num=32, train_loss_step=1.630]\n",
      "Epoch 0:  72%|███████▏  | 48/67 [00:44<00:17,  1.08it/s, loss=2.55, v_num=32, train_loss_step=1.630]\n",
      "Epoch 0:  73%|███████▎  | 49/67 [00:45<00:16,  1.07it/s, loss=2.55, v_num=32, train_loss_step=1.630]\n",
      "Epoch 0:  75%|███████▍  | 50/67 [00:47<00:16,  1.05it/s, loss=2.55, v_num=32, train_loss_step=1.630]\n",
      "Epoch 0:  76%|███████▌  | 51/67 [00:49<00:15,  1.04it/s, loss=2.55, v_num=32, train_loss_step=1.630]\n",
      "Epoch 0:  78%|███████▊  | 52/67 [00:50<00:14,  1.02it/s, loss=2.55, v_num=32, train_loss_step=1.630]\n",
      "Epoch 0:  79%|███████▉  | 53/67 [00:52<00:13,  1.01it/s, loss=2.55, v_num=32, train_loss_step=1.630]\n",
      "Epoch 0:  81%|████████  | 54/67 [00:53<00:12,  1.00it/s, loss=2.55, v_num=32, train_loss_step=1.630]\n",
      "Epoch 0:  82%|████████▏ | 55/67 [00:55<00:12,  1.01s/it, loss=2.55, v_num=32, train_loss_step=1.630]\n",
      "Epoch 0:  84%|████████▎ | 56/67 [00:56<00:11,  1.02s/it, loss=2.55, v_num=32, train_loss_step=1.630]\n",
      "Epoch 0:  85%|████████▌ | 57/67 [00:58<00:10,  1.03s/it, loss=2.55, v_num=32, train_loss_step=1.630]\n",
      "Epoch 0:  87%|████████▋ | 58/67 [01:00<00:09,  1.04s/it, loss=2.55, v_num=32, train_loss_step=1.630]\n",
      "Epoch 0:  88%|████████▊ | 59/67 [01:01<00:08,  1.05s/it, loss=2.55, v_num=32, train_loss_step=1.630]\n",
      "Epoch 0:  90%|████████▉ | 60/67 [01:03<00:07,  1.06s/it, loss=2.55, v_num=32, train_loss_step=1.630]\n",
      "Epoch 0:  91%|█████████ | 61/67 [01:04<00:06,  1.07s/it, loss=2.55, v_num=32, train_loss_step=1.630]\n",
      "Epoch 0:  93%|█████████▎| 62/67 [01:06<00:05,  1.07s/it, loss=2.55, v_num=32, train_loss_step=1.630]\n",
      "Epoch 0:  94%|█████████▍| 63/67 [01:08<00:04,  1.08s/it, loss=2.55, v_num=32, train_loss_step=1.630]\n",
      "Epoch 0:  96%|█████████▌| 64/67 [01:09<00:03,  1.09s/it, loss=2.55, v_num=32, train_loss_step=1.630]\n",
      "Epoch 0:  97%|█████████▋| 65/67 [01:11<00:02,  1.09s/it, loss=2.55, v_num=32, train_loss_step=1.630]\n",
      "Epoch 0:  99%|█████████▊| 66/67 [01:12<00:01,  1.10s/it, loss=2.55, v_num=32, train_loss_step=1.630]\n",
      "Epoch 0: 100%|██████████| 67/67 [01:14<00:00,  1.12s/it, loss=2.55, v_num=32, train_loss_step=1.630, val_loss=3.220]\n",
      "Epoch 1:  45%|████▍     | 30/67 [00:16<00:19,  1.87it/s, loss=3.68, v_num=32, train_loss_step=2.060, val_loss=3.220, train_loss_epoch=3.650]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  46%|████▋     | 31/67 [00:17<00:20,  1.74it/s, loss=3.68, v_num=32, train_loss_step=2.060, val_loss=3.220, train_loss_epoch=3.650]\n",
      "Epoch 1:  48%|████▊     | 32/67 [00:19<00:21,  1.65it/s, loss=3.68, v_num=32, train_loss_step=2.060, val_loss=3.220, train_loss_epoch=3.650]\n",
      "Epoch 1:  49%|████▉     | 33/67 [00:20<00:21,  1.58it/s, loss=3.68, v_num=32, train_loss_step=2.060, val_loss=3.220, train_loss_epoch=3.650]\n",
      "Epoch 1:  51%|█████     | 34/67 [00:22<00:22,  1.50it/s, loss=3.68, v_num=32, train_loss_step=2.060, val_loss=3.220, train_loss_epoch=3.650]\n",
      "Epoch 1:  52%|█████▏    | 35/67 [00:24<00:22,  1.44it/s, loss=3.68, v_num=32, train_loss_step=2.060, val_loss=3.220, train_loss_epoch=3.650]\n",
      "Epoch 1:  54%|█████▎    | 36/67 [00:25<00:22,  1.39it/s, loss=3.68, v_num=32, train_loss_step=2.060, val_loss=3.220, train_loss_epoch=3.650]\n",
      "Epoch 1:  55%|█████▌    | 37/67 [00:27<00:22,  1.35it/s, loss=3.68, v_num=32, train_loss_step=2.060, val_loss=3.220, train_loss_epoch=3.650]\n",
      "Epoch 1:  57%|█████▋    | 38/67 [00:28<00:22,  1.31it/s, loss=3.68, v_num=32, train_loss_step=2.060, val_loss=3.220, train_loss_epoch=3.650]\n",
      "Epoch 1:  58%|█████▊    | 39/67 [00:30<00:21,  1.28it/s, loss=3.68, v_num=32, train_loss_step=2.060, val_loss=3.220, train_loss_epoch=3.650]\n",
      "Epoch 1:  60%|█████▉    | 40/67 [00:32<00:21,  1.25it/s, loss=3.68, v_num=32, train_loss_step=2.060, val_loss=3.220, train_loss_epoch=3.650]\n",
      "Epoch 1:  61%|██████    | 41/67 [00:33<00:21,  1.22it/s, loss=3.68, v_num=32, train_loss_step=2.060, val_loss=3.220, train_loss_epoch=3.650]\n",
      "Epoch 1:  63%|██████▎   | 42/67 [00:35<00:21,  1.18it/s, loss=3.68, v_num=32, train_loss_step=2.060, val_loss=3.220, train_loss_epoch=3.650]\n",
      "Epoch 1:  64%|██████▍   | 43/67 [00:36<00:20,  1.16it/s, loss=3.68, v_num=32, train_loss_step=2.060, val_loss=3.220, train_loss_epoch=3.650]\n",
      "Epoch 1:  66%|██████▌   | 44/67 [00:38<00:20,  1.14it/s, loss=3.68, v_num=32, train_loss_step=2.060, val_loss=3.220, train_loss_epoch=3.650]\n",
      "Epoch 1:  67%|██████▋   | 45/67 [00:40<00:19,  1.12it/s, loss=3.68, v_num=32, train_loss_step=2.060, val_loss=3.220, train_loss_epoch=3.650]\n",
      "Epoch 1:  69%|██████▊   | 46/67 [00:41<00:18,  1.11it/s, loss=3.68, v_num=32, train_loss_step=2.060, val_loss=3.220, train_loss_epoch=3.650]\n",
      "Epoch 1:  70%|███████   | 47/67 [00:43<00:18,  1.09it/s, loss=3.68, v_num=32, train_loss_step=2.060, val_loss=3.220, train_loss_epoch=3.650]\n",
      "Epoch 1:  72%|███████▏  | 48/67 [00:44<00:17,  1.07it/s, loss=3.68, v_num=32, train_loss_step=2.060, val_loss=3.220, train_loss_epoch=3.650]\n",
      "Epoch 1:  73%|███████▎  | 49/67 [00:46<00:16,  1.06it/s, loss=3.68, v_num=32, train_loss_step=2.060, val_loss=3.220, train_loss_epoch=3.650]\n",
      "Epoch 1:  75%|███████▍  | 50/67 [00:47<00:16,  1.04it/s, loss=3.68, v_num=32, train_loss_step=2.060, val_loss=3.220, train_loss_epoch=3.650]\n",
      "Epoch 1:  76%|███████▌  | 51/67 [00:49<00:15,  1.03it/s, loss=3.68, v_num=32, train_loss_step=2.060, val_loss=3.220, train_loss_epoch=3.650]\n",
      "Epoch 1:  78%|███████▊  | 52/67 [00:51<00:14,  1.02it/s, loss=3.68, v_num=32, train_loss_step=2.060, val_loss=3.220, train_loss_epoch=3.650]\n",
      "Epoch 1:  79%|███████▉  | 53/67 [00:52<00:13,  1.01it/s, loss=3.68, v_num=32, train_loss_step=2.060, val_loss=3.220, train_loss_epoch=3.650]\n",
      "Epoch 1:  81%|████████  | 54/67 [00:54<00:13,  1.01s/it, loss=3.68, v_num=32, train_loss_step=2.060, val_loss=3.220, train_loss_epoch=3.650]\n",
      "Epoch 1:  82%|████████▏ | 55/67 [00:55<00:12,  1.02s/it, loss=3.68, v_num=32, train_loss_step=2.060, val_loss=3.220, train_loss_epoch=3.650]\n",
      "Epoch 1:  84%|████████▎ | 56/67 [00:57<00:11,  1.02s/it, loss=3.68, v_num=32, train_loss_step=2.060, val_loss=3.220, train_loss_epoch=3.650]\n",
      "Epoch 1:  85%|████████▌ | 57/67 [00:58<00:10,  1.03s/it, loss=3.68, v_num=32, train_loss_step=2.060, val_loss=3.220, train_loss_epoch=3.650]\n",
      "Epoch 1:  87%|████████▋ | 58/67 [01:00<00:09,  1.05s/it, loss=3.68, v_num=32, train_loss_step=2.060, val_loss=3.220, train_loss_epoch=3.650]\n",
      "Epoch 1:  88%|████████▊ | 59/67 [01:02<00:08,  1.06s/it, loss=3.68, v_num=32, train_loss_step=2.060, val_loss=3.220, train_loss_epoch=3.650]\n",
      "Epoch 1:  90%|████████▉ | 60/67 [01:03<00:07,  1.06s/it, loss=3.68, v_num=32, train_loss_step=2.060, val_loss=3.220, train_loss_epoch=3.650]\n",
      "Epoch 1:  91%|█████████ | 61/67 [01:05<00:06,  1.07s/it, loss=3.68, v_num=32, train_loss_step=2.060, val_loss=3.220, train_loss_epoch=3.650]\n",
      "Epoch 1:  93%|█████████▎| 62/67 [01:07<00:05,  1.08s/it, loss=3.68, v_num=32, train_loss_step=2.060, val_loss=3.220, train_loss_epoch=3.650]\n",
      "Epoch 1:  94%|█████████▍| 63/67 [01:08<00:04,  1.09s/it, loss=3.68, v_num=32, train_loss_step=2.060, val_loss=3.220, train_loss_epoch=3.650]\n",
      "Epoch 1:  96%|█████████▌| 64/67 [01:10<00:03,  1.10s/it, loss=3.68, v_num=32, train_loss_step=2.060, val_loss=3.220, train_loss_epoch=3.650]\n",
      "Epoch 1:  97%|█████████▋| 65/67 [01:11<00:02,  1.10s/it, loss=3.68, v_num=32, train_loss_step=2.060, val_loss=3.220, train_loss_epoch=3.650]\n",
      "Epoch 1:  99%|█████████▊| 66/67 [01:13<00:01,  1.11s/it, loss=3.68, v_num=32, train_loss_step=2.060, val_loss=3.220, train_loss_epoch=3.650]\n",
      "Epoch 1: 100%|██████████| 67/67 [01:15<00:00,  1.13s/it, loss=3.68, v_num=32, train_loss_step=2.060, val_loss=3.210, train_loss_epoch=3.650]\n",
      "Epoch 2:  45%|████▍     | 30/67 [00:15<00:19,  1.88it/s, loss=4.58, v_num=32, train_loss_step=1.330, val_loss=3.210, train_loss_epoch=3.650]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  46%|████▋     | 31/67 [00:17<00:20,  1.76it/s, loss=4.58, v_num=32, train_loss_step=1.330, val_loss=3.210, train_loss_epoch=3.650]\n",
      "Epoch 2:  48%|████▊     | 32/67 [00:19<00:21,  1.66it/s, loss=4.58, v_num=32, train_loss_step=1.330, val_loss=3.210, train_loss_epoch=3.650]\n",
      "Epoch 2:  49%|████▉     | 33/67 [00:20<00:21,  1.59it/s, loss=4.58, v_num=32, train_loss_step=1.330, val_loss=3.210, train_loss_epoch=3.650]\n",
      "Epoch 2:  51%|█████     | 34/67 [00:22<00:21,  1.53it/s, loss=4.58, v_num=32, train_loss_step=1.330, val_loss=3.210, train_loss_epoch=3.650]\n",
      "Epoch 2:  52%|█████▏    | 35/67 [00:24<00:22,  1.44it/s, loss=4.58, v_num=32, train_loss_step=1.330, val_loss=3.210, train_loss_epoch=3.650]\n",
      "Epoch 2:  54%|█████▎    | 36/67 [00:25<00:22,  1.40it/s, loss=4.58, v_num=32, train_loss_step=1.330, val_loss=3.210, train_loss_epoch=3.650]\n",
      "Epoch 2:  55%|█████▌    | 37/67 [00:27<00:22,  1.35it/s, loss=4.58, v_num=32, train_loss_step=1.330, val_loss=3.210, train_loss_epoch=3.650]\n",
      "Epoch 2:  57%|█████▋    | 38/67 [00:28<00:22,  1.32it/s, loss=4.58, v_num=32, train_loss_step=1.330, val_loss=3.210, train_loss_epoch=3.650]\n",
      "Epoch 2:  58%|█████▊    | 39/67 [00:30<00:21,  1.29it/s, loss=4.58, v_num=32, train_loss_step=1.330, val_loss=3.210, train_loss_epoch=3.650]\n",
      "Epoch 2:  60%|█████▉    | 40/67 [00:31<00:21,  1.26it/s, loss=4.58, v_num=32, train_loss_step=1.330, val_loss=3.210, train_loss_epoch=3.650]\n",
      "Epoch 2:  61%|██████    | 41/67 [00:33<00:21,  1.23it/s, loss=4.58, v_num=32, train_loss_step=1.330, val_loss=3.210, train_loss_epoch=3.650]\n",
      "Epoch 2:  63%|██████▎   | 42/67 [00:34<00:20,  1.20it/s, loss=4.58, v_num=32, train_loss_step=1.330, val_loss=3.210, train_loss_epoch=3.650]\n",
      "Epoch 2:  64%|██████▍   | 43/67 [00:36<00:20,  1.17it/s, loss=4.58, v_num=32, train_loss_step=1.330, val_loss=3.210, train_loss_epoch=3.650]\n",
      "Epoch 2:  66%|██████▌   | 44/67 [00:38<00:20,  1.15it/s, loss=4.58, v_num=32, train_loss_step=1.330, val_loss=3.210, train_loss_epoch=3.650]\n",
      "Epoch 2:  67%|██████▋   | 45/67 [00:39<00:19,  1.13it/s, loss=4.58, v_num=32, train_loss_step=1.330, val_loss=3.210, train_loss_epoch=3.650]\n",
      "Epoch 2:  69%|██████▊   | 46/67 [00:41<00:18,  1.11it/s, loss=4.58, v_num=32, train_loss_step=1.330, val_loss=3.210, train_loss_epoch=3.650]\n",
      "Epoch 2:  70%|███████   | 47/67 [00:42<00:18,  1.10it/s, loss=4.58, v_num=32, train_loss_step=1.330, val_loss=3.210, train_loss_epoch=3.650]\n",
      "Epoch 2:  72%|███████▏  | 48/67 [00:44<00:17,  1.08it/s, loss=4.58, v_num=32, train_loss_step=1.330, val_loss=3.210, train_loss_epoch=3.650]\n",
      "Epoch 2:  73%|███████▎  | 49/67 [00:45<00:16,  1.07it/s, loss=4.58, v_num=32, train_loss_step=1.330, val_loss=3.210, train_loss_epoch=3.650]\n",
      "Epoch 2:  75%|███████▍  | 50/67 [00:47<00:16,  1.05it/s, loss=4.58, v_num=32, train_loss_step=1.330, val_loss=3.210, train_loss_epoch=3.650]\n",
      "Epoch 2:  76%|███████▌  | 51/67 [00:49<00:15,  1.04it/s, loss=4.58, v_num=32, train_loss_step=1.330, val_loss=3.210, train_loss_epoch=3.650]\n",
      "Epoch 2:  78%|███████▊  | 52/67 [00:51<00:14,  1.02it/s, loss=4.58, v_num=32, train_loss_step=1.330, val_loss=3.210, train_loss_epoch=3.650]\n",
      "Epoch 2:  79%|███████▉  | 53/67 [00:52<00:13,  1.01it/s, loss=4.58, v_num=32, train_loss_step=1.330, val_loss=3.210, train_loss_epoch=3.650]\n",
      "Epoch 2:  81%|████████  | 54/67 [00:54<00:13,  1.00s/it, loss=4.58, v_num=32, train_loss_step=1.330, val_loss=3.210, train_loss_epoch=3.650]\n",
      "Epoch 2:  82%|████████▏ | 55/67 [00:55<00:12,  1.01s/it, loss=4.58, v_num=32, train_loss_step=1.330, val_loss=3.210, train_loss_epoch=3.650]\n",
      "Epoch 2:  84%|████████▎ | 56/67 [00:57<00:11,  1.02s/it, loss=4.58, v_num=32, train_loss_step=1.330, val_loss=3.210, train_loss_epoch=3.650]\n",
      "Epoch 2:  85%|████████▌ | 57/67 [00:58<00:10,  1.03s/it, loss=4.58, v_num=32, train_loss_step=1.330, val_loss=3.210, train_loss_epoch=3.650]\n",
      "Epoch 2:  87%|████████▋ | 58/67 [01:00<00:09,  1.04s/it, loss=4.58, v_num=32, train_loss_step=1.330, val_loss=3.210, train_loss_epoch=3.650]\n",
      "Epoch 2:  88%|████████▊ | 59/67 [01:01<00:08,  1.05s/it, loss=4.58, v_num=32, train_loss_step=1.330, val_loss=3.210, train_loss_epoch=3.650]\n",
      "Epoch 2:  90%|████████▉ | 60/67 [01:03<00:07,  1.06s/it, loss=4.58, v_num=32, train_loss_step=1.330, val_loss=3.210, train_loss_epoch=3.650]\n",
      "Epoch 2:  91%|█████████ | 61/67 [01:05<00:06,  1.07s/it, loss=4.58, v_num=32, train_loss_step=1.330, val_loss=3.210, train_loss_epoch=3.650]\n",
      "Epoch 2:  93%|█████████▎| 62/67 [01:06<00:05,  1.08s/it, loss=4.58, v_num=32, train_loss_step=1.330, val_loss=3.210, train_loss_epoch=3.650]\n",
      "Epoch 2:  94%|█████████▍| 63/67 [01:08<00:04,  1.09s/it, loss=4.58, v_num=32, train_loss_step=1.330, val_loss=3.210, train_loss_epoch=3.650]\n",
      "Epoch 2:  96%|█████████▌| 64/67 [01:09<00:03,  1.09s/it, loss=4.58, v_num=32, train_loss_step=1.330, val_loss=3.210, train_loss_epoch=3.650]\n",
      "Epoch 2:  97%|█████████▋| 65/67 [01:11<00:02,  1.10s/it, loss=4.58, v_num=32, train_loss_step=1.330, val_loss=3.210, train_loss_epoch=3.650]\n",
      "Epoch 2:  99%|█████████▊| 66/67 [01:12<00:01,  1.11s/it, loss=4.58, v_num=32, train_loss_step=1.330, val_loss=3.210, train_loss_epoch=3.650]\n",
      "Epoch 2: 100%|██████████| 67/67 [01:14<00:00,  1.12s/it, loss=4.58, v_num=32, train_loss_step=1.330, val_loss=3.210, train_loss_epoch=3.650]\n",
      "Epoch 3:  45%|████▍     | 30/67 [00:16<00:20,  1.83it/s, loss=2.66, v_num=32, train_loss_step=2.840, val_loss=3.210, train_loss_epoch=4.200]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  46%|████▋     | 31/67 [00:18<00:20,  1.72it/s, loss=2.66, v_num=32, train_loss_step=2.840, val_loss=3.210, train_loss_epoch=4.200]\n",
      "Epoch 3:  48%|████▊     | 32/67 [00:19<00:21,  1.63it/s, loss=2.66, v_num=32, train_loss_step=2.840, val_loss=3.210, train_loss_epoch=4.200]\n",
      "Epoch 3:  49%|████▉     | 33/67 [00:21<00:21,  1.56it/s, loss=2.66, v_num=32, train_loss_step=2.840, val_loss=3.210, train_loss_epoch=4.200]\n",
      "Epoch 3:  51%|█████     | 34/67 [00:22<00:22,  1.50it/s, loss=2.66, v_num=32, train_loss_step=2.840, val_loss=3.210, train_loss_epoch=4.200]\n",
      "Epoch 3:  52%|█████▏    | 35/67 [00:24<00:22,  1.44it/s, loss=2.66, v_num=32, train_loss_step=2.840, val_loss=3.210, train_loss_epoch=4.200]\n",
      "Epoch 3:  54%|█████▎    | 36/67 [00:25<00:22,  1.40it/s, loss=2.66, v_num=32, train_loss_step=2.840, val_loss=3.210, train_loss_epoch=4.200]\n",
      "Epoch 3:  55%|█████▌    | 37/67 [00:27<00:22,  1.35it/s, loss=2.66, v_num=32, train_loss_step=2.840, val_loss=3.210, train_loss_epoch=4.200]\n",
      "Epoch 3:  57%|█████▋    | 38/67 [00:29<00:22,  1.30it/s, loss=2.66, v_num=32, train_loss_step=2.840, val_loss=3.210, train_loss_epoch=4.200]\n",
      "Epoch 3:  58%|█████▊    | 39/67 [00:30<00:22,  1.27it/s, loss=2.66, v_num=32, train_loss_step=2.840, val_loss=3.210, train_loss_epoch=4.200]\n",
      "Epoch 3:  60%|█████▉    | 40/67 [00:32<00:21,  1.24it/s, loss=2.66, v_num=32, train_loss_step=2.840, val_loss=3.210, train_loss_epoch=4.200]\n",
      "Epoch 3:  61%|██████    | 41/67 [00:33<00:21,  1.21it/s, loss=2.66, v_num=32, train_loss_step=2.840, val_loss=3.210, train_loss_epoch=4.200]\n",
      "Epoch 3:  63%|██████▎   | 42/67 [00:35<00:21,  1.18it/s, loss=2.66, v_num=32, train_loss_step=2.840, val_loss=3.210, train_loss_epoch=4.200]\n",
      "Epoch 3:  64%|██████▍   | 43/67 [00:36<00:20,  1.16it/s, loss=2.66, v_num=32, train_loss_step=2.840, val_loss=3.210, train_loss_epoch=4.200]\n",
      "Epoch 3:  66%|██████▌   | 44/67 [00:38<00:20,  1.15it/s, loss=2.66, v_num=32, train_loss_step=2.840, val_loss=3.210, train_loss_epoch=4.200]\n",
      "Epoch 3:  67%|██████▋   | 45/67 [00:39<00:19,  1.13it/s, loss=2.66, v_num=32, train_loss_step=2.840, val_loss=3.210, train_loss_epoch=4.200]\n",
      "Epoch 3:  69%|██████▊   | 46/67 [00:41<00:18,  1.11it/s, loss=2.66, v_num=32, train_loss_step=2.840, val_loss=3.210, train_loss_epoch=4.200]\n",
      "Epoch 3:  70%|███████   | 47/67 [00:42<00:18,  1.10it/s, loss=2.66, v_num=32, train_loss_step=2.840, val_loss=3.210, train_loss_epoch=4.200]\n",
      "Epoch 3:  72%|███████▏  | 48/67 [00:44<00:17,  1.07it/s, loss=2.66, v_num=32, train_loss_step=2.840, val_loss=3.210, train_loss_epoch=4.200]\n",
      "Epoch 3:  73%|███████▎  | 49/67 [00:46<00:16,  1.06it/s, loss=2.66, v_num=32, train_loss_step=2.840, val_loss=3.210, train_loss_epoch=4.200]\n",
      "Epoch 3:  75%|███████▍  | 50/67 [00:47<00:16,  1.05it/s, loss=2.66, v_num=32, train_loss_step=2.840, val_loss=3.210, train_loss_epoch=4.200]\n",
      "Epoch 3:  76%|███████▌  | 51/67 [00:49<00:15,  1.03it/s, loss=2.66, v_num=32, train_loss_step=2.840, val_loss=3.210, train_loss_epoch=4.200]\n",
      "Epoch 3:  78%|███████▊  | 52/67 [00:50<00:14,  1.02it/s, loss=2.66, v_num=32, train_loss_step=2.840, val_loss=3.210, train_loss_epoch=4.200]\n",
      "Epoch 3:  79%|███████▉  | 53/67 [00:52<00:13,  1.01it/s, loss=2.66, v_num=32, train_loss_step=2.840, val_loss=3.210, train_loss_epoch=4.200]\n",
      "Epoch 3:  81%|████████  | 54/67 [00:53<00:12,  1.00it/s, loss=2.66, v_num=32, train_loss_step=2.840, val_loss=3.210, train_loss_epoch=4.200]\n",
      "Epoch 3:  82%|████████▏ | 55/67 [00:55<00:12,  1.01s/it, loss=2.66, v_num=32, train_loss_step=2.840, val_loss=3.210, train_loss_epoch=4.200]\n",
      "Epoch 3:  84%|████████▎ | 56/67 [00:57<00:11,  1.02s/it, loss=2.66, v_num=32, train_loss_step=2.840, val_loss=3.210, train_loss_epoch=4.200]\n",
      "Epoch 3:  85%|████████▌ | 57/67 [00:58<00:10,  1.03s/it, loss=2.66, v_num=32, train_loss_step=2.840, val_loss=3.210, train_loss_epoch=4.200]\n",
      "Epoch 3:  87%|████████▋ | 58/67 [01:00<00:09,  1.04s/it, loss=2.66, v_num=32, train_loss_step=2.840, val_loss=3.210, train_loss_epoch=4.200]\n",
      "Epoch 3:  88%|████████▊ | 59/67 [01:02<00:08,  1.05s/it, loss=2.66, v_num=32, train_loss_step=2.840, val_loss=3.210, train_loss_epoch=4.200]\n",
      "Epoch 3:  90%|████████▉ | 60/67 [01:03<00:07,  1.06s/it, loss=2.66, v_num=32, train_loss_step=2.840, val_loss=3.210, train_loss_epoch=4.200]\n",
      "Epoch 3:  91%|█████████ | 61/67 [01:05<00:06,  1.07s/it, loss=2.66, v_num=32, train_loss_step=2.840, val_loss=3.210, train_loss_epoch=4.200]\n",
      "Epoch 3:  93%|█████████▎| 62/67 [01:06<00:05,  1.08s/it, loss=2.66, v_num=32, train_loss_step=2.840, val_loss=3.210, train_loss_epoch=4.200]\n",
      "Epoch 3:  94%|█████████▍| 63/67 [01:08<00:04,  1.08s/it, loss=2.66, v_num=32, train_loss_step=2.840, val_loss=3.210, train_loss_epoch=4.200]\n",
      "Epoch 3:  96%|█████████▌| 64/67 [01:09<00:03,  1.09s/it, loss=2.66, v_num=32, train_loss_step=2.840, val_loss=3.210, train_loss_epoch=4.200]\n",
      "Epoch 3:  97%|█████████▋| 65/67 [01:11<00:02,  1.10s/it, loss=2.66, v_num=32, train_loss_step=2.840, val_loss=3.210, train_loss_epoch=4.200]\n",
      "Epoch 3:  99%|█████████▊| 66/67 [01:12<00:01,  1.10s/it, loss=2.66, v_num=32, train_loss_step=2.840, val_loss=3.210, train_loss_epoch=4.200]\n",
      "Epoch 3: 100%|██████████| 67/67 [01:15<00:00,  1.12s/it, loss=2.66, v_num=32, train_loss_step=2.840, val_loss=3.210, train_loss_epoch=4.200]\n",
      "Epoch 4:  45%|████▍     | 30/67 [00:16<00:19,  1.87it/s, loss=6.04, v_num=32, train_loss_step=3.600, val_loss=3.210, train_loss_epoch=2.910]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  46%|████▋     | 31/67 [00:17<00:20,  1.74it/s, loss=6.04, v_num=32, train_loss_step=3.600, val_loss=3.210, train_loss_epoch=2.910]\n",
      "Epoch 4:  48%|████▊     | 32/67 [00:19<00:21,  1.66it/s, loss=6.04, v_num=32, train_loss_step=3.600, val_loss=3.210, train_loss_epoch=2.910]\n",
      "Epoch 4:  49%|████▉     | 33/67 [00:20<00:21,  1.58it/s, loss=6.04, v_num=32, train_loss_step=3.600, val_loss=3.210, train_loss_epoch=2.910]\n",
      "Epoch 4:  51%|█████     | 34/67 [00:22<00:21,  1.52it/s, loss=6.04, v_num=32, train_loss_step=3.600, val_loss=3.210, train_loss_epoch=2.910]\n",
      "Epoch 4:  52%|█████▏    | 35/67 [00:23<00:21,  1.47it/s, loss=6.04, v_num=32, train_loss_step=3.600, val_loss=3.210, train_loss_epoch=2.910]\n",
      "Epoch 4:  54%|█████▎    | 36/67 [00:25<00:21,  1.42it/s, loss=6.04, v_num=32, train_loss_step=3.600, val_loss=3.210, train_loss_epoch=2.910]\n",
      "Epoch 4:  55%|█████▌    | 37/67 [00:27<00:22,  1.35it/s, loss=6.04, v_num=32, train_loss_step=3.600, val_loss=3.210, train_loss_epoch=2.910]\n",
      "Epoch 4:  57%|█████▋    | 38/67 [00:28<00:22,  1.32it/s, loss=6.04, v_num=32, train_loss_step=3.600, val_loss=3.210, train_loss_epoch=2.910]\n",
      "Epoch 4:  58%|█████▊    | 39/67 [00:30<00:21,  1.29it/s, loss=6.04, v_num=32, train_loss_step=3.600, val_loss=3.210, train_loss_epoch=2.910]\n",
      "Epoch 4:  60%|█████▉    | 40/67 [00:31<00:21,  1.26it/s, loss=6.04, v_num=32, train_loss_step=3.600, val_loss=3.210, train_loss_epoch=2.910]\n",
      "Epoch 4:  61%|██████    | 41/67 [00:33<00:21,  1.22it/s, loss=6.04, v_num=32, train_loss_step=3.600, val_loss=3.210, train_loss_epoch=2.910]\n",
      "Epoch 4:  63%|██████▎   | 42/67 [00:35<00:20,  1.20it/s, loss=6.04, v_num=32, train_loss_step=3.600, val_loss=3.210, train_loss_epoch=2.910]\n",
      "Epoch 4:  64%|██████▍   | 43/67 [00:36<00:20,  1.18it/s, loss=6.04, v_num=32, train_loss_step=3.600, val_loss=3.210, train_loss_epoch=2.910]\n",
      "Epoch 4:  66%|██████▌   | 44/67 [00:38<00:19,  1.16it/s, loss=6.04, v_num=32, train_loss_step=3.600, val_loss=3.210, train_loss_epoch=2.910]\n",
      "Epoch 4:  67%|██████▋   | 45/67 [00:39<00:19,  1.14it/s, loss=6.04, v_num=32, train_loss_step=3.600, val_loss=3.210, train_loss_epoch=2.910]\n",
      "Epoch 4:  69%|██████▊   | 46/67 [00:40<00:18,  1.12it/s, loss=6.04, v_num=32, train_loss_step=3.600, val_loss=3.210, train_loss_epoch=2.910]\n",
      "Epoch 4:  70%|███████   | 47/67 [00:42<00:18,  1.10it/s, loss=6.04, v_num=32, train_loss_step=3.600, val_loss=3.210, train_loss_epoch=2.910]\n",
      "Epoch 4:  72%|███████▏  | 48/67 [00:44<00:17,  1.08it/s, loss=6.04, v_num=32, train_loss_step=3.600, val_loss=3.210, train_loss_epoch=2.910]\n",
      "Epoch 4:  73%|███████▎  | 49/67 [00:45<00:16,  1.07it/s, loss=6.04, v_num=32, train_loss_step=3.600, val_loss=3.210, train_loss_epoch=2.910]\n",
      "Epoch 4:  75%|███████▍  | 50/67 [00:47<00:16,  1.06it/s, loss=6.04, v_num=32, train_loss_step=3.600, val_loss=3.210, train_loss_epoch=2.910]\n",
      "Epoch 4:  76%|███████▌  | 51/67 [00:49<00:15,  1.04it/s, loss=6.04, v_num=32, train_loss_step=3.600, val_loss=3.210, train_loss_epoch=2.910]\n",
      "Epoch 4:  78%|███████▊  | 52/67 [00:50<00:14,  1.03it/s, loss=6.04, v_num=32, train_loss_step=3.600, val_loss=3.210, train_loss_epoch=2.910]\n",
      "Epoch 4:  79%|███████▉  | 53/67 [00:52<00:13,  1.02it/s, loss=6.04, v_num=32, train_loss_step=3.600, val_loss=3.210, train_loss_epoch=2.910]\n",
      "Epoch 4:  81%|████████  | 54/67 [00:53<00:12,  1.01it/s, loss=6.04, v_num=32, train_loss_step=3.600, val_loss=3.210, train_loss_epoch=2.910]\n",
      "Epoch 4:  82%|████████▏ | 55/67 [00:55<00:12,  1.00s/it, loss=6.04, v_num=32, train_loss_step=3.600, val_loss=3.210, train_loss_epoch=2.910]\n",
      "Epoch 4:  84%|████████▎ | 56/67 [00:56<00:11,  1.01s/it, loss=6.04, v_num=32, train_loss_step=3.600, val_loss=3.210, train_loss_epoch=2.910]\n",
      "Epoch 4:  85%|████████▌ | 57/67 [00:58<00:10,  1.03s/it, loss=6.04, v_num=32, train_loss_step=3.600, val_loss=3.210, train_loss_epoch=2.910]\n",
      "Epoch 4:  87%|████████▋ | 58/67 [01:00<00:09,  1.04s/it, loss=6.04, v_num=32, train_loss_step=3.600, val_loss=3.210, train_loss_epoch=2.910]\n",
      "Epoch 4:  88%|████████▊ | 59/67 [01:01<00:08,  1.05s/it, loss=6.04, v_num=32, train_loss_step=3.600, val_loss=3.210, train_loss_epoch=2.910]\n",
      "Epoch 4:  90%|████████▉ | 60/67 [01:03<00:07,  1.06s/it, loss=6.04, v_num=32, train_loss_step=3.600, val_loss=3.210, train_loss_epoch=2.910]\n",
      "Epoch 4:  91%|█████████ | 61/67 [01:05<00:06,  1.07s/it, loss=6.04, v_num=32, train_loss_step=3.600, val_loss=3.210, train_loss_epoch=2.910]\n",
      "Epoch 4:  93%|█████████▎| 62/67 [01:06<00:05,  1.08s/it, loss=6.04, v_num=32, train_loss_step=3.600, val_loss=3.210, train_loss_epoch=2.910]\n",
      "Epoch 4:  94%|█████████▍| 63/67 [01:08<00:04,  1.08s/it, loss=6.04, v_num=32, train_loss_step=3.600, val_loss=3.210, train_loss_epoch=2.910]\n",
      "Epoch 4:  96%|█████████▌| 64/67 [01:09<00:03,  1.09s/it, loss=6.04, v_num=32, train_loss_step=3.600, val_loss=3.210, train_loss_epoch=2.910]\n",
      "Epoch 4:  97%|█████████▋| 65/67 [01:11<00:02,  1.10s/it, loss=6.04, v_num=32, train_loss_step=3.600, val_loss=3.210, train_loss_epoch=2.910]\n",
      "Epoch 4:  99%|█████████▊| 66/67 [01:12<00:01,  1.10s/it, loss=6.04, v_num=32, train_loss_step=3.600, val_loss=3.210, train_loss_epoch=2.910]\n",
      "Epoch 4: 100%|██████████| 67/67 [01:15<00:00,  1.12s/it, loss=6.04, v_num=32, train_loss_step=3.600, val_loss=3.210, train_loss_epoch=2.910]\n",
      "Epoch 5:  45%|████▍     | 30/67 [00:16<00:19,  1.87it/s, loss=3.56, v_num=32, train_loss_step=5.470, val_loss=3.210, train_loss_epoch=5.390]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5:  46%|████▋     | 31/67 [00:17<00:20,  1.75it/s, loss=3.56, v_num=32, train_loss_step=5.470, val_loss=3.210, train_loss_epoch=5.390]\n",
      "Epoch 5:  48%|████▊     | 32/67 [00:19<00:21,  1.66it/s, loss=3.56, v_num=32, train_loss_step=5.470, val_loss=3.210, train_loss_epoch=5.390]\n",
      "Epoch 5:  49%|████▉     | 33/67 [00:20<00:21,  1.59it/s, loss=3.56, v_num=32, train_loss_step=5.470, val_loss=3.210, train_loss_epoch=5.390]\n",
      "Epoch 5:  51%|█████     | 34/67 [00:22<00:21,  1.52it/s, loss=3.56, v_num=32, train_loss_step=5.470, val_loss=3.210, train_loss_epoch=5.390]\n",
      "Epoch 5:  52%|█████▏    | 35/67 [00:23<00:21,  1.47it/s, loss=3.56, v_num=32, train_loss_step=5.470, val_loss=3.210, train_loss_epoch=5.390]\n",
      "Epoch 5:  54%|█████▎    | 36/67 [00:25<00:21,  1.42it/s, loss=3.56, v_num=32, train_loss_step=5.470, val_loss=3.210, train_loss_epoch=5.390]\n",
      "Epoch 5:  55%|█████▌    | 37/67 [00:26<00:21,  1.37it/s, loss=3.56, v_num=32, train_loss_step=5.470, val_loss=3.210, train_loss_epoch=5.390]\n",
      "Epoch 5:  57%|█████▋    | 38/67 [00:28<00:22,  1.32it/s, loss=3.56, v_num=32, train_loss_step=5.470, val_loss=3.210, train_loss_epoch=5.390]\n",
      "Epoch 5:  58%|█████▊    | 39/67 [00:30<00:21,  1.28it/s, loss=3.56, v_num=32, train_loss_step=5.470, val_loss=3.210, train_loss_epoch=5.390]\n",
      "Epoch 5:  60%|█████▉    | 40/67 [00:31<00:21,  1.25it/s, loss=3.56, v_num=32, train_loss_step=5.470, val_loss=3.210, train_loss_epoch=5.390]\n",
      "Epoch 5:  61%|██████    | 41/67 [00:33<00:21,  1.22it/s, loss=3.56, v_num=32, train_loss_step=5.470, val_loss=3.210, train_loss_epoch=5.390]\n",
      "Epoch 5:  63%|██████▎   | 42/67 [00:35<00:20,  1.20it/s, loss=3.56, v_num=32, train_loss_step=5.470, val_loss=3.210, train_loss_epoch=5.390]\n",
      "Epoch 5:  64%|██████▍   | 43/67 [00:36<00:20,  1.18it/s, loss=3.56, v_num=32, train_loss_step=5.470, val_loss=3.210, train_loss_epoch=5.390]\n",
      "Epoch 5:  66%|██████▌   | 44/67 [00:38<00:19,  1.16it/s, loss=3.56, v_num=32, train_loss_step=5.470, val_loss=3.210, train_loss_epoch=5.390]\n",
      "Epoch 5:  67%|██████▋   | 45/67 [00:39<00:19,  1.14it/s, loss=3.56, v_num=32, train_loss_step=5.470, val_loss=3.210, train_loss_epoch=5.390]\n",
      "Epoch 5:  69%|██████▊   | 46/67 [00:41<00:18,  1.12it/s, loss=3.56, v_num=32, train_loss_step=5.470, val_loss=3.210, train_loss_epoch=5.390]\n",
      "Epoch 5:  70%|███████   | 47/67 [00:42<00:18,  1.10it/s, loss=3.56, v_num=32, train_loss_step=5.470, val_loss=3.210, train_loss_epoch=5.390]\n",
      "Epoch 5:  72%|███████▏  | 48/67 [00:44<00:17,  1.09it/s, loss=3.56, v_num=32, train_loss_step=5.470, val_loss=3.210, train_loss_epoch=5.390]\n",
      "Epoch 5:  73%|███████▎  | 49/67 [00:46<00:16,  1.07it/s, loss=3.56, v_num=32, train_loss_step=5.470, val_loss=3.210, train_loss_epoch=5.390]\n",
      "Epoch 5:  75%|███████▍  | 50/67 [00:47<00:16,  1.05it/s, loss=3.56, v_num=32, train_loss_step=5.470, val_loss=3.210, train_loss_epoch=5.390]\n",
      "Epoch 5:  76%|███████▌  | 51/67 [00:49<00:15,  1.04it/s, loss=3.56, v_num=32, train_loss_step=5.470, val_loss=3.210, train_loss_epoch=5.390]\n",
      "Epoch 5:  78%|███████▊  | 52/67 [00:50<00:14,  1.03it/s, loss=3.56, v_num=32, train_loss_step=5.470, val_loss=3.210, train_loss_epoch=5.390]\n",
      "Epoch 5:  79%|███████▉  | 53/67 [00:52<00:13,  1.02it/s, loss=3.56, v_num=32, train_loss_step=5.470, val_loss=3.210, train_loss_epoch=5.390]\n",
      "Epoch 5:  81%|████████  | 54/67 [00:53<00:12,  1.01it/s, loss=3.56, v_num=32, train_loss_step=5.470, val_loss=3.210, train_loss_epoch=5.390]\n",
      "Epoch 5:  82%|████████▏ | 55/67 [00:55<00:12,  1.00s/it, loss=3.56, v_num=32, train_loss_step=5.470, val_loss=3.210, train_loss_epoch=5.390]\n",
      "Epoch 5:  84%|████████▎ | 56/67 [00:56<00:11,  1.01s/it, loss=3.56, v_num=32, train_loss_step=5.470, val_loss=3.210, train_loss_epoch=5.390]\n",
      "Epoch 5:  85%|████████▌ | 57/67 [00:58<00:10,  1.02s/it, loss=3.56, v_num=32, train_loss_step=5.470, val_loss=3.210, train_loss_epoch=5.390]\n",
      "Epoch 5:  87%|████████▋ | 58/67 [00:59<00:09,  1.03s/it, loss=3.56, v_num=32, train_loss_step=5.470, val_loss=3.210, train_loss_epoch=5.390]\n",
      "Epoch 5:  88%|████████▊ | 59/67 [01:01<00:08,  1.04s/it, loss=3.56, v_num=32, train_loss_step=5.470, val_loss=3.210, train_loss_epoch=5.390]\n",
      "Epoch 5:  90%|████████▉ | 60/67 [01:03<00:07,  1.05s/it, loss=3.56, v_num=32, train_loss_step=5.470, val_loss=3.210, train_loss_epoch=5.390]\n",
      "Epoch 5:  91%|█████████ | 61/67 [01:04<00:06,  1.06s/it, loss=3.56, v_num=32, train_loss_step=5.470, val_loss=3.210, train_loss_epoch=5.390]\n",
      "Epoch 5:  93%|█████████▎| 62/67 [01:06<00:05,  1.07s/it, loss=3.56, v_num=32, train_loss_step=5.470, val_loss=3.210, train_loss_epoch=5.390]\n",
      "Epoch 5:  94%|█████████▍| 63/67 [01:07<00:04,  1.08s/it, loss=3.56, v_num=32, train_loss_step=5.470, val_loss=3.210, train_loss_epoch=5.390]\n",
      "Epoch 5:  96%|█████████▌| 64/67 [01:09<00:03,  1.08s/it, loss=3.56, v_num=32, train_loss_step=5.470, val_loss=3.210, train_loss_epoch=5.390]\n",
      "Epoch 5:  97%|█████████▋| 65/67 [01:10<00:02,  1.09s/it, loss=3.56, v_num=32, train_loss_step=5.470, val_loss=3.210, train_loss_epoch=5.390]\n",
      "Epoch 5:  99%|█████████▊| 66/67 [01:12<00:01,  1.10s/it, loss=3.56, v_num=32, train_loss_step=5.470, val_loss=3.210, train_loss_epoch=5.390]\n",
      "Epoch 5: 100%|██████████| 67/67 [01:14<00:00,  1.11s/it, loss=3.56, v_num=32, train_loss_step=5.470, val_loss=3.210, train_loss_epoch=5.390]\n",
      "Epoch 6:  45%|████▍     | 30/67 [00:15<00:19,  1.88it/s, loss=4.34, v_num=32, train_loss_step=1.450, val_loss=3.210, train_loss_epoch=4.410]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6:  46%|████▋     | 31/67 [00:17<00:20,  1.75it/s, loss=4.34, v_num=32, train_loss_step=1.450, val_loss=3.210, train_loss_epoch=4.410]\n",
      "Epoch 6:  48%|████▊     | 32/67 [00:19<00:21,  1.62it/s, loss=4.34, v_num=32, train_loss_step=1.450, val_loss=3.210, train_loss_epoch=4.410]\n",
      "Epoch 6:  49%|████▉     | 33/67 [00:21<00:21,  1.56it/s, loss=4.34, v_num=32, train_loss_step=1.450, val_loss=3.210, train_loss_epoch=4.410]\n",
      "Epoch 6:  51%|█████     | 34/67 [00:22<00:22,  1.49it/s, loss=4.34, v_num=32, train_loss_step=1.450, val_loss=3.210, train_loss_epoch=4.410]\n",
      "Epoch 6:  52%|█████▏    | 35/67 [00:24<00:22,  1.44it/s, loss=4.34, v_num=32, train_loss_step=1.450, val_loss=3.210, train_loss_epoch=4.410]\n",
      "Epoch 6:  54%|█████▎    | 36/67 [00:25<00:22,  1.39it/s, loss=4.34, v_num=32, train_loss_step=1.450, val_loss=3.210, train_loss_epoch=4.410]\n",
      "Epoch 6:  55%|█████▌    | 37/67 [00:27<00:22,  1.35it/s, loss=4.34, v_num=32, train_loss_step=1.450, val_loss=3.210, train_loss_epoch=4.410]\n",
      "Epoch 6:  57%|█████▋    | 38/67 [00:29<00:22,  1.31it/s, loss=4.34, v_num=32, train_loss_step=1.450, val_loss=3.210, train_loss_epoch=4.410]\n",
      "Epoch 6:  58%|█████▊    | 39/67 [00:30<00:21,  1.28it/s, loss=4.34, v_num=32, train_loss_step=1.450, val_loss=3.210, train_loss_epoch=4.410]\n",
      "Epoch 6:  60%|█████▉    | 40/67 [00:32<00:21,  1.25it/s, loss=4.34, v_num=32, train_loss_step=1.450, val_loss=3.210, train_loss_epoch=4.410]\n",
      "Epoch 6:  61%|██████    | 41/67 [00:33<00:21,  1.21it/s, loss=4.34, v_num=32, train_loss_step=1.450, val_loss=3.210, train_loss_epoch=4.410]\n",
      "Epoch 6:  63%|██████▎   | 42/67 [00:35<00:21,  1.19it/s, loss=4.34, v_num=32, train_loss_step=1.450, val_loss=3.210, train_loss_epoch=4.410]\n",
      "Epoch 6:  64%|██████▍   | 43/67 [00:37<00:20,  1.15it/s, loss=4.34, v_num=32, train_loss_step=1.450, val_loss=3.210, train_loss_epoch=4.410]\n",
      "Epoch 6:  66%|██████▌   | 44/67 [00:38<00:20,  1.13it/s, loss=4.34, v_num=32, train_loss_step=1.450, val_loss=3.210, train_loss_epoch=4.410]\n",
      "Epoch 6:  67%|██████▋   | 45/67 [00:40<00:19,  1.12it/s, loss=4.34, v_num=32, train_loss_step=1.450, val_loss=3.210, train_loss_epoch=4.410]\n",
      "Epoch 6:  69%|██████▊   | 46/67 [00:41<00:19,  1.10it/s, loss=4.34, v_num=32, train_loss_step=1.450, val_loss=3.210, train_loss_epoch=4.410]\n",
      "Epoch 6:  70%|███████   | 47/67 [00:43<00:18,  1.09it/s, loss=4.34, v_num=32, train_loss_step=1.450, val_loss=3.210, train_loss_epoch=4.410]\n",
      "Epoch 6:  72%|███████▏  | 48/67 [00:44<00:17,  1.07it/s, loss=4.34, v_num=32, train_loss_step=1.450, val_loss=3.210, train_loss_epoch=4.410]\n",
      "Epoch 6:  73%|███████▎  | 49/67 [00:46<00:16,  1.06it/s, loss=4.34, v_num=32, train_loss_step=1.450, val_loss=3.210, train_loss_epoch=4.410]\n",
      "Epoch 6:  75%|███████▍  | 50/67 [00:47<00:16,  1.05it/s, loss=4.34, v_num=32, train_loss_step=1.450, val_loss=3.210, train_loss_epoch=4.410]\n",
      "Epoch 6:  76%|███████▌  | 51/67 [00:49<00:15,  1.03it/s, loss=4.34, v_num=32, train_loss_step=1.450, val_loss=3.210, train_loss_epoch=4.410]\n",
      "Epoch 6:  78%|███████▊  | 52/67 [00:50<00:14,  1.02it/s, loss=4.34, v_num=32, train_loss_step=1.450, val_loss=3.210, train_loss_epoch=4.410]\n",
      "Epoch 6:  79%|███████▉  | 53/67 [00:52<00:13,  1.01it/s, loss=4.34, v_num=32, train_loss_step=1.450, val_loss=3.210, train_loss_epoch=4.410]\n",
      "Epoch 6:  81%|████████  | 54/67 [00:53<00:12,  1.00it/s, loss=4.34, v_num=32, train_loss_step=1.450, val_loss=3.210, train_loss_epoch=4.410]\n",
      "Epoch 6:  82%|████████▏ | 55/67 [00:55<00:12,  1.01s/it, loss=4.34, v_num=32, train_loss_step=1.450, val_loss=3.210, train_loss_epoch=4.410]\n",
      "Epoch 6:  84%|████████▎ | 56/67 [00:57<00:11,  1.02s/it, loss=4.34, v_num=32, train_loss_step=1.450, val_loss=3.210, train_loss_epoch=4.410]\n",
      "Epoch 6:  85%|████████▌ | 57/67 [00:58<00:10,  1.03s/it, loss=4.34, v_num=32, train_loss_step=1.450, val_loss=3.210, train_loss_epoch=4.410]\n",
      "Epoch 6:  87%|████████▋ | 58/67 [01:00<00:09,  1.04s/it, loss=4.34, v_num=32, train_loss_step=1.450, val_loss=3.210, train_loss_epoch=4.410]\n",
      "Epoch 6:  88%|████████▊ | 59/67 [01:01<00:08,  1.05s/it, loss=4.34, v_num=32, train_loss_step=1.450, val_loss=3.210, train_loss_epoch=4.410]\n",
      "Epoch 6:  90%|████████▉ | 60/67 [01:03<00:07,  1.06s/it, loss=4.34, v_num=32, train_loss_step=1.450, val_loss=3.210, train_loss_epoch=4.410]\n",
      "Epoch 6:  91%|█████████ | 61/67 [01:05<00:06,  1.07s/it, loss=4.34, v_num=32, train_loss_step=1.450, val_loss=3.210, train_loss_epoch=4.410]\n",
      "Epoch 6:  93%|█████████▎| 62/67 [01:06<00:05,  1.07s/it, loss=4.34, v_num=32, train_loss_step=1.450, val_loss=3.210, train_loss_epoch=4.410]\n",
      "Epoch 6:  94%|█████████▍| 63/67 [01:08<00:04,  1.08s/it, loss=4.34, v_num=32, train_loss_step=1.450, val_loss=3.210, train_loss_epoch=4.410]\n",
      "Epoch 6:  96%|█████████▌| 64/67 [01:09<00:03,  1.09s/it, loss=4.34, v_num=32, train_loss_step=1.450, val_loss=3.210, train_loss_epoch=4.410]\n",
      "Epoch 6:  97%|█████████▋| 65/67 [01:11<00:02,  1.09s/it, loss=4.34, v_num=32, train_loss_step=1.450, val_loss=3.210, train_loss_epoch=4.410]\n",
      "Epoch 6:  99%|█████████▊| 66/67 [01:12<00:01,  1.10s/it, loss=4.34, v_num=32, train_loss_step=1.450, val_loss=3.210, train_loss_epoch=4.410]\n",
      "Epoch 6: 100%|██████████| 67/67 [01:14<00:00,  1.12s/it, loss=4.34, v_num=32, train_loss_step=1.450, val_loss=3.210, train_loss_epoch=4.410]\n",
      "Epoch 7:  45%|████▍     | 30/67 [00:15<00:19,  1.90it/s, loss=4.88, v_num=32, train_loss_step=1.470, val_loss=3.210, train_loss_epoch=3.790]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7:  46%|████▋     | 31/67 [00:17<00:20,  1.77it/s, loss=4.88, v_num=32, train_loss_step=1.470, val_loss=3.210, train_loss_epoch=3.790]\n",
      "Epoch 7:  48%|████▊     | 32/67 [00:19<00:20,  1.68it/s, loss=4.88, v_num=32, train_loss_step=1.470, val_loss=3.210, train_loss_epoch=3.790]\n",
      "Epoch 7:  49%|████▉     | 33/67 [00:20<00:21,  1.60it/s, loss=4.88, v_num=32, train_loss_step=1.470, val_loss=3.210, train_loss_epoch=3.790]\n",
      "Epoch 7:  51%|█████     | 34/67 [00:22<00:21,  1.54it/s, loss=4.88, v_num=32, train_loss_step=1.470, val_loss=3.210, train_loss_epoch=3.790]\n",
      "Epoch 7:  52%|█████▏    | 35/67 [00:23<00:21,  1.48it/s, loss=4.88, v_num=32, train_loss_step=1.470, val_loss=3.210, train_loss_epoch=3.790]\n",
      "Epoch 7:  54%|█████▎    | 36/67 [00:25<00:21,  1.43it/s, loss=4.88, v_num=32, train_loss_step=1.470, val_loss=3.210, train_loss_epoch=3.790]\n",
      "Epoch 7:  55%|█████▌    | 37/67 [00:26<00:21,  1.39it/s, loss=4.88, v_num=32, train_loss_step=1.470, val_loss=3.210, train_loss_epoch=3.790]\n",
      "Epoch 7:  57%|█████▋    | 38/67 [00:28<00:21,  1.34it/s, loss=4.88, v_num=32, train_loss_step=1.470, val_loss=3.210, train_loss_epoch=3.790]\n",
      "Epoch 7:  58%|█████▊    | 39/67 [00:30<00:21,  1.29it/s, loss=4.88, v_num=32, train_loss_step=1.470, val_loss=3.210, train_loss_epoch=3.790]\n",
      "Epoch 7:  60%|█████▉    | 40/67 [00:31<00:21,  1.26it/s, loss=4.88, v_num=32, train_loss_step=1.470, val_loss=3.210, train_loss_epoch=3.790]\n",
      "Epoch 7:  61%|██████    | 41/67 [00:33<00:21,  1.22it/s, loss=4.88, v_num=32, train_loss_step=1.470, val_loss=3.210, train_loss_epoch=3.790]\n",
      "Epoch 7:  63%|██████▎   | 42/67 [00:34<00:20,  1.20it/s, loss=4.88, v_num=32, train_loss_step=1.470, val_loss=3.210, train_loss_epoch=3.790]\n",
      "Epoch 7:  64%|██████▍   | 43/67 [00:36<00:20,  1.18it/s, loss=4.88, v_num=32, train_loss_step=1.470, val_loss=3.210, train_loss_epoch=3.790]\n",
      "Epoch 7:  66%|██████▌   | 44/67 [00:37<00:19,  1.16it/s, loss=4.88, v_num=32, train_loss_step=1.470, val_loss=3.210, train_loss_epoch=3.790]\n",
      "Epoch 7:  67%|██████▋   | 45/67 [00:39<00:19,  1.14it/s, loss=4.88, v_num=32, train_loss_step=1.470, val_loss=3.210, train_loss_epoch=3.790]\n",
      "Epoch 7:  69%|██████▊   | 46/67 [00:40<00:18,  1.12it/s, loss=4.88, v_num=32, train_loss_step=1.470, val_loss=3.210, train_loss_epoch=3.790]\n",
      "Epoch 7:  70%|███████   | 47/67 [00:42<00:18,  1.11it/s, loss=4.88, v_num=32, train_loss_step=1.470, val_loss=3.210, train_loss_epoch=3.790]\n",
      "Epoch 7:  72%|███████▏  | 48/67 [00:44<00:17,  1.09it/s, loss=4.88, v_num=32, train_loss_step=1.470, val_loss=3.210, train_loss_epoch=3.790]\n",
      "Epoch 7:  73%|███████▎  | 49/67 [00:45<00:16,  1.07it/s, loss=4.88, v_num=32, train_loss_step=1.470, val_loss=3.210, train_loss_epoch=3.790]\n",
      "Epoch 7:  75%|███████▍  | 50/67 [00:47<00:16,  1.06it/s, loss=4.88, v_num=32, train_loss_step=1.470, val_loss=3.210, train_loss_epoch=3.790]\n",
      "Epoch 7:  76%|███████▌  | 51/67 [00:49<00:15,  1.04it/s, loss=4.88, v_num=32, train_loss_step=1.470, val_loss=3.210, train_loss_epoch=3.790]\n",
      "Epoch 7:  78%|███████▊  | 52/67 [00:50<00:14,  1.03it/s, loss=4.88, v_num=32, train_loss_step=1.470, val_loss=3.210, train_loss_epoch=3.790]\n",
      "Epoch 7:  79%|███████▉  | 53/67 [00:52<00:13,  1.01it/s, loss=4.88, v_num=32, train_loss_step=1.470, val_loss=3.210, train_loss_epoch=3.790]\n",
      "Epoch 7:  81%|████████  | 54/67 [00:53<00:12,  1.00it/s, loss=4.88, v_num=32, train_loss_step=1.470, val_loss=3.210, train_loss_epoch=3.790]\n",
      "Epoch 7:  82%|████████▏ | 55/67 [00:55<00:12,  1.01s/it, loss=4.88, v_num=32, train_loss_step=1.470, val_loss=3.210, train_loss_epoch=3.790]\n",
      "Epoch 7:  84%|████████▎ | 56/67 [00:56<00:11,  1.02s/it, loss=4.88, v_num=32, train_loss_step=1.470, val_loss=3.210, train_loss_epoch=3.790]\n",
      "Epoch 7:  85%|████████▌ | 57/67 [00:58<00:10,  1.02s/it, loss=4.88, v_num=32, train_loss_step=1.470, val_loss=3.210, train_loss_epoch=3.790]\n",
      "Epoch 7:  87%|████████▋ | 58/67 [00:59<00:09,  1.03s/it, loss=4.88, v_num=32, train_loss_step=1.470, val_loss=3.210, train_loss_epoch=3.790]\n",
      "Epoch 7:  88%|████████▊ | 59/67 [01:01<00:08,  1.04s/it, loss=4.88, v_num=32, train_loss_step=1.470, val_loss=3.210, train_loss_epoch=3.790]\n",
      "Epoch 7:  90%|████████▉ | 60/67 [01:02<00:07,  1.05s/it, loss=4.88, v_num=32, train_loss_step=1.470, val_loss=3.210, train_loss_epoch=3.790]\n",
      "Epoch 7:  91%|█████████ | 61/67 [01:04<00:06,  1.06s/it, loss=4.88, v_num=32, train_loss_step=1.470, val_loss=3.210, train_loss_epoch=3.790]\n",
      "Epoch 7:  93%|█████████▎| 62/67 [01:06<00:05,  1.07s/it, loss=4.88, v_num=32, train_loss_step=1.470, val_loss=3.210, train_loss_epoch=3.790]\n",
      "Epoch 7:  94%|█████████▍| 63/67 [01:08<00:04,  1.08s/it, loss=4.88, v_num=32, train_loss_step=1.470, val_loss=3.210, train_loss_epoch=3.790]\n",
      "Epoch 7:  96%|█████████▌| 64/67 [01:09<00:03,  1.09s/it, loss=4.88, v_num=32, train_loss_step=1.470, val_loss=3.210, train_loss_epoch=3.790]\n",
      "Epoch 7:  97%|█████████▋| 65/67 [01:11<00:02,  1.09s/it, loss=4.88, v_num=32, train_loss_step=1.470, val_loss=3.210, train_loss_epoch=3.790]\n",
      "Epoch 7:  99%|█████████▊| 66/67 [01:12<00:01,  1.10s/it, loss=4.88, v_num=32, train_loss_step=1.470, val_loss=3.210, train_loss_epoch=3.790]\n",
      "Epoch 7: 100%|██████████| 67/67 [01:13<00:00,  1.10s/it, loss=4.88, v_num=32, train_loss_step=1.470, val_loss=3.210, train_loss_epoch=3.790]"
     ]
    }
   ],
   "source": [
    "# fit network\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")\n",
    "#TweedieLoss p=1.9  Epoch 0: 100%|██████████| 49/49 [01:24<00:00,  1.72s/it, loss=1.11e+04, v_num=19, train_loss_step=10.70, val_loss=6.19e+8, train_loss_epoch=8.9e+12]\n",
    "#TweedieLoss p=1.01 Epoch 0: 100%|██████████| 49/49 [01:25<00:00,  1.74s/it, loss=1.66e+04, v_num=20, train_loss_step=110.0, val_loss=2.37e+19, train_loss_epoch=1.05e+21]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model according to the validation loss\n",
    "# (given that we use early stopping, this is not necessarily the last epoch)\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "print(best_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# raw predictions are a dictionary from which all kind of information including quantiles can be extracted\n",
    "raw_predictions, x = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)\n",
    "print('hola')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx in range(5):  # plot 10 examples\n",
    "    best_tft.plot_prediction(x, raw_predictions, idx=idx, add_loss_to_title=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions, x = best_tft.predict(val_dataloader, return_x=True)\n",
    "predictions_vs_actuals = best_tft.calculate_prediction_actual_by_variable(x, predictions)\n",
    "best_tft.plot_prediction_actual_by_variable(predictions_vs_actuals);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcualte root mean squared error on validation set\n",
    "actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
    "val_predictions = best_tft.predict(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "rmse_val = torch.sqrt(criterion(actuals,val_predictions)).item()\n",
    "print('rmse_val = ',rmse_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    print(actuals[i],val_predictions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('max_prediction_length:',max_prediction_length)\n",
    "print('max_encoder_length   :',max_encoder_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select last 30 days from data (max_encoder_length is 24)\n",
    "encoder_data = df_train[lambda x: x.date_block_num > x.date_block_num.max() - max_encoder_length]\n",
    "\n",
    "print(encoder_data['date_block_num'].min(),encoder_data['date_block_num'].max())\n",
    "#print(encoder_data['DATE'].min(),encoder_data['DATE'].max())\n",
    "encoder_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_prediction_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "last_data = df_train[df_train['date_block_num'].isin([idx  -  max_prediction_length for idx in df_test['date_block_num'].unique()])]\n",
    "last_data['date_block_num'] = last_data['date_block_num'] + max_prediction_length\n",
    "\n",
    "decoder_data = pd.merge(df_test[[col for col in df_test.columns if 'Demanda' not in col]], \n",
    "        last_data[['date_block_num','Z_MODELO','Z_PUNTO_VENTA','Z_GAMA',\"Demanda\"]+statistics_columns],\n",
    "        on = ['date_block_num', 'Z_MODELO','Z_PUNTO_VENTA','Z_GAMA',],\n",
    "                        how='left'\n",
    "        )\n",
    "\n",
    "\n",
    "encoder_data.replace([np.inf, -np.inf, np.nan],0,inplace=True)\n",
    "decoder_data.replace([np.inf, -np.inf, np.nan],0,inplace=True)\n",
    "\n",
    "\n",
    "# combine encoder and decoder data\n",
    "new_prediction_data = pd.concat([encoder_data, decoder_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_data[['date_block_num','Z_MODELO','Z_PUNTO_VENTA','Z_GAMA',\"Demanda\"]+statistics_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "aux = encoder_data['Demanda'].values\n",
    "\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "plt.plot(aux,'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "aux = decoder_data['Demanda'].values\n",
    "\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "plt.plot(aux,'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prediction_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_raw_predictions, new_x = best_tft.predict(new_prediction_data, mode=\"raw\", return_x=True)\n",
    "\n",
    "for idx in range(10):  # plot 10 examples\n",
    "    best_tft.plot_prediction(new_x, new_raw_predictions, idx=idx, show_future_observed=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "interpretation = best_tft.interpret_output(new_raw_predictions, reduction=\"sum\")\n",
    "best_tft.plot_interpretation(interpretation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_raw_predictions = best_tft.predict(new_prediction_data, mode=\"prediction\", return_x=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(new_raw_predictions.numpy()).T\n",
    "predictions['date_block_num'] = sorted(df_test['date_block_num'].unique())\n",
    "predictions = pd.melt(predictions, id_vars=['date_block_num'])\n",
    "predictions = predictions.sort_values(['date_block_num', 'variable']).reset_index(drop=True)\n",
    "df_test[['date_block_num','Z_MODELO','Z_PUNTO_VENTA','Z_GAMA']].sort_values(['date_block_num', 'Z_MODELO','Z_PUNTO_VENTA','Z_GAMA']).reset_index(drop=True)\n",
    "df_test2 = df_test.join(predictions['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "reverse_mapping_file = '../../utils/reverse_dict_mapping_list.txt'\n",
    "\n",
    "with open(reverse_mapping_file, 'rb') as f:\n",
    "    reverse_mapping = pickle.load( f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reverse_mapping#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#descriptive_columns = ['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA']\n",
    "descriptive_columns = ['Z_MARCA', 'Z_GAMA', 'Z_MODELO',\n",
    "                       'Z_DEPARTAMENTO', 'Z_PUNTO_VENTA']\n",
    "i=0\n",
    "for column in descriptive_columns:\n",
    "    if column in df_test2.columns:\n",
    "        df_test2[column] = df_test2[column].map(reverse_mapping[i])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inv_dict_dates = {v: k for k, v in dict_dates.items()}\n",
    "df_test2['Z_WEEK'] = df_test2['date_block_num'].map(inv_dict_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test2['ID'] = df_test2['Z_MODELO'] + '|' + df_test2['Z_PUNTO_VENTA'] + '|' + df_test2['Z_GAMA'] + '|' + df_test2['Z_WEEK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test2['Demanda'] = np.maximum(df_test2['value'],0)\n",
    "submission = df_test2[['Z_WEEK','ID','Demanda']]#.groupby('ID').sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.boxplot(['Demanda'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "aux = df_train['Demanda'].values\n",
    "\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "plt.plot(aux,'go')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "prediction = submission['Demanda'].values\n",
    "df_baseline = pd.read_csv('../../results/Submission_37.csv')\n",
    "real = df_baseline['Demanda']\n",
    "\n",
    "fig = plt.figure(figsize=(30,5))\n",
    "\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(prediction,'bo')\n",
    "plt.title('prediction')\n",
    "\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(real,'go')\n",
    "plt.title('real')\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(prediction,'bo',alpha=0.7,label='prediction')\n",
    "plt.plot(real,'go',alpha=0.1,label='real')\n",
    "plt.title('real')\n",
    "\n",
    "plt.suptitle('analysis de predicciones')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_actual = df_baseline['Demanda']\n",
    "y_predicted = submission['Demanda'].values\n",
    "\n",
    "rms = mean_squared_error(y_actual, y_predicted, squared=False)\n",
    "rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rmse_val = 3\n",
    "submission['Demanda_real'] = df_baseline['Demanda']\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "tendencia_semanal = submission[['Z_WEEK','Demanda','Demanda_real']].groupby(['Z_WEEK']).sum().reset_index()\n",
    "\n",
    "graph = pd.melt(tendencia_semanal,id_vars=['Z_WEEK'],value_vars=['Demanda','Demanda_real'],)\n",
    "\n",
    "#fig = px.line(graph,x='date_block_num',y='value',color='variable')\n",
    "#fig.show()\n",
    "import seaborn as sns\n",
    "\n",
    "sns.catplot(x=\"Z_WEEK\", y=\"value\", hue=\"variable\", kind=\"point\", data=graph, height=4.27, aspect=19.7/8.27)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submission['Demanda'] = 0.9#submission['value']\n",
    "import numpy as np\n",
    "\n",
    "submission[['ID', 'Demanda']].to_csv('../../results/Submission_tft_rmse_epoch100_val_'+str(np.round(rmse_val,2))+'_refencia_best_pred_rmse_'+str(np.round(rms,2))+'.csv', index = False, sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "63e0b83efdf8191a659559277734df523ac963ca42a147be12244d5a52bc1968"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
