{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from time import sleep\n",
    "\n",
    "def get_distance_from_paydays(date):\n",
    "    end_of_month = date.daysinmonth\n",
    "    distance_to_1st = 0 if date.day >=15 else 15 - date.day\n",
    "    distance_to15th = 0 if date.day < 15 else end_of_month - date.day\n",
    "    return distance_to_1st + distance_to15th\n",
    "\n",
    "def std(x): return np.std(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../../dataset/train/train_converted.csv')\n",
    "df_test  = pd.read_csv('../../dataset/test/test_converted.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA','Z_WEEK','Z_WEEK_DATE','Demanda']].groupby(['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA','Z_WEEK','Z_WEEK_DATE']).sum().reset_index()\n",
    "df_test = df_test[['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA','Z_WEEK','Z_WEEK_DATE','Demanda']].groupby(['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA','Z_WEEK','Z_WEEK_DATE']).sum().reset_index()\n",
    "df_train.replace(['',np.inf, -np.inf, np.nan],0,inplace=True)\n",
    "df_test.replace(['',np.inf, -np.inf, np.nan],0,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print('Creating date_block_num ...')\n",
    "N_submission = df_test.shape[0]\n",
    "N_sales      = df_train.shape[0]\n",
    "\n",
    "print(df_train.shape,df_test.shape)\n",
    "df_auxiliar = pd.concat([df_train,df_test])\n",
    "df_auxiliar.replace([np.inf, -np.inf, np.nan],0,inplace=True)\n",
    "\n",
    "\n",
    "dates = df_auxiliar['Z_WEEK'].unique()\n",
    "\n",
    "date = df_auxiliar['Z_WEEK'].min()\n",
    "maxi = df_auxiliar['Z_WEEK'].max()\n",
    "\n",
    "\n",
    "dict_dates = {}\n",
    "for idx,date in enumerate(dates):\n",
    "    dict_dates[date] =idx\n",
    "    \n",
    "    \n",
    "df_auxiliar['date_block_num'] = df_auxiliar['Z_WEEK'].replace(dict_dates)\n",
    "\n",
    "df_train, df_test = df_auxiliar[:N_sales], df_auxiliar[N_sales:]\n",
    "print(df_train.shape,df_test.shape)\n",
    "\n",
    "df_train.replace(['',np.inf, -np.inf, np.nan],0,inplace=True)\n",
    "df_test.replace(['',np.inf, -np.inf, np.nan],0,inplace=True)\n",
    "\n",
    "print('Creating date_block_num completed!')\n",
    "\n",
    "\n",
    "print('Preprocessing TRAINING DATASET ...')\n",
    "\n",
    "\n",
    "df_train['Z_WEEK_DATE'] = pd.to_datetime(df_train['Z_WEEK_DATE'])\n",
    "df_train['days_from_payday'] = df_train['Z_WEEK_DATE'].apply(get_distance_from_paydays)\n",
    "\n",
    "\n",
    "statistics_columns = [ ]\n",
    "\n",
    "bar1 = tqdm([\n",
    "    ['Z_MODELO'],\n",
    "    ['Z_PUNTO_VENTA'],\n",
    "    ['Z_GAMA'],\n",
    "    ['Z_MODELO','Z_PUNTO_VENTA'],\n",
    "    ['Z_MODELO','Z_GAMA'],\n",
    "    ['Z_PUNTO_VENTA','Z_GAMA'],\n",
    "    ['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA']], position=0, desc=\"i\",colour='green', ncols=80)\n",
    "time.sleep(1)\n",
    "\n",
    "bar2 = tqdm(['mean','std','max','min'], position=1, desc=\"j\", colour='red', ncols=80, leave=False)\n",
    "time.sleep(1)\n",
    "\n",
    "unique_columns = [ ]\n",
    "        \n",
    "for column_names in bar1:\n",
    "    bar1.update()\n",
    "    bar2.refresh()  #force print final state\n",
    "    bar2.reset()  #reuse bar\n",
    "    time.sleep(0.1)\n",
    "    for statistic in bar2:\n",
    "        \n",
    "        new_column_name = statistic+'_sales_by_'+'_'.join(column_names)\n",
    "        #df_train[new_column_name] = df_train.groupby([\"Z_WEEK_DATE\"]+column_names, observed=True).Demanda.transform(statistic)\n",
    "        if statistic == 'mean':\n",
    "            df_agg = df_train.groupby([\"Z_WEEK\"]+column_names, observed=True).Demanda.mean()\n",
    "        if statistic == 'std':\n",
    "            df_agg = df_train.groupby([\"Z_WEEK\"]+column_names, observed=True).Demanda.std(ddof=0)\n",
    "        if statistic == 'max':\n",
    "            df_agg = df_train.groupby([\"Z_WEEK\"]+column_names, observed=True).Demanda.max()\n",
    "        if statistic == 'min':\n",
    "            df_agg = df_train.groupby([\"Z_WEEK\"]+column_names, observed=True).Demanda.min()\n",
    "        if df_agg.shape[0] >= df_train.shape[0]*0.7:\n",
    "            unique_columns.append([[\"Z_WEEK\"]+column_names,new_column_name])\n",
    "            continue\n",
    "        \n",
    "        df_agg = df_agg.reset_index()\n",
    "        df_agg.columns = df_agg.columns.str.replace('Demanda', new_column_name)\n",
    "        \n",
    "        df_train = df_train.merge(df_agg,on=[\"Z_WEEK\"]+column_names,how='left')\n",
    "        statistics_columns.append(new_column_name)\n",
    "        bar2.update()\n",
    "        time.sleep(0.05)\n",
    "    \n",
    "df_train['dayofweek'] = df_train['Z_WEEK_DATE'].dt.dayofweek.astype('str').astype('category')\n",
    "df_train['month'] = df_train['Z_WEEK_DATE'].dt.month.astype('str').astype('category')\n",
    "df_train['dayofyear'] = df_train['Z_WEEK_DATE'].dt.dayofyear.astype('str').astype('category')\n",
    "\n",
    "df_train.drop(columns=['Z_WEEK_DATE'],inplace=True)\n",
    "df_train.drop(columns=['Z_WEEK'],inplace=True)\n",
    "\n",
    "print('Preprocessing TRAINING DATASET COMPLETED!')\n",
    "print('Preprocessing TESTING DATASET ...')\n",
    "\n",
    "\n",
    "df_test['Z_WEEK_DATE'] = pd.to_datetime(df_test['Z_WEEK_DATE'])\n",
    "df_test['days_from_payday'] = df_test['Z_WEEK_DATE'].apply(get_distance_from_paydays)\n",
    "\n",
    "inv_dict_dates = {v: k for k, v in dict_dates.items()}\n",
    "#df_test['Z_WEEK'] = df_test['date_block_num'].map(inv_dict_dates)\n",
    "df_test = df_test[['date_block_num','Z_MODELO','Z_PUNTO_VENTA','Z_GAMA',\"Demanda\",\"Z_WEEK_DATE\"]]\n",
    "\n",
    "df_test['dayofweek'] = df_test['Z_WEEK_DATE'].dt.dayofweek.astype('str').astype('category')\n",
    "df_test['month'] = df_test['Z_WEEK_DATE'].dt.month.astype('str').astype('category')\n",
    "df_test['dayofyear'] = df_test['Z_WEEK_DATE'].dt.dayofyear.astype('str').astype('category')\n",
    "\n",
    "\n",
    "\n",
    "df_test['days_from_payday'] = df_test['Z_WEEK_DATE'].apply(get_distance_from_paydays)\n",
    "df_test.drop(columns=['Z_WEEK_DATE'],inplace=True)\n",
    "\n",
    "print('Preprocessing TESTING DATASET COMPLETED!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.groupby([\"date_block_num\"]+['Z_MODELO','Z_GAMA'], observed=True).Demanda.std(ddof=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info(verbose=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.shape)\n",
    "print(list(df_train.columns))\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('statistics_columns : ',len(statistics_columns))\n",
    "print(df_test.shape)\n",
    "print(list(df_test.columns))\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA','date_block_num','Demanda']][(df_train['Z_MODELO']=='MOD_10')&(df_train['date_block_num']==0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Z_MODELO\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Z_GAMA\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for z_modelo in list(df_train[\"Z_MODELO\"].value_counts().index.values)[:10]+['MOD_102']:\n",
    "    aux = df_train[(df_train['Z_MODELO']==z_modelo)]['Z_GAMA'].value_counts()\n",
    "    print('Z_MODELO = ',z_modelo,' # GAMAS =',aux.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for z_modelo in list(df_train[\"Z_MODELO\"].value_counts().index.values)[:10]+['MOD_102']:\n",
    "    aux = df_train[(df_train['Z_MODELO']==z_modelo)]['Z_PUNTO_VENTA'].value_counts()\n",
    "    print('Z_MODELO = ',z_modelo,' # Z_PUNTO_VENTA =',aux.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA','date_block_num','Demanda']][(df_train['Z_MODELO']=='MOD_10')]['Z_GAMA'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = df_train[\"Z_MODELO\"].astype(str) + df_train[\"Z_GAMA\"]\n",
    "aux.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train[['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA','date_block_num','Demanda']][(df_train['Z_MODELO']=='MOD_10')&(df_train['Z_PUNTO_VENTA']=='PVENT_1')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA','date_block_num','Demanda']][(df_train['Z_MODELO']=='MOD_10')&(df_train['Z_PUNTO_VENTA']=='PVENT_1')&(df_train['date_block_num']==10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train[['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA','date_block_num','Demanda']][(df_train['Z_MODELO']=='MOD_10')&(df_train['Z_PUNTO_VENTA']=='PVENT_2')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(df_train['Demanda'],bins=100)\n",
    "plt.title('train demanda hist')\n",
    "plt.subplot(1,2,2)\n",
    "plt.boxplot(df_train['Demanda'],vert=False)\n",
    "plt.title('train demanda boxplot')\n",
    "plt.suptitle('TRAIN Demanda distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(np.log10(df_train['Demanda']+0.1),bins=100)\n",
    "plt.title('train demanda hist')\n",
    "plt.subplot(1,2,2)\n",
    "plt.boxplot(np.log10(df_train['Demanda']+0.1),vert=False)\n",
    "plt.title('train demanda boxplot')\n",
    "plt.suptitle('TRAIN Demanda distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "import pytorch_forecasting\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer,EncoderNormalizer\n",
    "\n",
    "from pytorch_forecasting.metrics import SMAPE, PoissonLoss, QuantileLoss\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "\"\"\"Point metrics for forecasting a single point per time step.\"\"\"\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from pytorch_forecasting.metrics import SMAPE, MAE,RMSE\n",
    "\n",
    "import scipy.stats\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import rnn\n",
    "\n",
    "from pytorch_forecasting.metrics import MultiHorizonMetric\n",
    "from pytorch_forecasting.utils import create_mask, unpack_sequence, unsqueeze_like\n",
    "\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "# configure network and trainer\n",
    "pl.seed_everything(42)\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_steps = df_test['date_block_num'].nunique()\n",
    "prediction_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['date_block_num'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'day_of_week', 'day', 'is_month_end', 'day_of_year',  'is_quarter_start', 'year', 'month', 'is_year_start', 'is_month_start', \n",
    "# 'I103','S103', 'C101','I100' , 'C100', 'ID', 'I102','S102',, 'S101', 'S100', 'item_id', 'date_block_num', 'I101'\n",
    "max_prediction_length = prediction_steps\n",
    "\n",
    "max_encoder_length = 40\n",
    "\n",
    "training_cutoff = df_train['date_block_num'].max() - max_prediction_length\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    df_train[lambda x: x['date_block_num'] <= training_cutoff],\n",
    "    time_idx='date_block_num',\n",
    "    target=\"Demanda\",\n",
    "    group_ids=['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA'],\n",
    "    min_encoder_length= max_encoder_length // 2,   \n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "        \n",
    "    static_categoricals=['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA'],\n",
    "    \n",
    "    time_varying_unknown_categoricals=[\n",
    "                                     \"month\", \n",
    "                                     \"dayofweek\",\n",
    "                                     \"dayofyear\"],\n",
    "    \n",
    "    time_varying_unknown_reals=[\"date_block_num\",'days_from_payday'],\n",
    "                                #[\"date_block_num\",\"Demanda\"],\n",
    "    time_varying_known_categoricals=[],  \n",
    "\n",
    "    time_varying_known_reals= statistics_columns,#'date_block_num'],\n",
    "       \n",
    "    #target_normalizer=GroupNormalizer(\n",
    "    #    groups=['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA'], transformation=\"softplus\"\n",
    "    #),  # use softplus and normalize by group    \n",
    "    \n",
    "    categorical_encoders={\n",
    "        #\"Z_MARCA\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "                          \n",
    "                          \"Z_GAMA\":  pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "                          \"Z_MODELO\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "                          #\"Z_DEPARTAMENTO\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "                          \"Z_PUNTO_VENTA\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "                          \"dayofweek\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "                          \"month\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "                          \"dayofyear\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "                         #\"date_block_num\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "                         },\n",
    "    #,\n",
    "    #                      \"item_id\":pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "    #                     },\n",
    "    #'day_of_week', 'day', 'is_month_end', 'day_of_year',  'is_quarter_start', 'year', 'month', 'is_year_start', 'is_month_start']},\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = TimeSeriesDataSet.from_dataset(training, df_train, predict=True, stop_randomization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "val_dataloader   = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_forecasting.metrics import QuantileLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch\n",
    "\n",
    "# calculate baseline mean absolute error, i.e. predict next value as the last available value from the history\n",
    "actuals = torch.cat([y for x, (y, weight) in iter(val_dataloader)])\n",
    "baseline_predictions = Baseline().predict(val_dataloader)\n",
    "mase_val = (actuals - baseline_predictions).abs().mean().item()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "rmse_val = torch.sqrt(criterion(actuals,baseline_predictions))\n",
    "print('baseline - mase_val = ',mase_val)\n",
    "print('baseline - rmse_val = ',rmse_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    import pickle\n",
    "\n",
    "    from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "\n",
    "    # create study\n",
    "    study = optimize_hyperparameters(\n",
    "        train_dataloader,\n",
    "        val_dataloader,\n",
    "        model_path=\"optuna_test\",\n",
    "        n_trials=50,\n",
    "        max_epochs=20,\n",
    "        gradient_clip_val_range=(0.01, 1.0),\n",
    "        hidden_size_range=(8, 64),\n",
    "        hidden_continuous_size_range=(8, 64),\n",
    "        attention_head_size_range=(1, 4),\n",
    "        learning_rate_range=(0.001, 0.1),\n",
    "        dropout_range=(0.1, 0.3),\n",
    "        trainer_kwargs=dict(limit_train_batches=30, log_every_n_steps=15, gpus=1),\n",
    "        reduce_on_plateau_patience=4,\n",
    "        use_learning_rate_finder=False,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n",
    "        timeout=4200,\n",
    "        loss=TweedieLoss()\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # show best hyperparameters\n",
    "    print(study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Early Stopping \n",
    "MIN_DELTA  = 1e-4\n",
    "PATIENCE   = 20\n",
    "\n",
    "#PL Trainer\n",
    "MAX_EPOCHS = 500\n",
    "GPUS = 1\n",
    "\n",
    "if False:\n",
    "    #study = {'gradient_clip_val': 0.5899880996240897, 'hidden_size': 24, 'dropout': 0.18045896986283255, \n",
    "    # 'hidden_continuous_size': 11, 'attention_head_size': 2, 'learning_rate': 0.012427775478680268}\n",
    "    \n",
    "    study = {'gradient_clip_val': 0.3468483254885978, 'hidden_size': 32, 'dropout': 0.11031796015695508, \n",
    "             'hidden_continuous_size': 19, 'attention_head_size': 15, 'learning_rate': 0.04289257757804779}\n",
    "    \n",
    "    GRADIENT_CLIP_VAL=study['gradient_clip_val']\n",
    "    LIMIT_TRAIN_BATCHES=30\n",
    "\n",
    "    #Fusion Transformer\n",
    "    LR = study['learning_rate']\n",
    "    HIDDEN_SIZE = study['hidden_size']\n",
    "    DROPOUT = study['dropout']\n",
    "    ATTENTION_HEAD_SIZE = study['attention_head_size']\n",
    "    HIDDEN_CONTINUOUS_SIZE = study['hidden_continuous_size']\n",
    "else:\n",
    "    GRADIENT_CLIP_VAL=study.best_trial.params['gradient_clip_val']\n",
    "    LIMIT_TRAIN_BATCHES=30\n",
    "\n",
    "    #Fusion Transformer\n",
    "    LR = study.best_trial.params['learning_rate']\n",
    "    HIDDEN_SIZE = study.best_trial.params['hidden_size']\n",
    "    DROPOUT = study.best_trial.params['dropout']\n",
    "    ATTENTION_HEAD_SIZE = study.best_trial.params['attention_head_size']\n",
    "    HIDDEN_CONTINUOUS_SIZE = study.best_trial.params['hidden_continuous_size']\n",
    "\n",
    "OUTPUT_SIZE= 1\n",
    "REDUCE_ON_PLATEAU_PATIENCE=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTENTION_HEAD_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_prediction_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "composite_metric = SMAPE() + 1e-4 * MAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSE2(MultiHorizonMetric):\n",
    "    \"\"\"\n",
    "    Root mean square error\n",
    "\n",
    "    Defined as ``(y_pred - target)**2``\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, reduction=\"sqrt-mean\", **kwargs):\n",
    "        super().__init__(reduction=reduction, **kwargs)\n",
    "\n",
    "    def loss(self, y_pred: Dict[str, torch.Tensor], target):\n",
    "        loss = torch.pow(self.to_prediction(y_pred) - target, 2)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweedieLoss(MultiHorizonMetric):\n",
    "    \"\"\"\n",
    "    Tweedie loss\n",
    "\n",
    "    Tweedie regression with log-link. It might be useful, e.g., for modeling total\n",
    "    loss in insurance, or for any target that might be tweedie-distributed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, reduction=\"mean\", p: float = 1.5, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            p (float, optional): tweedie variance power which is greater equal\n",
    "                1.0 and smaller 2.0. Close to ``2`` shifts to\n",
    "                Gamma distribution and close to ``1`` shifts to Poisson distribution.\n",
    "                Defaults to 1.5.\n",
    "            reduction (str, optional): How to reduce the loss. Defaults to \"mean\".\n",
    "        \"\"\"\n",
    "        super().__init__(reduction=reduction, **kwargs)\n",
    "        assert 1 <= p < 2, \"p must be in range [1, 2]\"\n",
    "        self.p = p\n",
    "\n",
    "    def to_prediction(self, out: Dict[str, torch.Tensor]):\n",
    "        rate = torch.exp(super().to_prediction(out))\n",
    "        return rate\n",
    "\n",
    "\n",
    "    def loss(self, y_pred, y_true):\n",
    "        y_pred = super().to_prediction(y_pred)\n",
    "        a = y_true * torch.exp(y_pred * (1 - self.p)) / (1 - self.p)\n",
    "        b = torch.exp(y_pred * (2 - self.p)) / (2 - self.p)\n",
    "        loss = -a + b\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "\n",
    "# configure network and trainer\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=MIN_DELTA, patience=PATIENCE, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    gpus=GPUS,\n",
    "    weights_summary=\"top\",\n",
    "    gradient_clip_val=GRADIENT_CLIP_VAL,\n",
    "    limit_train_batches=LIMIT_TRAIN_BATCHES,#oment in for training, running valiation every 30 batches\n",
    "    #fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    "    log_every_n_steps=10\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=LR,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    attention_head_size=ATTENTION_HEAD_SIZE,\n",
    "    dropout=DROPOUT,\n",
    "    hidden_continuous_size=HIDDEN_CONTINUOUS_SIZE,\n",
    "    output_size=OUTPUT_SIZE,# 7 quantiles by default\n",
    "    \n",
    "    #loss=QuantileLoss(),\n",
    "    #loss=RMSE(),\n",
    "    loss=TweedieLoss(),\n",
    "    log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "    reduce_on_plateau_patience=REDUCE_ON_PLATEAU_PATIENCE,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit network\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model according to the validation loss\n",
    "# (given that we use early stopping, this is not necessarily the last epoch)\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "print(best_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# raw predictions are a dictionary from which all kind of information including quantiles can be extracted\n",
    "raw_predictions, x = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)\n",
    "print('hola')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(5):  # plot 10 examples\n",
    "    best_tft.plot_prediction(x, raw_predictions, idx=idx, add_loss_to_title=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, x = best_tft.predict(val_dataloader, return_x=True)\n",
    "predictions_vs_actuals = best_tft.calculate_prediction_actual_by_variable(x, predictions)\n",
    "best_tft.plot_prediction_actual_by_variable(predictions_vs_actuals);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcualte root mean squared error on validation set\n",
    "actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
    "val_predictions = best_tft.predict(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "torch.sqrt(criterion(actuals,val_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    print(actuals[i],val_predictions[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('max_prediction_length:',max_prediction_length)\n",
    "print('max_encoder_length   :',max_encoder_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select last 30 days from data (max_encoder_length is 24)\n",
    "encoder_data = df_train[lambda x: x.date_block_num > x.date_block_num.max() - max_encoder_length]\n",
    "\n",
    "print(encoder_data['date_block_num'].min(),encoder_data['date_block_num'].max())\n",
    "#print(encoder_data['DATE'].min(),encoder_data['DATE'].max())\n",
    "encoder_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_prediction_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "last_data = df_train[df_train['date_block_num'].isin([idx  -  max_prediction_length for idx in df_test['date_block_num'].unique()])]\n",
    "last_data['date_block_num'] = last_data['date_block_num'] + max_prediction_length\n",
    "\n",
    "decoder_data = pd.merge(df_test[[col for col in df_test.columns if 'Demanda' not in col]], \n",
    "        last_data[['date_block_num','Z_MODELO','Z_PUNTO_VENTA','Z_GAMA',\"Demanda\",\n",
    "                   'average_sales_by_Z_MODELO','average_sales_by_Z_PUNTO_VENTA', \n",
    "                   'average_sales_by_Z_GAMA','average_sales_by_Z_WEEK', 'average_sales_by_Z_WEEK_DATE']],\n",
    "        \n",
    "        on = ['date_block_num', 'Z_MODELO','Z_PUNTO_VENTA','Z_GAMA',],\n",
    "                        how='left'\n",
    "        )\n",
    "\n",
    "\n",
    "encoder_data.replace([np.inf, -np.inf, np.nan],0,inplace=True)\n",
    "decoder_data.replace([np.inf, -np.inf, np.nan],0,inplace=True)\n",
    "\n",
    "\n",
    "# combine encoder and decoder data\n",
    "new_prediction_data = pd.concat([encoder_data, decoder_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_data[['date_block_num','Z_MODELO','Z_PUNTO_VENTA','Z_GAMA',\"Demanda\",\n",
    "                   'average_sales_by_Z_MODELO','average_sales_by_Z_PUNTO_VENTA', \n",
    "                   'average_sales_by_Z_GAMA','average_sales_by_Z_WEEK', 'average_sales_by_Z_WEEK_DATE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_data['average_sales_by_Z_WEEK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "aux = encoder_data['Demanda'].values\n",
    "\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "plt.plot(aux,'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "aux = decoder_data['Demanda'].values\n",
    "\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "plt.plot(aux,'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prediction_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_raw_predictions, new_x = best_tft.predict(new_prediction_data, mode=\"raw\", return_x=True)\n",
    "\n",
    "for idx in range(10):  # plot 10 examples\n",
    "    best_tft.plot_prediction(new_x, new_raw_predictions, idx=idx, show_future_observed=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_raw_predictions.shape\n",
    "#torch.Size([47173, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_raw_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpretation = best_tft.interpret_output(new_raw_predictions, reduction=\"sum\")\n",
    "best_tft.plot_interpretation(interpretation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_raw_predictions = best_tft.predict(new_prediction_data, mode=\"prediction\", return_x=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(new_raw_predictions.numpy()).T\n",
    "predictions['date_block_num'] = sorted(df_test['date_block_num'].unique())\n",
    "predictions = pd.melt(predictions, id_vars=['date_block_num'])\n",
    "predictions = predictions.sort_values(['date_block_num', 'variable']).reset_index(drop=True)\n",
    "df_test[['date_block_num','Z_MODELO','Z_PUNTO_VENTA','Z_GAMA']].sort_values(['date_block_num', 'Z_MODELO','Z_PUNTO_VENTA','Z_GAMA']).reset_index(drop=True)\n",
    "df_test2 = df_test.join(predictions['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "reverse_mapping_file = '../../utils/reverse_dict_mapping_list.txt'\n",
    "\n",
    "with open(reverse_mapping_file, 'rb') as f:\n",
    "    reverse_mapping = pickle.load( f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reverse_mapping#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#descriptive_columns = ['Z_MODELO','Z_PUNTO_VENTA','Z_GAMA']\n",
    "descriptive_columns = ['Z_MARCA', 'Z_GAMA', 'Z_MODELO',\n",
    "                       'Z_DEPARTAMENTO', 'Z_PUNTO_VENTA']\n",
    "i=0\n",
    "for column in descriptive_columns:\n",
    "    if column in df_test2.columns:\n",
    "        df_test2[column] = df_test2[column].map(reverse_mapping[i])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inv_dict_dates = {v: k for k, v in dict_dates.items()}\n",
    "df_test2['Z_WEEK'] = df_test2['date_block_num'].map(inv_dict_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test2['ID'] = df_test2['Z_MODELO'] + '|' + df_test2['Z_PUNTO_VENTA'] + '|' + df_test2['Z_GAMA'] + '|' + df_test2['Z_WEEK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test2['Demanda'] = df_test2['value']\n",
    "submission = df_test2[['ID','Demanda']]#.groupby('ID').sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submission['Demanda'] = 0.9#submission['value']\n",
    "\n",
    "submission[['ID', 'Demanda']].to_csv('../../results/Submission_tft_v2_.csv', index = False, sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.boxplot(['Demanda'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "aux = df_train['Demanda'].values\n",
    "\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "plt.plot(aux,'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "aux = df_test['Demanda'].values\n",
    "\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "plt.plot(aux,'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "aux = submission['Demanda'].values\n",
    "\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "plt.plot(aux,'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux[aux>=300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_baseline = pd.read_csv('../../results/Submission_28.csv')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "aux = df_baseline['Demanda']\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "df_baseline.boxplot(['Demanda'])\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "plt.plot(aux,'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_actual = df_baseline['Demanda']\n",
    "y_predicted = submission['Demanda'].values\n",
    "\n",
    "rms = mean_squared_error(y_actual, y_predicted, squared=False)\n",
    "rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "63e0b83efdf8191a659559277734df523ac963ca42a147be12244d5a52bc1968"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
